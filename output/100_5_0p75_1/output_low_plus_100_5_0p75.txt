nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [43:00<70:57:33, 2580.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [1:22:08<66:31:44, 2443.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [2:11:01<71:51:38, 2667.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [3:01:56<75:12:28, 2820.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [3:54:56<77:51:04, 2950.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [4:42:18<76:04:19, 2913.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [5:29:01<74:19:29, 2877.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [6:12:59<71:34:52, 2801.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [6:55:05<68:37:45, 2715.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [7:50:37<72:38:19, 2905.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [8:40:27<72:28:19, 2931.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [9:25:12<69:49:16, 2856.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [10:19:31<71:58:39, 2978.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [11:13:47<73:09:23, 3062.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [12:02:23<71:15:37, 3018.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [12:56:36<72:04:13, 3088.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [13:45:24<70:06:00, 3040.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [14:30:16<66:52:25, 2935.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [15:16:06<64:47:46, 2879.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [16:04:26<64:07:52, 2885.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [16:49:40<62:11:54, 2834.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [17:43:48<64:06:16, 2958.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [18:32:01<62:51:34, 2938.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [19:24:38<63:25:32, 3004.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [20:10:07<60:51:56, 2921.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [20:55:39<58:53:10, 2864.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [21:41:10<57:16:46, 2824.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [22:25:54<55:38:55, 2782.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [23:14:20<55:36:22, 2819.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [24:03:56<55:44:21, 2866.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [24:50:49<54:38:05, 2850.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [25:35:05<52:44:21, 2792.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [26:23:49<52:41:55, 2831.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [27:19:08<54:35:30, 2977.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -10930.875646933191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42043.6484375
Iteration 100: Loss = -27311.919921875
Iteration 200: Loss = -17163.220703125
Iteration 300: Loss = -12769.3125
Iteration 400: Loss = -11561.337890625
Iteration 500: Loss = -11221.2607421875
Iteration 600: Loss = -11142.9658203125
Iteration 700: Loss = -11112.32421875
Iteration 800: Loss = -11093.2890625
Iteration 900: Loss = -11085.5546875
Iteration 1000: Loss = -11077.380859375
Iteration 1100: Loss = -11073.6279296875
Iteration 1200: Loss = -11070.529296875
Iteration 1300: Loss = -11065.32421875
Iteration 1400: Loss = -11063.1396484375
Iteration 1500: Loss = -11061.9404296875
Iteration 1600: Loss = -11061.0234375
Iteration 1700: Loss = -11060.287109375
Iteration 1800: Loss = -11059.66796875
Iteration 1900: Loss = -11059.1474609375
Iteration 2000: Loss = -11058.6953125
Iteration 2100: Loss = -11058.3037109375
Iteration 2200: Loss = -11057.9609375
Iteration 2300: Loss = -11057.66015625
Iteration 2400: Loss = -11057.392578125
Iteration 2500: Loss = -11057.1552734375
Iteration 2600: Loss = -11056.9404296875
Iteration 2700: Loss = -11056.74609375
Iteration 2800: Loss = -11056.5732421875
Iteration 2900: Loss = -11056.4150390625
Iteration 3000: Loss = -11056.26953125
Iteration 3100: Loss = -11056.138671875
Iteration 3200: Loss = -11056.015625
Iteration 3300: Loss = -11055.9052734375
Iteration 3400: Loss = -11055.8056640625
Iteration 3500: Loss = -11055.7099609375
Iteration 3600: Loss = -11055.6220703125
Iteration 3700: Loss = -11055.5439453125
Iteration 3800: Loss = -11055.4677734375
Iteration 3900: Loss = -11055.3984375
Iteration 4000: Loss = -11055.3349609375
Iteration 4100: Loss = -11055.2724609375
Iteration 4200: Loss = -11055.2158203125
Iteration 4300: Loss = -11055.1640625
Iteration 4400: Loss = -11055.1142578125
Iteration 4500: Loss = -11055.0693359375
Iteration 4600: Loss = -11055.0244140625
Iteration 4700: Loss = -11054.986328125
Iteration 4800: Loss = -11054.947265625
Iteration 4900: Loss = -11054.912109375
Iteration 5000: Loss = -11054.87890625
Iteration 5100: Loss = -11054.8466796875
Iteration 5200: Loss = -11054.8173828125
Iteration 5300: Loss = -11054.791015625
Iteration 5400: Loss = -11054.7646484375
Iteration 5500: Loss = -11054.7392578125
Iteration 5600: Loss = -11054.7158203125
Iteration 5700: Loss = -11054.693359375
Iteration 5800: Loss = -11054.671875
Iteration 5900: Loss = -11054.654296875
Iteration 6000: Loss = -11054.634765625
Iteration 6100: Loss = -11054.6171875
Iteration 6200: Loss = -11054.6005859375
Iteration 6300: Loss = -11054.5849609375
Iteration 6400: Loss = -11054.568359375
Iteration 6500: Loss = -11054.5556640625
Iteration 6600: Loss = -11054.541015625
Iteration 6700: Loss = -11054.529296875
Iteration 6800: Loss = -11054.5185546875
Iteration 6900: Loss = -11054.505859375
Iteration 7000: Loss = -11054.494140625
Iteration 7100: Loss = -11054.484375
Iteration 7200: Loss = -11054.4736328125
Iteration 7300: Loss = -11054.46484375
Iteration 7400: Loss = -11054.4560546875
Iteration 7500: Loss = -11054.4462890625
Iteration 7600: Loss = -11054.4384765625
Iteration 7700: Loss = -11054.4306640625
Iteration 7800: Loss = -11054.4228515625
Iteration 7900: Loss = -11054.4169921875
Iteration 8000: Loss = -11054.4111328125
Iteration 8100: Loss = -11054.4033203125
Iteration 8200: Loss = -11054.3974609375
Iteration 8300: Loss = -11054.390625
Iteration 8400: Loss = -11054.38671875
Iteration 8500: Loss = -11054.380859375
Iteration 8600: Loss = -11054.375
Iteration 8700: Loss = -11054.37109375
Iteration 8800: Loss = -11054.3662109375
Iteration 8900: Loss = -11054.3603515625
Iteration 9000: Loss = -11054.3544921875
Iteration 9100: Loss = -11054.3505859375
Iteration 9200: Loss = -11054.3466796875
Iteration 9300: Loss = -11054.3388671875
Iteration 9400: Loss = -11054.33203125
Iteration 9500: Loss = -11054.322265625
Iteration 9600: Loss = -11054.3095703125
Iteration 9700: Loss = -11054.291015625
Iteration 9800: Loss = -11054.2724609375
Iteration 9900: Loss = -11054.255859375
Iteration 10000: Loss = -11054.23828125
Iteration 10100: Loss = -11054.2236328125
Iteration 10200: Loss = -11054.2080078125
Iteration 10300: Loss = -11054.1923828125
Iteration 10400: Loss = -11054.177734375
Iteration 10500: Loss = -11054.1630859375
Iteration 10600: Loss = -11054.14453125
Iteration 10700: Loss = -11054.1279296875
Iteration 10800: Loss = -11054.11328125
Iteration 10900: Loss = -11054.0986328125
Iteration 11000: Loss = -11054.076171875
Iteration 11100: Loss = -11054.0302734375
Iteration 11200: Loss = -11053.96484375
Iteration 11300: Loss = -11053.9208984375
Iteration 11400: Loss = -11053.9013671875
Iteration 11500: Loss = -11053.8837890625
Iteration 11600: Loss = -11053.8681640625
Iteration 11700: Loss = -11053.8564453125
Iteration 11800: Loss = -11053.83984375
Iteration 11900: Loss = -11053.822265625
Iteration 12000: Loss = -11053.8046875
Iteration 12100: Loss = -11053.783203125
Iteration 12200: Loss = -11053.7587890625
Iteration 12300: Loss = -11053.734375
Iteration 12400: Loss = -11053.705078125
Iteration 12500: Loss = -11053.677734375
Iteration 12600: Loss = -11053.64453125
Iteration 12700: Loss = -11053.61328125
Iteration 12800: Loss = -11053.5791015625
Iteration 12900: Loss = -11053.5498046875
Iteration 13000: Loss = -11053.5205078125
Iteration 13100: Loss = -11053.4970703125
Iteration 13200: Loss = -11053.4775390625
Iteration 13300: Loss = -11053.4599609375
Iteration 13400: Loss = -11053.4404296875
Iteration 13500: Loss = -11052.478515625
Iteration 13600: Loss = -11052.419921875
Iteration 13700: Loss = -11052.396484375
Iteration 13800: Loss = -11052.3935546875
Iteration 13900: Loss = -11052.1015625
Iteration 14000: Loss = -11052.013671875
Iteration 14100: Loss = -11052.0107421875
Iteration 14200: Loss = -11051.978515625
Iteration 14300: Loss = -11051.9130859375
Iteration 14400: Loss = -11051.8994140625
Iteration 14500: Loss = -11051.76171875
Iteration 14600: Loss = -11051.4306640625
Iteration 14700: Loss = -11050.9658203125
Iteration 14800: Loss = -11050.8837890625
Iteration 14900: Loss = -11050.849609375
Iteration 15000: Loss = -11050.7060546875
Iteration 15100: Loss = -11050.5068359375
Iteration 15200: Loss = -11050.412109375
Iteration 15300: Loss = -11050.3740234375
Iteration 15400: Loss = -11050.3486328125
Iteration 15500: Loss = -11050.322265625
Iteration 15600: Loss = -11050.3056640625
Iteration 15700: Loss = -11050.265625
Iteration 15800: Loss = -11050.232421875
Iteration 15900: Loss = -11050.1767578125
Iteration 16000: Loss = -11050.115234375
Iteration 16100: Loss = -11050.0810546875
Iteration 16200: Loss = -11050.0703125
Iteration 16300: Loss = -11050.0390625
Iteration 16400: Loss = -11050.02734375
Iteration 16500: Loss = -11050.0087890625
Iteration 16600: Loss = -11050.0029296875
Iteration 16700: Loss = -11049.982421875
Iteration 16800: Loss = -11049.9716796875
Iteration 16900: Loss = -11049.966796875
Iteration 17000: Loss = -11049.9609375
Iteration 17100: Loss = -11049.9521484375
Iteration 17200: Loss = -11049.9443359375
Iteration 17300: Loss = -11049.9443359375
Iteration 17400: Loss = -11049.94140625
Iteration 17500: Loss = -11049.9404296875
Iteration 17600: Loss = -11049.939453125
Iteration 17700: Loss = -11049.9404296875
1
Iteration 17800: Loss = -11049.94140625
2
Iteration 17900: Loss = -11049.939453125
Iteration 18000: Loss = -11049.9384765625
Iteration 18100: Loss = -11049.9384765625
Iteration 18200: Loss = -11049.9404296875
1
Iteration 18300: Loss = -11049.939453125
2
Iteration 18400: Loss = -11049.9384765625
Iteration 18500: Loss = -11049.94140625
1
Iteration 18600: Loss = -11049.9384765625
Iteration 18700: Loss = -11049.9384765625
Iteration 18800: Loss = -11049.9384765625
Iteration 18900: Loss = -11049.9384765625
Iteration 19000: Loss = -11049.9365234375
Iteration 19100: Loss = -11049.9384765625
1
Iteration 19200: Loss = -11049.939453125
2
Iteration 19300: Loss = -11049.9384765625
3
Iteration 19400: Loss = -11049.94140625
4
Iteration 19500: Loss = -11049.9375
5
Iteration 19600: Loss = -11049.939453125
6
Iteration 19700: Loss = -11049.9384765625
7
Iteration 19800: Loss = -11049.9375
8
Iteration 19900: Loss = -11049.9384765625
9
Iteration 20000: Loss = -11049.9384765625
10
Iteration 20100: Loss = -11049.9404296875
11
Iteration 20200: Loss = -11049.9384765625
12
Iteration 20300: Loss = -11049.9404296875
13
Iteration 20400: Loss = -11049.9404296875
14
Iteration 20500: Loss = -11049.9384765625
15
Stopping early at iteration 20500 due to no improvement.
pi: tensor([[0.9476, 0.0524],
        [0.8691, 0.1309]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9922, 0.0078], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1613, 0.1133],
         [0.9300, 0.3399]],

        [[0.9168, 0.2257],
         [0.6883, 0.0654]],

        [[0.3473, 0.2226],
         [0.0195, 0.0284]],

        [[0.6008, 0.2191],
         [0.1113, 0.7868]],

        [[0.9112, 0.0823],
         [0.4166, 0.0484]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: -0.012494332208171696
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
Global Adjusted Rand Index: -0.0003002266693751667
Average Adjusted Rand Index: -0.0033899997555783324
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36026.84375
Iteration 100: Loss = -20303.951171875
Iteration 200: Loss = -12682.0546875
Iteration 300: Loss = -11449.8154296875
Iteration 400: Loss = -11259.84765625
Iteration 500: Loss = -11194.6162109375
Iteration 600: Loss = -11157.494140625
Iteration 700: Loss = -11127.9560546875
Iteration 800: Loss = -11112.708984375
Iteration 900: Loss = -11100.0859375
Iteration 1000: Loss = -11092.7900390625
Iteration 1100: Loss = -11087.232421875
Iteration 1200: Loss = -11082.595703125
Iteration 1300: Loss = -11078.8251953125
Iteration 1400: Loss = -11075.9033203125
Iteration 1500: Loss = -11073.4208984375
Iteration 1600: Loss = -11068.494140625
Iteration 1700: Loss = -11066.4375
Iteration 1800: Loss = -11065.119140625
Iteration 1900: Loss = -11064.056640625
Iteration 2000: Loss = -11063.162109375
Iteration 2100: Loss = -11062.38671875
Iteration 2200: Loss = -11061.7119140625
Iteration 2300: Loss = -11061.11328125
Iteration 2400: Loss = -11060.580078125
Iteration 2500: Loss = -11060.1005859375
Iteration 2600: Loss = -11059.6748046875
Iteration 2700: Loss = -11059.287109375
Iteration 2800: Loss = -11058.9345703125
Iteration 2900: Loss = -11058.6171875
Iteration 3000: Loss = -11058.32421875
Iteration 3100: Loss = -11058.0576171875
Iteration 3200: Loss = -11057.8115234375
Iteration 3300: Loss = -11057.5859375
Iteration 3400: Loss = -11057.376953125
Iteration 3500: Loss = -11057.1826171875
Iteration 3600: Loss = -11057.00390625
Iteration 3700: Loss = -11056.8388671875
Iteration 3800: Loss = -11056.68359375
Iteration 3900: Loss = -11056.541015625
Iteration 4000: Loss = -11056.408203125
Iteration 4100: Loss = -11056.28515625
Iteration 4200: Loss = -11056.166015625
Iteration 4300: Loss = -11056.056640625
Iteration 4400: Loss = -11055.9560546875
Iteration 4500: Loss = -11055.8603515625
Iteration 4600: Loss = -11055.771484375
Iteration 4700: Loss = -11055.6875
Iteration 4800: Loss = -11055.609375
Iteration 4900: Loss = -11055.5341796875
Iteration 5000: Loss = -11055.4658203125
Iteration 5100: Loss = -11055.3984375
Iteration 5200: Loss = -11055.337890625
Iteration 5300: Loss = -11055.2783203125
Iteration 5400: Loss = -11055.2236328125
Iteration 5500: Loss = -11055.1708984375
Iteration 5600: Loss = -11055.12109375
Iteration 5700: Loss = -11055.0771484375
Iteration 5800: Loss = -11055.03125
Iteration 5900: Loss = -11054.98828125
Iteration 6000: Loss = -11054.9482421875
Iteration 6100: Loss = -11054.9111328125
Iteration 6200: Loss = -11054.8759765625
Iteration 6300: Loss = -11054.8408203125
Iteration 6400: Loss = -11054.8095703125
Iteration 6500: Loss = -11054.7763671875
Iteration 6600: Loss = -11054.74609375
Iteration 6700: Loss = -11054.716796875
Iteration 6800: Loss = -11054.6904296875
Iteration 6900: Loss = -11054.6650390625
Iteration 7000: Loss = -11054.642578125
Iteration 7100: Loss = -11054.619140625
Iteration 7200: Loss = -11054.599609375
Iteration 7300: Loss = -11054.578125
Iteration 7400: Loss = -11054.5615234375
Iteration 7500: Loss = -11054.54296875
Iteration 7600: Loss = -11054.52734375
Iteration 7700: Loss = -11054.51171875
Iteration 7800: Loss = -11054.4970703125
Iteration 7900: Loss = -11054.4833984375
Iteration 8000: Loss = -11054.470703125
Iteration 8100: Loss = -11054.45703125
Iteration 8200: Loss = -11054.4462890625
Iteration 8300: Loss = -11054.4345703125
Iteration 8400: Loss = -11054.423828125
Iteration 8500: Loss = -11054.412109375
Iteration 8600: Loss = -11054.40234375
Iteration 8700: Loss = -11054.3896484375
Iteration 8800: Loss = -11054.369140625
Iteration 8900: Loss = -11054.3466796875
Iteration 9000: Loss = -11054.314453125
Iteration 9100: Loss = -11054.26171875
Iteration 9200: Loss = -11054.1953125
Iteration 9300: Loss = -11054.1259765625
Iteration 9400: Loss = -11054.0693359375
Iteration 9500: Loss = -11054.0302734375
Iteration 9600: Loss = -11053.9970703125
Iteration 9700: Loss = -11053.9677734375
Iteration 9800: Loss = -11053.939453125
Iteration 9900: Loss = -11053.908203125
Iteration 10000: Loss = -11053.8759765625
Iteration 10100: Loss = -11053.8359375
Iteration 10200: Loss = -11053.7548828125
Iteration 10300: Loss = -11053.58984375
Iteration 10400: Loss = -11053.482421875
Iteration 10500: Loss = -11053.3935546875
Iteration 10600: Loss = -11053.3037109375
Iteration 10700: Loss = -11053.2060546875
Iteration 10800: Loss = -11053.08984375
Iteration 10900: Loss = -11052.953125
Iteration 11000: Loss = -11052.779296875
Iteration 11100: Loss = -11052.5537109375
Iteration 11200: Loss = -11052.2607421875
Iteration 11300: Loss = -11051.8837890625
Iteration 11400: Loss = -11051.4345703125
Iteration 11500: Loss = -11051.0185546875
Iteration 11600: Loss = -11050.712890625
Iteration 11700: Loss = -11050.4833984375
Iteration 11800: Loss = -11050.32421875
Iteration 11900: Loss = -11050.185546875
Iteration 12000: Loss = -11050.0634765625
Iteration 12100: Loss = -11049.9208984375
Iteration 12200: Loss = -11049.798828125
Iteration 12300: Loss = -11049.716796875
Iteration 12400: Loss = -11049.6435546875
Iteration 12500: Loss = -11049.587890625
Iteration 12600: Loss = -11049.544921875
Iteration 12700: Loss = -11049.5205078125
Iteration 12800: Loss = -11049.501953125
Iteration 12900: Loss = -11049.48828125
Iteration 13000: Loss = -11049.4794921875
Iteration 13100: Loss = -11049.349609375
Iteration 13200: Loss = -11049.3447265625
Iteration 13300: Loss = -11049.34375
Iteration 13400: Loss = -11049.341796875
Iteration 13500: Loss = -11049.341796875
Iteration 13600: Loss = -11049.3388671875
Iteration 13700: Loss = -11049.3388671875
Iteration 13800: Loss = -11049.337890625
Iteration 13900: Loss = -11049.3369140625
Iteration 14000: Loss = -11049.337890625
1
Iteration 14100: Loss = -11049.3369140625
Iteration 14200: Loss = -11049.3349609375
Iteration 14300: Loss = -11049.3359375
1
Iteration 14400: Loss = -11049.333984375
Iteration 14500: Loss = -11049.33203125
Iteration 14600: Loss = -11048.6435546875
Iteration 14700: Loss = -11046.63671875
Iteration 14800: Loss = -10945.84375
Iteration 14900: Loss = -10945.2578125
Iteration 15000: Loss = -10945.107421875
Iteration 15100: Loss = -10945.037109375
Iteration 15200: Loss = -10944.998046875
Iteration 15300: Loss = -10944.974609375
Iteration 15400: Loss = -10944.9560546875
Iteration 15500: Loss = -10944.9453125
Iteration 15600: Loss = -10944.9375
Iteration 15700: Loss = -10944.927734375
Iteration 15800: Loss = -10944.9228515625
Iteration 15900: Loss = -10944.9189453125
Iteration 16000: Loss = -10944.916015625
Iteration 16100: Loss = -10944.9130859375
Iteration 16200: Loss = -10944.908203125
Iteration 16300: Loss = -10944.908203125
Iteration 16400: Loss = -10944.9052734375
Iteration 16500: Loss = -10944.90234375
Iteration 16600: Loss = -10944.900390625
Iteration 16700: Loss = -10944.8984375
Iteration 16800: Loss = -10944.896484375
Iteration 16900: Loss = -10944.8896484375
Iteration 17000: Loss = -10944.8271484375
Iteration 17100: Loss = -10940.6494140625
Iteration 17200: Loss = -10936.3251953125
Iteration 17300: Loss = -10933.96875
Iteration 17400: Loss = -10933.9609375
Iteration 17500: Loss = -10933.9580078125
Iteration 17600: Loss = -10933.9560546875
Iteration 17700: Loss = -10933.9541015625
Iteration 17800: Loss = -10933.953125
Iteration 17900: Loss = -10933.9521484375
Iteration 18000: Loss = -10933.9521484375
Iteration 18100: Loss = -10933.9521484375
Iteration 18200: Loss = -10933.9501953125
Iteration 18300: Loss = -10933.951171875
1
Iteration 18400: Loss = -10933.94921875
Iteration 18500: Loss = -10933.9453125
Iteration 18600: Loss = -10933.9443359375
Iteration 18700: Loss = -10933.943359375
Iteration 18800: Loss = -10933.9423828125
Iteration 18900: Loss = -10933.943359375
1
Iteration 19000: Loss = -10933.943359375
2
Iteration 19100: Loss = -10933.943359375
3
Iteration 19200: Loss = -10933.943359375
4
Iteration 19300: Loss = -10933.943359375
5
Iteration 19400: Loss = -10933.9423828125
Iteration 19500: Loss = -10933.9423828125
Iteration 19600: Loss = -10933.9423828125
Iteration 19700: Loss = -10933.943359375
1
Iteration 19800: Loss = -10933.9404296875
Iteration 19900: Loss = -10933.9423828125
1
Iteration 20000: Loss = -10933.9404296875
Iteration 20100: Loss = -10933.9384765625
Iteration 20200: Loss = -10933.939453125
1
Iteration 20300: Loss = -10933.939453125
2
Iteration 20400: Loss = -10933.9375
Iteration 20500: Loss = -10933.9384765625
1
Iteration 20600: Loss = -10933.9384765625
2
Iteration 20700: Loss = -10933.9384765625
3
Iteration 20800: Loss = -10933.9384765625
4
Iteration 20900: Loss = -10933.939453125
5
Iteration 21000: Loss = -10933.9375
Iteration 21100: Loss = -10933.9384765625
1
Iteration 21200: Loss = -10933.9384765625
2
Iteration 21300: Loss = -10933.9384765625
3
Iteration 21400: Loss = -10933.939453125
4
Iteration 21500: Loss = -10933.939453125
5
Iteration 21600: Loss = -10933.9384765625
6
Iteration 21700: Loss = -10933.9384765625
7
Iteration 21800: Loss = -10933.9365234375
Iteration 21900: Loss = -10933.9365234375
Iteration 22000: Loss = -10933.935546875
Iteration 22100: Loss = -10933.9365234375
1
Iteration 22200: Loss = -10933.935546875
Iteration 22300: Loss = -10933.9365234375
1
Iteration 22400: Loss = -10933.9365234375
2
Iteration 22500: Loss = -10933.9375
3
Iteration 22600: Loss = -10933.9375
4
Iteration 22700: Loss = -10933.9375
5
Iteration 22800: Loss = -10933.9365234375
6
Iteration 22900: Loss = -10933.9365234375
7
Iteration 23000: Loss = -10933.9365234375
8
Iteration 23100: Loss = -10933.935546875
Iteration 23200: Loss = -10933.9365234375
1
Iteration 23300: Loss = -10933.9365234375
2
Iteration 23400: Loss = -10933.9365234375
3
Iteration 23500: Loss = -10933.9365234375
4
Iteration 23600: Loss = -10933.9365234375
5
Iteration 23700: Loss = -10933.9365234375
6
Iteration 23800: Loss = -10933.935546875
Iteration 23900: Loss = -10933.9365234375
1
Iteration 24000: Loss = -10933.9365234375
2
Iteration 24100: Loss = -10933.9365234375
3
Iteration 24200: Loss = -10933.9365234375
4
Iteration 24300: Loss = -10933.9365234375
5
Iteration 24400: Loss = -10933.9375
6
Iteration 24500: Loss = -10933.9365234375
7
Iteration 24600: Loss = -10933.9365234375
8
Iteration 24700: Loss = -10933.9365234375
9
Iteration 24800: Loss = -10933.9365234375
10
Iteration 24900: Loss = -10933.9365234375
11
Iteration 25000: Loss = -10933.9365234375
12
Iteration 25100: Loss = -10933.9365234375
13
Iteration 25200: Loss = -10933.9365234375
14
Iteration 25300: Loss = -10933.9365234375
15
Stopping early at iteration 25300 due to no improvement.
pi: tensor([[0.7374, 0.2626],
        [0.3499, 0.6501]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2111, 0.7889], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2566, 0.0937],
         [0.1295, 0.1929]],

        [[0.9889, 0.1077],
         [0.0072, 0.9916]],

        [[0.7826, 0.0944],
         [0.4885, 0.9796]],

        [[0.4171, 0.1037],
         [0.0097, 0.0548]],

        [[0.0530, 0.0973],
         [0.4050, 0.5528]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.18545454545454546
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 2
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.736960421744899
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
Global Adjusted Rand Index: 0.41872858519797923
Average Adjusted Rand Index: 0.714576579700063
[-0.0003002266693751667, 0.41872858519797923] [-0.0033899997555783324, 0.714576579700063] [11049.9384765625, 10933.9365234375]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11016.022295498846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33803.5
Iteration 100: Loss = -21285.84375
Iteration 200: Loss = -13654.5166015625
Iteration 300: Loss = -11594.42578125
Iteration 400: Loss = -11372.5087890625
Iteration 500: Loss = -11301.5322265625
Iteration 600: Loss = -11261.0537109375
Iteration 700: Loss = -11236.068359375
Iteration 800: Loss = -11222.3525390625
Iteration 900: Loss = -11212.845703125
Iteration 1000: Loss = -11205.8349609375
Iteration 1100: Loss = -11200.47265625
Iteration 1200: Loss = -11196.2578125
Iteration 1300: Loss = -11192.873046875
Iteration 1400: Loss = -11190.1044921875
Iteration 1500: Loss = -11187.806640625
Iteration 1600: Loss = -11185.87890625
Iteration 1700: Loss = -11184.2412109375
Iteration 1800: Loss = -11182.837890625
Iteration 1900: Loss = -11181.6259765625
Iteration 2000: Loss = -11180.5693359375
Iteration 2100: Loss = -11179.6455078125
Iteration 2200: Loss = -11178.8310546875
Iteration 2300: Loss = -11178.1083984375
Iteration 2400: Loss = -11177.466796875
Iteration 2500: Loss = -11176.8916015625
Iteration 2600: Loss = -11176.375
Iteration 2700: Loss = -11175.912109375
Iteration 2800: Loss = -11175.490234375
Iteration 2900: Loss = -11175.1015625
Iteration 3000: Loss = -11174.7568359375
Iteration 3100: Loss = -11174.44140625
Iteration 3200: Loss = -11174.1552734375
Iteration 3300: Loss = -11173.890625
Iteration 3400: Loss = -11173.6474609375
Iteration 3500: Loss = -11173.423828125
Iteration 3600: Loss = -11173.21484375
Iteration 3700: Loss = -11173.0244140625
Iteration 3800: Loss = -11172.8447265625
Iteration 3900: Loss = -11172.6787109375
Iteration 4000: Loss = -11172.5244140625
Iteration 4100: Loss = -11172.3828125
Iteration 4200: Loss = -11172.24609375
Iteration 4300: Loss = -11172.1201171875
Iteration 4400: Loss = -11172.0029296875
Iteration 4500: Loss = -11171.892578125
Iteration 4600: Loss = -11171.787109375
Iteration 4700: Loss = -11171.689453125
Iteration 4800: Loss = -11171.5986328125
Iteration 4900: Loss = -11171.5146484375
Iteration 5000: Loss = -11171.4326171875
Iteration 5100: Loss = -11171.349609375
Iteration 5200: Loss = -11171.259765625
Iteration 5300: Loss = -11171.1640625
Iteration 5400: Loss = -11171.0927734375
Iteration 5500: Loss = -11171.025390625
Iteration 5600: Loss = -11170.962890625
Iteration 5700: Loss = -11170.9052734375
Iteration 5800: Loss = -11170.8515625
Iteration 5900: Loss = -11170.8046875
Iteration 6000: Loss = -11170.7587890625
Iteration 6100: Loss = -11170.7158203125
Iteration 6200: Loss = -11170.681640625
Iteration 6300: Loss = -11170.64453125
Iteration 6400: Loss = -11170.6123046875
Iteration 6500: Loss = -11170.578125
Iteration 6600: Loss = -11170.541015625
Iteration 6700: Loss = -11170.501953125
Iteration 6800: Loss = -11170.4599609375
Iteration 6900: Loss = -11170.4140625
Iteration 7000: Loss = -11170.3583984375
Iteration 7100: Loss = -11170.298828125
Iteration 7200: Loss = -11170.22265625
Iteration 7300: Loss = -11170.126953125
Iteration 7400: Loss = -11170.0283203125
Iteration 7500: Loss = -11169.939453125
Iteration 7600: Loss = -11169.859375
Iteration 7700: Loss = -11169.7841796875
Iteration 7800: Loss = -11169.7119140625
Iteration 7900: Loss = -11169.6396484375
Iteration 8000: Loss = -11169.572265625
Iteration 8100: Loss = -11169.5078125
Iteration 8200: Loss = -11169.44140625
Iteration 8300: Loss = -11169.37109375
Iteration 8400: Loss = -11169.294921875
Iteration 8500: Loss = -11169.2099609375
Iteration 8600: Loss = -11169.1142578125
Iteration 8700: Loss = -11168.99609375
Iteration 8800: Loss = -11168.8203125
Iteration 8900: Loss = -11168.56640625
Iteration 9000: Loss = -11168.353515625
Iteration 9100: Loss = -11168.1611328125
Iteration 9200: Loss = -11167.9638671875
Iteration 9300: Loss = -11167.7529296875
Iteration 9400: Loss = -11167.5263671875
Iteration 9500: Loss = -11167.306640625
Iteration 9600: Loss = -11167.1162109375
Iteration 9700: Loss = -11166.9482421875
Iteration 9800: Loss = -11166.796875
Iteration 9900: Loss = -11166.6669921875
Iteration 10000: Loss = -11166.556640625
Iteration 10100: Loss = -11166.462890625
Iteration 10200: Loss = -11166.384765625
Iteration 10300: Loss = -11166.3173828125
Iteration 10400: Loss = -11166.251953125
Iteration 10500: Loss = -11166.2021484375
Iteration 10600: Loss = -11166.158203125
Iteration 10700: Loss = -11166.1220703125
Iteration 10800: Loss = -11166.0947265625
Iteration 10900: Loss = -11166.0712890625
Iteration 11000: Loss = -11166.05078125
Iteration 11100: Loss = -11166.033203125
Iteration 11200: Loss = -11166.017578125
Iteration 11300: Loss = -11166.0048828125
Iteration 11400: Loss = -11165.9921875
Iteration 11500: Loss = -11165.982421875
Iteration 11600: Loss = -11165.9677734375
Iteration 11700: Loss = -11165.9599609375
Iteration 11800: Loss = -11165.951171875
Iteration 11900: Loss = -11165.9453125
Iteration 12000: Loss = -11165.9404296875
Iteration 12100: Loss = -11165.9345703125
Iteration 12200: Loss = -11165.927734375
Iteration 12300: Loss = -11165.9228515625
Iteration 12400: Loss = -11165.919921875
Iteration 12500: Loss = -11165.916015625
Iteration 12600: Loss = -11165.9111328125
Iteration 12700: Loss = -11165.908203125
Iteration 12800: Loss = -11165.9033203125
Iteration 12900: Loss = -11165.9013671875
Iteration 13000: Loss = -11165.8984375
Iteration 13100: Loss = -11165.8974609375
Iteration 13200: Loss = -11165.89453125
Iteration 13300: Loss = -11165.890625
Iteration 13400: Loss = -11165.890625
Iteration 13500: Loss = -11165.8876953125
Iteration 13600: Loss = -11165.888671875
1
Iteration 13700: Loss = -11165.8876953125
Iteration 13800: Loss = -11165.88671875
Iteration 13900: Loss = -11165.8857421875
Iteration 14000: Loss = -11165.884765625
Iteration 14100: Loss = -11165.884765625
Iteration 14200: Loss = -11165.884765625
Iteration 14300: Loss = -11165.8857421875
1
Iteration 14400: Loss = -11165.884765625
Iteration 14500: Loss = -11165.884765625
Iteration 14600: Loss = -11165.8857421875
1
Iteration 14700: Loss = -11165.884765625
Iteration 14800: Loss = -11165.8837890625
Iteration 14900: Loss = -11165.8828125
Iteration 15000: Loss = -11165.884765625
1
Iteration 15100: Loss = -11165.884765625
2
Iteration 15200: Loss = -11165.884765625
3
Iteration 15300: Loss = -11165.8828125
Iteration 15400: Loss = -11165.884765625
1
Iteration 15500: Loss = -11165.884765625
2
Iteration 15600: Loss = -11165.884765625
3
Iteration 15700: Loss = -11165.8837890625
4
Iteration 15800: Loss = -11165.8837890625
5
Iteration 15900: Loss = -11165.8837890625
6
Iteration 16000: Loss = -11165.884765625
7
Iteration 16100: Loss = -11165.884765625
8
Iteration 16200: Loss = -11165.8837890625
9
Iteration 16300: Loss = -11165.8818359375
Iteration 16400: Loss = -11165.884765625
1
Iteration 16500: Loss = -11165.884765625
2
Iteration 16600: Loss = -11165.884765625
3
Iteration 16700: Loss = -11165.8837890625
4
Iteration 16800: Loss = -11165.8837890625
5
Iteration 16900: Loss = -11165.8837890625
6
Iteration 17000: Loss = -11165.884765625
7
Iteration 17100: Loss = -11165.884765625
8
Iteration 17200: Loss = -11165.8837890625
9
Iteration 17300: Loss = -11165.8837890625
10
Iteration 17400: Loss = -11165.8837890625
11
Iteration 17500: Loss = -11165.884765625
12
Iteration 17600: Loss = -11165.8828125
13
Iteration 17700: Loss = -11165.8837890625
14
Iteration 17800: Loss = -11165.8837890625
15
Stopping early at iteration 17800 due to no improvement.
pi: tensor([[0.2251, 0.7749],
        [0.0497, 0.9503]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0587, 0.9413], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3581, 0.2210],
         [0.0187, 0.1624]],

        [[0.3871, 0.2020],
         [0.6600, 0.1615]],

        [[0.9069, 0.2275],
         [0.2393, 0.6734]],

        [[0.6974, 0.2101],
         [0.0430, 0.6153]],

        [[0.1002, 0.0848],
         [0.9842, 0.4377]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.025916162480371957
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.010091437982433116
Global Adjusted Rand Index: -0.004179222223921156
Average Adjusted Rand Index: -0.006852451533255359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30666.734375
Iteration 100: Loss = -20230.1484375
Iteration 200: Loss = -13505.400390625
Iteration 300: Loss = -11698.5205078125
Iteration 400: Loss = -11435.185546875
Iteration 500: Loss = -11334.904296875
Iteration 600: Loss = -11288.2421875
Iteration 700: Loss = -11257.4599609375
Iteration 800: Loss = -11239.2236328125
Iteration 900: Loss = -11225.61328125
Iteration 1000: Loss = -11214.7900390625
Iteration 1100: Loss = -11207.33984375
Iteration 1200: Loss = -11201.5751953125
Iteration 1300: Loss = -11196.94921875
Iteration 1400: Loss = -11193.1474609375
Iteration 1500: Loss = -11189.986328125
Iteration 1600: Loss = -11187.3017578125
Iteration 1700: Loss = -11184.990234375
Iteration 1800: Loss = -11182.97265625
Iteration 1900: Loss = -11181.224609375
Iteration 2000: Loss = -11179.701171875
Iteration 2100: Loss = -11178.375
Iteration 2200: Loss = -11177.2255859375
Iteration 2300: Loss = -11176.21875
Iteration 2400: Loss = -11175.3349609375
Iteration 2500: Loss = -11174.548828125
Iteration 2600: Loss = -11173.8466796875
Iteration 2700: Loss = -11173.2119140625
Iteration 2800: Loss = -11172.6259765625
Iteration 2900: Loss = -11172.080078125
Iteration 3000: Loss = -11171.5732421875
Iteration 3100: Loss = -11171.107421875
Iteration 3200: Loss = -11170.68359375
Iteration 3300: Loss = -11170.2880859375
Iteration 3400: Loss = -11169.912109375
Iteration 3500: Loss = -11169.54296875
Iteration 3600: Loss = -11169.1796875
Iteration 3700: Loss = -11168.8330078125
Iteration 3800: Loss = -11168.498046875
Iteration 3900: Loss = -11168.1669921875
Iteration 4000: Loss = -11167.84375
Iteration 4100: Loss = -11167.5234375
Iteration 4200: Loss = -11167.1982421875
Iteration 4300: Loss = -11166.865234375
Iteration 4400: Loss = -11166.505859375
Iteration 4500: Loss = -11166.052734375
Iteration 4600: Loss = -11165.3427734375
Iteration 4700: Loss = -11164.177734375
Iteration 4800: Loss = -11162.3955078125
Iteration 4900: Loss = -11160.888671875
Iteration 5000: Loss = -11159.783203125
Iteration 5100: Loss = -11158.4140625
Iteration 5200: Loss = -11154.833984375
Iteration 5300: Loss = -11145.6591796875
Iteration 5400: Loss = -11131.22265625
Iteration 5500: Loss = -11103.9873046875
Iteration 5600: Loss = -11080.818359375
Iteration 5700: Loss = -11071.5400390625
Iteration 5800: Loss = -11062.6259765625
Iteration 5900: Loss = -11058.2939453125
Iteration 6000: Loss = -11057.748046875
Iteration 6100: Loss = -11057.306640625
Iteration 6200: Loss = -11056.33984375
Iteration 6300: Loss = -11056.21484375
Iteration 6400: Loss = -11056.1318359375
Iteration 6500: Loss = -11056.068359375
Iteration 6600: Loss = -11056.015625
Iteration 6700: Loss = -11055.9677734375
Iteration 6800: Loss = -11055.9248046875
Iteration 6900: Loss = -11055.88671875
Iteration 7000: Loss = -11055.8505859375
Iteration 7100: Loss = -11055.8056640625
Iteration 7200: Loss = -11052.884765625
Iteration 7300: Loss = -11052.8271484375
Iteration 7400: Loss = -11052.7900390625
Iteration 7500: Loss = -11052.7548828125
Iteration 7600: Loss = -11051.8388671875
Iteration 7700: Loss = -11051.76171875
Iteration 7800: Loss = -11051.732421875
Iteration 7900: Loss = -11051.70703125
Iteration 8000: Loss = -11051.685546875
Iteration 8100: Loss = -11051.6650390625
Iteration 8200: Loss = -11051.634765625
Iteration 8300: Loss = -11051.22265625
Iteration 8400: Loss = -11043.7626953125
Iteration 8500: Loss = -11043.6875
Iteration 8600: Loss = -11043.65625
Iteration 8700: Loss = -11043.63671875
Iteration 8800: Loss = -11043.62109375
Iteration 8900: Loss = -11043.607421875
Iteration 9000: Loss = -11043.595703125
Iteration 9100: Loss = -11043.5849609375
Iteration 9200: Loss = -11043.576171875
Iteration 9300: Loss = -11043.5673828125
Iteration 9400: Loss = -11043.55859375
Iteration 9500: Loss = -11043.5478515625
Iteration 9600: Loss = -11043.4736328125
Iteration 9700: Loss = -11043.4619140625
Iteration 9800: Loss = -11043.44921875
Iteration 9900: Loss = -11043.4345703125
Iteration 10000: Loss = -11043.423828125
Iteration 10100: Loss = -11043.4013671875
Iteration 10200: Loss = -11043.3935546875
Iteration 10300: Loss = -11043.388671875
Iteration 10400: Loss = -11043.3837890625
Iteration 10500: Loss = -11043.3798828125
Iteration 10600: Loss = -11043.375
Iteration 10700: Loss = -11043.3701171875
Iteration 10800: Loss = -11043.365234375
Iteration 10900: Loss = -11043.3603515625
Iteration 11000: Loss = -11043.3564453125
Iteration 11100: Loss = -11043.353515625
Iteration 11200: Loss = -11043.3515625
Iteration 11300: Loss = -11043.3486328125
Iteration 11400: Loss = -11043.3447265625
Iteration 11500: Loss = -11043.3427734375
Iteration 11600: Loss = -11043.3369140625
Iteration 11700: Loss = -11043.333984375
Iteration 11800: Loss = -11043.330078125
Iteration 11900: Loss = -11043.3212890625
Iteration 12000: Loss = -11043.306640625
Iteration 12100: Loss = -11043.302734375
Iteration 12200: Loss = -11043.294921875
Iteration 12300: Loss = -11043.2353515625
Iteration 12400: Loss = -11043.23046875
Iteration 12500: Loss = -11043.228515625
Iteration 12600: Loss = -11043.2275390625
Iteration 12700: Loss = -11043.2265625
Iteration 12800: Loss = -11043.224609375
Iteration 12900: Loss = -11043.224609375
Iteration 13000: Loss = -11043.2236328125
Iteration 13100: Loss = -11043.220703125
Iteration 13200: Loss = -11043.220703125
Iteration 13300: Loss = -11043.220703125
Iteration 13400: Loss = -11043.21875
Iteration 13500: Loss = -11043.2177734375
Iteration 13600: Loss = -11043.212890625
Iteration 13700: Loss = -11043.2041015625
Iteration 13800: Loss = -11043.203125
Iteration 13900: Loss = -11043.2021484375
Iteration 14000: Loss = -11043.2021484375
Iteration 14100: Loss = -11043.201171875
Iteration 14200: Loss = -11043.2001953125
Iteration 14300: Loss = -11043.2001953125
Iteration 14400: Loss = -11043.19921875
Iteration 14500: Loss = -11043.1982421875
Iteration 14600: Loss = -11043.1982421875
Iteration 14700: Loss = -11043.19921875
1
Iteration 14800: Loss = -11043.1962890625
Iteration 14900: Loss = -11043.1962890625
Iteration 15000: Loss = -11043.1962890625
Iteration 15100: Loss = -11043.193359375
Iteration 15200: Loss = -11043.1923828125
Iteration 15300: Loss = -11043.1923828125
Iteration 15400: Loss = -11043.189453125
Iteration 15500: Loss = -11043.189453125
Iteration 15600: Loss = -11043.189453125
Iteration 15700: Loss = -11043.189453125
Iteration 15800: Loss = -11043.1884765625
Iteration 15900: Loss = -11043.189453125
1
Iteration 16000: Loss = -11043.1884765625
Iteration 16100: Loss = -11043.1875
Iteration 16200: Loss = -11043.1875
Iteration 16300: Loss = -11043.1865234375
Iteration 16400: Loss = -11043.1865234375
Iteration 16500: Loss = -11043.171875
Iteration 16600: Loss = -11043.1708984375
Iteration 16700: Loss = -11043.169921875
Iteration 16800: Loss = -11043.1708984375
1
Iteration 16900: Loss = -11043.171875
2
Iteration 17000: Loss = -11043.171875
3
Iteration 17100: Loss = -11043.169921875
Iteration 17200: Loss = -11043.1708984375
1
Iteration 17300: Loss = -11043.169921875
Iteration 17400: Loss = -11043.16796875
Iteration 17500: Loss = -11043.1650390625
Iteration 17600: Loss = -11043.166015625
1
Iteration 17700: Loss = -11043.166015625
2
Iteration 17800: Loss = -11043.166015625
3
Iteration 17900: Loss = -11043.1650390625
Iteration 18000: Loss = -11043.166015625
1
Iteration 18100: Loss = -11043.1650390625
Iteration 18200: Loss = -11043.1650390625
Iteration 18300: Loss = -11043.1650390625
Iteration 18400: Loss = -11043.1650390625
Iteration 18500: Loss = -11043.1650390625
Iteration 18600: Loss = -11043.1640625
Iteration 18700: Loss = -11043.1640625
Iteration 18800: Loss = -11043.1650390625
1
Iteration 18900: Loss = -11043.1650390625
2
Iteration 19000: Loss = -11043.166015625
3
Iteration 19100: Loss = -11043.1640625
Iteration 19200: Loss = -11043.1630859375
Iteration 19300: Loss = -11043.1640625
1
Iteration 19400: Loss = -11043.1650390625
2
Iteration 19500: Loss = -11043.1640625
3
Iteration 19600: Loss = -11043.1630859375
Iteration 19700: Loss = -11043.1640625
1
Iteration 19800: Loss = -11043.1630859375
Iteration 19900: Loss = -11043.1162109375
Iteration 20000: Loss = -11043.1171875
1
Iteration 20100: Loss = -11043.115234375
Iteration 20200: Loss = -11043.1171875
1
Iteration 20300: Loss = -11043.1162109375
2
Iteration 20400: Loss = -11043.1171875
3
Iteration 20500: Loss = -11043.1171875
4
Iteration 20600: Loss = -11043.1162109375
5
Iteration 20700: Loss = -11043.1171875
6
Iteration 20800: Loss = -11043.1181640625
7
Iteration 20900: Loss = -11043.1171875
8
Iteration 21000: Loss = -11043.103515625
Iteration 21100: Loss = -11043.1044921875
1
Iteration 21200: Loss = -11043.1044921875
2
Iteration 21300: Loss = -11043.103515625
Iteration 21400: Loss = -11043.1025390625
Iteration 21500: Loss = -11043.1064453125
1
Iteration 21600: Loss = -11043.103515625
2
Iteration 21700: Loss = -11043.103515625
3
Iteration 21800: Loss = -11043.1044921875
4
Iteration 21900: Loss = -11043.10546875
5
Iteration 22000: Loss = -11043.103515625
6
Iteration 22100: Loss = -11043.1044921875
7
Iteration 22200: Loss = -11043.1044921875
8
Iteration 22300: Loss = -11043.10546875
9
Iteration 22400: Loss = -11043.103515625
10
Iteration 22500: Loss = -11043.1044921875
11
Iteration 22600: Loss = -11043.103515625
12
Iteration 22700: Loss = -11043.103515625
13
Iteration 22800: Loss = -11043.1025390625
Iteration 22900: Loss = -11043.103515625
1
Iteration 23000: Loss = -11043.1025390625
Iteration 23100: Loss = -11043.103515625
1
Iteration 23200: Loss = -11043.103515625
2
Iteration 23300: Loss = -11043.1044921875
3
Iteration 23400: Loss = -11043.103515625
4
Iteration 23500: Loss = -11043.103515625
5
Iteration 23600: Loss = -11043.103515625
6
Iteration 23700: Loss = -11043.103515625
7
Iteration 23800: Loss = -11043.103515625
8
Iteration 23900: Loss = -11043.103515625
9
Iteration 24000: Loss = -11043.103515625
10
Iteration 24100: Loss = -11043.1044921875
11
Iteration 24200: Loss = -11043.1044921875
12
Iteration 24300: Loss = -11043.1044921875
13
Iteration 24400: Loss = -11043.1044921875
14
Iteration 24500: Loss = -11043.103515625
15
Stopping early at iteration 24500 due to no improvement.
pi: tensor([[0.6170, 0.3830],
        [0.2569, 0.7431]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.7882e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1766, 0.2325],
         [0.9840, 0.2671]],

        [[0.1790, 0.1069],
         [0.9857, 0.4974]],

        [[0.9694, 0.1023],
         [0.9869, 0.8835]],

        [[0.0134, 0.1025],
         [0.9070, 0.0616]],

        [[0.5458, 0.0970],
         [0.4751, 0.9866]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6690831299615492
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9206245835695015
time is 3
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080875752406894
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.49448572600396196
Average Adjusted Rand Index: 0.6560426163260333
[-0.004179222223921156, 0.49448572600396196] [-0.006852451533255359, 0.6560426163260333] [11165.8837890625, 11043.103515625]
-------------------------------------
This iteration is 2
True Objective function: Loss = -10841.400245710178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -60192.8515625
Iteration 100: Loss = -40251.79296875
Iteration 200: Loss = -23518.93359375
Iteration 300: Loss = -14795.703125
Iteration 400: Loss = -12160.548828125
Iteration 500: Loss = -11356.416015625
Iteration 600: Loss = -11118.7607421875
Iteration 700: Loss = -11042.517578125
Iteration 800: Loss = -11007.1884765625
Iteration 900: Loss = -10993.1494140625
Iteration 1000: Loss = -10984.1318359375
Iteration 1100: Loss = -10977.572265625
Iteration 1200: Loss = -10972.52734375
Iteration 1300: Loss = -10968.5341796875
Iteration 1400: Loss = -10965.3056640625
Iteration 1500: Loss = -10962.650390625
Iteration 1600: Loss = -10960.4375
Iteration 1700: Loss = -10958.5703125
Iteration 1800: Loss = -10956.9794921875
Iteration 1900: Loss = -10955.611328125
Iteration 2000: Loss = -10954.4228515625
Iteration 2100: Loss = -10953.3876953125
Iteration 2200: Loss = -10952.4775390625
Iteration 2300: Loss = -10951.6728515625
Iteration 2400: Loss = -10950.958984375
Iteration 2500: Loss = -10950.322265625
Iteration 2600: Loss = -10949.751953125
Iteration 2700: Loss = -10949.23828125
Iteration 2800: Loss = -10948.775390625
Iteration 2900: Loss = -10948.357421875
Iteration 3000: Loss = -10947.9765625
Iteration 3100: Loss = -10947.62890625
Iteration 3200: Loss = -10947.314453125
Iteration 3300: Loss = -10947.0244140625
Iteration 3400: Loss = -10946.7578125
Iteration 3500: Loss = -10946.513671875
Iteration 3600: Loss = -10946.291015625
Iteration 3700: Loss = -10946.0810546875
Iteration 3800: Loss = -10945.8896484375
Iteration 3900: Loss = -10945.7109375
Iteration 4000: Loss = -10945.546875
Iteration 4100: Loss = -10945.392578125
Iteration 4200: Loss = -10945.2490234375
Iteration 4300: Loss = -10945.1162109375
Iteration 4400: Loss = -10944.9951171875
Iteration 4500: Loss = -10944.8779296875
Iteration 4600: Loss = -10944.7705078125
Iteration 4700: Loss = -10944.669921875
Iteration 4800: Loss = -10944.5732421875
Iteration 4900: Loss = -10944.4853515625
Iteration 5000: Loss = -10944.40234375
Iteration 5100: Loss = -10944.32421875
Iteration 5200: Loss = -10944.2509765625
Iteration 5300: Loss = -10944.1806640625
Iteration 5400: Loss = -10944.1162109375
Iteration 5500: Loss = -10944.0546875
Iteration 5600: Loss = -10943.9970703125
Iteration 5700: Loss = -10943.9423828125
Iteration 5800: Loss = -10943.892578125
Iteration 5900: Loss = -10943.84375
Iteration 6000: Loss = -10943.7978515625
Iteration 6100: Loss = -10943.755859375
Iteration 6200: Loss = -10943.71484375
Iteration 6300: Loss = -10943.677734375
Iteration 6400: Loss = -10943.6416015625
Iteration 6500: Loss = -10943.6064453125
Iteration 6600: Loss = -10943.57421875
Iteration 6700: Loss = -10943.5439453125
Iteration 6800: Loss = -10943.515625
Iteration 6900: Loss = -10943.48828125
Iteration 7000: Loss = -10943.4619140625
Iteration 7100: Loss = -10943.4375
Iteration 7200: Loss = -10943.4130859375
Iteration 7300: Loss = -10943.3896484375
Iteration 7400: Loss = -10943.3701171875
Iteration 7500: Loss = -10943.3505859375
Iteration 7600: Loss = -10943.3310546875
Iteration 7700: Loss = -10943.3134765625
Iteration 7800: Loss = -10943.2958984375
Iteration 7900: Loss = -10943.2802734375
Iteration 8000: Loss = -10943.2666015625
Iteration 8100: Loss = -10943.2509765625
Iteration 8200: Loss = -10943.236328125
Iteration 8300: Loss = -10943.224609375
Iteration 8400: Loss = -10943.2138671875
Iteration 8500: Loss = -10943.2001953125
Iteration 8600: Loss = -10943.1904296875
Iteration 8700: Loss = -10943.177734375
Iteration 8800: Loss = -10943.16796875
Iteration 8900: Loss = -10943.16015625
Iteration 9000: Loss = -10943.1494140625
Iteration 9100: Loss = -10943.1396484375
Iteration 9200: Loss = -10943.1328125
Iteration 9300: Loss = -10943.1259765625
Iteration 9400: Loss = -10943.119140625
Iteration 9500: Loss = -10943.1123046875
Iteration 9600: Loss = -10943.10546875
Iteration 9700: Loss = -10943.09765625
Iteration 9800: Loss = -10943.091796875
Iteration 9900: Loss = -10943.0869140625
Iteration 10000: Loss = -10943.08203125
Iteration 10100: Loss = -10943.076171875
Iteration 10200: Loss = -10943.0712890625
Iteration 10300: Loss = -10943.06640625
Iteration 10400: Loss = -10943.0615234375
Iteration 10500: Loss = -10943.056640625
Iteration 10600: Loss = -10943.0537109375
Iteration 10700: Loss = -10943.0498046875
Iteration 10800: Loss = -10943.046875
Iteration 10900: Loss = -10943.04296875
Iteration 11000: Loss = -10943.0400390625
Iteration 11100: Loss = -10943.03515625
Iteration 11200: Loss = -10943.033203125
Iteration 11300: Loss = -10943.0302734375
Iteration 11400: Loss = -10943.0283203125
Iteration 11500: Loss = -10943.0263671875
Iteration 11600: Loss = -10943.0244140625
Iteration 11700: Loss = -10943.021484375
Iteration 11800: Loss = -10943.0205078125
Iteration 11900: Loss = -10943.017578125
Iteration 12000: Loss = -10943.015625
Iteration 12100: Loss = -10943.013671875
Iteration 12200: Loss = -10943.0126953125
Iteration 12300: Loss = -10943.0087890625
Iteration 12400: Loss = -10943.0078125
Iteration 12500: Loss = -10943.0068359375
Iteration 12600: Loss = -10943.005859375
Iteration 12700: Loss = -10943.001953125
Iteration 12800: Loss = -10943.00390625
1
Iteration 12900: Loss = -10943.0
Iteration 13000: Loss = -10943.0
Iteration 13100: Loss = -10943.0
Iteration 13200: Loss = -10942.998046875
Iteration 13300: Loss = -10942.998046875
Iteration 13400: Loss = -10942.9951171875
Iteration 13500: Loss = -10942.994140625
Iteration 13600: Loss = -10942.9931640625
Iteration 13700: Loss = -10942.9931640625
Iteration 13800: Loss = -10942.9931640625
Iteration 13900: Loss = -10942.9912109375
Iteration 14000: Loss = -10942.9912109375
Iteration 14100: Loss = -10942.990234375
Iteration 14200: Loss = -10942.990234375
Iteration 14300: Loss = -10942.9892578125
Iteration 14400: Loss = -10942.98828125
Iteration 14500: Loss = -10942.9873046875
Iteration 14600: Loss = -10942.9873046875
Iteration 14700: Loss = -10942.986328125
Iteration 14800: Loss = -10942.9873046875
1
Iteration 14900: Loss = -10942.9853515625
Iteration 15000: Loss = -10942.9853515625
Iteration 15100: Loss = -10942.984375
Iteration 15200: Loss = -10942.9833984375
Iteration 15300: Loss = -10942.984375
1
Iteration 15400: Loss = -10942.9833984375
Iteration 15500: Loss = -10942.9833984375
Iteration 15600: Loss = -10942.9833984375
Iteration 15700: Loss = -10942.982421875
Iteration 15800: Loss = -10942.9833984375
1
Iteration 15900: Loss = -10942.9833984375
2
Iteration 16000: Loss = -10942.982421875
Iteration 16100: Loss = -10942.982421875
Iteration 16200: Loss = -10942.9814453125
Iteration 16300: Loss = -10942.9814453125
Iteration 16400: Loss = -10942.982421875
1
Iteration 16500: Loss = -10942.98046875
Iteration 16600: Loss = -10942.9794921875
Iteration 16700: Loss = -10942.98046875
1
Iteration 16800: Loss = -10942.98046875
2
Iteration 16900: Loss = -10942.98046875
3
Iteration 17000: Loss = -10942.9794921875
Iteration 17100: Loss = -10942.978515625
Iteration 17200: Loss = -10942.9794921875
1
Iteration 17300: Loss = -10942.9794921875
2
Iteration 17400: Loss = -10942.98046875
3
Iteration 17500: Loss = -10942.98046875
4
Iteration 17600: Loss = -10942.978515625
Iteration 17700: Loss = -10942.9794921875
1
Iteration 17800: Loss = -10942.98046875
2
Iteration 17900: Loss = -10942.9814453125
3
Iteration 18000: Loss = -10942.98046875
4
Iteration 18100: Loss = -10942.98046875
5
Iteration 18200: Loss = -10942.978515625
Iteration 18300: Loss = -10942.9794921875
1
Iteration 18400: Loss = -10942.9794921875
2
Iteration 18500: Loss = -10942.978515625
Iteration 18600: Loss = -10942.9794921875
1
Iteration 18700: Loss = -10942.9794921875
2
Iteration 18800: Loss = -10942.98046875
3
Iteration 18900: Loss = -10942.978515625
Iteration 19000: Loss = -10942.978515625
Iteration 19100: Loss = -10942.9794921875
1
Iteration 19200: Loss = -10942.978515625
Iteration 19300: Loss = -10942.978515625
Iteration 19400: Loss = -10942.978515625
Iteration 19500: Loss = -10942.9794921875
1
Iteration 19600: Loss = -10942.9794921875
2
Iteration 19700: Loss = -10942.978515625
Iteration 19800: Loss = -10942.978515625
Iteration 19900: Loss = -10942.978515625
Iteration 20000: Loss = -10942.978515625
Iteration 20100: Loss = -10942.978515625
Iteration 20200: Loss = -10942.978515625
Iteration 20300: Loss = -10942.978515625
Iteration 20400: Loss = -10942.9775390625
Iteration 20500: Loss = -10942.978515625
1
Iteration 20600: Loss = -10942.978515625
2
Iteration 20700: Loss = -10942.9794921875
3
Iteration 20800: Loss = -10942.978515625
4
Iteration 20900: Loss = -10942.978515625
5
Iteration 21000: Loss = -10942.9794921875
6
Iteration 21100: Loss = -10942.978515625
7
Iteration 21200: Loss = -10942.978515625
8
Iteration 21300: Loss = -10942.978515625
9
Iteration 21400: Loss = -10942.978515625
10
Iteration 21500: Loss = -10942.978515625
11
Iteration 21600: Loss = -10942.9775390625
Iteration 21700: Loss = -10942.978515625
1
Iteration 21800: Loss = -10942.9775390625
Iteration 21900: Loss = -10942.978515625
1
Iteration 22000: Loss = -10942.98046875
2
Iteration 22100: Loss = -10943.0029296875
3
Iteration 22200: Loss = -10942.978515625
4
Iteration 22300: Loss = -10942.98046875
5
Iteration 22400: Loss = -10942.98046875
6
Iteration 22500: Loss = -10942.98046875
7
Iteration 22600: Loss = -10942.98046875
8
Iteration 22700: Loss = -10942.98046875
9
Iteration 22800: Loss = -10942.98046875
10
Iteration 22900: Loss = -10942.98046875
11
Iteration 23000: Loss = -10942.9892578125
12
Iteration 23100: Loss = -10942.978515625
13
Iteration 23200: Loss = -10942.98046875
14
Iteration 23300: Loss = -10942.978515625
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[9.9999e-01, 6.0153e-06],
        [9.6224e-01, 3.7757e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 7.7468e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1615, 0.1880],
         [0.0347, 0.9298]],

        [[0.0238, 0.1879],
         [0.8340, 0.9891]],

        [[0.9718, 0.1767],
         [0.8825, 0.0859]],

        [[0.3070, 0.2181],
         [0.0578, 0.7895]],

        [[0.8554, 0.2580],
         [0.0335, 0.9662]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -66791.734375
Iteration 100: Loss = -49887.296875
Iteration 200: Loss = -36203.44140625
Iteration 300: Loss = -25985.578125
Iteration 400: Loss = -18882.078125
Iteration 500: Loss = -14688.0947265625
Iteration 600: Loss = -12602.6171875
Iteration 700: Loss = -11653.58984375
Iteration 800: Loss = -11257.140625
Iteration 900: Loss = -11102.669921875
Iteration 1000: Loss = -11050.2666015625
Iteration 1100: Loss = -11012.55078125
Iteration 1200: Loss = -10995.974609375
Iteration 1300: Loss = -10988.59375
Iteration 1400: Loss = -10982.447265625
Iteration 1500: Loss = -10975.52734375
Iteration 1600: Loss = -10972.619140625
Iteration 1700: Loss = -10969.90625
Iteration 1800: Loss = -10966.3564453125
Iteration 1900: Loss = -10963.2744140625
Iteration 2000: Loss = -10960.8486328125
Iteration 2100: Loss = -10958.1865234375
Iteration 2200: Loss = -10955.9482421875
Iteration 2300: Loss = -10954.89453125
Iteration 2400: Loss = -10954.3046875
Iteration 2500: Loss = -10953.8369140625
Iteration 2600: Loss = -10953.357421875
Iteration 2700: Loss = -10952.4267578125
Iteration 2800: Loss = -10947.203125
Iteration 2900: Loss = -10945.3046875
Iteration 3000: Loss = -10944.650390625
Iteration 3100: Loss = -10944.296875
Iteration 3200: Loss = -10944.0673828125
Iteration 3300: Loss = -10943.8994140625
Iteration 3400: Loss = -10943.771484375
Iteration 3500: Loss = -10943.6669921875
Iteration 3600: Loss = -10943.5810546875
Iteration 3700: Loss = -10943.50390625
Iteration 3800: Loss = -10943.4384765625
Iteration 3900: Loss = -10943.3818359375
Iteration 4000: Loss = -10943.3271484375
Iteration 4100: Loss = -10943.28125
Iteration 4200: Loss = -10943.2373046875
Iteration 4300: Loss = -10943.19921875
Iteration 4400: Loss = -10943.162109375
Iteration 4500: Loss = -10943.126953125
Iteration 4600: Loss = -10943.0966796875
Iteration 4700: Loss = -10943.0673828125
Iteration 4800: Loss = -10943.041015625
Iteration 4900: Loss = -10943.017578125
Iteration 5000: Loss = -10942.9921875
Iteration 5100: Loss = -10942.970703125
Iteration 5200: Loss = -10942.951171875
Iteration 5300: Loss = -10942.93359375
Iteration 5400: Loss = -10942.9150390625
Iteration 5500: Loss = -10942.8984375
Iteration 5600: Loss = -10942.8837890625
Iteration 5700: Loss = -10942.87109375
Iteration 5800: Loss = -10942.8564453125
Iteration 5900: Loss = -10942.84375
Iteration 6000: Loss = -10942.8291015625
Iteration 6100: Loss = -10942.8154296875
Iteration 6200: Loss = -10942.80078125
Iteration 6300: Loss = -10942.791015625
Iteration 6400: Loss = -10942.77734375
Iteration 6500: Loss = -10942.7666015625
Iteration 6600: Loss = -10942.755859375
Iteration 6700: Loss = -10942.7412109375
Iteration 6800: Loss = -10942.72265625
Iteration 6900: Loss = -10942.7060546875
Iteration 7000: Loss = -10942.693359375
Iteration 7100: Loss = -10942.6826171875
Iteration 7200: Loss = -10942.673828125
Iteration 7300: Loss = -10942.6650390625
Iteration 7400: Loss = -10942.6494140625
Iteration 7500: Loss = -10942.642578125
Iteration 7600: Loss = -10942.6328125
Iteration 7700: Loss = -10942.62109375
Iteration 7800: Loss = -10942.6123046875
Iteration 7900: Loss = -10942.60546875
Iteration 8000: Loss = -10942.5986328125
Iteration 8100: Loss = -10942.587890625
Iteration 8200: Loss = -10942.576171875
Iteration 8300: Loss = -10942.5458984375
Iteration 8400: Loss = -10942.50390625
Iteration 8500: Loss = -10942.4951171875
Iteration 8600: Loss = -10942.484375
Iteration 8700: Loss = -10942.474609375
Iteration 8800: Loss = -10942.46484375
Iteration 8900: Loss = -10942.4541015625
Iteration 9000: Loss = -10942.4423828125
Iteration 9100: Loss = -10942.4267578125
Iteration 9200: Loss = -10942.3935546875
Iteration 9300: Loss = -10942.3603515625
Iteration 9400: Loss = -10942.314453125
Iteration 9500: Loss = -10942.2451171875
Iteration 9600: Loss = -10942.1904296875
Iteration 9700: Loss = -10942.1435546875
Iteration 9800: Loss = -10942.0947265625
Iteration 9900: Loss = -10942.060546875
Iteration 10000: Loss = -10942.02734375
Iteration 10100: Loss = -10941.9833984375
Iteration 10200: Loss = -10941.9521484375
Iteration 10300: Loss = -10941.92578125
Iteration 10400: Loss = -10941.7685546875
Iteration 10500: Loss = -10941.705078125
Iteration 10600: Loss = -10941.66796875
Iteration 10700: Loss = -10941.642578125
Iteration 10800: Loss = -10941.6162109375
Iteration 10900: Loss = -10941.587890625
Iteration 11000: Loss = -10941.560546875
Iteration 11100: Loss = -10941.53125
Iteration 11200: Loss = -10941.5048828125
Iteration 11300: Loss = -10941.4794921875
Iteration 11400: Loss = -10941.455078125
Iteration 11500: Loss = -10941.43359375
Iteration 11600: Loss = -10941.4150390625
Iteration 11700: Loss = -10941.3935546875
Iteration 11800: Loss = -10941.3388671875
Iteration 11900: Loss = -10940.12890625
Iteration 12000: Loss = -10939.57421875
Iteration 12100: Loss = -10939.4384765625
Iteration 12200: Loss = -10939.390625
Iteration 12300: Loss = -10939.3662109375
Iteration 12400: Loss = -10939.3525390625
Iteration 12500: Loss = -10939.341796875
Iteration 12600: Loss = -10939.333984375
Iteration 12700: Loss = -10939.328125
Iteration 12800: Loss = -10939.32421875
Iteration 12900: Loss = -10939.322265625
Iteration 13000: Loss = -10939.3173828125
Iteration 13100: Loss = -10939.314453125
Iteration 13200: Loss = -10939.30859375
Iteration 13300: Loss = -10938.212890625
Iteration 13400: Loss = -10938.125
Iteration 13500: Loss = -10938.083984375
Iteration 13600: Loss = -10938.05859375
Iteration 13700: Loss = -10938.0400390625
Iteration 13800: Loss = -10938.0302734375
Iteration 13900: Loss = -10938.0244140625
Iteration 14000: Loss = -10938.0185546875
Iteration 14100: Loss = -10938.017578125
Iteration 14200: Loss = -10938.0146484375
Iteration 14300: Loss = -10938.013671875
Iteration 14400: Loss = -10938.013671875
Iteration 14500: Loss = -10938.01171875
Iteration 14600: Loss = -10938.0107421875
Iteration 14700: Loss = -10938.009765625
Iteration 14800: Loss = -10938.0107421875
1
Iteration 14900: Loss = -10938.0078125
Iteration 15000: Loss = -10938.0078125
Iteration 15100: Loss = -10938.0087890625
1
Iteration 15200: Loss = -10938.0087890625
2
Iteration 15300: Loss = -10938.0078125
Iteration 15400: Loss = -10938.0078125
Iteration 15500: Loss = -10938.0068359375
Iteration 15600: Loss = -10938.0068359375
Iteration 15700: Loss = -10938.0068359375
Iteration 15800: Loss = -10938.0068359375
Iteration 15900: Loss = -10938.0068359375
Iteration 16000: Loss = -10938.0068359375
Iteration 16100: Loss = -10938.0068359375
Iteration 16200: Loss = -10938.0048828125
Iteration 16300: Loss = -10938.0048828125
Iteration 16400: Loss = -10938.0048828125
Iteration 16500: Loss = -10938.0048828125
Iteration 16600: Loss = -10938.005859375
1
Iteration 16700: Loss = -10938.0048828125
Iteration 16800: Loss = -10938.005859375
1
Iteration 16900: Loss = -10938.005859375
2
Iteration 17000: Loss = -10938.0068359375
3
Iteration 17100: Loss = -10938.00390625
Iteration 17200: Loss = -10938.0048828125
1
Iteration 17300: Loss = -10938.00390625
Iteration 17400: Loss = -10938.0048828125
1
Iteration 17500: Loss = -10938.00390625
Iteration 17600: Loss = -10938.00390625
Iteration 17700: Loss = -10938.00390625
Iteration 17800: Loss = -10938.0048828125
1
Iteration 17900: Loss = -10938.00390625
Iteration 18000: Loss = -10938.00390625
Iteration 18100: Loss = -10938.00390625
Iteration 18200: Loss = -10938.00390625
Iteration 18300: Loss = -10938.0048828125
1
Iteration 18400: Loss = -10938.00390625
Iteration 18500: Loss = -10938.0029296875
Iteration 18600: Loss = -10938.0029296875
Iteration 18700: Loss = -10938.001953125
Iteration 18800: Loss = -10937.720703125
Iteration 18900: Loss = -10937.7216796875
1
Iteration 19000: Loss = -10937.7197265625
Iteration 19100: Loss = -10937.7216796875
1
Iteration 19200: Loss = -10937.71875
Iteration 19300: Loss = -10937.7197265625
1
Iteration 19400: Loss = -10937.7197265625
2
Iteration 19500: Loss = -10937.7197265625
3
Iteration 19600: Loss = -10937.7197265625
4
Iteration 19700: Loss = -10937.71875
Iteration 19800: Loss = -10937.7197265625
1
Iteration 19900: Loss = -10937.720703125
2
Iteration 20000: Loss = -10937.7197265625
3
Iteration 20100: Loss = -10937.720703125
4
Iteration 20200: Loss = -10937.7197265625
5
Iteration 20300: Loss = -10937.7197265625
6
Iteration 20400: Loss = -10937.71875
Iteration 20500: Loss = -10937.7197265625
1
Iteration 20600: Loss = -10937.720703125
2
Iteration 20700: Loss = -10937.7197265625
3
Iteration 20800: Loss = -10937.7197265625
4
Iteration 20900: Loss = -10937.7216796875
5
Iteration 21000: Loss = -10937.71875
Iteration 21100: Loss = -10937.708984375
Iteration 21200: Loss = -10937.7099609375
1
Iteration 21300: Loss = -10937.7099609375
2
Iteration 21400: Loss = -10937.708984375
Iteration 21500: Loss = -10937.7080078125
Iteration 21600: Loss = -10937.708984375
1
Iteration 21700: Loss = -10937.70703125
Iteration 21800: Loss = -10937.7080078125
1
Iteration 21900: Loss = -10937.7080078125
2
Iteration 22000: Loss = -10937.7080078125
3
Iteration 22100: Loss = -10937.708984375
4
Iteration 22200: Loss = -10937.708984375
5
Iteration 22300: Loss = -10937.7080078125
6
Iteration 22400: Loss = -10937.7080078125
7
Iteration 22500: Loss = -10937.7099609375
8
Iteration 22600: Loss = -10937.7080078125
9
Iteration 22700: Loss = -10937.7080078125
10
Iteration 22800: Loss = -10937.7080078125
11
Iteration 22900: Loss = -10937.70703125
Iteration 23000: Loss = -10937.7080078125
1
Iteration 23100: Loss = -10937.7080078125
2
Iteration 23200: Loss = -10937.70703125
Iteration 23300: Loss = -10937.70703125
Iteration 23400: Loss = -10937.7080078125
1
Iteration 23500: Loss = -10937.708984375
2
Iteration 23600: Loss = -10937.7041015625
Iteration 23700: Loss = -10937.705078125
1
Iteration 23800: Loss = -10937.705078125
2
Iteration 23900: Loss = -10937.7060546875
3
Iteration 24000: Loss = -10937.705078125
4
Iteration 24100: Loss = -10937.705078125
5
Iteration 24200: Loss = -10937.705078125
6
Iteration 24300: Loss = -10937.7001953125
Iteration 24400: Loss = -10937.7001953125
Iteration 24500: Loss = -10937.7021484375
1
Iteration 24600: Loss = -10937.7001953125
Iteration 24700: Loss = -10937.7001953125
Iteration 24800: Loss = -10937.69921875
Iteration 24900: Loss = -10937.697265625
Iteration 25000: Loss = -10937.6982421875
1
Iteration 25100: Loss = -10937.697265625
Iteration 25200: Loss = -10937.697265625
Iteration 25300: Loss = -10937.6962890625
Iteration 25400: Loss = -10937.697265625
1
Iteration 25500: Loss = -10937.6962890625
Iteration 25600: Loss = -10937.697265625
1
Iteration 25700: Loss = -10937.6962890625
Iteration 25800: Loss = -10937.697265625
1
Iteration 25900: Loss = -10937.697265625
2
Iteration 26000: Loss = -10937.6962890625
Iteration 26100: Loss = -10937.6962890625
Iteration 26200: Loss = -10937.6962890625
Iteration 26300: Loss = -10937.697265625
1
Iteration 26400: Loss = -10937.697265625
2
Iteration 26500: Loss = -10937.6962890625
Iteration 26600: Loss = -10937.6962890625
Iteration 26700: Loss = -10937.6962890625
Iteration 26800: Loss = -10937.697265625
1
Iteration 26900: Loss = -10937.6962890625
Iteration 27000: Loss = -10937.6962890625
Iteration 27100: Loss = -10937.6943359375
Iteration 27200: Loss = -10937.6982421875
1
Iteration 27300: Loss = -10937.6953125
2
Iteration 27400: Loss = -10937.6953125
3
Iteration 27500: Loss = -10937.6962890625
4
Iteration 27600: Loss = -10937.6962890625
5
Iteration 27700: Loss = -10937.6953125
6
Iteration 27800: Loss = -10937.6962890625
7
Iteration 27900: Loss = -10937.6953125
8
Iteration 28000: Loss = -10937.697265625
9
Iteration 28100: Loss = -10937.6953125
10
Iteration 28200: Loss = -10937.697265625
11
Iteration 28300: Loss = -10937.6953125
12
Iteration 28400: Loss = -10937.6962890625
13
Iteration 28500: Loss = -10937.6943359375
Iteration 28600: Loss = -10937.697265625
1
Iteration 28700: Loss = -10937.6962890625
2
Iteration 28800: Loss = -10937.6953125
3
Iteration 28900: Loss = -10937.6953125
4
Iteration 29000: Loss = -10937.6953125
5
Iteration 29100: Loss = -10937.6962890625
6
Iteration 29200: Loss = -10937.6962890625
7
Iteration 29300: Loss = -10937.6962890625
8
Iteration 29400: Loss = -10937.6962890625
9
Iteration 29500: Loss = -10937.6962890625
10
Iteration 29600: Loss = -10937.6943359375
Iteration 29700: Loss = -10937.6962890625
1
Iteration 29800: Loss = -10937.6962890625
2
Iteration 29900: Loss = -10937.6962890625
3
pi: tensor([[1.0000e+00, 2.3731e-06],
        [1.2579e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0270, 0.9730], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1413, 0.1049],
         [0.9241, 0.1615]],

        [[0.9843, 0.0865],
         [0.9874, 0.6808]],

        [[0.9102, 0.1951],
         [0.9352, 0.0156]],

        [[0.9684, 0.1839],
         [0.0443, 0.3481]],

        [[0.9730, 0.2352],
         [0.9441, 0.0166]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: -0.00021731722590199396
Average Adjusted Rand Index: -0.002164923978724115
[0.0, -0.00021731722590199396] [0.0, -0.002164923978724115] [10942.978515625, 10937.6962890625]
-------------------------------------
This iteration is 3
True Objective function: Loss = -10910.183756692655
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21941.962890625
Iteration 100: Loss = -15463.3037109375
Iteration 200: Loss = -12165.044921875
Iteration 300: Loss = -11361.48828125
Iteration 400: Loss = -11176.5908203125
Iteration 500: Loss = -11133.5576171875
Iteration 600: Loss = -11107.892578125
Iteration 700: Loss = -11089.37890625
Iteration 800: Loss = -11075.3203125
Iteration 900: Loss = -11062.986328125
Iteration 1000: Loss = -11051.400390625
Iteration 1100: Loss = -11041.171875
Iteration 1200: Loss = -11031.6630859375
Iteration 1300: Loss = -11024.2880859375
Iteration 1400: Loss = -11018.4560546875
Iteration 1500: Loss = -11013.9775390625
Iteration 1600: Loss = -11010.5146484375
Iteration 1700: Loss = -11007.779296875
Iteration 1800: Loss = -11005.2119140625
Iteration 1900: Loss = -11003.5419921875
Iteration 2000: Loss = -11002.4326171875
Iteration 2100: Loss = -11001.5830078125
Iteration 2200: Loss = -11000.890625
Iteration 2300: Loss = -11000.26171875
Iteration 2400: Loss = -10999.7939453125
Iteration 2500: Loss = -10999.4208984375
Iteration 2600: Loss = -10999.1064453125
Iteration 2700: Loss = -10998.8359375
Iteration 2800: Loss = -10998.5986328125
Iteration 2900: Loss = -10998.3876953125
Iteration 3000: Loss = -10998.1923828125
Iteration 3100: Loss = -10997.9921875
Iteration 3200: Loss = -10997.79296875
Iteration 3300: Loss = -10997.595703125
Iteration 3400: Loss = -10997.3232421875
Iteration 3500: Loss = -10997.1796875
Iteration 3600: Loss = -10997.0556640625
Iteration 3700: Loss = -10996.9375
Iteration 3800: Loss = -10996.8193359375
Iteration 3900: Loss = -10996.705078125
Iteration 4000: Loss = -10996.59375
Iteration 4100: Loss = -10996.4921875
Iteration 4200: Loss = -10996.3994140625
Iteration 4300: Loss = -10996.3173828125
Iteration 4400: Loss = -10996.2451171875
Iteration 4500: Loss = -10996.1806640625
Iteration 4600: Loss = -10996.125
Iteration 4700: Loss = -10996.0751953125
Iteration 4800: Loss = -10996.0322265625
Iteration 4900: Loss = -10995.994140625
Iteration 5000: Loss = -10995.958984375
Iteration 5100: Loss = -10995.9287109375
Iteration 5200: Loss = -10995.8994140625
Iteration 5300: Loss = -10995.875
Iteration 5400: Loss = -10995.8515625
Iteration 5500: Loss = -10995.8291015625
Iteration 5600: Loss = -10995.80859375
Iteration 5700: Loss = -10995.791015625
Iteration 5800: Loss = -10995.7724609375
Iteration 5900: Loss = -10995.7568359375
Iteration 6000: Loss = -10995.740234375
Iteration 6100: Loss = -10995.7255859375
Iteration 6200: Loss = -10995.7080078125
Iteration 6300: Loss = -10995.3173828125
Iteration 6400: Loss = -10995.2236328125
Iteration 6500: Loss = -10995.1865234375
Iteration 6600: Loss = -10995.1640625
Iteration 6700: Loss = -10995.150390625
Iteration 6800: Loss = -10995.1396484375
Iteration 6900: Loss = -10995.1298828125
Iteration 7000: Loss = -10995.1220703125
Iteration 7100: Loss = -10995.1142578125
Iteration 7200: Loss = -10995.1064453125
Iteration 7300: Loss = -10995.0986328125
Iteration 7400: Loss = -10995.0908203125
Iteration 7500: Loss = -10995.0849609375
Iteration 7600: Loss = -10995.080078125
Iteration 7700: Loss = -10995.072265625
Iteration 7800: Loss = -10995.0693359375
Iteration 7900: Loss = -10995.064453125
Iteration 8000: Loss = -10995.0595703125
Iteration 8100: Loss = -10995.052734375
Iteration 8200: Loss = -10995.05078125
Iteration 8300: Loss = -10995.0458984375
Iteration 8400: Loss = -10995.04296875
Iteration 8500: Loss = -10995.0380859375
Iteration 8600: Loss = -10995.0341796875
Iteration 8700: Loss = -10995.03125
Iteration 8800: Loss = -10995.029296875
Iteration 8900: Loss = -10995.025390625
Iteration 9000: Loss = -10995.021484375
Iteration 9100: Loss = -10995.0205078125
Iteration 9200: Loss = -10995.017578125
Iteration 9300: Loss = -10995.0146484375
Iteration 9400: Loss = -10995.0126953125
Iteration 9500: Loss = -10995.01171875
Iteration 9600: Loss = -10995.0087890625
Iteration 9700: Loss = -10995.005859375
Iteration 9800: Loss = -10995.00390625
Iteration 9900: Loss = -10995.00390625
Iteration 10000: Loss = -10995.001953125
Iteration 10100: Loss = -10994.9990234375
Iteration 10200: Loss = -10994.9990234375
Iteration 10300: Loss = -10994.9970703125
Iteration 10400: Loss = -10994.9951171875
Iteration 10500: Loss = -10994.9951171875
Iteration 10600: Loss = -10994.994140625
Iteration 10700: Loss = -10994.9931640625
Iteration 10800: Loss = -10994.9912109375
Iteration 10900: Loss = -10994.990234375
Iteration 11000: Loss = -10994.98828125
Iteration 11100: Loss = -10994.98828125
Iteration 11200: Loss = -10994.9873046875
Iteration 11300: Loss = -10994.986328125
Iteration 11400: Loss = -10994.9853515625
Iteration 11500: Loss = -10994.984375
Iteration 11600: Loss = -10994.9853515625
1
Iteration 11700: Loss = -10994.9833984375
Iteration 11800: Loss = -10994.9833984375
Iteration 11900: Loss = -10994.9814453125
Iteration 12000: Loss = -10994.98046875
Iteration 12100: Loss = -10994.98046875
Iteration 12200: Loss = -10994.98046875
Iteration 12300: Loss = -10994.98046875
Iteration 12400: Loss = -10994.9794921875
Iteration 12500: Loss = -10994.9794921875
Iteration 12600: Loss = -10994.978515625
Iteration 12700: Loss = -10994.978515625
Iteration 12800: Loss = -10994.9775390625
Iteration 12900: Loss = -10994.978515625
1
Iteration 13000: Loss = -10994.9765625
Iteration 13100: Loss = -10994.9765625
Iteration 13200: Loss = -10994.9765625
Iteration 13300: Loss = -10994.9755859375
Iteration 13400: Loss = -10994.9765625
1
Iteration 13500: Loss = -10994.9755859375
Iteration 13600: Loss = -10994.9755859375
Iteration 13700: Loss = -10994.9755859375
Iteration 13800: Loss = -10994.9755859375
Iteration 13900: Loss = -10994.9736328125
Iteration 14000: Loss = -10994.974609375
1
Iteration 14100: Loss = -10994.9736328125
Iteration 14200: Loss = -10994.9736328125
Iteration 14300: Loss = -10994.974609375
1
Iteration 14400: Loss = -10994.97265625
Iteration 14500: Loss = -10994.97265625
Iteration 14600: Loss = -10994.97265625
Iteration 14700: Loss = -10994.9716796875
Iteration 14800: Loss = -10994.97265625
1
Iteration 14900: Loss = -10994.9736328125
2
Iteration 15000: Loss = -10994.97265625
3
Iteration 15100: Loss = -10994.97265625
4
Iteration 15200: Loss = -10994.97265625
5
Iteration 15300: Loss = -10994.97265625
6
Iteration 15400: Loss = -10994.97265625
7
Iteration 15500: Loss = -10994.970703125
Iteration 15600: Loss = -10994.9716796875
1
Iteration 15700: Loss = -10994.97265625
2
Iteration 15800: Loss = -10994.970703125
Iteration 15900: Loss = -10994.970703125
Iteration 16000: Loss = -10994.9716796875
1
Iteration 16100: Loss = -10994.970703125
Iteration 16200: Loss = -10994.970703125
Iteration 16300: Loss = -10994.9716796875
1
Iteration 16400: Loss = -10994.9697265625
Iteration 16500: Loss = -10994.970703125
1
Iteration 16600: Loss = -10994.9697265625
Iteration 16700: Loss = -10994.9716796875
1
Iteration 16800: Loss = -10994.970703125
2
Iteration 16900: Loss = -10994.970703125
3
Iteration 17000: Loss = -10994.97265625
4
Iteration 17100: Loss = -10994.970703125
5
Iteration 17200: Loss = -10994.9697265625
Iteration 17300: Loss = -10994.970703125
1
Iteration 17400: Loss = -10994.9697265625
Iteration 17500: Loss = -10994.9716796875
1
Iteration 17600: Loss = -10994.970703125
2
Iteration 17700: Loss = -10994.970703125
3
Iteration 17800: Loss = -10994.9697265625
Iteration 17900: Loss = -10994.970703125
1
Iteration 18000: Loss = -10994.970703125
2
Iteration 18100: Loss = -10994.9697265625
Iteration 18200: Loss = -10994.9716796875
1
Iteration 18300: Loss = -10994.9716796875
2
Iteration 18400: Loss = -10994.9599609375
Iteration 18500: Loss = -10993.65625
Iteration 18600: Loss = -10993.61328125
Iteration 18700: Loss = -10993.6044921875
Iteration 18800: Loss = -10993.59765625
Iteration 18900: Loss = -10993.5947265625
Iteration 19000: Loss = -10993.5927734375
Iteration 19100: Loss = -10993.591796875
Iteration 19200: Loss = -10993.5908203125
Iteration 19300: Loss = -10993.58984375
Iteration 19400: Loss = -10993.5888671875
Iteration 19500: Loss = -10993.5888671875
Iteration 19600: Loss = -10993.587890625
Iteration 19700: Loss = -10993.587890625
Iteration 19800: Loss = -10993.5869140625
Iteration 19900: Loss = -10993.587890625
1
Iteration 20000: Loss = -10993.5869140625
Iteration 20100: Loss = -10993.5869140625
Iteration 20200: Loss = -10993.5869140625
Iteration 20300: Loss = -10993.5869140625
Iteration 20400: Loss = -10993.5859375
Iteration 20500: Loss = -10993.5859375
Iteration 20600: Loss = -10993.5859375
Iteration 20700: Loss = -10993.5859375
Iteration 20800: Loss = -10993.5849609375
Iteration 20900: Loss = -10993.5859375
1
Iteration 21000: Loss = -10993.5859375
2
Iteration 21100: Loss = -10993.5859375
3
Iteration 21200: Loss = -10993.5859375
4
Iteration 21300: Loss = -10993.5859375
5
Iteration 21400: Loss = -10993.5859375
6
Iteration 21500: Loss = -10993.5849609375
Iteration 21600: Loss = -10993.5849609375
Iteration 21700: Loss = -10993.5849609375
Iteration 21800: Loss = -10993.5849609375
Iteration 21900: Loss = -10993.5849609375
Iteration 22000: Loss = -10993.5849609375
Iteration 22100: Loss = -10993.5849609375
Iteration 22200: Loss = -10993.583984375
Iteration 22300: Loss = -10993.5849609375
1
Iteration 22400: Loss = -10993.5830078125
Iteration 22500: Loss = -10993.5849609375
1
Iteration 22600: Loss = -10993.5849609375
2
Iteration 22700: Loss = -10993.5849609375
3
Iteration 22800: Loss = -10993.5849609375
4
Iteration 22900: Loss = -10993.583984375
5
Iteration 23000: Loss = -10993.583984375
6
Iteration 23100: Loss = -10993.5849609375
7
Iteration 23200: Loss = -10993.5849609375
8
Iteration 23300: Loss = -10993.5849609375
9
Iteration 23400: Loss = -10993.5849609375
10
Iteration 23500: Loss = -10993.583984375
11
Iteration 23600: Loss = -10993.5849609375
12
Iteration 23700: Loss = -10993.5830078125
Iteration 23800: Loss = -10993.5830078125
Iteration 23900: Loss = -10993.583984375
1
Iteration 24000: Loss = -10993.583984375
2
Iteration 24100: Loss = -10993.5849609375
3
Iteration 24200: Loss = -10993.5830078125
Iteration 24300: Loss = -10993.583984375
1
Iteration 24400: Loss = -10993.583984375
2
Iteration 24500: Loss = -10993.5849609375
3
Iteration 24600: Loss = -10993.583984375
4
Iteration 24700: Loss = -10993.5869140625
5
Iteration 24800: Loss = -10993.5830078125
Iteration 24900: Loss = -10993.583984375
1
Iteration 25000: Loss = -10993.5859375
2
Iteration 25100: Loss = -10993.583984375
3
Iteration 25200: Loss = -10993.583984375
4
Iteration 25300: Loss = -10993.583984375
5
Iteration 25400: Loss = -10993.583984375
6
Iteration 25500: Loss = -10993.583984375
7
Iteration 25600: Loss = -10993.583984375
8
Iteration 25700: Loss = -10993.583984375
9
Iteration 25800: Loss = -10993.583984375
10
Iteration 25900: Loss = -10993.583984375
11
Iteration 26000: Loss = -10993.5859375
12
Iteration 26100: Loss = -10993.583984375
13
Iteration 26200: Loss = -10993.5859375
14
Iteration 26300: Loss = -10993.583984375
15
Stopping early at iteration 26300 due to no improvement.
pi: tensor([[3.2596e-05, 9.9997e-01],
        [1.7285e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9983e-01, 1.6995e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1665, 0.1667],
         [0.1256, 0.1618]],

        [[0.7184, 0.1281],
         [0.0201, 0.1168]],

        [[0.8988, 0.1707],
         [0.9055, 0.9917]],

        [[0.9281, 0.2537],
         [0.1685, 0.7068]],

        [[0.0682, 0.1585],
         [0.9173, 0.0670]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001550223838615892
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -46462.38671875
Iteration 100: Loss = -29217.099609375
Iteration 200: Loss = -15350.4599609375
Iteration 300: Loss = -12246.970703125
Iteration 400: Loss = -11720.3740234375
Iteration 500: Loss = -11463.314453125
Iteration 600: Loss = -11350.712890625
Iteration 700: Loss = -11279.5947265625
Iteration 800: Loss = -11229.728515625
Iteration 900: Loss = -11191.2548828125
Iteration 1000: Loss = -11158.9794921875
Iteration 1100: Loss = -11136.4755859375
Iteration 1200: Loss = -11117.5859375
Iteration 1300: Loss = -11099.5283203125
Iteration 1400: Loss = -11088.3271484375
Iteration 1500: Loss = -11079.90234375
Iteration 1600: Loss = -11072.7021484375
Iteration 1700: Loss = -11066.087890625
Iteration 1800: Loss = -11060.20703125
Iteration 1900: Loss = -11055.611328125
Iteration 2000: Loss = -11052.083984375
Iteration 2100: Loss = -11049.2119140625
Iteration 2200: Loss = -11046.771484375
Iteration 2300: Loss = -11044.326171875
Iteration 2400: Loss = -11035.8994140625
Iteration 2500: Loss = -11033.4111328125
Iteration 2600: Loss = -11031.8916015625
Iteration 2700: Loss = -11030.5751953125
Iteration 2800: Loss = -11029.384765625
Iteration 2900: Loss = -11028.2255859375
Iteration 3000: Loss = -11026.064453125
Iteration 3100: Loss = -11023.1552734375
Iteration 3200: Loss = -11021.892578125
Iteration 3300: Loss = -11020.93359375
Iteration 3400: Loss = -11017.3984375
Iteration 3500: Loss = -11013.841796875
Iteration 3600: Loss = -11013.130859375
Iteration 3700: Loss = -11012.5546875
Iteration 3800: Loss = -11012.0439453125
Iteration 3900: Loss = -11011.5810546875
Iteration 4000: Loss = -11011.158203125
Iteration 4100: Loss = -11010.7646484375
Iteration 4200: Loss = -11010.4091796875
Iteration 4300: Loss = -11010.0771484375
Iteration 4400: Loss = -11009.76953125
Iteration 4500: Loss = -11009.4775390625
Iteration 4600: Loss = -11009.193359375
Iteration 4700: Loss = -11004.7021484375
Iteration 4800: Loss = -10998.767578125
Iteration 4900: Loss = -10998.0947265625
Iteration 5000: Loss = -10997.7373046875
Iteration 5100: Loss = -10997.4619140625
Iteration 5200: Loss = -10997.22265625
Iteration 5300: Loss = -10997.0107421875
Iteration 5400: Loss = -10996.818359375
Iteration 5500: Loss = -10996.64453125
Iteration 5600: Loss = -10996.4814453125
Iteration 5700: Loss = -10996.330078125
Iteration 5800: Loss = -10996.1904296875
Iteration 5900: Loss = -10996.0595703125
Iteration 6000: Loss = -10995.9365234375
Iteration 6100: Loss = -10995.8232421875
Iteration 6200: Loss = -10995.71484375
Iteration 6300: Loss = -10995.615234375
Iteration 6400: Loss = -10995.521484375
Iteration 6500: Loss = -10995.431640625
Iteration 6600: Loss = -10995.349609375
Iteration 6700: Loss = -10995.271484375
Iteration 6800: Loss = -10995.1953125
Iteration 6900: Loss = -10995.126953125
Iteration 7000: Loss = -10995.060546875
Iteration 7100: Loss = -10994.998046875
Iteration 7200: Loss = -10994.939453125
Iteration 7300: Loss = -10994.8837890625
Iteration 7400: Loss = -10994.83203125
Iteration 7500: Loss = -10994.783203125
Iteration 7600: Loss = -10994.7373046875
Iteration 7700: Loss = -10994.6943359375
Iteration 7800: Loss = -10994.6513671875
Iteration 7900: Loss = -10994.611328125
Iteration 8000: Loss = -10994.5751953125
Iteration 8100: Loss = -10994.5390625
Iteration 8200: Loss = -10994.505859375
Iteration 8300: Loss = -10994.474609375
Iteration 8400: Loss = -10994.4423828125
Iteration 8500: Loss = -10994.416015625
Iteration 8600: Loss = -10994.3876953125
Iteration 8700: Loss = -10994.3642578125
Iteration 8800: Loss = -10994.3388671875
Iteration 8900: Loss = -10994.3154296875
Iteration 9000: Loss = -10994.294921875
Iteration 9100: Loss = -10994.2744140625
Iteration 9200: Loss = -10994.2548828125
Iteration 9300: Loss = -10994.2373046875
Iteration 9400: Loss = -10994.2177734375
Iteration 9500: Loss = -10994.2021484375
Iteration 9600: Loss = -10994.185546875
Iteration 9700: Loss = -10994.171875
Iteration 9800: Loss = -10994.1572265625
Iteration 9900: Loss = -10994.1435546875
Iteration 10000: Loss = -10994.130859375
Iteration 10100: Loss = -10994.119140625
Iteration 10200: Loss = -10994.107421875
Iteration 10300: Loss = -10994.0966796875
Iteration 10400: Loss = -10994.0859375
Iteration 10500: Loss = -10994.076171875
Iteration 10600: Loss = -10994.0693359375
Iteration 10700: Loss = -10994.05859375
Iteration 10800: Loss = -10994.0517578125
Iteration 10900: Loss = -10994.0439453125
Iteration 11000: Loss = -10994.03515625
Iteration 11100: Loss = -10994.0283203125
Iteration 11200: Loss = -10994.021484375
Iteration 11300: Loss = -10994.015625
Iteration 11400: Loss = -10994.0087890625
Iteration 11500: Loss = -10994.001953125
Iteration 11600: Loss = -10993.9970703125
Iteration 11700: Loss = -10993.9921875
Iteration 11800: Loss = -10993.98828125
Iteration 11900: Loss = -10993.9833984375
Iteration 12000: Loss = -10993.9765625
Iteration 12100: Loss = -10993.974609375
Iteration 12200: Loss = -10993.9697265625
Iteration 12300: Loss = -10993.9658203125
Iteration 12400: Loss = -10993.9619140625
Iteration 12500: Loss = -10993.958984375
Iteration 12600: Loss = -10993.958984375
Iteration 12700: Loss = -10993.953125
Iteration 12800: Loss = -10993.9501953125
Iteration 12900: Loss = -10993.947265625
Iteration 13000: Loss = -10993.9453125
Iteration 13100: Loss = -10993.94140625
Iteration 13200: Loss = -10993.9404296875
Iteration 13300: Loss = -10993.9375
Iteration 13400: Loss = -10993.935546875
Iteration 13500: Loss = -10993.9326171875
Iteration 13600: Loss = -10993.931640625
Iteration 13700: Loss = -10993.9296875
Iteration 13800: Loss = -10993.927734375
Iteration 13900: Loss = -10993.92578125
Iteration 14000: Loss = -10993.9248046875
Iteration 14100: Loss = -10993.9228515625
Iteration 14200: Loss = -10993.921875
Iteration 14300: Loss = -10993.9208984375
Iteration 14400: Loss = -10993.91796875
Iteration 14500: Loss = -10993.91796875
Iteration 14600: Loss = -10993.9169921875
Iteration 14700: Loss = -10993.9150390625
Iteration 14800: Loss = -10993.9150390625
Iteration 14900: Loss = -10993.9140625
Iteration 15000: Loss = -10993.9130859375
Iteration 15100: Loss = -10993.9130859375
Iteration 15200: Loss = -10993.9111328125
Iteration 15300: Loss = -10993.9111328125
Iteration 15400: Loss = -10993.91015625
Iteration 15500: Loss = -10993.9091796875
Iteration 15600: Loss = -10993.908203125
Iteration 15700: Loss = -10993.9091796875
1
Iteration 15800: Loss = -10993.9072265625
Iteration 15900: Loss = -10993.9072265625
Iteration 16000: Loss = -10993.9052734375
Iteration 16100: Loss = -10993.90625
1
Iteration 16200: Loss = -10993.904296875
Iteration 16300: Loss = -10993.9052734375
1
Iteration 16400: Loss = -10993.90625
2
Iteration 16500: Loss = -10993.9033203125
Iteration 16600: Loss = -10993.904296875
1
Iteration 16700: Loss = -10993.9033203125
Iteration 16800: Loss = -10993.9033203125
Iteration 16900: Loss = -10993.9033203125
Iteration 17000: Loss = -10993.900390625
Iteration 17100: Loss = -10993.90234375
1
Iteration 17200: Loss = -10993.90234375
2
Iteration 17300: Loss = -10993.9013671875
3
Iteration 17400: Loss = -10993.900390625
Iteration 17500: Loss = -10993.900390625
Iteration 17600: Loss = -10993.900390625
Iteration 17700: Loss = -10993.8994140625
Iteration 17800: Loss = -10993.8994140625
Iteration 17900: Loss = -10993.900390625
1
Iteration 18000: Loss = -10993.900390625
2
Iteration 18100: Loss = -10993.8994140625
Iteration 18200: Loss = -10993.8984375
Iteration 18300: Loss = -10993.8984375
Iteration 18400: Loss = -10993.8994140625
1
Iteration 18500: Loss = -10993.8994140625
2
Iteration 18600: Loss = -10993.8994140625
3
Iteration 18700: Loss = -10993.896484375
Iteration 18800: Loss = -10993.8974609375
1
Iteration 18900: Loss = -10993.896484375
Iteration 19000: Loss = -10993.896484375
Iteration 19100: Loss = -10993.8984375
1
Iteration 19200: Loss = -10993.896484375
Iteration 19300: Loss = -10993.8974609375
1
Iteration 19400: Loss = -10993.8974609375
2
Iteration 19500: Loss = -10993.8974609375
3
Iteration 19600: Loss = -10993.8984375
4
Iteration 19700: Loss = -10993.8984375
5
Iteration 19800: Loss = -10993.8984375
6
Iteration 19900: Loss = -10993.8974609375
7
Iteration 20000: Loss = -10993.8974609375
8
Iteration 20100: Loss = -10993.896484375
Iteration 20200: Loss = -10993.8974609375
1
Iteration 20300: Loss = -10993.896484375
Iteration 20400: Loss = -10993.8974609375
1
Iteration 20500: Loss = -10993.8974609375
2
Iteration 20600: Loss = -10993.8974609375
3
Iteration 20700: Loss = -10993.8984375
4
Iteration 20800: Loss = -10993.896484375
Iteration 20900: Loss = -10993.8974609375
1
Iteration 21000: Loss = -10993.8955078125
Iteration 21100: Loss = -10993.8955078125
Iteration 21200: Loss = -10993.896484375
1
Iteration 21300: Loss = -10993.896484375
2
Iteration 21400: Loss = -10993.7587890625
Iteration 21500: Loss = -10993.369140625
Iteration 21600: Loss = -10993.359375
Iteration 21700: Loss = -10993.333984375
Iteration 21800: Loss = -10993.328125
Iteration 21900: Loss = -10993.3251953125
Iteration 22000: Loss = -10993.3251953125
Iteration 22100: Loss = -10993.3212890625
Iteration 22200: Loss = -10993.322265625
1
Iteration 22300: Loss = -10993.3193359375
Iteration 22400: Loss = -10993.3193359375
Iteration 22500: Loss = -10993.318359375
Iteration 22600: Loss = -10993.3173828125
Iteration 22700: Loss = -10993.31640625
Iteration 22800: Loss = -10993.3173828125
1
Iteration 22900: Loss = -10993.31640625
Iteration 23000: Loss = -10993.3154296875
Iteration 23100: Loss = -10993.31640625
1
Iteration 23200: Loss = -10993.3173828125
2
Iteration 23300: Loss = -10993.3154296875
Iteration 23400: Loss = -10993.3173828125
1
Iteration 23500: Loss = -10993.31640625
2
Iteration 23600: Loss = -10993.3173828125
3
Iteration 23700: Loss = -10993.3154296875
Iteration 23800: Loss = -10993.314453125
Iteration 23900: Loss = -10993.3173828125
1
Iteration 24000: Loss = -10993.31640625
2
Iteration 24100: Loss = -10993.3173828125
3
Iteration 24200: Loss = -10993.318359375
4
Iteration 24300: Loss = -10993.31640625
5
Iteration 24400: Loss = -10993.318359375
6
Iteration 24500: Loss = -10993.31640625
7
Iteration 24600: Loss = -10993.31640625
8
Iteration 24700: Loss = -10991.630859375
Iteration 24800: Loss = -10991.572265625
Iteration 24900: Loss = -10991.5634765625
Iteration 25000: Loss = -10991.5595703125
Iteration 25100: Loss = -10991.556640625
Iteration 25200: Loss = -10991.5556640625
Iteration 25300: Loss = -10991.55859375
1
Iteration 25400: Loss = -10991.5546875
Iteration 25500: Loss = -10991.5556640625
1
Iteration 25600: Loss = -10991.552734375
Iteration 25700: Loss = -10991.552734375
Iteration 25800: Loss = -10991.5517578125
Iteration 25900: Loss = -10991.552734375
1
Iteration 26000: Loss = -10991.5517578125
Iteration 26100: Loss = -10991.552734375
1
Iteration 26200: Loss = -10991.5517578125
Iteration 26300: Loss = -10991.5517578125
Iteration 26400: Loss = -10991.552734375
1
Iteration 26500: Loss = -10991.5625
2
Iteration 26600: Loss = -10991.55078125
Iteration 26700: Loss = -10991.552734375
1
Iteration 26800: Loss = -10991.552734375
2
Iteration 26900: Loss = -10991.5517578125
3
Iteration 27000: Loss = -10991.5517578125
4
Iteration 27100: Loss = -10991.55078125
Iteration 27200: Loss = -10991.55078125
Iteration 27300: Loss = -10991.5517578125
1
Iteration 27400: Loss = -10991.5498046875
Iteration 27500: Loss = -10991.5498046875
Iteration 27600: Loss = -10991.5517578125
1
Iteration 27700: Loss = -10991.5498046875
Iteration 27800: Loss = -10991.5498046875
Iteration 27900: Loss = -10991.5498046875
Iteration 28000: Loss = -10991.55078125
1
Iteration 28100: Loss = -10991.55078125
2
Iteration 28200: Loss = -10991.5517578125
3
Iteration 28300: Loss = -10991.5498046875
Iteration 28400: Loss = -10991.5517578125
1
Iteration 28500: Loss = -10991.5498046875
Iteration 28600: Loss = -10991.5517578125
1
Iteration 28700: Loss = -10991.5498046875
Iteration 28800: Loss = -10991.5498046875
Iteration 28900: Loss = -10991.5498046875
Iteration 29000: Loss = -10991.5498046875
Iteration 29100: Loss = -10991.5517578125
1
Iteration 29200: Loss = -10991.55078125
2
Iteration 29300: Loss = -10991.5498046875
Iteration 29400: Loss = -10991.5458984375
Iteration 29500: Loss = -10991.5244140625
Iteration 29600: Loss = -10991.521484375
Iteration 29700: Loss = -10991.5234375
1
Iteration 29800: Loss = -10991.5234375
2
Iteration 29900: Loss = -10991.521484375
pi: tensor([[1.0000e+00, 1.4856e-07],
        [9.6912e-05, 9.9990e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1616, 0.2808],
         [0.0094, 0.0017]],

        [[0.1389, 0.1914],
         [0.0235, 0.8257]],

        [[0.3434, 0.2517],
         [0.9927, 0.3499]],

        [[0.0375, 0.1810],
         [0.6281, 0.0140]],

        [[0.0707, 0.1905],
         [0.1736, 0.2812]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: 0.0014729757840434622
Average Adjusted Rand Index: 0.0012057210812004612
[-0.001550223838615892, 0.0014729757840434622] [0.0, 0.0012057210812004612] [10993.583984375, 10991.5224609375]
-------------------------------------
This iteration is 4
True Objective function: Loss = -10927.656958504685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40178.140625
Iteration 100: Loss = -21497.9375
Iteration 200: Loss = -13190.083984375
Iteration 300: Loss = -11598.7861328125
Iteration 400: Loss = -11331.5869140625
Iteration 500: Loss = -11218.578125
Iteration 600: Loss = -11162.7529296875
Iteration 700: Loss = -11128.7607421875
Iteration 800: Loss = -11109.9365234375
Iteration 900: Loss = -11096.7626953125
Iteration 1000: Loss = -11087.0126953125
Iteration 1100: Loss = -11079.5322265625
Iteration 1200: Loss = -11073.6435546875
Iteration 1300: Loss = -11068.90625
Iteration 1400: Loss = -11065.033203125
Iteration 1500: Loss = -11061.818359375
Iteration 1600: Loss = -11059.119140625
Iteration 1700: Loss = -11056.8291015625
Iteration 1800: Loss = -11054.8662109375
Iteration 1900: Loss = -11053.1708984375
Iteration 2000: Loss = -11051.6962890625
Iteration 2100: Loss = -11050.40625
Iteration 2200: Loss = -11049.2705078125
Iteration 2300: Loss = -11048.265625
Iteration 2400: Loss = -11047.3701171875
Iteration 2500: Loss = -11046.5732421875
Iteration 2600: Loss = -11045.859375
Iteration 2700: Loss = -11045.2138671875
Iteration 2800: Loss = -11044.6328125
Iteration 2900: Loss = -11044.1083984375
Iteration 3000: Loss = -11043.630859375
Iteration 3100: Loss = -11043.197265625
Iteration 3200: Loss = -11042.8017578125
Iteration 3300: Loss = -11042.44140625
Iteration 3400: Loss = -11042.111328125
Iteration 3500: Loss = -11041.8076171875
Iteration 3600: Loss = -11041.5283203125
Iteration 3700: Loss = -11041.2724609375
Iteration 3800: Loss = -11041.0361328125
Iteration 3900: Loss = -11040.8193359375
Iteration 4000: Loss = -11040.6181640625
Iteration 4100: Loss = -11040.431640625
Iteration 4200: Loss = -11040.2578125
Iteration 4300: Loss = -11040.09765625
Iteration 4400: Loss = -11039.9453125
Iteration 4500: Loss = -11039.8017578125
Iteration 4600: Loss = -11039.6591796875
Iteration 4700: Loss = -11039.5078125
Iteration 4800: Loss = -11039.373046875
Iteration 4900: Loss = -11039.267578125
Iteration 5000: Loss = -11039.1796875
Iteration 5100: Loss = -11039.0888671875
Iteration 5200: Loss = -11039.0185546875
Iteration 5300: Loss = -11038.955078125
Iteration 5400: Loss = -11038.8974609375
Iteration 5500: Loss = -11038.84375
Iteration 5600: Loss = -11038.7939453125
Iteration 5700: Loss = -11038.7470703125
Iteration 5800: Loss = -11038.7021484375
Iteration 5900: Loss = -11038.66015625
Iteration 6000: Loss = -11038.6220703125
Iteration 6100: Loss = -11038.5849609375
Iteration 6200: Loss = -11038.552734375
Iteration 6300: Loss = -11038.521484375
Iteration 6400: Loss = -11038.4892578125
Iteration 6500: Loss = -11038.4609375
Iteration 6600: Loss = -11038.43359375
Iteration 6700: Loss = -11038.4072265625
Iteration 6800: Loss = -11038.3818359375
Iteration 6900: Loss = -11038.357421875
Iteration 7000: Loss = -11038.333984375
Iteration 7100: Loss = -11038.306640625
Iteration 7200: Loss = -11038.28125
Iteration 7300: Loss = -11038.2509765625
Iteration 7400: Loss = -11038.2158203125
Iteration 7500: Loss = -11038.17578125
Iteration 7600: Loss = -11038.1240234375
Iteration 7700: Loss = -11038.05859375
Iteration 7800: Loss = -11037.990234375
Iteration 7900: Loss = -11037.9248046875
Iteration 8000: Loss = -11037.873046875
Iteration 8100: Loss = -11037.83203125
Iteration 8200: Loss = -11037.7939453125
Iteration 8300: Loss = -11037.7578125
Iteration 8400: Loss = -11037.7236328125
Iteration 8500: Loss = -11037.689453125
Iteration 8600: Loss = -11037.6591796875
Iteration 8700: Loss = -11037.6328125
Iteration 8800: Loss = -11037.607421875
Iteration 8900: Loss = -11037.5849609375
Iteration 9000: Loss = -11037.5625
Iteration 9100: Loss = -11037.5390625
Iteration 9200: Loss = -11037.517578125
Iteration 9300: Loss = -11037.4970703125
Iteration 9400: Loss = -11037.4736328125
Iteration 9500: Loss = -11037.455078125
Iteration 9600: Loss = -11037.431640625
Iteration 9700: Loss = -11037.4033203125
Iteration 9800: Loss = -11037.345703125
Iteration 9900: Loss = -11036.7802734375
Iteration 10000: Loss = -11036.2451171875
Iteration 10100: Loss = -11036.083984375
Iteration 10200: Loss = -11035.9765625
Iteration 10300: Loss = -11035.8935546875
Iteration 10400: Loss = -11035.828125
Iteration 10500: Loss = -11035.77734375
Iteration 10600: Loss = -11035.7294921875
Iteration 10700: Loss = -11035.693359375
Iteration 10800: Loss = -11035.662109375
Iteration 10900: Loss = -11035.634765625
Iteration 11000: Loss = -11035.611328125
Iteration 11100: Loss = -11035.587890625
Iteration 11200: Loss = -11035.5693359375
Iteration 11300: Loss = -11035.55078125
Iteration 11400: Loss = -11035.53515625
Iteration 11500: Loss = -11035.51953125
Iteration 11600: Loss = -11035.5078125
Iteration 11700: Loss = -11035.4951171875
Iteration 11800: Loss = -11035.484375
Iteration 11900: Loss = -11035.47265625
Iteration 12000: Loss = -11035.4619140625
Iteration 12100: Loss = -11035.4521484375
Iteration 12200: Loss = -11035.4443359375
Iteration 12300: Loss = -11035.43359375
Iteration 12400: Loss = -11035.423828125
Iteration 12500: Loss = -11035.412109375
Iteration 12600: Loss = -11035.4013671875
Iteration 12700: Loss = -11035.3896484375
Iteration 12800: Loss = -11035.373046875
Iteration 12900: Loss = -11035.35546875
Iteration 13000: Loss = -11035.33203125
Iteration 13100: Loss = -11035.2939453125
Iteration 13200: Loss = -11035.09765625
Iteration 13300: Loss = -11034.83984375
Iteration 13400: Loss = -11034.814453125
Iteration 13500: Loss = -11034.78125
Iteration 13600: Loss = -11034.7470703125
Iteration 13700: Loss = -11034.732421875
Iteration 13800: Loss = -11034.72265625
Iteration 13900: Loss = -11034.7099609375
Iteration 14000: Loss = -11034.697265625
Iteration 14100: Loss = -11034.6796875
Iteration 14200: Loss = -11034.65625
Iteration 14300: Loss = -11033.072265625
Iteration 14400: Loss = -11032.6259765625
Iteration 14500: Loss = -11032.55859375
Iteration 14600: Loss = -11032.447265625
Iteration 14700: Loss = -11031.8837890625
Iteration 14800: Loss = -11031.236328125
Iteration 14900: Loss = -11016.2890625
Iteration 15000: Loss = -11005.6240234375
Iteration 15100: Loss = -11001.732421875
Iteration 15200: Loss = -11001.4560546875
Iteration 15300: Loss = -10988.9326171875
Iteration 15400: Loss = -10982.3349609375
Iteration 15500: Loss = -10969.80078125
Iteration 15600: Loss = -10943.35546875
Iteration 15700: Loss = -10922.8330078125
Iteration 15800: Loss = -10905.984375
Iteration 15900: Loss = -10904.8447265625
Iteration 16000: Loss = -10904.640625
Iteration 16100: Loss = -10903.9990234375
Iteration 16200: Loss = -10903.9462890625
Iteration 16300: Loss = -10903.6572265625
Iteration 16400: Loss = -10903.375
Iteration 16500: Loss = -10903.3564453125
Iteration 16600: Loss = -10903.3408203125
Iteration 16700: Loss = -10903.3291015625
Iteration 16800: Loss = -10903.31640625
Iteration 16900: Loss = -10903.30859375
Iteration 17000: Loss = -10903.302734375
Iteration 17100: Loss = -10903.296875
Iteration 17200: Loss = -10903.2822265625
Iteration 17300: Loss = -10903.1884765625
Iteration 17400: Loss = -10903.1845703125
Iteration 17500: Loss = -10903.181640625
Iteration 17600: Loss = -10903.177734375
Iteration 17700: Loss = -10903.1748046875
Iteration 17800: Loss = -10898.859375
Iteration 17900: Loss = -10898.8095703125
Iteration 18000: Loss = -10898.8017578125
Iteration 18100: Loss = -10898.7978515625
Iteration 18200: Loss = -10898.794921875
Iteration 18300: Loss = -10898.7919921875
Iteration 18400: Loss = -10898.791015625
Iteration 18500: Loss = -10898.7900390625
Iteration 18600: Loss = -10898.7880859375
Iteration 18700: Loss = -10898.775390625
Iteration 18800: Loss = -10898.7724609375
Iteration 18900: Loss = -10898.7724609375
Iteration 19000: Loss = -10898.7705078125
Iteration 19100: Loss = -10898.771484375
1
Iteration 19200: Loss = -10898.767578125
Iteration 19300: Loss = -10898.7666015625
Iteration 19400: Loss = -10898.767578125
1
Iteration 19500: Loss = -10898.7666015625
Iteration 19600: Loss = -10898.765625
Iteration 19700: Loss = -10898.7646484375
Iteration 19800: Loss = -10898.763671875
Iteration 19900: Loss = -10898.763671875
Iteration 20000: Loss = -10898.7626953125
Iteration 20100: Loss = -10898.7626953125
Iteration 20200: Loss = -10898.76171875
Iteration 20300: Loss = -10898.7607421875
Iteration 20400: Loss = -10898.751953125
Iteration 20500: Loss = -10898.751953125
Iteration 20600: Loss = -10898.7490234375
Iteration 20700: Loss = -10898.7490234375
Iteration 20800: Loss = -10898.7490234375
Iteration 20900: Loss = -10898.701171875
Iteration 21000: Loss = -10898.697265625
Iteration 21100: Loss = -10896.533203125
Iteration 21200: Loss = -10896.5126953125
Iteration 21300: Loss = -10896.236328125
Iteration 21400: Loss = -10896.234375
Iteration 21500: Loss = -10896.234375
Iteration 21600: Loss = -10896.234375
Iteration 21700: Loss = -10896.2333984375
Iteration 21800: Loss = -10896.220703125
Iteration 21900: Loss = -10896.216796875
Iteration 22000: Loss = -10896.2177734375
1
Iteration 22100: Loss = -10895.6083984375
Iteration 22200: Loss = -10895.6044921875
Iteration 22300: Loss = -10895.6044921875
Iteration 22400: Loss = -10895.6025390625
Iteration 22500: Loss = -10895.572265625
Iteration 22600: Loss = -10895.5712890625
Iteration 22700: Loss = -10895.572265625
1
Iteration 22800: Loss = -10895.5703125
Iteration 22900: Loss = -10895.5693359375
Iteration 23000: Loss = -10895.4697265625
Iteration 23100: Loss = -10892.9169921875
Iteration 23200: Loss = -10892.9130859375
Iteration 23300: Loss = -10892.9130859375
Iteration 23400: Loss = -10892.9130859375
Iteration 23500: Loss = -10892.9130859375
Iteration 23600: Loss = -10892.7802734375
Iteration 23700: Loss = -10892.77734375
Iteration 23800: Loss = -10892.77734375
Iteration 23900: Loss = -10892.77734375
Iteration 24000: Loss = -10892.7763671875
Iteration 24100: Loss = -10892.77734375
1
Iteration 24200: Loss = -10892.7763671875
Iteration 24300: Loss = -10892.7763671875
Iteration 24400: Loss = -10892.77734375
1
Iteration 24500: Loss = -10892.7763671875
Iteration 24600: Loss = -10892.7763671875
Iteration 24700: Loss = -10892.77734375
1
Iteration 24800: Loss = -10892.767578125
Iteration 24900: Loss = -10892.765625
Iteration 25000: Loss = -10892.7666015625
1
Iteration 25100: Loss = -10892.7666015625
2
Iteration 25200: Loss = -10892.765625
Iteration 25300: Loss = -10892.763671875
Iteration 25400: Loss = -10892.7646484375
1
Iteration 25500: Loss = -10892.7646484375
2
Iteration 25600: Loss = -10891.7431640625
Iteration 25700: Loss = -10891.728515625
Iteration 25800: Loss = -10891.728515625
Iteration 25900: Loss = -10891.7294921875
1
Iteration 26000: Loss = -10891.7294921875
2
Iteration 26100: Loss = -10891.728515625
Iteration 26200: Loss = -10891.7294921875
1
Iteration 26300: Loss = -10891.669921875
Iteration 26400: Loss = -10891.646484375
Iteration 26500: Loss = -10891.6455078125
Iteration 26600: Loss = -10891.6455078125
Iteration 26700: Loss = -10891.6455078125
Iteration 26800: Loss = -10891.6455078125
Iteration 26900: Loss = -10891.6455078125
Iteration 27000: Loss = -10891.6455078125
Iteration 27100: Loss = -10891.646484375
1
Iteration 27200: Loss = -10891.6474609375
2
Iteration 27300: Loss = -10891.646484375
3
Iteration 27400: Loss = -10891.646484375
4
Iteration 27500: Loss = -10891.642578125
Iteration 27600: Loss = -10891.642578125
Iteration 27700: Loss = -10891.642578125
Iteration 27800: Loss = -10891.642578125
Iteration 27900: Loss = -10891.6416015625
Iteration 28000: Loss = -10891.642578125
1
Iteration 28100: Loss = -10891.6416015625
Iteration 28200: Loss = -10891.64453125
1
Iteration 28300: Loss = -10891.6416015625
Iteration 28400: Loss = -10891.6279296875
Iteration 28500: Loss = -10891.580078125
Iteration 28600: Loss = -10891.5810546875
1
Iteration 28700: Loss = -10891.5810546875
2
Iteration 28800: Loss = -10891.58203125
3
Iteration 28900: Loss = -10891.58203125
4
Iteration 29000: Loss = -10891.580078125
Iteration 29100: Loss = -10891.5791015625
Iteration 29200: Loss = -10891.580078125
1
Iteration 29300: Loss = -10891.580078125
2
Iteration 29400: Loss = -10891.580078125
3
Iteration 29500: Loss = -10891.580078125
4
Iteration 29600: Loss = -10891.5791015625
Iteration 29700: Loss = -10891.5263671875
Iteration 29800: Loss = -10891.236328125
Iteration 29900: Loss = -10891.2353515625
pi: tensor([[0.7840, 0.2160],
        [0.2199, 0.7801]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5788, 0.4212], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2409, 0.0957],
         [0.0867, 0.2095]],

        [[0.9931, 0.1013],
         [0.0079, 0.0335]],

        [[0.3614, 0.1062],
         [0.9020, 0.0146]],

        [[0.3703, 0.1001],
         [0.9638, 0.9012]],

        [[0.1198, 0.0937],
         [0.7170, 0.6947]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369913366172994
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369913366172994
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.809619625046958
Average Adjusted Rand Index: 0.8098628950664546
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30405.033203125
Iteration 100: Loss = -19751.66015625
Iteration 200: Loss = -12981.412109375
Iteration 300: Loss = -11711.78515625
Iteration 400: Loss = -11452.3056640625
Iteration 500: Loss = -11335.29296875
Iteration 600: Loss = -11271.6455078125
Iteration 700: Loss = -11233.3681640625
Iteration 800: Loss = -11204.3779296875
Iteration 900: Loss = -11185.50390625
Iteration 1000: Loss = -11173.3671875
Iteration 1100: Loss = -11164.8232421875
Iteration 1200: Loss = -11158.9091796875
Iteration 1300: Loss = -11153.9130859375
Iteration 1400: Loss = -11148.8505859375
Iteration 1500: Loss = -11144.8466796875
Iteration 1600: Loss = -11140.05078125
Iteration 1700: Loss = -11132.548828125
Iteration 1800: Loss = -11129.04296875
Iteration 1900: Loss = -11127.326171875
Iteration 2000: Loss = -11125.548828125
Iteration 2100: Loss = -11122.89453125
Iteration 2200: Loss = -11119.5703125
Iteration 2300: Loss = -11116.1796875
Iteration 2400: Loss = -11114.2626953125
Iteration 2500: Loss = -11112.650390625
Iteration 2600: Loss = -11109.6279296875
Iteration 2700: Loss = -11108.0302734375
Iteration 2800: Loss = -11106.1689453125
Iteration 2900: Loss = -11103.7578125
Iteration 3000: Loss = -11101.7861328125
Iteration 3100: Loss = -11100.7509765625
Iteration 3200: Loss = -11099.9453125
Iteration 3300: Loss = -11099.0009765625
Iteration 3400: Loss = -11093.0361328125
Iteration 3500: Loss = -11090.0576171875
Iteration 3600: Loss = -11086.2861328125
Iteration 3700: Loss = -11082.666015625
Iteration 3800: Loss = -11077.5234375
Iteration 3900: Loss = -11073.443359375
Iteration 4000: Loss = -11072.849609375
Iteration 4100: Loss = -11072.5048828125
Iteration 4200: Loss = -11072.234375
Iteration 4300: Loss = -11072.0078125
Iteration 4400: Loss = -11071.80078125
Iteration 4500: Loss = -11071.591796875
Iteration 4600: Loss = -11071.2099609375
Iteration 4700: Loss = -11065.048828125
Iteration 4800: Loss = -11060.724609375
Iteration 4900: Loss = -11059.66796875
Iteration 5000: Loss = -11058.9326171875
Iteration 5100: Loss = -11058.4599609375
Iteration 5200: Loss = -11058.1953125
Iteration 5300: Loss = -11058.0419921875
Iteration 5400: Loss = -11057.9443359375
Iteration 5500: Loss = -11057.87109375
Iteration 5600: Loss = -11057.7841796875
Iteration 5700: Loss = -11057.6982421875
Iteration 5800: Loss = -11057.625
Iteration 5900: Loss = -11054.8369140625
Iteration 6000: Loss = -11054.595703125
Iteration 6100: Loss = -11054.5166015625
Iteration 6200: Loss = -11054.4619140625
Iteration 6300: Loss = -11054.4150390625
Iteration 6400: Loss = -11054.3623046875
Iteration 6500: Loss = -11054.2666015625
Iteration 6600: Loss = -11054.201171875
Iteration 6700: Loss = -11054.140625
Iteration 6800: Loss = -11054.0224609375
Iteration 6900: Loss = -11053.9248046875
Iteration 7000: Loss = -11053.8828125
Iteration 7100: Loss = -11053.8310546875
Iteration 7200: Loss = -11053.7705078125
Iteration 7300: Loss = -11051.2529296875
Iteration 7400: Loss = -11050.53515625
Iteration 7500: Loss = -11050.4462890625
Iteration 7600: Loss = -11050.384765625
Iteration 7700: Loss = -11050.3466796875
Iteration 7800: Loss = -11050.3125
Iteration 7900: Loss = -11050.2802734375
Iteration 8000: Loss = -11050.251953125
Iteration 8100: Loss = -11050.2275390625
Iteration 8200: Loss = -11050.2041015625
Iteration 8300: Loss = -11050.1787109375
Iteration 8400: Loss = -11050.1513671875
Iteration 8500: Loss = -11050.11328125
Iteration 8600: Loss = -11046.85546875
Iteration 8700: Loss = -11046.763671875
Iteration 8800: Loss = -11046.736328125
Iteration 8900: Loss = -11046.712890625
Iteration 9000: Loss = -11046.6962890625
Iteration 9100: Loss = -11046.6796875
Iteration 9200: Loss = -11046.666015625
Iteration 9300: Loss = -11046.6533203125
Iteration 9400: Loss = -11046.6455078125
Iteration 9500: Loss = -11046.63671875
Iteration 9600: Loss = -11046.62890625
Iteration 9700: Loss = -11046.6220703125
Iteration 9800: Loss = -11046.6142578125
Iteration 9900: Loss = -11046.609375
Iteration 10000: Loss = -11046.6025390625
Iteration 10100: Loss = -11046.5986328125
Iteration 10200: Loss = -11046.421875
Iteration 10300: Loss = -11041.5908203125
Iteration 10400: Loss = -11041.498046875
Iteration 10500: Loss = -11041.4638671875
Iteration 10600: Loss = -11041.4462890625
Iteration 10700: Loss = -11041.435546875
Iteration 10800: Loss = -11041.427734375
Iteration 10900: Loss = -11041.421875
Iteration 11000: Loss = -11041.416015625
Iteration 11100: Loss = -11041.4111328125
Iteration 11200: Loss = -11041.4072265625
Iteration 11300: Loss = -11041.4033203125
Iteration 11400: Loss = -11041.40234375
Iteration 11500: Loss = -11041.3984375
Iteration 11600: Loss = -11041.3955078125
Iteration 11700: Loss = -11041.39453125
Iteration 11800: Loss = -11041.13671875
Iteration 11900: Loss = -11037.6240234375
Iteration 12000: Loss = -11037.568359375
Iteration 12100: Loss = -11037.5283203125
Iteration 12200: Loss = -11037.4677734375
Iteration 12300: Loss = -11037.353515625
Iteration 12400: Loss = -11037.12890625
Iteration 12500: Loss = -11037.0615234375
Iteration 12600: Loss = -11037.0546875
Iteration 12700: Loss = -11037.0498046875
Iteration 12800: Loss = -11037.0458984375
Iteration 12900: Loss = -11037.044921875
Iteration 13000: Loss = -11037.0439453125
Iteration 13100: Loss = -11037.04296875
Iteration 13200: Loss = -11037.0419921875
Iteration 13300: Loss = -11037.0400390625
Iteration 13400: Loss = -11037.0400390625
Iteration 13500: Loss = -11037.037109375
Iteration 13600: Loss = -11037.0322265625
Iteration 13700: Loss = -11036.814453125
Iteration 13800: Loss = -11036.8037109375
Iteration 13900: Loss = -11036.80078125
Iteration 14000: Loss = -11036.7978515625
Iteration 14100: Loss = -11036.7978515625
Iteration 14200: Loss = -11036.7978515625
Iteration 14300: Loss = -11036.7978515625
Iteration 14400: Loss = -11036.7958984375
Iteration 14500: Loss = -11036.794921875
Iteration 14600: Loss = -11036.7890625
Iteration 14700: Loss = -11036.7890625
Iteration 14800: Loss = -11036.7890625
Iteration 14900: Loss = -11036.7880859375
Iteration 15000: Loss = -11036.7841796875
Iteration 15100: Loss = -11036.76171875
Iteration 15200: Loss = -11035.0498046875
Iteration 15300: Loss = -11035.0302734375
Iteration 15400: Loss = -11035.025390625
Iteration 15500: Loss = -11035.0244140625
Iteration 15600: Loss = -11035.0234375
Iteration 15700: Loss = -11035.0224609375
Iteration 15800: Loss = -11035.0234375
1
Iteration 15900: Loss = -11035.021484375
Iteration 16000: Loss = -11035.0205078125
Iteration 16100: Loss = -11035.0205078125
Iteration 16200: Loss = -11035.01953125
Iteration 16300: Loss = -11035.01953125
Iteration 16400: Loss = -11035.0205078125
1
Iteration 16500: Loss = -11035.01953125
Iteration 16600: Loss = -11035.01953125
Iteration 16700: Loss = -11035.0185546875
Iteration 16800: Loss = -11035.01953125
1
Iteration 16900: Loss = -11035.0185546875
Iteration 17000: Loss = -11035.0185546875
Iteration 17100: Loss = -11035.017578125
Iteration 17200: Loss = -11035.017578125
Iteration 17300: Loss = -11035.017578125
Iteration 17400: Loss = -11035.017578125
Iteration 17500: Loss = -11035.0185546875
1
Iteration 17600: Loss = -11035.0224609375
2
Iteration 17700: Loss = -11035.0185546875
3
Iteration 17800: Loss = -11035.017578125
Iteration 17900: Loss = -11035.017578125
Iteration 18000: Loss = -11035.0185546875
1
Iteration 18100: Loss = -11035.0166015625
Iteration 18200: Loss = -11035.0185546875
1
Iteration 18300: Loss = -11035.0166015625
Iteration 18400: Loss = -11035.017578125
1
Iteration 18500: Loss = -11035.0166015625
Iteration 18600: Loss = -11035.0166015625
Iteration 18700: Loss = -11035.017578125
1
Iteration 18800: Loss = -11035.017578125
2
Iteration 18900: Loss = -11035.017578125
3
Iteration 19000: Loss = -11035.0166015625
Iteration 19100: Loss = -11035.0166015625
Iteration 19200: Loss = -11035.0185546875
1
Iteration 19300: Loss = -11035.017578125
2
Iteration 19400: Loss = -11035.017578125
3
Iteration 19500: Loss = -11035.017578125
4
Iteration 19600: Loss = -11035.015625
Iteration 19700: Loss = -11035.017578125
1
Iteration 19800: Loss = -11035.017578125
2
Iteration 19900: Loss = -11035.015625
Iteration 20000: Loss = -11035.017578125
1
Iteration 20100: Loss = -11035.015625
Iteration 20200: Loss = -11035.015625
Iteration 20300: Loss = -11035.015625
Iteration 20400: Loss = -11035.01953125
1
Iteration 20500: Loss = -11035.015625
Iteration 20600: Loss = -11035.017578125
1
Iteration 20700: Loss = -11035.0166015625
2
Iteration 20800: Loss = -11035.0166015625
3
Iteration 20900: Loss = -11035.0166015625
4
Iteration 21000: Loss = -11035.0166015625
5
Iteration 21100: Loss = -11035.017578125
6
Iteration 21200: Loss = -11035.0146484375
Iteration 21300: Loss = -11035.015625
1
Iteration 21400: Loss = -11035.0166015625
2
Iteration 21500: Loss = -11035.015625
3
Iteration 21600: Loss = -11035.017578125
4
Iteration 21700: Loss = -11035.015625
5
Iteration 21800: Loss = -11035.0166015625
6
Iteration 21900: Loss = -11035.0166015625
7
Iteration 22000: Loss = -11035.015625
8
Iteration 22100: Loss = -11035.0166015625
9
Iteration 22200: Loss = -11035.0166015625
10
Iteration 22300: Loss = -11035.015625
11
Iteration 22400: Loss = -11035.0166015625
12
Iteration 22500: Loss = -11035.015625
13
Iteration 22600: Loss = -11035.0166015625
14
Iteration 22700: Loss = -11035.0146484375
Iteration 22800: Loss = -11035.015625
1
Iteration 22900: Loss = -11035.017578125
2
Iteration 23000: Loss = -11035.015625
3
Iteration 23100: Loss = -11035.0146484375
Iteration 23200: Loss = -11035.013671875
Iteration 23300: Loss = -11035.013671875
Iteration 23400: Loss = -11035.013671875
Iteration 23500: Loss = -11035.013671875
Iteration 23600: Loss = -11035.0146484375
1
Iteration 23700: Loss = -11035.013671875
Iteration 23800: Loss = -11035.013671875
Iteration 23900: Loss = -11035.0166015625
1
Iteration 24000: Loss = -11035.0146484375
2
Iteration 24100: Loss = -11035.013671875
Iteration 24200: Loss = -11035.013671875
Iteration 24300: Loss = -11035.013671875
Iteration 24400: Loss = -11035.013671875
Iteration 24500: Loss = -11035.013671875
Iteration 24600: Loss = -11035.013671875
Iteration 24700: Loss = -11035.013671875
Iteration 24800: Loss = -11035.013671875
Iteration 24900: Loss = -11035.013671875
Iteration 25000: Loss = -11035.013671875
Iteration 25100: Loss = -11035.013671875
Iteration 25200: Loss = -11035.013671875
Iteration 25300: Loss = -11035.013671875
Iteration 25400: Loss = -11035.013671875
Iteration 25500: Loss = -11035.0146484375
1
Iteration 25600: Loss = -11035.013671875
Iteration 25700: Loss = -11035.013671875
Iteration 25800: Loss = -11035.013671875
Iteration 25900: Loss = -11035.0146484375
1
Iteration 26000: Loss = -11035.013671875
Iteration 26100: Loss = -11035.013671875
Iteration 26200: Loss = -11035.013671875
Iteration 26300: Loss = -11035.01171875
Iteration 26400: Loss = -11035.0146484375
1
Iteration 26500: Loss = -11035.013671875
2
Iteration 26600: Loss = -11035.013671875
3
Iteration 26700: Loss = -11035.0126953125
4
Iteration 26800: Loss = -11035.0146484375
5
Iteration 26900: Loss = -11035.0146484375
6
Iteration 27000: Loss = -11035.0146484375
7
Iteration 27100: Loss = -11035.0146484375
8
Iteration 27200: Loss = -11035.0126953125
9
Iteration 27300: Loss = -11035.0146484375
10
Iteration 27400: Loss = -11035.0146484375
11
Iteration 27500: Loss = -11035.013671875
12
Iteration 27600: Loss = -11035.013671875
13
Iteration 27700: Loss = -11035.013671875
14
Iteration 27800: Loss = -11035.0146484375
15
Stopping early at iteration 27800 due to no improvement.
pi: tensor([[3.6359e-06, 1.0000e+00],
        [2.2167e-03, 9.9778e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0586, 0.9414], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.9113, 0.1998],
         [0.0258, 0.1623]],

        [[0.6461, 0.1129],
         [0.1383, 0.7618]],

        [[0.1282, 0.5143],
         [0.9921, 0.0159]],

        [[0.0121, 0.2965],
         [0.0116, 0.9071]],

        [[0.9908, 0.1276],
         [0.1749, 0.0154]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0028959952356207414
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013652179134287512
Average Adjusted Rand Index: 0.0005791990471241483
[0.809619625046958, -0.0013652179134287512] [0.8098628950664546, 0.0005791990471241483] [10891.236328125, 11035.0146484375]
-------------------------------------
This iteration is 5
True Objective function: Loss = -10822.265315894521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43186.8671875
Iteration 100: Loss = -27312.91015625
Iteration 200: Loss = -16524.88671875
Iteration 300: Loss = -12475.0859375
Iteration 400: Loss = -11404.9833984375
Iteration 500: Loss = -11109.9921875
Iteration 600: Loss = -11041.0810546875
Iteration 700: Loss = -11018.931640625
Iteration 800: Loss = -11006.8251953125
Iteration 900: Loss = -10999.037109375
Iteration 1000: Loss = -10991.6025390625
Iteration 1100: Loss = -10987.091796875
Iteration 1200: Loss = -10983.70703125
Iteration 1300: Loss = -10979.892578125
Iteration 1400: Loss = -10976.205078125
Iteration 1500: Loss = -10974.07421875
Iteration 1600: Loss = -10972.38671875
Iteration 1700: Loss = -10968.39453125
Iteration 1800: Loss = -10961.91796875
Iteration 1900: Loss = -10956.0146484375
Iteration 2000: Loss = -10954.6904296875
Iteration 2100: Loss = -10953.935546875
Iteration 2200: Loss = -10953.333984375
Iteration 2300: Loss = -10952.830078125
Iteration 2400: Loss = -10952.3955078125
Iteration 2500: Loss = -10952.0166015625
Iteration 2600: Loss = -10951.6806640625
Iteration 2700: Loss = -10951.3828125
Iteration 2800: Loss = -10951.115234375
Iteration 2900: Loss = -10950.8759765625
Iteration 3000: Loss = -10950.658203125
Iteration 3100: Loss = -10950.462890625
Iteration 3200: Loss = -10950.28125
Iteration 3300: Loss = -10950.1171875
Iteration 3400: Loss = -10949.966796875
Iteration 3500: Loss = -10949.830078125
Iteration 3600: Loss = -10949.7021484375
Iteration 3700: Loss = -10949.583984375
Iteration 3800: Loss = -10949.4765625
Iteration 3900: Loss = -10949.376953125
Iteration 4000: Loss = -10949.283203125
Iteration 4100: Loss = -10949.197265625
Iteration 4200: Loss = -10949.1162109375
Iteration 4300: Loss = -10949.0439453125
Iteration 4400: Loss = -10948.9736328125
Iteration 4500: Loss = -10948.908203125
Iteration 4600: Loss = -10948.84765625
Iteration 4700: Loss = -10948.791015625
Iteration 4800: Loss = -10948.7373046875
Iteration 4900: Loss = -10948.6884765625
Iteration 5000: Loss = -10948.6396484375
Iteration 5100: Loss = -10948.5986328125
Iteration 5200: Loss = -10948.556640625
Iteration 5300: Loss = -10948.5185546875
Iteration 5400: Loss = -10948.482421875
Iteration 5500: Loss = -10948.44921875
Iteration 5600: Loss = -10948.4189453125
Iteration 5700: Loss = -10948.388671875
Iteration 5800: Loss = -10948.3603515625
Iteration 5900: Loss = -10948.3349609375
Iteration 6000: Loss = -10948.3115234375
Iteration 6100: Loss = -10948.2880859375
Iteration 6200: Loss = -10948.2666015625
Iteration 6300: Loss = -10948.2451171875
Iteration 6400: Loss = -10948.228515625
Iteration 6500: Loss = -10948.2080078125
Iteration 6600: Loss = -10948.1943359375
Iteration 6700: Loss = -10948.177734375
Iteration 6800: Loss = -10948.1630859375
Iteration 6900: Loss = -10948.1484375
Iteration 7000: Loss = -10948.1376953125
Iteration 7100: Loss = -10948.125
Iteration 7200: Loss = -10948.11328125
Iteration 7300: Loss = -10948.103515625
Iteration 7400: Loss = -10948.0927734375
Iteration 7500: Loss = -10948.083984375
Iteration 7600: Loss = -10948.0751953125
Iteration 7700: Loss = -10948.068359375
Iteration 7800: Loss = -10948.0595703125
Iteration 7900: Loss = -10948.052734375
Iteration 8000: Loss = -10948.044921875
Iteration 8100: Loss = -10948.0390625
Iteration 8200: Loss = -10948.033203125
Iteration 8300: Loss = -10948.0263671875
Iteration 8400: Loss = -10948.0224609375
Iteration 8500: Loss = -10948.017578125
Iteration 8600: Loss = -10948.013671875
Iteration 8700: Loss = -10948.0078125
Iteration 8800: Loss = -10948.00390625
Iteration 8900: Loss = -10948.0
Iteration 9000: Loss = -10947.99609375
Iteration 9100: Loss = -10947.9921875
Iteration 9200: Loss = -10947.9892578125
Iteration 9300: Loss = -10947.986328125
Iteration 9400: Loss = -10947.9814453125
Iteration 9500: Loss = -10947.978515625
Iteration 9600: Loss = -10947.9775390625
Iteration 9700: Loss = -10947.97265625
Iteration 9800: Loss = -10947.9716796875
Iteration 9900: Loss = -10947.9677734375
Iteration 10000: Loss = -10947.966796875
Iteration 10100: Loss = -10947.9638671875
Iteration 10200: Loss = -10947.9619140625
Iteration 10300: Loss = -10947.9599609375
Iteration 10400: Loss = -10947.958984375
Iteration 10500: Loss = -10947.955078125
Iteration 10600: Loss = -10947.9541015625
Iteration 10700: Loss = -10947.9541015625
Iteration 10800: Loss = -10947.9521484375
Iteration 10900: Loss = -10947.9501953125
Iteration 11000: Loss = -10947.9482421875
Iteration 11100: Loss = -10947.947265625
Iteration 11200: Loss = -10947.9453125
Iteration 11300: Loss = -10947.9443359375
Iteration 11400: Loss = -10947.9423828125
Iteration 11500: Loss = -10947.94140625
Iteration 11600: Loss = -10947.94140625
Iteration 11700: Loss = -10947.939453125
Iteration 11800: Loss = -10947.9375
Iteration 11900: Loss = -10947.9375
Iteration 12000: Loss = -10947.9375
Iteration 12100: Loss = -10947.9365234375
Iteration 12200: Loss = -10947.935546875
Iteration 12300: Loss = -10947.9345703125
Iteration 12400: Loss = -10947.9345703125
Iteration 12500: Loss = -10947.93359375
Iteration 12600: Loss = -10947.931640625
Iteration 12700: Loss = -10947.931640625
Iteration 12800: Loss = -10947.9296875
Iteration 12900: Loss = -10947.9306640625
1
Iteration 13000: Loss = -10947.9287109375
Iteration 13100: Loss = -10947.9287109375
Iteration 13200: Loss = -10947.9287109375
Iteration 13300: Loss = -10947.927734375
Iteration 13400: Loss = -10947.927734375
Iteration 13500: Loss = -10947.927734375
Iteration 13600: Loss = -10947.927734375
Iteration 13700: Loss = -10947.9267578125
Iteration 13800: Loss = -10947.92578125
Iteration 13900: Loss = -10947.92578125
Iteration 14000: Loss = -10947.9248046875
Iteration 14100: Loss = -10947.9248046875
Iteration 14200: Loss = -10947.92578125
1
Iteration 14300: Loss = -10947.9248046875
Iteration 14400: Loss = -10947.9228515625
Iteration 14500: Loss = -10947.923828125
1
Iteration 14600: Loss = -10947.921875
Iteration 14700: Loss = -10947.921875
Iteration 14800: Loss = -10947.921875
Iteration 14900: Loss = -10947.9228515625
1
Iteration 15000: Loss = -10947.9208984375
Iteration 15100: Loss = -10947.9208984375
Iteration 15200: Loss = -10947.921875
1
Iteration 15300: Loss = -10947.9208984375
Iteration 15400: Loss = -10947.9208984375
Iteration 15500: Loss = -10947.9189453125
Iteration 15600: Loss = -10947.9228515625
1
Iteration 15700: Loss = -10947.919921875
2
Iteration 15800: Loss = -10947.919921875
3
Iteration 15900: Loss = -10947.9189453125
Iteration 16000: Loss = -10947.9189453125
Iteration 16100: Loss = -10947.919921875
1
Iteration 16200: Loss = -10947.9189453125
Iteration 16300: Loss = -10947.9189453125
Iteration 16400: Loss = -10947.91796875
Iteration 16500: Loss = -10947.9189453125
1
Iteration 16600: Loss = -10947.919921875
2
Iteration 16700: Loss = -10947.9189453125
3
Iteration 16800: Loss = -10947.9169921875
Iteration 16900: Loss = -10947.9169921875
Iteration 17000: Loss = -10947.9169921875
Iteration 17100: Loss = -10947.9169921875
Iteration 17200: Loss = -10947.9169921875
Iteration 17300: Loss = -10947.9169921875
Iteration 17400: Loss = -10947.9169921875
Iteration 17500: Loss = -10947.9150390625
Iteration 17600: Loss = -10947.916015625
1
Iteration 17700: Loss = -10947.916015625
2
Iteration 17800: Loss = -10947.9169921875
3
Iteration 17900: Loss = -10947.9150390625
Iteration 18000: Loss = -10947.916015625
1
Iteration 18100: Loss = -10947.9150390625
Iteration 18200: Loss = -10947.9150390625
Iteration 18300: Loss = -10947.916015625
1
Iteration 18400: Loss = -10947.9150390625
Iteration 18500: Loss = -10947.9140625
Iteration 18600: Loss = -10947.9150390625
1
Iteration 18700: Loss = -10947.916015625
2
Iteration 18800: Loss = -10947.9140625
Iteration 18900: Loss = -10947.9150390625
1
Iteration 19000: Loss = -10947.9140625
Iteration 19100: Loss = -10947.9150390625
1
Iteration 19200: Loss = -10947.9140625
Iteration 19300: Loss = -10947.9140625
Iteration 19400: Loss = -10947.9150390625
1
Iteration 19500: Loss = -10947.9130859375
Iteration 19600: Loss = -10947.9150390625
1
Iteration 19700: Loss = -10947.9130859375
Iteration 19800: Loss = -10947.9130859375
Iteration 19900: Loss = -10947.9111328125
Iteration 20000: Loss = -10947.9111328125
Iteration 20100: Loss = -10947.9072265625
Iteration 20200: Loss = -10947.9033203125
Iteration 20300: Loss = -10947.890625
Iteration 20400: Loss = -10947.5224609375
Iteration 20500: Loss = -10947.4501953125
Iteration 20600: Loss = -10947.439453125
Iteration 20700: Loss = -10947.4326171875
Iteration 20800: Loss = -10947.431640625
Iteration 20900: Loss = -10947.4306640625
Iteration 21000: Loss = -10947.357421875
Iteration 21100: Loss = -10946.787109375
Iteration 21200: Loss = -10946.78515625
Iteration 21300: Loss = -10946.787109375
1
Iteration 21400: Loss = -10946.7861328125
2
Iteration 21500: Loss = -10946.7841796875
Iteration 21600: Loss = -10946.7861328125
1
Iteration 21700: Loss = -10946.78515625
2
Iteration 21800: Loss = -10946.78515625
3
Iteration 21900: Loss = -10946.7861328125
4
Iteration 22000: Loss = -10946.7841796875
Iteration 22100: Loss = -10946.7861328125
1
Iteration 22200: Loss = -10946.7861328125
2
Iteration 22300: Loss = -10946.78515625
3
Iteration 22400: Loss = -10946.7861328125
4
Iteration 22500: Loss = -10946.7861328125
5
Iteration 22600: Loss = -10946.7861328125
6
Iteration 22700: Loss = -10946.7841796875
Iteration 22800: Loss = -10946.7841796875
Iteration 22900: Loss = -10946.78515625
1
Iteration 23000: Loss = -10946.78515625
2
Iteration 23100: Loss = -10946.78515625
3
Iteration 23200: Loss = -10946.7705078125
Iteration 23300: Loss = -10946.1796875
Iteration 23400: Loss = -10946.087890625
Iteration 23500: Loss = -10946.0791015625
Iteration 23600: Loss = -10946.0751953125
Iteration 23700: Loss = -10946.0732421875
Iteration 23800: Loss = -10946.072265625
Iteration 23900: Loss = -10946.0732421875
1
Iteration 24000: Loss = -10946.0712890625
Iteration 24100: Loss = -10946.0712890625
Iteration 24200: Loss = -10946.0703125
Iteration 24300: Loss = -10946.0703125
Iteration 24400: Loss = -10946.0693359375
Iteration 24500: Loss = -10946.0703125
1
Iteration 24600: Loss = -10946.0693359375
Iteration 24700: Loss = -10946.0693359375
Iteration 24800: Loss = -10946.0703125
1
Iteration 24900: Loss = -10946.0703125
2
Iteration 25000: Loss = -10946.0703125
3
Iteration 25100: Loss = -10946.0703125
4
Iteration 25200: Loss = -10946.068359375
Iteration 25300: Loss = -10946.0703125
1
Iteration 25400: Loss = -10946.0693359375
2
Iteration 25500: Loss = -10946.0693359375
3
Iteration 25600: Loss = -10946.068359375
Iteration 25700: Loss = -10946.0693359375
1
Iteration 25800: Loss = -10946.0673828125
Iteration 25900: Loss = -10946.0673828125
Iteration 26000: Loss = -10946.0693359375
1
Iteration 26100: Loss = -10946.0703125
2
Iteration 26200: Loss = -10946.0703125
3
Iteration 26300: Loss = -10946.0693359375
4
Iteration 26400: Loss = -10946.068359375
5
Iteration 26500: Loss = -10946.0693359375
6
Iteration 26600: Loss = -10946.0693359375
7
Iteration 26700: Loss = -10946.068359375
8
Iteration 26800: Loss = -10946.0693359375
9
Iteration 26900: Loss = -10946.068359375
10
Iteration 27000: Loss = -10946.068359375
11
Iteration 27100: Loss = -10946.0703125
12
Iteration 27200: Loss = -10946.0693359375
13
Iteration 27300: Loss = -10946.068359375
14
Iteration 27400: Loss = -10946.068359375
15
Stopping early at iteration 27400 due to no improvement.
pi: tensor([[0.8751, 0.1249],
        [0.9862, 0.0138]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.8629e-05, 9.9997e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1641, 0.1585],
         [0.7663, 0.1569]],

        [[0.9544, 0.0672],
         [0.9675, 0.8284]],

        [[0.9791, 0.1449],
         [0.8867, 0.9824]],

        [[0.5196, 0.1751],
         [0.0159, 0.9879]],

        [[0.0365, 0.1623],
         [0.1381, 0.8388]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -4.1736150871935294e-05
Average Adjusted Rand Index: -0.0015521208230856337
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -57873.15234375
Iteration 100: Loss = -34531.8046875
Iteration 200: Loss = -17059.375
Iteration 300: Loss = -12898.0576171875
Iteration 400: Loss = -12057.7861328125
Iteration 500: Loss = -11641.095703125
Iteration 600: Loss = -11441.75
Iteration 700: Loss = -11259.3271484375
Iteration 800: Loss = -11196.7197265625
Iteration 900: Loss = -11146.0146484375
Iteration 1000: Loss = -11107.7587890625
Iteration 1100: Loss = -11085.7939453125
Iteration 1200: Loss = -11067.484375
Iteration 1300: Loss = -11044.7177734375
Iteration 1400: Loss = -11033.416015625
Iteration 1500: Loss = -11024.4306640625
Iteration 1600: Loss = -11012.919921875
Iteration 1700: Loss = -11000.888671875
Iteration 1800: Loss = -10994.615234375
Iteration 1900: Loss = -10990.0263671875
Iteration 2000: Loss = -10986.2822265625
Iteration 2100: Loss = -10983.0947265625
Iteration 2200: Loss = -10980.3212890625
Iteration 2300: Loss = -10977.87890625
Iteration 2400: Loss = -10975.7060546875
Iteration 2500: Loss = -10973.7578125
Iteration 2600: Loss = -10972.0146484375
Iteration 2700: Loss = -10970.4560546875
Iteration 2800: Loss = -10969.0439453125
Iteration 2900: Loss = -10967.7646484375
Iteration 3000: Loss = -10966.6015625
Iteration 3100: Loss = -10965.5390625
Iteration 3200: Loss = -10964.568359375
Iteration 3300: Loss = -10963.671875
Iteration 3400: Loss = -10962.8486328125
Iteration 3500: Loss = -10962.0888671875
Iteration 3600: Loss = -10961.37890625
Iteration 3700: Loss = -10960.712890625
Iteration 3800: Loss = -10960.072265625
Iteration 3900: Loss = -10959.43359375
Iteration 4000: Loss = -10958.7119140625
Iteration 4100: Loss = -10957.6201171875
Iteration 4200: Loss = -10955.9931640625
Iteration 4300: Loss = -10954.9228515625
Iteration 4400: Loss = -10954.2685546875
Iteration 4500: Loss = -10953.7734375
Iteration 4600: Loss = -10953.35546875
Iteration 4700: Loss = -10952.990234375
Iteration 4800: Loss = -10952.6630859375
Iteration 4900: Loss = -10952.3662109375
Iteration 5000: Loss = -10952.09375
Iteration 5100: Loss = -10951.841796875
Iteration 5200: Loss = -10951.6103515625
Iteration 5300: Loss = -10951.39453125
Iteration 5400: Loss = -10951.1923828125
Iteration 5500: Loss = -10951.0048828125
Iteration 5600: Loss = -10950.8291015625
Iteration 5700: Loss = -10950.6650390625
Iteration 5800: Loss = -10950.509765625
Iteration 5900: Loss = -10950.3671875
Iteration 6000: Loss = -10950.23046875
Iteration 6100: Loss = -10950.103515625
Iteration 6200: Loss = -10949.9833984375
Iteration 6300: Loss = -10949.8720703125
Iteration 6400: Loss = -10949.7646484375
Iteration 6500: Loss = -10949.6650390625
Iteration 6600: Loss = -10949.5693359375
Iteration 6700: Loss = -10949.48046875
Iteration 6800: Loss = -10949.3974609375
Iteration 6900: Loss = -10949.31640625
Iteration 7000: Loss = -10949.2421875
Iteration 7100: Loss = -10949.1708984375
Iteration 7200: Loss = -10949.10546875
Iteration 7300: Loss = -10949.0419921875
Iteration 7400: Loss = -10948.98046875
Iteration 7500: Loss = -10948.9248046875
Iteration 7600: Loss = -10948.87109375
Iteration 7700: Loss = -10948.8212890625
Iteration 7800: Loss = -10948.7734375
Iteration 7900: Loss = -10948.728515625
Iteration 8000: Loss = -10948.6845703125
Iteration 8100: Loss = -10948.6455078125
Iteration 8200: Loss = -10948.607421875
Iteration 8300: Loss = -10948.5712890625
Iteration 8400: Loss = -10948.5341796875
Iteration 8500: Loss = -10948.501953125
Iteration 8600: Loss = -10948.470703125
Iteration 8700: Loss = -10948.4423828125
Iteration 8800: Loss = -10948.416015625
Iteration 8900: Loss = -10948.388671875
Iteration 9000: Loss = -10948.36328125
Iteration 9100: Loss = -10948.3388671875
Iteration 9200: Loss = -10948.3173828125
Iteration 9300: Loss = -10948.2978515625
Iteration 9400: Loss = -10948.27734375
Iteration 9500: Loss = -10948.2568359375
Iteration 9600: Loss = -10948.240234375
Iteration 9700: Loss = -10948.2255859375
Iteration 9800: Loss = -10948.2080078125
Iteration 9900: Loss = -10948.19140625
Iteration 10000: Loss = -10948.1787109375
Iteration 10100: Loss = -10948.1650390625
Iteration 10200: Loss = -10948.1513671875
Iteration 10300: Loss = -10948.138671875
Iteration 10400: Loss = -10948.126953125
Iteration 10500: Loss = -10948.115234375
Iteration 10600: Loss = -10948.1064453125
Iteration 10700: Loss = -10948.0947265625
Iteration 10800: Loss = -10948.0859375
Iteration 10900: Loss = -10948.078125
Iteration 11000: Loss = -10948.0673828125
Iteration 11100: Loss = -10948.060546875
Iteration 11200: Loss = -10948.0537109375
Iteration 11300: Loss = -10948.046875
Iteration 11400: Loss = -10948.041015625
Iteration 11500: Loss = -10948.03515625
Iteration 11600: Loss = -10948.02734375
Iteration 11700: Loss = -10948.0205078125
Iteration 11800: Loss = -10948.0166015625
Iteration 11900: Loss = -10948.01171875
Iteration 12000: Loss = -10948.005859375
Iteration 12100: Loss = -10948.0009765625
Iteration 12200: Loss = -10947.998046875
Iteration 12300: Loss = -10947.994140625
Iteration 12400: Loss = -10947.9892578125
Iteration 12500: Loss = -10947.984375
Iteration 12600: Loss = -10947.9814453125
Iteration 12700: Loss = -10947.9775390625
Iteration 12800: Loss = -10947.974609375
Iteration 12900: Loss = -10947.9736328125
Iteration 13000: Loss = -10947.970703125
Iteration 13100: Loss = -10947.9658203125
Iteration 13200: Loss = -10947.9638671875
Iteration 13300: Loss = -10947.9619140625
Iteration 13400: Loss = -10947.9580078125
Iteration 13500: Loss = -10947.9580078125
Iteration 13600: Loss = -10947.955078125
Iteration 13700: Loss = -10947.9521484375
Iteration 13800: Loss = -10947.9501953125
Iteration 13900: Loss = -10947.94921875
Iteration 14000: Loss = -10947.947265625
Iteration 14100: Loss = -10947.9482421875
1
Iteration 14200: Loss = -10947.9453125
Iteration 14300: Loss = -10947.9423828125
Iteration 14400: Loss = -10947.9423828125
Iteration 14500: Loss = -10947.939453125
Iteration 14600: Loss = -10947.9375
Iteration 14700: Loss = -10947.9375
Iteration 14800: Loss = -10947.9375
Iteration 14900: Loss = -10947.9345703125
Iteration 15000: Loss = -10947.935546875
1
Iteration 15100: Loss = -10947.93359375
Iteration 15200: Loss = -10947.9326171875
Iteration 15300: Loss = -10947.9326171875
Iteration 15400: Loss = -10947.9296875
Iteration 15500: Loss = -10947.9296875
Iteration 15600: Loss = -10947.9296875
Iteration 15700: Loss = -10947.9296875
Iteration 15800: Loss = -10947.9267578125
Iteration 15900: Loss = -10947.927734375
1
Iteration 16000: Loss = -10947.9267578125
Iteration 16100: Loss = -10947.92578125
Iteration 16200: Loss = -10947.9248046875
Iteration 16300: Loss = -10947.923828125
Iteration 16400: Loss = -10947.9248046875
1
Iteration 16500: Loss = -10947.9248046875
2
Iteration 16600: Loss = -10947.9228515625
Iteration 16700: Loss = -10947.9228515625
Iteration 16800: Loss = -10947.9228515625
Iteration 16900: Loss = -10947.9228515625
Iteration 17000: Loss = -10947.9228515625
Iteration 17100: Loss = -10947.921875
Iteration 17200: Loss = -10947.9208984375
Iteration 17300: Loss = -10947.921875
1
Iteration 17400: Loss = -10947.9189453125
Iteration 17500: Loss = -10947.919921875
1
Iteration 17600: Loss = -10947.919921875
2
Iteration 17700: Loss = -10947.9189453125
Iteration 17800: Loss = -10947.9189453125
Iteration 17900: Loss = -10947.919921875
1
Iteration 18000: Loss = -10947.9189453125
Iteration 18100: Loss = -10947.9189453125
Iteration 18200: Loss = -10947.9208984375
1
Iteration 18300: Loss = -10947.9189453125
Iteration 18400: Loss = -10947.9189453125
Iteration 18500: Loss = -10947.9189453125
Iteration 18600: Loss = -10947.919921875
1
Iteration 18700: Loss = -10947.9189453125
Iteration 18800: Loss = -10947.916015625
Iteration 18900: Loss = -10947.9169921875
1
Iteration 19000: Loss = -10947.91796875
2
Iteration 19100: Loss = -10947.91796875
3
Iteration 19200: Loss = -10947.9189453125
4
Iteration 19300: Loss = -10947.9169921875
5
Iteration 19400: Loss = -10947.916015625
Iteration 19500: Loss = -10947.9189453125
1
Iteration 19600: Loss = -10947.9169921875
2
Iteration 19700: Loss = -10947.9169921875
3
Iteration 19800: Loss = -10947.916015625
Iteration 19900: Loss = -10947.9150390625
Iteration 20000: Loss = -10947.9169921875
1
Iteration 20100: Loss = -10947.9150390625
Iteration 20200: Loss = -10947.916015625
1
Iteration 20300: Loss = -10947.9150390625
Iteration 20400: Loss = -10947.9150390625
Iteration 20500: Loss = -10947.9150390625
Iteration 20600: Loss = -10947.916015625
1
Iteration 20700: Loss = -10947.9150390625
Iteration 20800: Loss = -10947.9140625
Iteration 20900: Loss = -10947.9130859375
Iteration 21000: Loss = -10947.9111328125
Iteration 21100: Loss = -10947.908203125
Iteration 21200: Loss = -10947.900390625
Iteration 21300: Loss = -10947.673828125
Iteration 21400: Loss = -10947.453125
Iteration 21500: Loss = -10947.4375
Iteration 21600: Loss = -10946.81640625
Iteration 21700: Loss = -10946.810546875
Iteration 21800: Loss = -10946.8076171875
Iteration 21900: Loss = -10946.80859375
1
Iteration 22000: Loss = -10946.8056640625
Iteration 22100: Loss = -10946.8037109375
Iteration 22200: Loss = -10946.7880859375
Iteration 22300: Loss = -10946.7861328125
Iteration 22400: Loss = -10946.78515625
Iteration 22500: Loss = -10946.787109375
1
Iteration 22600: Loss = -10946.7861328125
2
Iteration 22700: Loss = -10946.7861328125
3
Iteration 22800: Loss = -10946.787109375
4
Iteration 22900: Loss = -10946.7861328125
5
Iteration 23000: Loss = -10946.78515625
Iteration 23100: Loss = -10946.7841796875
Iteration 23200: Loss = -10946.783203125
Iteration 23300: Loss = -10946.78515625
1
Iteration 23400: Loss = -10946.7841796875
2
Iteration 23500: Loss = -10946.7841796875
3
Iteration 23600: Loss = -10946.7841796875
4
Iteration 23700: Loss = -10946.7841796875
5
Iteration 23800: Loss = -10946.78515625
6
Iteration 23900: Loss = -10946.7841796875
7
Iteration 24000: Loss = -10946.7841796875
8
Iteration 24100: Loss = -10946.7841796875
9
Iteration 24200: Loss = -10946.7861328125
10
Iteration 24300: Loss = -10946.7841796875
11
Iteration 24400: Loss = -10946.78515625
12
Iteration 24500: Loss = -10946.78515625
13
Iteration 24600: Loss = -10946.78515625
14
Iteration 24700: Loss = -10946.78515625
15
Stopping early at iteration 24700 due to no improvement.
pi: tensor([[1.0000e+00, 7.1130e-07],
        [9.8584e-01, 1.4161e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0693, 0.9307], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1634, 0.1591],
         [0.6723, 0.1566]],

        [[0.9830, 0.0668],
         [0.5729, 0.9889]],

        [[0.9753, 0.1678],
         [0.4825, 0.9901]],

        [[0.8250, 0.1258],
         [0.9435, 0.0837]],

        [[0.6795, 0.1701],
         [0.9798, 0.9725]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -4.1736150871935294e-05
Average Adjusted Rand Index: -0.0015521208230856337
[-4.1736150871935294e-05, -4.1736150871935294e-05] [-0.0015521208230856337, -0.0015521208230856337] [10946.068359375, 10946.78515625]
-------------------------------------
This iteration is 6
True Objective function: Loss = -10827.320372823497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52167.765625
Iteration 100: Loss = -33541.25
Iteration 200: Loss = -18853.435546875
Iteration 300: Loss = -13507.81640625
Iteration 400: Loss = -11898.6416015625
Iteration 500: Loss = -11404.4287109375
Iteration 600: Loss = -11221.478515625
Iteration 700: Loss = -11097.4609375
Iteration 800: Loss = -11058.4296875
Iteration 900: Loss = -11035.2158203125
Iteration 1000: Loss = -11020.5361328125
Iteration 1100: Loss = -11009.197265625
Iteration 1200: Loss = -11000.0732421875
Iteration 1300: Loss = -10993.421875
Iteration 1400: Loss = -10986.037109375
Iteration 1500: Loss = -10979.595703125
Iteration 1600: Loss = -10976.16796875
Iteration 1700: Loss = -10973.513671875
Iteration 1800: Loss = -10971.3662109375
Iteration 1900: Loss = -10969.5859375
Iteration 2000: Loss = -10968.0810546875
Iteration 2100: Loss = -10966.7880859375
Iteration 2200: Loss = -10965.6708984375
Iteration 2300: Loss = -10964.6962890625
Iteration 2400: Loss = -10963.8359375
Iteration 2500: Loss = -10963.07421875
Iteration 2600: Loss = -10962.396484375
Iteration 2700: Loss = -10961.7900390625
Iteration 2800: Loss = -10961.244140625
Iteration 2900: Loss = -10960.7509765625
Iteration 3000: Loss = -10960.3037109375
Iteration 3100: Loss = -10959.8955078125
Iteration 3200: Loss = -10959.525390625
Iteration 3300: Loss = -10959.1865234375
Iteration 3400: Loss = -10958.873046875
Iteration 3500: Loss = -10958.587890625
Iteration 3600: Loss = -10958.3212890625
Iteration 3700: Loss = -10958.078125
Iteration 3800: Loss = -10957.8525390625
Iteration 3900: Loss = -10957.642578125
Iteration 4000: Loss = -10957.44921875
Iteration 4100: Loss = -10957.2705078125
Iteration 4200: Loss = -10957.1015625
Iteration 4300: Loss = -10956.9443359375
Iteration 4400: Loss = -10956.798828125
Iteration 4500: Loss = -10956.662109375
Iteration 4600: Loss = -10956.533203125
Iteration 4700: Loss = -10956.4140625
Iteration 4800: Loss = -10956.3037109375
Iteration 4900: Loss = -10956.197265625
Iteration 5000: Loss = -10956.099609375
Iteration 5100: Loss = -10956.0078125
Iteration 5200: Loss = -10955.919921875
Iteration 5300: Loss = -10955.837890625
Iteration 5400: Loss = -10955.7607421875
Iteration 5500: Loss = -10955.6884765625
Iteration 5600: Loss = -10955.6171875
Iteration 5700: Loss = -10955.55078125
Iteration 5800: Loss = -10955.4912109375
Iteration 5900: Loss = -10955.4326171875
Iteration 6000: Loss = -10955.3759765625
Iteration 6100: Loss = -10955.322265625
Iteration 6200: Loss = -10955.271484375
Iteration 6300: Loss = -10955.2216796875
Iteration 6400: Loss = -10955.1748046875
Iteration 6500: Loss = -10955.1298828125
Iteration 6600: Loss = -10955.083984375
Iteration 6700: Loss = -10955.0400390625
Iteration 6800: Loss = -10954.994140625
Iteration 6900: Loss = -10954.9521484375
Iteration 7000: Loss = -10954.91015625
Iteration 7100: Loss = -10954.8701171875
Iteration 7200: Loss = -10954.8330078125
Iteration 7300: Loss = -10954.794921875
Iteration 7400: Loss = -10954.76171875
Iteration 7500: Loss = -10954.7314453125
Iteration 7600: Loss = -10954.7060546875
Iteration 7700: Loss = -10954.6806640625
Iteration 7800: Loss = -10954.658203125
Iteration 7900: Loss = -10954.6357421875
Iteration 8000: Loss = -10954.6162109375
Iteration 8100: Loss = -10954.59375
Iteration 8200: Loss = -10954.5693359375
Iteration 8300: Loss = -10954.5390625
Iteration 8400: Loss = -10954.48828125
Iteration 8500: Loss = -10954.3017578125
Iteration 8600: Loss = -10954.0537109375
Iteration 8700: Loss = -10953.8984375
Iteration 8800: Loss = -10953.759765625
Iteration 8900: Loss = -10953.7080078125
Iteration 9000: Loss = -10953.671875
Iteration 9100: Loss = -10953.6396484375
Iteration 9200: Loss = -10953.611328125
Iteration 9300: Loss = -10953.5830078125
Iteration 9400: Loss = -10953.5595703125
Iteration 9500: Loss = -10953.53515625
Iteration 9600: Loss = -10953.5078125
Iteration 9700: Loss = -10953.48046875
Iteration 9800: Loss = -10953.451171875
Iteration 9900: Loss = -10953.408203125
Iteration 10000: Loss = -10953.3623046875
Iteration 10100: Loss = -10953.3115234375
Iteration 10200: Loss = -10953.2607421875
Iteration 10300: Loss = -10953.216796875
Iteration 10400: Loss = -10953.1630859375
Iteration 10500: Loss = -10953.0859375
Iteration 10600: Loss = -10953.0244140625
Iteration 10700: Loss = -10952.96484375
Iteration 10800: Loss = -10952.6103515625
Iteration 10900: Loss = -10950.0546875
Iteration 11000: Loss = -10950.00390625
Iteration 11100: Loss = -10949.9755859375
Iteration 11200: Loss = -10949.9560546875
Iteration 11300: Loss = -10949.9375
Iteration 11400: Loss = -10949.9228515625
Iteration 11500: Loss = -10949.91015625
Iteration 11600: Loss = -10949.8974609375
Iteration 11700: Loss = -10949.8759765625
Iteration 11800: Loss = -10949.85546875
Iteration 11900: Loss = -10949.8447265625
Iteration 12000: Loss = -10949.8359375
Iteration 12100: Loss = -10949.8271484375
Iteration 12200: Loss = -10949.8193359375
Iteration 12300: Loss = -10949.80859375
Iteration 12400: Loss = -10949.8037109375
Iteration 12500: Loss = -10949.7978515625
Iteration 12600: Loss = -10949.79296875
Iteration 12700: Loss = -10949.7900390625
Iteration 12800: Loss = -10949.7880859375
Iteration 12900: Loss = -10949.783203125
Iteration 13000: Loss = -10949.783203125
Iteration 13100: Loss = -10949.7802734375
Iteration 13200: Loss = -10949.779296875
Iteration 13300: Loss = -10949.7763671875
Iteration 13400: Loss = -10949.775390625
Iteration 13500: Loss = -10949.7734375
Iteration 13600: Loss = -10949.7734375
Iteration 13700: Loss = -10949.771484375
Iteration 13800: Loss = -10949.7705078125
Iteration 13900: Loss = -10949.76953125
Iteration 14000: Loss = -10949.7685546875
Iteration 14100: Loss = -10949.767578125
Iteration 14200: Loss = -10949.767578125
Iteration 14300: Loss = -10949.7666015625
Iteration 14400: Loss = -10949.7666015625
Iteration 14500: Loss = -10949.763671875
Iteration 14600: Loss = -10949.7646484375
1
Iteration 14700: Loss = -10949.7646484375
2
Iteration 14800: Loss = -10949.763671875
Iteration 14900: Loss = -10949.763671875
Iteration 15000: Loss = -10949.71875
Iteration 15100: Loss = -10949.7158203125
Iteration 15200: Loss = -10949.7158203125
Iteration 15300: Loss = -10949.7138671875
Iteration 15400: Loss = -10949.7138671875
Iteration 15500: Loss = -10949.7138671875
Iteration 15600: Loss = -10949.7119140625
Iteration 15700: Loss = -10949.712890625
1
Iteration 15800: Loss = -10949.712890625
2
Iteration 15900: Loss = -10949.7109375
Iteration 16000: Loss = -10949.7109375
Iteration 16100: Loss = -10949.7119140625
1
Iteration 16200: Loss = -10949.7109375
Iteration 16300: Loss = -10949.7109375
Iteration 16400: Loss = -10949.7099609375
Iteration 16500: Loss = -10949.7099609375
Iteration 16600: Loss = -10949.708984375
Iteration 16700: Loss = -10949.7099609375
1
Iteration 16800: Loss = -10949.7109375
2
Iteration 16900: Loss = -10949.708984375
Iteration 17000: Loss = -10949.708984375
Iteration 17100: Loss = -10949.708984375
Iteration 17200: Loss = -10949.7080078125
Iteration 17300: Loss = -10949.7099609375
1
Iteration 17400: Loss = -10949.7080078125
Iteration 17500: Loss = -10949.7080078125
Iteration 17600: Loss = -10949.70703125
Iteration 17700: Loss = -10949.7080078125
1
Iteration 17800: Loss = -10949.7080078125
2
Iteration 17900: Loss = -10949.708984375
3
Iteration 18000: Loss = -10949.70703125
Iteration 18100: Loss = -10949.7080078125
1
Iteration 18200: Loss = -10949.7080078125
2
Iteration 18300: Loss = -10949.70703125
Iteration 18400: Loss = -10949.708984375
1
Iteration 18500: Loss = -10949.7080078125
2
Iteration 18600: Loss = -10949.7080078125
3
Iteration 18700: Loss = -10949.7060546875
Iteration 18800: Loss = -10949.70703125
1
Iteration 18900: Loss = -10949.70703125
2
Iteration 19000: Loss = -10949.7060546875
Iteration 19100: Loss = -10949.70703125
1
Iteration 19200: Loss = -10949.7080078125
2
Iteration 19300: Loss = -10949.7060546875
Iteration 19400: Loss = -10949.7060546875
Iteration 19500: Loss = -10949.7060546875
Iteration 19600: Loss = -10949.70703125
1
Iteration 19700: Loss = -10949.7060546875
Iteration 19800: Loss = -10949.705078125
Iteration 19900: Loss = -10949.7060546875
1
Iteration 20000: Loss = -10949.70703125
2
Iteration 20100: Loss = -10949.70703125
3
Iteration 20200: Loss = -10949.7060546875
4
Iteration 20300: Loss = -10949.70703125
5
Iteration 20400: Loss = -10949.70703125
6
Iteration 20500: Loss = -10949.7060546875
7
Iteration 20600: Loss = -10949.70703125
8
Iteration 20700: Loss = -10949.7060546875
9
Iteration 20800: Loss = -10949.7060546875
10
Iteration 20900: Loss = -10949.7080078125
11
Iteration 21000: Loss = -10949.7080078125
12
Iteration 21100: Loss = -10949.7060546875
13
Iteration 21200: Loss = -10949.70703125
14
Iteration 21300: Loss = -10949.7060546875
15
Stopping early at iteration 21300 due to no improvement.
pi: tensor([[9.7757e-01, 2.2428e-02],
        [9.9999e-01, 1.0051e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9846, 0.0154], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1623, 0.0648],
         [0.0093, 0.5959]],

        [[0.8330, 0.0687],
         [0.3433, 0.3798]],

        [[0.7010, 0.2474],
         [0.1344, 0.0441]],

        [[0.7248, 0.7538],
         [0.0236, 0.9924]],

        [[0.8428, 0.1122],
         [0.4195, 0.9758]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.007262881945936654
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.35429808991111e-05
Average Adjusted Rand Index: 0.0007893444549905863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22555.1484375
Iteration 100: Loss = -15256.33984375
Iteration 200: Loss = -11787.7578125
Iteration 300: Loss = -11283.07421875
Iteration 400: Loss = -11167.2685546875
Iteration 500: Loss = -11124.7548828125
Iteration 600: Loss = -11101.6513671875
Iteration 700: Loss = -11083.60546875
Iteration 800: Loss = -11069.8623046875
Iteration 900: Loss = -11059.5986328125
Iteration 1000: Loss = -11052.4072265625
Iteration 1100: Loss = -11044.140625
Iteration 1200: Loss = -11036.6552734375
Iteration 1300: Loss = -11031.6962890625
Iteration 1400: Loss = -11024.9599609375
Iteration 1500: Loss = -11020.0830078125
Iteration 1600: Loss = -11012.5966796875
Iteration 1700: Loss = -11008.5615234375
Iteration 1800: Loss = -11005.1279296875
Iteration 1900: Loss = -11001.716796875
Iteration 2000: Loss = -10998.8203125
Iteration 2100: Loss = -10995.09375
Iteration 2200: Loss = -10991.0673828125
Iteration 2300: Loss = -10987.0419921875
Iteration 2400: Loss = -10984.26171875
Iteration 2500: Loss = -10981.193359375
Iteration 2600: Loss = -10978.8388671875
Iteration 2700: Loss = -10977.736328125
Iteration 2800: Loss = -10976.845703125
Iteration 2900: Loss = -10976.0087890625
Iteration 3000: Loss = -10975.11328125
Iteration 3100: Loss = -10970.96484375
Iteration 3200: Loss = -10969.1728515625
Iteration 3300: Loss = -10967.44140625
Iteration 3400: Loss = -10966.6474609375
Iteration 3500: Loss = -10966.0986328125
Iteration 3600: Loss = -10965.6484375
Iteration 3700: Loss = -10965.1865234375
Iteration 3800: Loss = -10963.6669921875
Iteration 3900: Loss = -10962.4482421875
Iteration 4000: Loss = -10962.0693359375
Iteration 4100: Loss = -10961.8271484375
Iteration 4200: Loss = -10961.6357421875
Iteration 4300: Loss = -10961.4716796875
Iteration 4400: Loss = -10961.3291015625
Iteration 4500: Loss = -10961.193359375
Iteration 4600: Loss = -10961.0576171875
Iteration 4700: Loss = -10960.8603515625
Iteration 4800: Loss = -10958.7041015625
Iteration 4900: Loss = -10957.998046875
Iteration 5000: Loss = -10957.7783203125
Iteration 5100: Loss = -10957.6201171875
Iteration 5200: Loss = -10957.4775390625
Iteration 5300: Loss = -10957.3310546875
Iteration 5400: Loss = -10957.1552734375
Iteration 5500: Loss = -10956.8720703125
Iteration 5600: Loss = -10955.2568359375
Iteration 5700: Loss = -10954.15625
Iteration 5800: Loss = -10953.771484375
Iteration 5900: Loss = -10953.548828125
Iteration 6000: Loss = -10953.3955078125
Iteration 6100: Loss = -10953.2919921875
Iteration 6200: Loss = -10953.21875
Iteration 6300: Loss = -10953.166015625
Iteration 6400: Loss = -10953.125
Iteration 6500: Loss = -10953.091796875
Iteration 6600: Loss = -10953.0615234375
Iteration 6700: Loss = -10953.033203125
Iteration 6800: Loss = -10953.0087890625
Iteration 6900: Loss = -10952.984375
Iteration 7000: Loss = -10952.9599609375
Iteration 7100: Loss = -10952.9375
Iteration 7200: Loss = -10952.9140625
Iteration 7300: Loss = -10952.8935546875
Iteration 7400: Loss = -10952.8759765625
Iteration 7500: Loss = -10952.8583984375
Iteration 7600: Loss = -10952.8447265625
Iteration 7700: Loss = -10952.830078125
Iteration 7800: Loss = -10952.818359375
Iteration 7900: Loss = -10952.806640625
Iteration 8000: Loss = -10952.79296875
Iteration 8100: Loss = -10952.783203125
Iteration 8200: Loss = -10952.76953125
Iteration 8300: Loss = -10952.7470703125
Iteration 8400: Loss = -10952.6357421875
Iteration 8500: Loss = -10952.5888671875
Iteration 8600: Loss = -10952.5732421875
Iteration 8700: Loss = -10952.564453125
Iteration 8800: Loss = -10952.5556640625
Iteration 8900: Loss = -10952.5478515625
Iteration 9000: Loss = -10952.5419921875
Iteration 9100: Loss = -10952.537109375
Iteration 9200: Loss = -10952.5322265625
Iteration 9300: Loss = -10952.5263671875
Iteration 9400: Loss = -10952.521484375
Iteration 9500: Loss = -10952.517578125
Iteration 9600: Loss = -10952.5126953125
Iteration 9700: Loss = -10952.5068359375
Iteration 9800: Loss = -10949.7177734375
Iteration 9900: Loss = -10949.611328125
Iteration 10000: Loss = -10949.5966796875
Iteration 10100: Loss = -10949.58984375
Iteration 10200: Loss = -10949.5830078125
Iteration 10300: Loss = -10949.5791015625
Iteration 10400: Loss = -10949.5751953125
Iteration 10500: Loss = -10949.5732421875
Iteration 10600: Loss = -10949.5703125
Iteration 10700: Loss = -10949.568359375
Iteration 10800: Loss = -10949.56640625
Iteration 10900: Loss = -10949.564453125
Iteration 11000: Loss = -10949.5615234375
Iteration 11100: Loss = -10949.560546875
Iteration 11200: Loss = -10949.5556640625
Iteration 11300: Loss = -10949.5478515625
Iteration 11400: Loss = -10949.54296875
Iteration 11500: Loss = -10949.5380859375
Iteration 11600: Loss = -10949.5361328125
Iteration 11700: Loss = -10949.533203125
Iteration 11800: Loss = -10949.533203125
Iteration 11900: Loss = -10949.5322265625
Iteration 12000: Loss = -10949.53125
Iteration 12100: Loss = -10949.5302734375
Iteration 12200: Loss = -10949.529296875
Iteration 12300: Loss = -10949.5283203125
Iteration 12400: Loss = -10949.529296875
1
Iteration 12500: Loss = -10949.5263671875
Iteration 12600: Loss = -10949.5263671875
Iteration 12700: Loss = -10949.5244140625
Iteration 12800: Loss = -10949.5234375
Iteration 12900: Loss = -10949.5234375
Iteration 13000: Loss = -10949.521484375
Iteration 13100: Loss = -10949.5224609375
1
Iteration 13200: Loss = -10949.5224609375
2
Iteration 13300: Loss = -10949.521484375
Iteration 13400: Loss = -10949.521484375
Iteration 13500: Loss = -10949.51953125
Iteration 13600: Loss = -10949.5205078125
1
Iteration 13700: Loss = -10949.51953125
Iteration 13800: Loss = -10949.51953125
Iteration 13900: Loss = -10949.5185546875
Iteration 14000: Loss = -10949.5185546875
Iteration 14100: Loss = -10949.517578125
Iteration 14200: Loss = -10949.517578125
Iteration 14300: Loss = -10949.517578125
Iteration 14400: Loss = -10949.515625
Iteration 14500: Loss = -10949.5166015625
1
Iteration 14600: Loss = -10949.5166015625
2
Iteration 14700: Loss = -10949.5166015625
3
Iteration 14800: Loss = -10949.515625
Iteration 14900: Loss = -10949.515625
Iteration 15000: Loss = -10949.5166015625
1
Iteration 15100: Loss = -10949.5146484375
Iteration 15200: Loss = -10949.515625
1
Iteration 15300: Loss = -10949.5146484375
Iteration 15400: Loss = -10949.515625
1
Iteration 15500: Loss = -10949.5146484375
Iteration 15600: Loss = -10949.513671875
Iteration 15700: Loss = -10949.515625
1
Iteration 15800: Loss = -10949.51171875
Iteration 15900: Loss = -10949.4990234375
Iteration 16000: Loss = -10949.498046875
Iteration 16100: Loss = -10949.4990234375
1
Iteration 16200: Loss = -10949.4990234375
2
Iteration 16300: Loss = -10949.4990234375
3
Iteration 16400: Loss = -10949.498046875
Iteration 16500: Loss = -10949.498046875
Iteration 16600: Loss = -10949.498046875
Iteration 16700: Loss = -10949.4970703125
Iteration 16800: Loss = -10949.4970703125
Iteration 16900: Loss = -10949.4990234375
1
Iteration 17000: Loss = -10949.498046875
2
Iteration 17100: Loss = -10949.498046875
3
Iteration 17200: Loss = -10949.4970703125
Iteration 17300: Loss = -10949.4970703125
Iteration 17400: Loss = -10949.4970703125
Iteration 17500: Loss = -10949.498046875
1
Iteration 17600: Loss = -10949.49609375
Iteration 17700: Loss = -10949.4970703125
1
Iteration 17800: Loss = -10949.4970703125
2
Iteration 17900: Loss = -10949.4970703125
3
Iteration 18000: Loss = -10949.4970703125
4
Iteration 18100: Loss = -10949.49609375
Iteration 18200: Loss = -10949.498046875
1
Iteration 18300: Loss = -10949.4970703125
2
Iteration 18400: Loss = -10949.498046875
3
Iteration 18500: Loss = -10949.498046875
4
Iteration 18600: Loss = -10949.4970703125
5
Iteration 18700: Loss = -10945.041015625
Iteration 18800: Loss = -10944.341796875
Iteration 18900: Loss = -10944.326171875
Iteration 19000: Loss = -10944.3203125
Iteration 19100: Loss = -10944.318359375
Iteration 19200: Loss = -10944.3154296875
Iteration 19300: Loss = -10944.314453125
Iteration 19400: Loss = -10944.31640625
1
Iteration 19500: Loss = -10944.3134765625
Iteration 19600: Loss = -10944.3134765625
Iteration 19700: Loss = -10944.3125
Iteration 19800: Loss = -10944.3134765625
1
Iteration 19900: Loss = -10944.3125
Iteration 20000: Loss = -10944.3125
Iteration 20100: Loss = -10944.3115234375
Iteration 20200: Loss = -10944.3125
1
Iteration 20300: Loss = -10944.3125
2
Iteration 20400: Loss = -10944.3115234375
Iteration 20500: Loss = -10944.3125
1
Iteration 20600: Loss = -10944.3115234375
Iteration 20700: Loss = -10944.3115234375
Iteration 20800: Loss = -10944.3125
1
Iteration 20900: Loss = -10944.3125
2
Iteration 21000: Loss = -10944.3125
3
Iteration 21100: Loss = -10944.3115234375
Iteration 21200: Loss = -10944.310546875
Iteration 21300: Loss = -10944.3125
1
Iteration 21400: Loss = -10944.3125
2
Iteration 21500: Loss = -10944.3115234375
3
Iteration 21600: Loss = -10944.3115234375
4
Iteration 21700: Loss = -10944.310546875
Iteration 21800: Loss = -10944.310546875
Iteration 21900: Loss = -10944.310546875
Iteration 22000: Loss = -10944.3115234375
1
Iteration 22100: Loss = -10944.310546875
Iteration 22200: Loss = -10944.0908203125
Iteration 22300: Loss = -10943.84375
Iteration 22400: Loss = -10943.7275390625
Iteration 22500: Loss = -10943.55859375
Iteration 22600: Loss = -10943.5576171875
Iteration 22700: Loss = -10943.560546875
1
Iteration 22800: Loss = -10943.5556640625
Iteration 22900: Loss = -10943.3125
Iteration 23000: Loss = -10943.3125
Iteration 23100: Loss = -10943.259765625
Iteration 23200: Loss = -10943.220703125
Iteration 23300: Loss = -10943.185546875
Iteration 23400: Loss = -10943.0654296875
Iteration 23500: Loss = -10942.9765625
Iteration 23600: Loss = -10942.7998046875
Iteration 23700: Loss = -10942.5634765625
Iteration 23800: Loss = -10942.5322265625
Iteration 23900: Loss = -10942.4990234375
Iteration 24000: Loss = -10942.1904296875
Iteration 24100: Loss = -10942.1572265625
Iteration 24200: Loss = -10941.8896484375
Iteration 24300: Loss = -10941.7333984375
Iteration 24400: Loss = -10941.5166015625
Iteration 24500: Loss = -10941.32421875
Iteration 24600: Loss = -10941.23046875
Iteration 24700: Loss = -10941.1396484375
Iteration 24800: Loss = -10940.8701171875
Iteration 24900: Loss = -10940.6240234375
Iteration 25000: Loss = -10940.009765625
Iteration 25100: Loss = -10939.5126953125
Iteration 25200: Loss = -10939.4501953125
Iteration 25300: Loss = -10939.380859375
Iteration 25400: Loss = -10939.3046875
Iteration 25500: Loss = -10939.1708984375
Iteration 25600: Loss = -10939.1708984375
Iteration 25700: Loss = -10939.1689453125
Iteration 25800: Loss = -10939.1650390625
Iteration 25900: Loss = -10939.1650390625
Iteration 26000: Loss = -10939.1650390625
Iteration 26100: Loss = -10939.1650390625
Iteration 26200: Loss = -10939.1640625
Iteration 26300: Loss = -10939.1650390625
1
Iteration 26400: Loss = -10939.1650390625
2
Iteration 26500: Loss = -10939.1650390625
3
Iteration 26600: Loss = -10939.1640625
Iteration 26700: Loss = -10939.083984375
Iteration 26800: Loss = -10939.0830078125
Iteration 26900: Loss = -10939.083984375
1
Iteration 27000: Loss = -10939.083984375
2
Iteration 27100: Loss = -10939.0849609375
3
Iteration 27200: Loss = -10939.083984375
4
Iteration 27300: Loss = -10939.083984375
5
Iteration 27400: Loss = -10939.083984375
6
Iteration 27500: Loss = -10939.0830078125
Iteration 27600: Loss = -10939.08203125
Iteration 27700: Loss = -10939.0830078125
1
Iteration 27800: Loss = -10939.0830078125
2
Iteration 27900: Loss = -10939.083984375
3
Iteration 28000: Loss = -10939.0849609375
4
Iteration 28100: Loss = -10939.083984375
5
Iteration 28200: Loss = -10939.08203125
Iteration 28300: Loss = -10939.083984375
1
Iteration 28400: Loss = -10939.0830078125
2
Iteration 28500: Loss = -10939.083984375
3
Iteration 28600: Loss = -10939.0830078125
4
Iteration 28700: Loss = -10939.0830078125
5
Iteration 28800: Loss = -10939.0830078125
6
Iteration 28900: Loss = -10939.083984375
7
Iteration 29000: Loss = -10939.0849609375
8
Iteration 29100: Loss = -10939.0830078125
9
Iteration 29200: Loss = -10939.0830078125
10
Iteration 29300: Loss = -10939.083984375
11
Iteration 29400: Loss = -10939.083984375
12
Iteration 29500: Loss = -10939.083984375
13
Iteration 29600: Loss = -10939.0830078125
14
Iteration 29700: Loss = -10939.0830078125
15
Stopping early at iteration 29700 due to no improvement.
pi: tensor([[8.6771e-07, 1.0000e+00],
        [1.1396e-01, 8.8604e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2016, 0.7984], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2729, 0.0864],
         [0.9912, 0.1713]],

        [[0.0806, 0.0790],
         [0.9922, 0.6536]],

        [[0.9886, 0.2127],
         [0.7540, 0.0080]],

        [[0.6914, 0.1463],
         [0.7385, 0.3829]],

        [[0.9753, 0.0985],
         [0.0198, 0.4291]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 72
Adjusted Rand Index: 0.18545454545454546
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.08798777480938783
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.009791027327195674
Global Adjusted Rand Index: 0.02856469595288699
Average Adjusted Rand Index: 0.05456118408579584
[3.35429808991111e-05, 0.02856469595288699] [0.0007893444549905863, 0.05456118408579584] [10949.7060546875, 10939.0830078125]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11024.894890849544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27720.3828125
Iteration 100: Loss = -18130.935546875
Iteration 200: Loss = -12637.193359375
Iteration 300: Loss = -11418.361328125
Iteration 400: Loss = -11243.2958984375
Iteration 500: Loss = -11176.8564453125
Iteration 600: Loss = -11136.4111328125
Iteration 700: Loss = -11112.2060546875
Iteration 800: Loss = -11098.6142578125
Iteration 900: Loss = -11089.2919921875
Iteration 1000: Loss = -11082.546875
Iteration 1100: Loss = -11077.47265625
Iteration 1200: Loss = -11073.541015625
Iteration 1300: Loss = -11070.4228515625
Iteration 1400: Loss = -11067.8994140625
Iteration 1500: Loss = -11065.8232421875
Iteration 1600: Loss = -11064.0947265625
Iteration 1700: Loss = -11062.638671875
Iteration 1800: Loss = -11061.3974609375
Iteration 1900: Loss = -11060.3271484375
Iteration 2000: Loss = -11059.3994140625
Iteration 2100: Loss = -11058.591796875
Iteration 2200: Loss = -11057.87890625
Iteration 2300: Loss = -11057.2470703125
Iteration 2400: Loss = -11056.6875
Iteration 2500: Loss = -11056.185546875
Iteration 2600: Loss = -11055.7333984375
Iteration 2700: Loss = -11055.32421875
Iteration 2800: Loss = -11054.9521484375
Iteration 2900: Loss = -11054.6142578125
Iteration 3000: Loss = -11054.302734375
Iteration 3100: Loss = -11054.017578125
Iteration 3200: Loss = -11053.7529296875
Iteration 3300: Loss = -11053.5107421875
Iteration 3400: Loss = -11053.287109375
Iteration 3500: Loss = -11053.076171875
Iteration 3600: Loss = -11052.8828125
Iteration 3700: Loss = -11052.7001953125
Iteration 3800: Loss = -11052.53125
Iteration 3900: Loss = -11052.3701171875
Iteration 4000: Loss = -11052.21875
Iteration 4100: Loss = -11052.076171875
Iteration 4200: Loss = -11051.94140625
Iteration 4300: Loss = -11051.81640625
Iteration 4400: Loss = -11051.69921875
Iteration 4500: Loss = -11051.5888671875
Iteration 4600: Loss = -11051.4873046875
Iteration 4700: Loss = -11051.388671875
Iteration 4800: Loss = -11051.296875
Iteration 4900: Loss = -11051.212890625
Iteration 5000: Loss = -11051.1298828125
Iteration 5100: Loss = -11051.052734375
Iteration 5200: Loss = -11050.9775390625
Iteration 5300: Loss = -11050.9052734375
Iteration 5400: Loss = -11050.833984375
Iteration 5500: Loss = -11050.767578125
Iteration 5600: Loss = -11050.7021484375
Iteration 5700: Loss = -11050.63671875
Iteration 5800: Loss = -11050.5703125
Iteration 5900: Loss = -11050.5068359375
Iteration 6000: Loss = -11050.443359375
Iteration 6100: Loss = -11050.3779296875
Iteration 6200: Loss = -11050.3095703125
Iteration 6300: Loss = -11050.2421875
Iteration 6400: Loss = -11050.171875
Iteration 6500: Loss = -11050.09765625
Iteration 6600: Loss = -11050.0234375
Iteration 6700: Loss = -11049.9453125
Iteration 6800: Loss = -11049.8623046875
Iteration 6900: Loss = -11049.7802734375
Iteration 7000: Loss = -11049.689453125
Iteration 7100: Loss = -11049.599609375
Iteration 7200: Loss = -11049.5087890625
Iteration 7300: Loss = -11049.412109375
Iteration 7400: Loss = -11049.3154296875
Iteration 7500: Loss = -11049.2197265625
Iteration 7600: Loss = -11049.125
Iteration 7700: Loss = -11049.0302734375
Iteration 7800: Loss = -11048.9404296875
Iteration 7900: Loss = -11048.8544921875
Iteration 8000: Loss = -11048.7744140625
Iteration 8100: Loss = -11048.6982421875
Iteration 8200: Loss = -11048.630859375
Iteration 8300: Loss = -11048.5712890625
Iteration 8400: Loss = -11048.5166015625
Iteration 8500: Loss = -11048.466796875
Iteration 8600: Loss = -11048.4208984375
Iteration 8700: Loss = -11048.3798828125
Iteration 8800: Loss = -11048.337890625
Iteration 8900: Loss = -11048.29296875
Iteration 9000: Loss = -11048.236328125
Iteration 9100: Loss = -11048.1611328125
Iteration 9200: Loss = -11048.0458984375
Iteration 9300: Loss = -11047.890625
Iteration 9400: Loss = -11047.666015625
Iteration 9500: Loss = -11047.5751953125
Iteration 9600: Loss = -11047.5380859375
Iteration 9700: Loss = -11047.517578125
Iteration 9800: Loss = -11047.501953125
Iteration 9900: Loss = -11047.4931640625
Iteration 10000: Loss = -11047.486328125
Iteration 10100: Loss = -11047.4814453125
Iteration 10200: Loss = -11047.474609375
Iteration 10300: Loss = -11047.46875
Iteration 10400: Loss = -11047.4658203125
Iteration 10500: Loss = -11047.4599609375
Iteration 10600: Loss = -11047.4599609375
Iteration 10700: Loss = -11047.45703125
Iteration 10800: Loss = -11047.4560546875
Iteration 10900: Loss = -11047.4521484375
Iteration 11000: Loss = -11047.451171875
Iteration 11100: Loss = -11047.4501953125
Iteration 11200: Loss = -11047.447265625
Iteration 11300: Loss = -11047.4453125
Iteration 11400: Loss = -11047.443359375
Iteration 11500: Loss = -11047.44140625
Iteration 11600: Loss = -11047.4404296875
Iteration 11700: Loss = -11047.439453125
Iteration 11800: Loss = -11047.4375
Iteration 11900: Loss = -11047.439453125
1
Iteration 12000: Loss = -11047.4365234375
Iteration 12100: Loss = -11047.4345703125
Iteration 12200: Loss = -11047.4326171875
Iteration 12300: Loss = -11047.4326171875
Iteration 12400: Loss = -11047.4326171875
Iteration 12500: Loss = -11047.4306640625
Iteration 12600: Loss = -11047.4306640625
Iteration 12700: Loss = -11047.4296875
Iteration 12800: Loss = -11047.427734375
Iteration 12900: Loss = -11047.427734375
Iteration 13000: Loss = -11047.4267578125
Iteration 13100: Loss = -11047.4267578125
Iteration 13200: Loss = -11047.42578125
Iteration 13300: Loss = -11047.4248046875
Iteration 13400: Loss = -11047.4248046875
Iteration 13500: Loss = -11047.423828125
Iteration 13600: Loss = -11047.4228515625
Iteration 13700: Loss = -11047.421875
Iteration 13800: Loss = -11047.421875
Iteration 13900: Loss = -11047.421875
Iteration 14000: Loss = -11047.421875
Iteration 14100: Loss = -11047.419921875
Iteration 14200: Loss = -11047.4208984375
1
Iteration 14300: Loss = -11047.41796875
Iteration 14400: Loss = -11047.4189453125
1
Iteration 14500: Loss = -11047.419921875
2
Iteration 14600: Loss = -11047.4208984375
3
Iteration 14700: Loss = -11047.4189453125
4
Iteration 14800: Loss = -11047.4189453125
5
Iteration 14900: Loss = -11047.4189453125
6
Iteration 15000: Loss = -11047.419921875
7
Iteration 15100: Loss = -11047.4169921875
Iteration 15200: Loss = -11047.41796875
1
Iteration 15300: Loss = -11047.4169921875
Iteration 15400: Loss = -11047.416015625
Iteration 15500: Loss = -11047.4169921875
1
Iteration 15600: Loss = -11047.416015625
Iteration 15700: Loss = -11047.4150390625
Iteration 15800: Loss = -11047.416015625
1
Iteration 15900: Loss = -11047.416015625
2
Iteration 16000: Loss = -11047.416015625
3
Iteration 16100: Loss = -11047.4169921875
4
Iteration 16200: Loss = -11047.416015625
5
Iteration 16300: Loss = -11047.4150390625
Iteration 16400: Loss = -11047.416015625
1
Iteration 16500: Loss = -11047.4150390625
Iteration 16600: Loss = -11047.4169921875
1
Iteration 16700: Loss = -11047.4150390625
Iteration 16800: Loss = -11047.4150390625
Iteration 16900: Loss = -11047.4150390625
Iteration 17000: Loss = -11047.4150390625
Iteration 17100: Loss = -11047.4150390625
Iteration 17200: Loss = -11047.4140625
Iteration 17300: Loss = -11047.4130859375
Iteration 17400: Loss = -11047.4140625
1
Iteration 17500: Loss = -11047.4140625
2
Iteration 17600: Loss = -11047.4140625
3
Iteration 17700: Loss = -11047.4130859375
Iteration 17800: Loss = -11047.4140625
1
Iteration 17900: Loss = -11047.4150390625
2
Iteration 18000: Loss = -11047.4150390625
3
Iteration 18100: Loss = -11047.4140625
4
Iteration 18200: Loss = -11047.4140625
5
Iteration 18300: Loss = -11047.4130859375
Iteration 18400: Loss = -11047.4140625
1
Iteration 18500: Loss = -11047.4130859375
Iteration 18600: Loss = -11047.4140625
1
Iteration 18700: Loss = -11047.412109375
Iteration 18800: Loss = -11047.4130859375
1
Iteration 18900: Loss = -11047.4140625
2
Iteration 19000: Loss = -11047.4140625
3
Iteration 19100: Loss = -11047.4130859375
4
Iteration 19200: Loss = -11047.412109375
Iteration 19300: Loss = -11047.412109375
Iteration 19400: Loss = -11047.4130859375
1
Iteration 19500: Loss = -11047.4111328125
Iteration 19600: Loss = -11047.4130859375
1
Iteration 19700: Loss = -11047.4130859375
2
Iteration 19800: Loss = -11047.4130859375
3
Iteration 19900: Loss = -11047.4130859375
4
Iteration 20000: Loss = -11047.4140625
5
Iteration 20100: Loss = -11047.4140625
6
Iteration 20200: Loss = -11047.4130859375
7
Iteration 20300: Loss = -11047.4140625
8
Iteration 20400: Loss = -11047.4140625
9
Iteration 20500: Loss = -11047.412109375
10
Iteration 20600: Loss = -11047.412109375
11
Iteration 20700: Loss = -11047.4130859375
12
Iteration 20800: Loss = -11047.4130859375
13
Iteration 20900: Loss = -11047.4140625
14
Iteration 21000: Loss = -11047.4130859375
15
Stopping early at iteration 21000 due to no improvement.
pi: tensor([[8.7905e-01, 1.2095e-01],
        [9.9969e-01, 3.1189e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 7.7455e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1693, 0.2431],
         [0.2107, 0.1282]],

        [[0.1192, 0.1075],
         [0.0119, 0.9459]],

        [[0.9225, 0.1647],
         [0.5691, 0.1552]],

        [[0.0175, 0.1526],
         [0.2955, 0.2632]],

        [[0.9648, 0.1402],
         [0.0699, 0.0951]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.003422492374587702
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0007493607032682356
Average Adjusted Rand Index: -0.0006844984749175404
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42686.7890625
Iteration 100: Loss = -23513.630859375
Iteration 200: Loss = -13597.61328125
Iteration 300: Loss = -11844.8330078125
Iteration 400: Loss = -11539.52734375
Iteration 500: Loss = -11396.5517578125
Iteration 600: Loss = -11308.578125
Iteration 700: Loss = -11223.5986328125
Iteration 800: Loss = -11182.34765625
Iteration 900: Loss = -11153.9716796875
Iteration 1000: Loss = -11139.8310546875
Iteration 1100: Loss = -11129.0234375
Iteration 1200: Loss = -11119.6806640625
Iteration 1300: Loss = -11112.056640625
Iteration 1400: Loss = -11106.15625
Iteration 1500: Loss = -11100.8876953125
Iteration 1600: Loss = -11090.8427734375
Iteration 1700: Loss = -11086.525390625
Iteration 1800: Loss = -11083.880859375
Iteration 1900: Loss = -11081.8369140625
Iteration 2000: Loss = -11080.123046875
Iteration 2100: Loss = -11072.02734375
Iteration 2200: Loss = -11069.615234375
Iteration 2300: Loss = -11068.107421875
Iteration 2400: Loss = -11066.9091796875
Iteration 2500: Loss = -11065.8984375
Iteration 2600: Loss = -11065.01953125
Iteration 2700: Loss = -11064.23828125
Iteration 2800: Loss = -11063.5458984375
Iteration 2900: Loss = -11062.9306640625
Iteration 3000: Loss = -11062.3759765625
Iteration 3100: Loss = -11061.8740234375
Iteration 3200: Loss = -11061.4130859375
Iteration 3300: Loss = -11060.9912109375
Iteration 3400: Loss = -11060.6044921875
Iteration 3500: Loss = -11060.2490234375
Iteration 3600: Loss = -11059.9208984375
Iteration 3700: Loss = -11059.6162109375
Iteration 3800: Loss = -11059.3359375
Iteration 3900: Loss = -11059.0732421875
Iteration 4000: Loss = -11058.83203125
Iteration 4100: Loss = -11058.603515625
Iteration 4200: Loss = -11058.390625
Iteration 4300: Loss = -11058.18359375
Iteration 4400: Loss = -11057.98828125
Iteration 4500: Loss = -11057.81640625
Iteration 4600: Loss = -11057.654296875
Iteration 4700: Loss = -11057.5048828125
Iteration 4800: Loss = -11057.36328125
Iteration 4900: Loss = -11057.23046875
Iteration 5000: Loss = -11056.8095703125
Iteration 5100: Loss = -11053.345703125
Iteration 5200: Loss = -11053.1044921875
Iteration 5300: Loss = -11052.9404296875
Iteration 5400: Loss = -11052.8076171875
Iteration 5500: Loss = -11052.693359375
Iteration 5600: Loss = -11052.59375
Iteration 5700: Loss = -11052.501953125
Iteration 5800: Loss = -11052.4208984375
Iteration 5900: Loss = -11052.34375
Iteration 6000: Loss = -11052.2744140625
Iteration 6100: Loss = -11052.208984375
Iteration 6200: Loss = -11052.146484375
Iteration 6300: Loss = -11052.0888671875
Iteration 6400: Loss = -11052.0322265625
Iteration 6500: Loss = -11051.982421875
Iteration 6600: Loss = -11051.931640625
Iteration 6700: Loss = -11051.8876953125
Iteration 6800: Loss = -11051.845703125
Iteration 6900: Loss = -11051.8046875
Iteration 7000: Loss = -11051.765625
Iteration 7100: Loss = -11051.73046875
Iteration 7200: Loss = -11051.6953125
Iteration 7300: Loss = -11051.66015625
Iteration 7400: Loss = -11051.6318359375
Iteration 7500: Loss = -11051.6005859375
Iteration 7600: Loss = -11051.5732421875
Iteration 7700: Loss = -11051.544921875
Iteration 7800: Loss = -11051.51953125
Iteration 7900: Loss = -11051.49609375
Iteration 8000: Loss = -11051.4736328125
Iteration 8100: Loss = -11051.4521484375
Iteration 8200: Loss = -11051.431640625
Iteration 8300: Loss = -11051.412109375
Iteration 8400: Loss = -11051.39453125
Iteration 8500: Loss = -11051.3759765625
Iteration 8600: Loss = -11051.359375
Iteration 8700: Loss = -11051.3427734375
Iteration 8800: Loss = -11051.328125
Iteration 8900: Loss = -11051.3125
Iteration 9000: Loss = -11051.30078125
Iteration 9100: Loss = -11051.287109375
Iteration 9200: Loss = -11051.2744140625
Iteration 9300: Loss = -11051.2626953125
Iteration 9400: Loss = -11051.2509765625
Iteration 9500: Loss = -11051.2412109375
Iteration 9600: Loss = -11051.2294921875
Iteration 9700: Loss = -11051.220703125
Iteration 9800: Loss = -11051.2099609375
Iteration 9900: Loss = -11051.2021484375
Iteration 10000: Loss = -11051.193359375
Iteration 10100: Loss = -11051.185546875
Iteration 10200: Loss = -11051.1767578125
Iteration 10300: Loss = -11051.1708984375
Iteration 10400: Loss = -11051.1630859375
Iteration 10500: Loss = -11051.1572265625
Iteration 10600: Loss = -11051.1494140625
Iteration 10700: Loss = -11051.1435546875
Iteration 10800: Loss = -11051.13671875
Iteration 10900: Loss = -11051.1318359375
Iteration 11000: Loss = -11051.1279296875
Iteration 11100: Loss = -11051.1201171875
Iteration 11200: Loss = -11051.115234375
Iteration 11300: Loss = -11051.111328125
Iteration 11400: Loss = -11051.107421875
Iteration 11500: Loss = -11051.1025390625
Iteration 11600: Loss = -11051.0986328125
Iteration 11700: Loss = -11051.0947265625
Iteration 11800: Loss = -11051.091796875
Iteration 11900: Loss = -11051.087890625
Iteration 12000: Loss = -11051.083984375
Iteration 12100: Loss = -11051.080078125
Iteration 12200: Loss = -11051.0771484375
Iteration 12300: Loss = -11051.076171875
Iteration 12400: Loss = -11051.0732421875
Iteration 12500: Loss = -11051.0693359375
Iteration 12600: Loss = -11051.06640625
Iteration 12700: Loss = -11051.0634765625
Iteration 12800: Loss = -11051.0634765625
Iteration 12900: Loss = -11051.060546875
Iteration 13000: Loss = -11051.05859375
Iteration 13100: Loss = -11051.056640625
Iteration 13200: Loss = -11051.0556640625
Iteration 13300: Loss = -11051.0546875
Iteration 13400: Loss = -11051.05078125
Iteration 13500: Loss = -11051.05078125
Iteration 13600: Loss = -11051.0478515625
Iteration 13700: Loss = -11051.0458984375
Iteration 13800: Loss = -11051.046875
1
Iteration 13900: Loss = -11051.044921875
Iteration 14000: Loss = -11051.044921875
Iteration 14100: Loss = -11051.0419921875
Iteration 14200: Loss = -11051.0419921875
Iteration 14300: Loss = -11051.0400390625
Iteration 14400: Loss = -11051.0419921875
1
Iteration 14500: Loss = -11051.0380859375
Iteration 14600: Loss = -11051.0390625
1
Iteration 14700: Loss = -11051.037109375
Iteration 14800: Loss = -11051.0361328125
Iteration 14900: Loss = -11051.0361328125
Iteration 15000: Loss = -11051.0341796875
Iteration 15100: Loss = -11051.0341796875
Iteration 15200: Loss = -11051.03125
Iteration 15300: Loss = -11051.0322265625
1
Iteration 15400: Loss = -11051.0302734375
Iteration 15500: Loss = -11051.0302734375
Iteration 15600: Loss = -11051.0302734375
Iteration 15700: Loss = -11051.029296875
Iteration 15800: Loss = -11051.0283203125
Iteration 15900: Loss = -11051.0283203125
Iteration 16000: Loss = -11051.02734375
Iteration 16100: Loss = -11051.025390625
Iteration 16200: Loss = -11051.025390625
Iteration 16300: Loss = -11051.0244140625
Iteration 16400: Loss = -11051.021484375
Iteration 16500: Loss = -11051.0185546875
Iteration 16600: Loss = -11051.015625
Iteration 16700: Loss = -11051.01171875
Iteration 16800: Loss = -11051.009765625
Iteration 16900: Loss = -11051.0087890625
Iteration 17000: Loss = -11051.00390625
Iteration 17100: Loss = -11050.9501953125
Iteration 17200: Loss = -11050.92578125
Iteration 17300: Loss = -11050.8876953125
Iteration 17400: Loss = -11050.7822265625
Iteration 17500: Loss = -11050.2880859375
Iteration 17600: Loss = -11049.9130859375
Iteration 17700: Loss = -11049.1005859375
Iteration 17800: Loss = -11048.9873046875
Iteration 17900: Loss = -11048.9697265625
Iteration 18000: Loss = -11048.85546875
Iteration 18100: Loss = -11048.78515625
Iteration 18200: Loss = -11048.76171875
Iteration 18300: Loss = -11048.7119140625
Iteration 18400: Loss = -11048.603515625
Iteration 18500: Loss = -11048.498046875
Iteration 18600: Loss = -11048.4853515625
Iteration 18700: Loss = -11048.4150390625
Iteration 18800: Loss = -11048.369140625
Iteration 18900: Loss = -11048.2578125
Iteration 19000: Loss = -11047.9970703125
Iteration 19100: Loss = -11047.9814453125
Iteration 19200: Loss = -11047.9169921875
Iteration 19300: Loss = -11047.8994140625
Iteration 19400: Loss = -11047.8759765625
Iteration 19500: Loss = -11047.8349609375
Iteration 19600: Loss = -11047.8154296875
Iteration 19700: Loss = -11047.7822265625
Iteration 19800: Loss = -11047.763671875
Iteration 19900: Loss = -11047.728515625
Iteration 20000: Loss = -11047.70703125
Iteration 20100: Loss = -11047.6943359375
Iteration 20200: Loss = -11047.6884765625
Iteration 20300: Loss = -11047.6875
Iteration 20400: Loss = -11047.6787109375
Iteration 20500: Loss = -11047.6552734375
Iteration 20600: Loss = -11047.6484375
Iteration 20700: Loss = -11047.650390625
1
Iteration 20800: Loss = -11047.6474609375
Iteration 20900: Loss = -11047.6484375
1
Iteration 21000: Loss = -11047.6474609375
Iteration 21100: Loss = -11047.6474609375
Iteration 21200: Loss = -11047.6484375
1
Iteration 21300: Loss = -11047.6474609375
Iteration 21400: Loss = -11047.6484375
1
Iteration 21500: Loss = -11047.6484375
2
Iteration 21600: Loss = -11047.6474609375
Iteration 21700: Loss = -11047.646484375
Iteration 21800: Loss = -11047.646484375
Iteration 21900: Loss = -11047.6484375
1
Iteration 22000: Loss = -11047.6474609375
2
Iteration 22100: Loss = -11047.646484375
Iteration 22200: Loss = -11047.6484375
1
Iteration 22300: Loss = -11047.6474609375
2
Iteration 22400: Loss = -11047.6474609375
3
Iteration 22500: Loss = -11047.6474609375
4
Iteration 22600: Loss = -11047.6396484375
Iteration 22700: Loss = -11047.638671875
Iteration 22800: Loss = -11047.6376953125
Iteration 22900: Loss = -11047.638671875
1
Iteration 23000: Loss = -11047.6376953125
Iteration 23100: Loss = -11047.6396484375
1
Iteration 23200: Loss = -11047.6376953125
Iteration 23300: Loss = -11047.6376953125
Iteration 23400: Loss = -11047.638671875
1
Iteration 23500: Loss = -11047.6396484375
2
Iteration 23600: Loss = -11047.6396484375
3
Iteration 23700: Loss = -11047.6376953125
Iteration 23800: Loss = -11047.638671875
1
Iteration 23900: Loss = -11047.6376953125
Iteration 24000: Loss = -11047.63671875
Iteration 24100: Loss = -11047.6357421875
Iteration 24200: Loss = -11047.63671875
1
Iteration 24300: Loss = -11047.63671875
2
Iteration 24400: Loss = -11047.63671875
3
Iteration 24500: Loss = -11047.6357421875
Iteration 24600: Loss = -11047.638671875
1
Iteration 24700: Loss = -11047.6376953125
2
Iteration 24800: Loss = -11047.63671875
3
Iteration 24900: Loss = -11047.6357421875
Iteration 25000: Loss = -11047.6357421875
Iteration 25100: Loss = -11047.6357421875
Iteration 25200: Loss = -11047.634765625
Iteration 25300: Loss = -11047.6376953125
1
Iteration 25400: Loss = -11047.63671875
2
Iteration 25500: Loss = -11047.6357421875
3
Iteration 25600: Loss = -11047.6357421875
4
Iteration 25700: Loss = -11047.6357421875
5
Iteration 25800: Loss = -11047.63671875
6
Iteration 25900: Loss = -11047.6357421875
7
Iteration 26000: Loss = -11047.63671875
8
Iteration 26100: Loss = -11047.6357421875
9
Iteration 26200: Loss = -11047.6357421875
10
Iteration 26300: Loss = -11047.63671875
11
Iteration 26400: Loss = -11047.6357421875
12
Iteration 26500: Loss = -11047.6357421875
13
Iteration 26600: Loss = -11047.63671875
14
Iteration 26700: Loss = -11047.63671875
15
Stopping early at iteration 26700 due to no improvement.
pi: tensor([[1.4758e-05, 9.9999e-01],
        [5.7703e-02, 9.4230e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1494, 0.8506], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2359, 0.1902],
         [0.7945, 0.1605]],

        [[0.8272, 0.0935],
         [0.0709, 0.5903]],

        [[0.9112, 0.1998],
         [0.9808, 0.0097]],

        [[0.5395, 0.2168],
         [0.9856, 0.0549]],

        [[0.5458, 0.2046],
         [0.9931, 0.9564]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00021110461213990435
Average Adjusted Rand Index: -0.0009070175532987862
[0.0007493607032682356, 0.00021110461213990435] [-0.0006844984749175404, -0.0009070175532987862] [11047.4130859375, 11047.63671875]
-------------------------------------
This iteration is 8
True Objective function: Loss = -10909.16611699616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24505.55078125
Iteration 100: Loss = -16814.05859375
Iteration 200: Loss = -12410.3046875
Iteration 300: Loss = -11486.19921875
Iteration 400: Loss = -11261.494140625
Iteration 500: Loss = -11187.775390625
Iteration 600: Loss = -11150.8212890625
Iteration 700: Loss = -11128.533203125
Iteration 800: Loss = -11112.4951171875
Iteration 900: Loss = -11100.0263671875
Iteration 1000: Loss = -11089.5380859375
Iteration 1100: Loss = -11081.1474609375
Iteration 1200: Loss = -11075.2275390625
Iteration 1300: Loss = -11070.5830078125
Iteration 1400: Loss = -11065.36328125
Iteration 1500: Loss = -11059.8671875
Iteration 1600: Loss = -11056.03515625
Iteration 1700: Loss = -11051.2734375
Iteration 1800: Loss = -11047.0537109375
Iteration 1900: Loss = -11043.83203125
Iteration 2000: Loss = -11040.8984375
Iteration 2100: Loss = -11037.1552734375
Iteration 2200: Loss = -11034.7646484375
Iteration 2300: Loss = -11032.7685546875
Iteration 2400: Loss = -11030.869140625
Iteration 2500: Loss = -11029.162109375
Iteration 2600: Loss = -11027.1005859375
Iteration 2700: Loss = -11024.8798828125
Iteration 2800: Loss = -11023.15234375
Iteration 2900: Loss = -11021.603515625
Iteration 3000: Loss = -11020.0634765625
Iteration 3100: Loss = -11018.5244140625
Iteration 3200: Loss = -11017.177734375
Iteration 3300: Loss = -11016.0712890625
Iteration 3400: Loss = -11015.125
Iteration 3500: Loss = -11014.4638671875
Iteration 3600: Loss = -11013.97265625
Iteration 3700: Loss = -11013.58984375
Iteration 3800: Loss = -11013.271484375
Iteration 3900: Loss = -11012.9912109375
Iteration 4000: Loss = -11012.7197265625
Iteration 4100: Loss = -11012.365234375
Iteration 4200: Loss = -11011.56640625
Iteration 4300: Loss = -11010.8359375
Iteration 4400: Loss = -11010.4794921875
Iteration 4500: Loss = -11010.244140625
Iteration 4600: Loss = -11010.060546875
Iteration 4700: Loss = -11009.90234375
Iteration 4800: Loss = -11009.7607421875
Iteration 4900: Loss = -11009.6298828125
Iteration 5000: Loss = -11009.5048828125
Iteration 5100: Loss = -11009.369140625
Iteration 5200: Loss = -11009.1884765625
Iteration 5300: Loss = -11008.189453125
Iteration 5400: Loss = -11005.6123046875
Iteration 5500: Loss = -11005.0322265625
Iteration 5600: Loss = -11004.7666015625
Iteration 5700: Loss = -11004.6005859375
Iteration 5800: Loss = -11004.484375
Iteration 5900: Loss = -11004.3955078125
Iteration 6000: Loss = -11004.3203125
Iteration 6100: Loss = -11004.2568359375
Iteration 6200: Loss = -11004.19921875
Iteration 6300: Loss = -11004.146484375
Iteration 6400: Loss = -11004.0927734375
Iteration 6500: Loss = -11004.0439453125
Iteration 6600: Loss = -11003.998046875
Iteration 6700: Loss = -11003.9541015625
Iteration 6800: Loss = -11003.9140625
Iteration 6900: Loss = -11003.876953125
Iteration 7000: Loss = -11003.841796875
Iteration 7100: Loss = -11003.8095703125
Iteration 7200: Loss = -11003.77734375
Iteration 7300: Loss = -11003.74609375
Iteration 7400: Loss = -11003.7177734375
Iteration 7500: Loss = -11003.6953125
Iteration 7600: Loss = -11003.6748046875
Iteration 7700: Loss = -11003.654296875
Iteration 7800: Loss = -11003.6357421875
Iteration 7900: Loss = -11003.62109375
Iteration 8000: Loss = -11003.6064453125
Iteration 8100: Loss = -11003.5927734375
Iteration 8200: Loss = -11003.5810546875
Iteration 8300: Loss = -11003.5673828125
Iteration 8400: Loss = -11003.5556640625
Iteration 8500: Loss = -11003.546875
Iteration 8600: Loss = -11003.5361328125
Iteration 8700: Loss = -11003.5283203125
Iteration 8800: Loss = -11003.517578125
Iteration 8900: Loss = -11003.509765625
Iteration 9000: Loss = -11003.501953125
Iteration 9100: Loss = -11003.4931640625
Iteration 9200: Loss = -11003.48828125
Iteration 9300: Loss = -11003.4814453125
Iteration 9400: Loss = -11003.4755859375
Iteration 9500: Loss = -11003.4697265625
Iteration 9600: Loss = -11003.4638671875
Iteration 9700: Loss = -11003.458984375
Iteration 9800: Loss = -11003.4541015625
Iteration 9900: Loss = -11003.4501953125
Iteration 10000: Loss = -11003.4443359375
Iteration 10100: Loss = -11003.4404296875
Iteration 10200: Loss = -11003.4365234375
Iteration 10300: Loss = -11003.43359375
Iteration 10400: Loss = -11003.4306640625
Iteration 10500: Loss = -11003.42578125
Iteration 10600: Loss = -11003.4248046875
Iteration 10700: Loss = -11003.4208984375
Iteration 10800: Loss = -11003.41796875
Iteration 10900: Loss = -11003.4169921875
Iteration 11000: Loss = -11003.4140625
Iteration 11100: Loss = -11003.4111328125
Iteration 11200: Loss = -11003.3984375
Iteration 11300: Loss = -11003.3896484375
Iteration 11400: Loss = -11003.37109375
Iteration 11500: Loss = -11003.3671875
Iteration 11600: Loss = -11003.365234375
Iteration 11700: Loss = -11003.3623046875
Iteration 11800: Loss = -11003.3623046875
Iteration 11900: Loss = -11003.359375
Iteration 12000: Loss = -11003.357421875
Iteration 12100: Loss = -11003.35546875
Iteration 12200: Loss = -11003.3564453125
1
Iteration 12300: Loss = -11003.35546875
Iteration 12400: Loss = -11003.3525390625
Iteration 12500: Loss = -11003.349609375
Iteration 12600: Loss = -11003.333984375
Iteration 12700: Loss = -11003.33203125
Iteration 12800: Loss = -11003.3310546875
Iteration 12900: Loss = -11003.3291015625
Iteration 13000: Loss = -11003.3291015625
Iteration 13100: Loss = -11003.3291015625
Iteration 13200: Loss = -11003.3291015625
Iteration 13300: Loss = -11003.3271484375
Iteration 13400: Loss = -11003.328125
1
Iteration 13500: Loss = -11003.326171875
Iteration 13600: Loss = -11003.3251953125
Iteration 13700: Loss = -11003.3251953125
Iteration 13800: Loss = -11003.326171875
1
Iteration 13900: Loss = -11003.3232421875
Iteration 14000: Loss = -11003.3232421875
Iteration 14100: Loss = -11003.322265625
Iteration 14200: Loss = -11003.3232421875
1
Iteration 14300: Loss = -11003.3212890625
Iteration 14400: Loss = -11003.3203125
Iteration 14500: Loss = -11003.3203125
Iteration 14600: Loss = -11003.3203125
Iteration 14700: Loss = -11003.3203125
Iteration 14800: Loss = -11003.3193359375
Iteration 14900: Loss = -11003.3203125
1
Iteration 15000: Loss = -11003.3193359375
Iteration 15100: Loss = -11003.3173828125
Iteration 15200: Loss = -11003.3193359375
1
Iteration 15300: Loss = -11003.318359375
2
Iteration 15400: Loss = -11003.318359375
3
Iteration 15500: Loss = -11003.3173828125
Iteration 15600: Loss = -11003.3134765625
Iteration 15700: Loss = -11003.3134765625
Iteration 15800: Loss = -11003.314453125
1
Iteration 15900: Loss = -11003.3134765625
Iteration 16000: Loss = -11003.3125
Iteration 16100: Loss = -11003.3134765625
1
Iteration 16200: Loss = -11003.3125
Iteration 16300: Loss = -11003.3125
Iteration 16400: Loss = -11003.3125
Iteration 16500: Loss = -11003.3115234375
Iteration 16600: Loss = -11003.3115234375
Iteration 16700: Loss = -11003.3115234375
Iteration 16800: Loss = -11003.3115234375
Iteration 16900: Loss = -11003.3115234375
Iteration 17000: Loss = -11003.310546875
Iteration 17100: Loss = -11003.3115234375
1
Iteration 17200: Loss = -11003.3125
2
Iteration 17300: Loss = -11003.310546875
Iteration 17400: Loss = -11003.310546875
Iteration 17500: Loss = -11003.310546875
Iteration 17600: Loss = -11003.310546875
Iteration 17700: Loss = -11003.3125
1
Iteration 17800: Loss = -11003.310546875
Iteration 17900: Loss = -11003.310546875
Iteration 18000: Loss = -11003.310546875
Iteration 18100: Loss = -11003.3095703125
Iteration 18200: Loss = -11003.310546875
1
Iteration 18300: Loss = -11003.310546875
2
Iteration 18400: Loss = -11003.3095703125
Iteration 18500: Loss = -11003.310546875
1
Iteration 18600: Loss = -11003.3095703125
Iteration 18700: Loss = -11003.30859375
Iteration 18800: Loss = -11003.310546875
1
Iteration 18900: Loss = -11003.310546875
2
Iteration 19000: Loss = -11003.3095703125
3
Iteration 19100: Loss = -11003.3095703125
4
Iteration 19200: Loss = -11003.3095703125
5
Iteration 19300: Loss = -11003.3095703125
6
Iteration 19400: Loss = -11003.310546875
7
Iteration 19500: Loss = -11003.3095703125
8
Iteration 19600: Loss = -11003.310546875
9
Iteration 19700: Loss = -11003.3115234375
10
Iteration 19800: Loss = -11003.3095703125
11
Iteration 19900: Loss = -11003.3095703125
12
Iteration 20000: Loss = -11003.3115234375
13
Iteration 20100: Loss = -11003.3115234375
14
Iteration 20200: Loss = -11003.310546875
15
Stopping early at iteration 20200 due to no improvement.
pi: tensor([[4.5896e-07, 1.0000e+00],
        [2.0359e-02, 9.7964e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8276, 0.1724], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.1027],
         [0.3234, 0.1624]],

        [[0.0120, 0.7366],
         [0.2972, 0.9824]],

        [[0.9927, 0.2191],
         [0.6074, 0.9880]],

        [[0.0164, 0.1082],
         [0.4814, 0.9925]],

        [[0.9826, 0.1897],
         [0.3098, 0.0110]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.052993146457920554
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.006504298374695772
Average Adjusted Rand Index: 0.010598629291584111
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -49196.86328125
Iteration 100: Loss = -28256.916015625
Iteration 200: Loss = -14926.318359375
Iteration 300: Loss = -12119.9140625
Iteration 400: Loss = -11569.3154296875
Iteration 500: Loss = -11350.19921875
Iteration 600: Loss = -11249.4404296875
Iteration 700: Loss = -11182.3125
Iteration 800: Loss = -11137.70703125
Iteration 900: Loss = -11104.7138671875
Iteration 1000: Loss = -11084.13671875
Iteration 1100: Loss = -11072.775390625
Iteration 1200: Loss = -11053.392578125
Iteration 1300: Loss = -11045.4150390625
Iteration 1400: Loss = -11038.189453125
Iteration 1500: Loss = -11033.9697265625
Iteration 1600: Loss = -11030.91015625
Iteration 1700: Loss = -11028.4072265625
Iteration 1800: Loss = -11026.2919921875
Iteration 1900: Loss = -11024.474609375
Iteration 2000: Loss = -11022.8974609375
Iteration 2100: Loss = -11021.5126953125
Iteration 2200: Loss = -11020.2919921875
Iteration 2300: Loss = -11019.2041015625
Iteration 2400: Loss = -11018.2353515625
Iteration 2500: Loss = -11017.369140625
Iteration 2600: Loss = -11016.5908203125
Iteration 2700: Loss = -11015.88671875
Iteration 2800: Loss = -11015.25
Iteration 2900: Loss = -11014.669921875
Iteration 3000: Loss = -11014.14453125
Iteration 3100: Loss = -11013.6611328125
Iteration 3200: Loss = -11013.216796875
Iteration 3300: Loss = -11012.80859375
Iteration 3400: Loss = -11012.43359375
Iteration 3500: Loss = -11012.087890625
Iteration 3600: Loss = -11011.7666015625
Iteration 3700: Loss = -11011.466796875
Iteration 3800: Loss = -11011.1923828125
Iteration 3900: Loss = -11010.9365234375
Iteration 4000: Loss = -11010.69921875
Iteration 4100: Loss = -11010.478515625
Iteration 4200: Loss = -11010.271484375
Iteration 4300: Loss = -11010.0771484375
Iteration 4400: Loss = -11009.8955078125
Iteration 4500: Loss = -11009.728515625
Iteration 4600: Loss = -11009.5712890625
Iteration 4700: Loss = -11009.4228515625
Iteration 4800: Loss = -11009.2861328125
Iteration 4900: Loss = -11009.1552734375
Iteration 5000: Loss = -11009.0322265625
Iteration 5100: Loss = -11008.91796875
Iteration 5200: Loss = -11008.8076171875
Iteration 5300: Loss = -11008.7060546875
Iteration 5400: Loss = -11008.609375
Iteration 5500: Loss = -11008.5205078125
Iteration 5600: Loss = -11008.431640625
Iteration 5700: Loss = -11008.3486328125
Iteration 5800: Loss = -11008.263671875
Iteration 5900: Loss = -11008.1787109375
Iteration 6000: Loss = -11008.103515625
Iteration 6100: Loss = -11008.0380859375
Iteration 6200: Loss = -11007.978515625
Iteration 6300: Loss = -11007.921875
Iteration 6400: Loss = -11007.8671875
Iteration 6500: Loss = -11007.818359375
Iteration 6600: Loss = -11007.7734375
Iteration 6700: Loss = -11007.73046875
Iteration 6800: Loss = -11007.69140625
Iteration 6900: Loss = -11007.6533203125
Iteration 7000: Loss = -11007.62109375
Iteration 7100: Loss = -11007.587890625
Iteration 7200: Loss = -11007.5595703125
Iteration 7300: Loss = -11007.529296875
Iteration 7400: Loss = -11007.505859375
Iteration 7500: Loss = -11007.478515625
Iteration 7600: Loss = -11007.455078125
Iteration 7700: Loss = -11007.431640625
Iteration 7800: Loss = -11007.412109375
Iteration 7900: Loss = -11007.3896484375
Iteration 8000: Loss = -11007.3701171875
Iteration 8100: Loss = -11007.3525390625
Iteration 8200: Loss = -11007.3359375
Iteration 8300: Loss = -11007.3193359375
Iteration 8400: Loss = -11007.3037109375
Iteration 8500: Loss = -11007.2890625
Iteration 8600: Loss = -11007.2744140625
Iteration 8700: Loss = -11007.26171875
Iteration 8800: Loss = -11007.248046875
Iteration 8900: Loss = -11007.2373046875
Iteration 9000: Loss = -11007.2255859375
Iteration 9100: Loss = -11007.2138671875
Iteration 9200: Loss = -11007.203125
Iteration 9300: Loss = -11007.193359375
Iteration 9400: Loss = -11007.185546875
Iteration 9500: Loss = -11007.173828125
Iteration 9600: Loss = -11007.1669921875
Iteration 9700: Loss = -11007.16015625
Iteration 9800: Loss = -11007.1513671875
Iteration 9900: Loss = -11007.1435546875
Iteration 10000: Loss = -11007.13671875
Iteration 10100: Loss = -11007.130859375
Iteration 10200: Loss = -11007.123046875
Iteration 10300: Loss = -11007.1171875
Iteration 10400: Loss = -11007.1103515625
Iteration 10500: Loss = -11007.1064453125
Iteration 10600: Loss = -11007.1005859375
Iteration 10700: Loss = -11007.095703125
Iteration 10800: Loss = -11007.0908203125
Iteration 10900: Loss = -11007.0849609375
Iteration 11000: Loss = -11007.0810546875
Iteration 11100: Loss = -11007.0751953125
Iteration 11200: Loss = -11007.0712890625
Iteration 11300: Loss = -11007.0673828125
Iteration 11400: Loss = -11007.0615234375
Iteration 11500: Loss = -11007.044921875
Iteration 11600: Loss = -11006.89453125
Iteration 11700: Loss = -11006.845703125
Iteration 11800: Loss = -11006.818359375
Iteration 11900: Loss = -11006.802734375
Iteration 12000: Loss = -11006.7900390625
Iteration 12100: Loss = -11006.7783203125
Iteration 12200: Loss = -11006.7666015625
Iteration 12300: Loss = -11006.7568359375
Iteration 12400: Loss = -11006.744140625
Iteration 12500: Loss = -11006.732421875
Iteration 12600: Loss = -11006.7216796875
Iteration 12700: Loss = -11006.708984375
Iteration 12800: Loss = -11006.6962890625
Iteration 12900: Loss = -11006.6796875
Iteration 13000: Loss = -11006.662109375
Iteration 13100: Loss = -11006.640625
Iteration 13200: Loss = -11006.6162109375
Iteration 13300: Loss = -11006.5869140625
Iteration 13400: Loss = -11006.546875
Iteration 13500: Loss = -11006.498046875
Iteration 13600: Loss = -11006.4306640625
Iteration 13700: Loss = -11006.3349609375
Iteration 13800: Loss = -11006.1943359375
Iteration 13900: Loss = -11006.00390625
Iteration 14000: Loss = -11005.8046875
Iteration 14100: Loss = -11005.6728515625
Iteration 14200: Loss = -11005.6201171875
Iteration 14300: Loss = -11005.5947265625
Iteration 14400: Loss = -11005.5830078125
Iteration 14500: Loss = -11005.576171875
Iteration 14600: Loss = -11005.5693359375
Iteration 14700: Loss = -11005.56640625
Iteration 14800: Loss = -11005.5615234375
Iteration 14900: Loss = -11005.55859375
Iteration 15000: Loss = -11005.556640625
Iteration 15100: Loss = -11005.5546875
Iteration 15200: Loss = -11005.552734375
Iteration 15300: Loss = -11005.5498046875
Iteration 15400: Loss = -11005.55078125
1
Iteration 15500: Loss = -11005.5498046875
Iteration 15600: Loss = -11005.5498046875
Iteration 15700: Loss = -11005.548828125
Iteration 15800: Loss = -11005.548828125
Iteration 15900: Loss = -11005.5478515625
Iteration 16000: Loss = -11005.546875
Iteration 16100: Loss = -11005.546875
Iteration 16200: Loss = -11005.5458984375
Iteration 16300: Loss = -11005.5458984375
Iteration 16400: Loss = -11005.546875
1
Iteration 16500: Loss = -11005.5439453125
Iteration 16600: Loss = -11005.5439453125
Iteration 16700: Loss = -11005.5439453125
Iteration 16800: Loss = -11005.5439453125
Iteration 16900: Loss = -11005.5439453125
Iteration 17000: Loss = -11005.5439453125
Iteration 17100: Loss = -11005.5458984375
1
Iteration 17200: Loss = -11005.54296875
Iteration 17300: Loss = -11005.54296875
Iteration 17400: Loss = -11005.54296875
Iteration 17500: Loss = -11005.5419921875
Iteration 17600: Loss = -11005.5419921875
Iteration 17700: Loss = -11005.541015625
Iteration 17800: Loss = -11005.54296875
1
Iteration 17900: Loss = -11005.541015625
Iteration 18000: Loss = -11005.5419921875
1
Iteration 18100: Loss = -11005.5419921875
2
Iteration 18200: Loss = -11005.5419921875
3
Iteration 18300: Loss = -11005.541015625
Iteration 18400: Loss = -11005.541015625
Iteration 18500: Loss = -11005.5400390625
Iteration 18600: Loss = -11005.541015625
1
Iteration 18700: Loss = -11005.541015625
2
Iteration 18800: Loss = -11005.5400390625
Iteration 18900: Loss = -11005.5390625
Iteration 19000: Loss = -11005.541015625
1
Iteration 19100: Loss = -11005.5380859375
Iteration 19200: Loss = -11005.5400390625
1
Iteration 19300: Loss = -11005.5400390625
2
Iteration 19400: Loss = -11005.5390625
3
Iteration 19500: Loss = -11005.5390625
4
Iteration 19600: Loss = -11005.5400390625
5
Iteration 19700: Loss = -11005.5380859375
Iteration 19800: Loss = -11005.541015625
1
Iteration 19900: Loss = -11005.5390625
2
Iteration 20000: Loss = -11005.5390625
3
Iteration 20100: Loss = -11005.5380859375
Iteration 20200: Loss = -11005.5380859375
Iteration 20300: Loss = -11005.5380859375
Iteration 20400: Loss = -11005.5380859375
Iteration 20500: Loss = -11005.5390625
1
Iteration 20600: Loss = -11005.5390625
2
Iteration 20700: Loss = -11005.5380859375
Iteration 20800: Loss = -11005.5380859375
Iteration 20900: Loss = -11005.5390625
1
Iteration 21000: Loss = -11005.5380859375
Iteration 21100: Loss = -11005.537109375
Iteration 21200: Loss = -11005.537109375
Iteration 21300: Loss = -11005.5390625
1
Iteration 21400: Loss = -11005.5380859375
2
Iteration 21500: Loss = -11005.537109375
Iteration 21600: Loss = -11005.537109375
Iteration 21700: Loss = -11005.5390625
1
Iteration 21800: Loss = -11005.5380859375
2
Iteration 21900: Loss = -11005.537109375
Iteration 22000: Loss = -11005.5380859375
1
Iteration 22100: Loss = -11005.537109375
Iteration 22200: Loss = -11005.537109375
Iteration 22300: Loss = -11005.3828125
Iteration 22400: Loss = -11005.3291015625
Iteration 22500: Loss = -11005.2958984375
Iteration 22600: Loss = -11005.2861328125
Iteration 22700: Loss = -11005.2744140625
Iteration 22800: Loss = -11005.275390625
1
Iteration 22900: Loss = -11005.2734375
Iteration 23000: Loss = -11005.271484375
Iteration 23100: Loss = -11005.2724609375
1
Iteration 23200: Loss = -11005.2724609375
2
Iteration 23300: Loss = -11005.26953125
Iteration 23400: Loss = -11005.2685546875
Iteration 23500: Loss = -11005.2685546875
Iteration 23600: Loss = -11005.2685546875
Iteration 23700: Loss = -11005.2666015625
Iteration 23800: Loss = -11005.267578125
1
Iteration 23900: Loss = -11005.267578125
2
Iteration 24000: Loss = -11005.265625
Iteration 24100: Loss = -11005.2685546875
1
Iteration 24200: Loss = -11005.2646484375
Iteration 24300: Loss = -11005.2666015625
1
Iteration 24400: Loss = -11005.267578125
2
Iteration 24500: Loss = -11005.265625
3
Iteration 24600: Loss = -11005.265625
4
Iteration 24700: Loss = -11005.265625
5
Iteration 24800: Loss = -11005.2666015625
6
Iteration 24900: Loss = -11005.265625
7
Iteration 25000: Loss = -11005.2666015625
8
Iteration 25100: Loss = -11005.265625
9
Iteration 25200: Loss = -11005.267578125
10
Iteration 25300: Loss = -11005.2666015625
11
Iteration 25400: Loss = -11005.2666015625
12
Iteration 25500: Loss = -11005.265625
13
Iteration 25600: Loss = -11005.2666015625
14
Iteration 25700: Loss = -11005.2666015625
15
Stopping early at iteration 25700 due to no improvement.
pi: tensor([[1.0000e+00, 3.2762e-06],
        [3.3120e-01, 6.6880e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9607, 0.0393], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1622, 0.2403],
         [0.9521, 0.2707]],

        [[0.9732, 0.1105],
         [0.4129, 0.9911]],

        [[0.2521, 0.1722],
         [0.9384, 0.3588]],

        [[0.6450, 0.1398],
         [0.5035, 0.8959]],

        [[0.9917, 0.2101],
         [0.1633, 0.1187]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0009173200856414475
Average Adjusted Rand Index: -0.001932531571646376
[0.006504298374695772, -0.0009173200856414475] [0.010598629291584111, -0.001932531571646376] [11003.310546875, 11005.2666015625]
-------------------------------------
This iteration is 9
True Objective function: Loss = -10993.14918705235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27752.86328125
Iteration 100: Loss = -17307.640625
Iteration 200: Loss = -12423.974609375
Iteration 300: Loss = -11589.4814453125
Iteration 400: Loss = -11417.80078125
Iteration 500: Loss = -11336.6005859375
Iteration 600: Loss = -11291.6669921875
Iteration 700: Loss = -11260.62890625
Iteration 800: Loss = -11238.140625
Iteration 900: Loss = -11213.8076171875
Iteration 1000: Loss = -11198.1142578125
Iteration 1100: Loss = -11183.8056640625
Iteration 1200: Loss = -11174.466796875
Iteration 1300: Loss = -11155.9404296875
Iteration 1400: Loss = -11146.4912109375
Iteration 1500: Loss = -11141.521484375
Iteration 1600: Loss = -11137.294921875
Iteration 1700: Loss = -11134.244140625
Iteration 1800: Loss = -11129.62109375
Iteration 1900: Loss = -11126.35546875
Iteration 2000: Loss = -11122.7099609375
Iteration 2100: Loss = -11118.3427734375
Iteration 2200: Loss = -11115.2998046875
Iteration 2300: Loss = -11113.470703125
Iteration 2400: Loss = -11112.2802734375
Iteration 2500: Loss = -11111.3818359375
Iteration 2600: Loss = -11110.6455078125
Iteration 2700: Loss = -11110.0400390625
Iteration 2800: Loss = -11109.462890625
Iteration 2900: Loss = -11108.6982421875
Iteration 3000: Loss = -11105.232421875
Iteration 3100: Loss = -11102.6708984375
Iteration 3200: Loss = -11100.9560546875
Iteration 3300: Loss = -11099.8193359375
Iteration 3400: Loss = -11096.576171875
Iteration 3500: Loss = -11096.0126953125
Iteration 3600: Loss = -11095.5947265625
Iteration 3700: Loss = -11095.251953125
Iteration 3800: Loss = -11094.955078125
Iteration 3900: Loss = -11094.697265625
Iteration 4000: Loss = -11094.466796875
Iteration 4100: Loss = -11094.2568359375
Iteration 4200: Loss = -11094.0625
Iteration 4300: Loss = -11093.8857421875
Iteration 4400: Loss = -11093.7255859375
Iteration 4500: Loss = -11093.58203125
Iteration 4600: Loss = -11093.455078125
Iteration 4700: Loss = -11093.337890625
Iteration 4800: Loss = -11093.2314453125
Iteration 4900: Loss = -11093.130859375
Iteration 5000: Loss = -11093.0400390625
Iteration 5100: Loss = -11092.953125
Iteration 5200: Loss = -11092.87109375
Iteration 5300: Loss = -11092.796875
Iteration 5400: Loss = -11092.7236328125
Iteration 5500: Loss = -11092.6591796875
Iteration 5600: Loss = -11092.5966796875
Iteration 5700: Loss = -11092.5400390625
Iteration 5800: Loss = -11092.4853515625
Iteration 5900: Loss = -11092.4345703125
Iteration 6000: Loss = -11092.3857421875
Iteration 6100: Loss = -11092.3408203125
Iteration 6200: Loss = -11092.2958984375
Iteration 6300: Loss = -11092.2578125
Iteration 6400: Loss = -11092.21875
Iteration 6500: Loss = -11092.1806640625
Iteration 6600: Loss = -11092.1474609375
Iteration 6700: Loss = -11092.115234375
Iteration 6800: Loss = -11092.0830078125
Iteration 6900: Loss = -11092.0537109375
Iteration 7000: Loss = -11092.025390625
Iteration 7100: Loss = -11092.0
Iteration 7200: Loss = -11091.9716796875
Iteration 7300: Loss = -11091.94921875
Iteration 7400: Loss = -11091.92578125
Iteration 7500: Loss = -11091.9052734375
Iteration 7600: Loss = -11091.8828125
Iteration 7700: Loss = -11091.8623046875
Iteration 7800: Loss = -11091.8427734375
Iteration 7900: Loss = -11091.8271484375
Iteration 8000: Loss = -11091.8076171875
Iteration 8100: Loss = -11091.7919921875
Iteration 8200: Loss = -11091.7763671875
Iteration 8300: Loss = -11091.7607421875
Iteration 8400: Loss = -11091.748046875
Iteration 8500: Loss = -11091.7333984375
Iteration 8600: Loss = -11091.7197265625
Iteration 8700: Loss = -11091.7099609375
Iteration 8800: Loss = -11091.6982421875
Iteration 8900: Loss = -11091.6875
Iteration 9000: Loss = -11091.677734375
Iteration 9100: Loss = -11091.666015625
Iteration 9200: Loss = -11091.6572265625
Iteration 9300: Loss = -11091.6474609375
Iteration 9400: Loss = -11091.6396484375
Iteration 9500: Loss = -11091.630859375
Iteration 9600: Loss = -11091.623046875
Iteration 9700: Loss = -11091.6142578125
Iteration 9800: Loss = -11091.607421875
Iteration 9900: Loss = -11091.6015625
Iteration 10000: Loss = -11091.59375
Iteration 10100: Loss = -11091.5849609375
Iteration 10200: Loss = -11091.5810546875
Iteration 10300: Loss = -11091.57421875
Iteration 10400: Loss = -11091.568359375
Iteration 10500: Loss = -11091.5634765625
Iteration 10600: Loss = -11091.5576171875
Iteration 10700: Loss = -11091.552734375
Iteration 10800: Loss = -11091.546875
Iteration 10900: Loss = -11091.541015625
Iteration 11000: Loss = -11091.537109375
Iteration 11100: Loss = -11091.5322265625
Iteration 11200: Loss = -11091.529296875
Iteration 11300: Loss = -11091.5263671875
Iteration 11400: Loss = -11091.5224609375
Iteration 11500: Loss = -11091.51953125
Iteration 11600: Loss = -11091.5166015625
Iteration 11700: Loss = -11091.5146484375
Iteration 11800: Loss = -11091.5107421875
Iteration 11900: Loss = -11091.5087890625
Iteration 12000: Loss = -11091.5078125
Iteration 12100: Loss = -11091.50390625
Iteration 12200: Loss = -11091.5029296875
Iteration 12300: Loss = -11091.5
Iteration 12400: Loss = -11091.498046875
Iteration 12500: Loss = -11091.4970703125
Iteration 12600: Loss = -11091.4951171875
Iteration 12700: Loss = -11091.494140625
Iteration 12800: Loss = -11091.4931640625
Iteration 12900: Loss = -11091.4912109375
Iteration 13000: Loss = -11091.490234375
Iteration 13100: Loss = -11091.48828125
Iteration 13200: Loss = -11091.4873046875
Iteration 13300: Loss = -11091.4755859375
Iteration 13400: Loss = -11091.4736328125
Iteration 13500: Loss = -11091.470703125
Iteration 13600: Loss = -11091.4716796875
1
Iteration 13700: Loss = -11091.4697265625
Iteration 13800: Loss = -11091.4697265625
Iteration 13900: Loss = -11091.46875
Iteration 14000: Loss = -11091.466796875
Iteration 14100: Loss = -11091.4677734375
1
Iteration 14200: Loss = -11091.466796875
Iteration 14300: Loss = -11091.4658203125
Iteration 14400: Loss = -11091.46484375
Iteration 14500: Loss = -11091.46484375
Iteration 14600: Loss = -11091.462890625
Iteration 14700: Loss = -11091.462890625
Iteration 14800: Loss = -11091.4638671875
1
Iteration 14900: Loss = -11091.4619140625
Iteration 15000: Loss = -11091.4619140625
Iteration 15100: Loss = -11091.4599609375
Iteration 15200: Loss = -11091.4609375
1
Iteration 15300: Loss = -11091.4599609375
Iteration 15400: Loss = -11091.458984375
Iteration 15500: Loss = -11091.458984375
Iteration 15600: Loss = -11091.458984375
Iteration 15700: Loss = -11091.45703125
Iteration 15800: Loss = -11091.45703125
Iteration 15900: Loss = -11091.45703125
Iteration 16000: Loss = -11091.45703125
Iteration 16100: Loss = -11091.45703125
Iteration 16200: Loss = -11091.4560546875
Iteration 16300: Loss = -11091.4541015625
Iteration 16400: Loss = -11091.455078125
1
Iteration 16500: Loss = -11091.455078125
2
Iteration 16600: Loss = -11091.4541015625
Iteration 16700: Loss = -11091.453125
Iteration 16800: Loss = -11091.4541015625
1
Iteration 16900: Loss = -11091.4541015625
2
Iteration 17000: Loss = -11091.4541015625
3
Iteration 17100: Loss = -11091.455078125
4
Iteration 17200: Loss = -11091.453125
Iteration 17300: Loss = -11091.453125
Iteration 17400: Loss = -11091.4521484375
Iteration 17500: Loss = -11091.4541015625
1
Iteration 17600: Loss = -11091.453125
2
Iteration 17700: Loss = -11091.4521484375
Iteration 17800: Loss = -11091.4521484375
Iteration 17900: Loss = -11091.4248046875
Iteration 18000: Loss = -11091.42578125
1
Iteration 18100: Loss = -11091.423828125
Iteration 18200: Loss = -11091.4228515625
Iteration 18300: Loss = -11091.421875
Iteration 18400: Loss = -11091.421875
Iteration 18500: Loss = -11091.421875
Iteration 18600: Loss = -11091.4208984375
Iteration 18700: Loss = -11091.4208984375
Iteration 18800: Loss = -11091.4208984375
Iteration 18900: Loss = -11091.4228515625
1
Iteration 19000: Loss = -11091.4228515625
2
Iteration 19100: Loss = -11091.4208984375
Iteration 19200: Loss = -11091.4208984375
Iteration 19300: Loss = -11091.4208984375
Iteration 19400: Loss = -11091.419921875
Iteration 19500: Loss = -11091.4189453125
Iteration 19600: Loss = -11091.419921875
1
Iteration 19700: Loss = -11091.4189453125
Iteration 19800: Loss = -11091.419921875
1
Iteration 19900: Loss = -11091.41796875
Iteration 20000: Loss = -11091.419921875
1
Iteration 20100: Loss = -11091.4208984375
2
Iteration 20200: Loss = -11091.4189453125
3
Iteration 20300: Loss = -11091.4208984375
4
Iteration 20400: Loss = -11091.419921875
5
Iteration 20500: Loss = -11091.4189453125
6
Iteration 20600: Loss = -11091.419921875
7
Iteration 20700: Loss = -11091.4189453125
8
Iteration 20800: Loss = -11091.4189453125
9
Iteration 20900: Loss = -11091.41796875
Iteration 21000: Loss = -11091.41796875
Iteration 21100: Loss = -11091.4189453125
1
Iteration 21200: Loss = -11091.4189453125
2
Iteration 21300: Loss = -11091.4189453125
3
Iteration 21400: Loss = -11091.41796875
Iteration 21500: Loss = -11091.4189453125
1
Iteration 21600: Loss = -11091.4208984375
2
Iteration 21700: Loss = -11091.4189453125
3
Iteration 21800: Loss = -11091.4208984375
4
Iteration 21900: Loss = -11091.41796875
Iteration 22000: Loss = -11091.4189453125
1
Iteration 22100: Loss = -11091.4189453125
2
Iteration 22200: Loss = -11091.4189453125
3
Iteration 22300: Loss = -11091.4208984375
4
Iteration 22400: Loss = -11091.41796875
Iteration 22500: Loss = -11091.4189453125
1
Iteration 22600: Loss = -11091.4189453125
2
Iteration 22700: Loss = -11091.419921875
3
Iteration 22800: Loss = -11091.4189453125
4
Iteration 22900: Loss = -11091.419921875
5
Iteration 23000: Loss = -11091.4189453125
6
Iteration 23100: Loss = -11091.4189453125
7
Iteration 23200: Loss = -11091.419921875
8
Iteration 23300: Loss = -11091.41796875
Iteration 23400: Loss = -11091.419921875
1
Iteration 23500: Loss = -11091.4189453125
2
Iteration 23600: Loss = -11091.41796875
Iteration 23700: Loss = -11091.4208984375
1
Iteration 23800: Loss = -11091.41796875
Iteration 23900: Loss = -11091.41796875
Iteration 24000: Loss = -11091.4189453125
1
Iteration 24100: Loss = -11091.4189453125
2
Iteration 24200: Loss = -11091.4189453125
3
Iteration 24300: Loss = -11091.4189453125
4
Iteration 24400: Loss = -11091.419921875
5
Iteration 24500: Loss = -11091.419921875
6
Iteration 24600: Loss = -11091.4189453125
7
Iteration 24700: Loss = -11091.41796875
Iteration 24800: Loss = -11091.4189453125
1
Iteration 24900: Loss = -11091.41796875
Iteration 25000: Loss = -11091.41796875
Iteration 25100: Loss = -11091.41796875
Iteration 25200: Loss = -11090.8349609375
Iteration 25300: Loss = -11089.1181640625
Iteration 25400: Loss = -11085.4990234375
Iteration 25500: Loss = -11082.77734375
Iteration 25600: Loss = -11082.6884765625
Iteration 25700: Loss = -11082.625
Iteration 25800: Loss = -11079.076171875
Iteration 25900: Loss = -11078.0869140625
Iteration 26000: Loss = -11077.9716796875
Iteration 26100: Loss = -11077.267578125
Iteration 26200: Loss = -11076.5576171875
Iteration 26300: Loss = -11072.48046875
Iteration 26400: Loss = -11061.8974609375
Iteration 26500: Loss = -11001.4970703125
Iteration 26600: Loss = -10988.0869140625
Iteration 26700: Loss = -10987.736328125
Iteration 26800: Loss = -10987.404296875
Iteration 26900: Loss = -10982.4365234375
Iteration 27000: Loss = -10982.3681640625
Iteration 27100: Loss = -10982.3408203125
Iteration 27200: Loss = -10982.326171875
Iteration 27300: Loss = -10980.892578125
Iteration 27400: Loss = -10980.830078125
Iteration 27500: Loss = -10980.81640625
Iteration 27600: Loss = -10980.8076171875
Iteration 27700: Loss = -10977.41015625
Iteration 27800: Loss = -10976.6181640625
Iteration 27900: Loss = -10976.5986328125
Iteration 28000: Loss = -10976.591796875
Iteration 28100: Loss = -10976.392578125
Iteration 28200: Loss = -10971.5634765625
Iteration 28300: Loss = -10971.548828125
Iteration 28400: Loss = -10971.5400390625
Iteration 28500: Loss = -10971.537109375
Iteration 28600: Loss = -10971.5341796875
Iteration 28700: Loss = -10971.533203125
Iteration 28800: Loss = -10971.53125
Iteration 28900: Loss = -10971.529296875
Iteration 29000: Loss = -10971.5126953125
Iteration 29100: Loss = -10971.375
Iteration 29200: Loss = -10971.373046875
Iteration 29300: Loss = -10971.3720703125
Iteration 29400: Loss = -10971.373046875
1
Iteration 29500: Loss = -10971.37109375
Iteration 29600: Loss = -10971.3720703125
1
Iteration 29700: Loss = -10971.37109375
Iteration 29800: Loss = -10971.16796875
Iteration 29900: Loss = -10967.529296875
pi: tensor([[0.7528, 0.2472],
        [0.2280, 0.7720]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5124, 0.4876], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2520, 0.1056],
         [0.8275, 0.2008]],

        [[0.0120, 0.0965],
         [0.9689, 0.0117]],

        [[0.0717, 0.1000],
         [0.8057, 0.4969]],

        [[0.1595, 0.1192],
         [0.4339, 0.6098]],

        [[0.1984, 0.0945],
         [0.9905, 0.0300]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 13
Adjusted Rand Index: 0.543005236161102
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
Global Adjusted Rand Index: 0.8241115899082239
Average Adjusted Rand Index: 0.8302115810071017
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29372.267578125
Iteration 100: Loss = -19557.451171875
Iteration 200: Loss = -13090.89453125
Iteration 300: Loss = -11677.3994140625
Iteration 400: Loss = -11424.0615234375
Iteration 500: Loss = -11335.0869140625
Iteration 600: Loss = -11290.8740234375
Iteration 700: Loss = -11264.26953125
Iteration 800: Loss = -11243.505859375
Iteration 900: Loss = -11227.8564453125
Iteration 1000: Loss = -11217.638671875
Iteration 1100: Loss = -11209.904296875
Iteration 1200: Loss = -11202.32421875
Iteration 1300: Loss = -11197.677734375
Iteration 1400: Loss = -11193.67578125
Iteration 1500: Loss = -11187.525390625
Iteration 1600: Loss = -11177.8671875
Iteration 1700: Loss = -11173.6640625
Iteration 1800: Loss = -11171.0068359375
Iteration 1900: Loss = -11168.78125
Iteration 2000: Loss = -11166.744140625
Iteration 2100: Loss = -11164.580078125
Iteration 2200: Loss = -11162.146484375
Iteration 2300: Loss = -11159.3388671875
Iteration 2400: Loss = -11154.822265625
Iteration 2500: Loss = -11152.640625
Iteration 2600: Loss = -11150.455078125
Iteration 2700: Loss = -11145.2001953125
Iteration 2800: Loss = -11143.3046875
Iteration 2900: Loss = -11140.1884765625
Iteration 3000: Loss = -11138.796875
Iteration 3100: Loss = -11137.5361328125
Iteration 3200: Loss = -11136.513671875
Iteration 3300: Loss = -11135.162109375
Iteration 3400: Loss = -11134.3486328125
Iteration 3500: Loss = -11132.4873046875
Iteration 3600: Loss = -11130.3779296875
Iteration 3700: Loss = -11128.75
Iteration 3800: Loss = -11125.8369140625
Iteration 3900: Loss = -11124.8779296875
Iteration 4000: Loss = -11123.9189453125
Iteration 4100: Loss = -11123.2763671875
Iteration 4200: Loss = -11121.5791015625
Iteration 4300: Loss = -11116.2021484375
Iteration 4400: Loss = -11115.322265625
Iteration 4500: Loss = -11114.9208984375
Iteration 4600: Loss = -11114.640625
Iteration 4700: Loss = -11114.4169921875
Iteration 4800: Loss = -11114.193359375
Iteration 4900: Loss = -11111.9423828125
Iteration 5000: Loss = -11111.7841796875
Iteration 5100: Loss = -11111.6533203125
Iteration 5200: Loss = -11111.537109375
Iteration 5300: Loss = -11111.4267578125
Iteration 5400: Loss = -11110.568359375
Iteration 5500: Loss = -11107.4873046875
Iteration 5600: Loss = -11107.0869140625
Iteration 5700: Loss = -11102.046875
Iteration 5800: Loss = -11101.5283203125
Iteration 5900: Loss = -11101.2861328125
Iteration 6000: Loss = -11101.1171875
Iteration 6100: Loss = -11100.982421875
Iteration 6200: Loss = -11100.8583984375
Iteration 6300: Loss = -11099.240234375
Iteration 6400: Loss = -11096.1220703125
Iteration 6500: Loss = -11094.765625
Iteration 6600: Loss = -11094.470703125
Iteration 6700: Loss = -11094.3037109375
Iteration 6800: Loss = -11094.1845703125
Iteration 6900: Loss = -11094.09375
Iteration 7000: Loss = -11094.021484375
Iteration 7100: Loss = -11093.95703125
Iteration 7200: Loss = -11093.904296875
Iteration 7300: Loss = -11093.859375
Iteration 7400: Loss = -11093.8154296875
Iteration 7500: Loss = -11093.7763671875
Iteration 7600: Loss = -11093.7421875
Iteration 7700: Loss = -11093.7119140625
Iteration 7800: Loss = -11093.6826171875
Iteration 7900: Loss = -11093.6572265625
Iteration 8000: Loss = -11093.634765625
Iteration 8100: Loss = -11093.6123046875
Iteration 8200: Loss = -11093.591796875
Iteration 8300: Loss = -11093.5712890625
Iteration 8400: Loss = -11093.552734375
Iteration 8500: Loss = -11093.5361328125
Iteration 8600: Loss = -11093.521484375
Iteration 8700: Loss = -11093.505859375
Iteration 8800: Loss = -11093.4931640625
Iteration 8900: Loss = -11093.482421875
Iteration 9000: Loss = -11093.4677734375
Iteration 9100: Loss = -11093.4560546875
Iteration 9200: Loss = -11093.4462890625
Iteration 9300: Loss = -11093.4375
Iteration 9400: Loss = -11093.4296875
Iteration 9500: Loss = -11093.4189453125
Iteration 9600: Loss = -11093.4111328125
Iteration 9700: Loss = -11093.4033203125
Iteration 9800: Loss = -11093.3955078125
Iteration 9900: Loss = -11093.3876953125
Iteration 10000: Loss = -11093.3818359375
Iteration 10100: Loss = -11093.375
Iteration 10200: Loss = -11093.369140625
Iteration 10300: Loss = -11093.36328125
Iteration 10400: Loss = -11093.3583984375
Iteration 10500: Loss = -11093.353515625
Iteration 10600: Loss = -11093.3486328125
Iteration 10700: Loss = -11093.3427734375
Iteration 10800: Loss = -11093.337890625
Iteration 10900: Loss = -11093.333984375
Iteration 11000: Loss = -11093.3310546875
Iteration 11100: Loss = -11093.328125
Iteration 11200: Loss = -11093.3251953125
Iteration 11300: Loss = -11093.322265625
Iteration 11400: Loss = -11093.318359375
Iteration 11500: Loss = -11093.31640625
Iteration 11600: Loss = -11093.3125
Iteration 11700: Loss = -11093.3095703125
Iteration 11800: Loss = -11093.3076171875
Iteration 11900: Loss = -11093.3056640625
Iteration 12000: Loss = -11093.3017578125
Iteration 12100: Loss = -11093.2998046875
Iteration 12200: Loss = -11093.2978515625
Iteration 12300: Loss = -11093.298828125
1
Iteration 12400: Loss = -11093.2939453125
Iteration 12500: Loss = -11093.2939453125
Iteration 12600: Loss = -11093.291015625
Iteration 12700: Loss = -11093.2890625
Iteration 12800: Loss = -11093.2861328125
Iteration 12900: Loss = -11093.2880859375
1
Iteration 13000: Loss = -11093.28515625
Iteration 13100: Loss = -11093.28515625
Iteration 13200: Loss = -11093.283203125
Iteration 13300: Loss = -11093.2822265625
Iteration 13400: Loss = -11093.2822265625
Iteration 13500: Loss = -11093.2802734375
Iteration 13600: Loss = -11093.279296875
Iteration 13700: Loss = -11093.279296875
Iteration 13800: Loss = -11093.27734375
Iteration 13900: Loss = -11093.2763671875
Iteration 14000: Loss = -11093.2763671875
Iteration 14100: Loss = -11093.2744140625
Iteration 14200: Loss = -11093.275390625
1
Iteration 14300: Loss = -11093.2734375
Iteration 14400: Loss = -11093.271484375
Iteration 14500: Loss = -11093.271484375
Iteration 14600: Loss = -11093.271484375
Iteration 14700: Loss = -11093.271484375
Iteration 14800: Loss = -11093.271484375
Iteration 14900: Loss = -11093.26953125
Iteration 15000: Loss = -11093.2685546875
Iteration 15100: Loss = -11093.2685546875
Iteration 15200: Loss = -11093.2685546875
Iteration 15300: Loss = -11093.2685546875
Iteration 15400: Loss = -11093.267578125
Iteration 15500: Loss = -11093.2685546875
1
Iteration 15600: Loss = -11093.267578125
Iteration 15700: Loss = -11093.265625
Iteration 15800: Loss = -11093.2666015625
1
Iteration 15900: Loss = -11093.2646484375
Iteration 16000: Loss = -11093.2646484375
Iteration 16100: Loss = -11093.265625
1
Iteration 16200: Loss = -11093.263671875
Iteration 16300: Loss = -11093.265625
1
Iteration 16400: Loss = -11093.2646484375
2
Iteration 16500: Loss = -11093.263671875
Iteration 16600: Loss = -11093.263671875
Iteration 16700: Loss = -11093.265625
1
Iteration 16800: Loss = -11093.263671875
Iteration 16900: Loss = -11093.2646484375
1
Iteration 17000: Loss = -11093.263671875
Iteration 17100: Loss = -11093.26171875
Iteration 17200: Loss = -11093.263671875
1
Iteration 17300: Loss = -11093.26171875
Iteration 17400: Loss = -11093.263671875
1
Iteration 17500: Loss = -11093.26171875
Iteration 17600: Loss = -11093.2626953125
1
Iteration 17700: Loss = -11093.2607421875
Iteration 17800: Loss = -11093.2626953125
1
Iteration 17900: Loss = -11093.2626953125
2
Iteration 18000: Loss = -11093.2626953125
3
Iteration 18100: Loss = -11093.2626953125
4
Iteration 18200: Loss = -11093.2607421875
Iteration 18300: Loss = -11093.26171875
1
Iteration 18400: Loss = -11093.26171875
2
Iteration 18500: Loss = -11093.2626953125
3
Iteration 18600: Loss = -11093.2607421875
Iteration 18700: Loss = -11093.2607421875
Iteration 18800: Loss = -11093.2607421875
Iteration 18900: Loss = -11093.26171875
1
Iteration 19000: Loss = -11093.26171875
2
Iteration 19100: Loss = -11093.2607421875
Iteration 19200: Loss = -11093.26171875
1
Iteration 19300: Loss = -11093.26171875
2
Iteration 19400: Loss = -11093.2607421875
Iteration 19500: Loss = -11093.2607421875
Iteration 19600: Loss = -11093.2607421875
Iteration 19700: Loss = -11093.2626953125
1
Iteration 19800: Loss = -11093.2607421875
Iteration 19900: Loss = -11093.2626953125
1
Iteration 20000: Loss = -11093.2626953125
2
Iteration 20100: Loss = -11093.2607421875
Iteration 20200: Loss = -11093.2607421875
Iteration 20300: Loss = -11093.26171875
1
Iteration 20400: Loss = -11093.2607421875
Iteration 20500: Loss = -11093.2607421875
Iteration 20600: Loss = -11093.26171875
1
Iteration 20700: Loss = -11093.2607421875
Iteration 20800: Loss = -11093.2607421875
Iteration 20900: Loss = -11093.26171875
1
Iteration 21000: Loss = -11093.26171875
2
Iteration 21100: Loss = -11093.2626953125
3
Iteration 21200: Loss = -11093.2607421875
Iteration 21300: Loss = -11093.2607421875
Iteration 21400: Loss = -11093.26171875
1
Iteration 21500: Loss = -11093.26171875
2
Iteration 21600: Loss = -11093.2626953125
3
Iteration 21700: Loss = -11093.2626953125
4
Iteration 21800: Loss = -11093.26171875
5
Iteration 21900: Loss = -11093.2607421875
Iteration 22000: Loss = -11093.2607421875
Iteration 22100: Loss = -11093.26171875
1
Iteration 22200: Loss = -11093.2607421875
Iteration 22300: Loss = -11093.2626953125
1
Iteration 22400: Loss = -11093.2626953125
2
Iteration 22500: Loss = -11093.2607421875
Iteration 22600: Loss = -11093.26171875
1
Iteration 22700: Loss = -11093.2607421875
Iteration 22800: Loss = -11093.26171875
1
Iteration 22900: Loss = -11093.2607421875
Iteration 23000: Loss = -11093.2607421875
Iteration 23100: Loss = -11093.2626953125
1
Iteration 23200: Loss = -11093.2626953125
2
Iteration 23300: Loss = -11093.2607421875
Iteration 23400: Loss = -11093.2626953125
1
Iteration 23500: Loss = -11093.2626953125
2
Iteration 23600: Loss = -11093.2626953125
3
Iteration 23700: Loss = -11093.2607421875
Iteration 23800: Loss = -11093.26171875
1
Iteration 23900: Loss = -11093.2607421875
Iteration 24000: Loss = -11093.2607421875
Iteration 24100: Loss = -11093.2626953125
1
Iteration 24200: Loss = -11093.26171875
2
Iteration 24300: Loss = -11093.26171875
3
Iteration 24400: Loss = -11093.26171875
4
Iteration 24500: Loss = -11093.2626953125
5
Iteration 24600: Loss = -11093.2607421875
Iteration 24700: Loss = -11093.2607421875
Iteration 24800: Loss = -11093.2607421875
Iteration 24900: Loss = -11093.2607421875
Iteration 25000: Loss = -11093.2626953125
1
Iteration 25100: Loss = -11093.2607421875
Iteration 25200: Loss = -11093.2607421875
Iteration 25300: Loss = -11093.2607421875
Iteration 25400: Loss = -11093.2607421875
Iteration 25500: Loss = -11093.2607421875
Iteration 25600: Loss = -11093.2607421875
Iteration 25700: Loss = -11093.26171875
1
Iteration 25800: Loss = -11093.2607421875
Iteration 25900: Loss = -11093.2626953125
1
Iteration 26000: Loss = -11093.2626953125
2
Iteration 26100: Loss = -11093.2607421875
Iteration 26200: Loss = -11093.2626953125
1
Iteration 26300: Loss = -11093.2607421875
Iteration 26400: Loss = -11093.2607421875
Iteration 26500: Loss = -11093.2607421875
Iteration 26600: Loss = -11093.2607421875
Iteration 26700: Loss = -11093.2607421875
Iteration 26800: Loss = -11093.2607421875
Iteration 26900: Loss = -11093.2607421875
Iteration 27000: Loss = -11093.2607421875
Iteration 27100: Loss = -11093.2607421875
Iteration 27200: Loss = -11093.26171875
1
Iteration 27300: Loss = -11093.2607421875
Iteration 27400: Loss = -11093.2626953125
1
Iteration 27500: Loss = -11093.2626953125
2
Iteration 27600: Loss = -11093.2626953125
3
Iteration 27700: Loss = -11093.2607421875
Iteration 27800: Loss = -11093.2607421875
Iteration 27900: Loss = -11093.26171875
1
Iteration 28000: Loss = -11093.2626953125
2
Iteration 28100: Loss = -11093.2607421875
Iteration 28200: Loss = -11093.2607421875
Iteration 28300: Loss = -11093.2607421875
Iteration 28400: Loss = -11093.2607421875
Iteration 28500: Loss = -11093.26171875
1
Iteration 28600: Loss = -11093.26171875
2
Iteration 28700: Loss = -11093.2607421875
Iteration 28800: Loss = -11093.2607421875
Iteration 28900: Loss = -11093.2626953125
1
Iteration 29000: Loss = -11093.2626953125
2
Iteration 29100: Loss = -11093.2607421875
Iteration 29200: Loss = -11093.2607421875
Iteration 29300: Loss = -11093.2607421875
Iteration 29400: Loss = -11093.2607421875
Iteration 29500: Loss = -11093.2607421875
Iteration 29600: Loss = -11093.2607421875
Iteration 29700: Loss = -11093.26171875
1
Iteration 29800: Loss = -11093.26171875
2
Iteration 29900: Loss = -11093.2607421875
pi: tensor([[7.2795e-03, 9.9272e-01],
        [2.9191e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.2123e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2585, 0.1808],
         [0.9792, 0.1652]],

        [[0.0237, 0.1500],
         [0.6190, 0.1278]],

        [[0.9103, 0.2121],
         [0.0544, 0.6251]],

        [[0.3595, 0.2145],
         [0.3552, 0.9419]],

        [[0.2722, 0.1568],
         [0.9644, 0.0285]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.8241115899082239, 0.0] [0.8302115810071017, 0.0] [10967.513671875, 11093.2607421875]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11076.898128394521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37516.49609375
Iteration 100: Loss = -22453.408203125
Iteration 200: Loss = -13533.3857421875
Iteration 300: Loss = -11809.4091796875
Iteration 400: Loss = -11523.197265625
Iteration 500: Loss = -11394.068359375
Iteration 600: Loss = -11322.5634765625
Iteration 700: Loss = -11280.8134765625
Iteration 800: Loss = -11250.7607421875
Iteration 900: Loss = -11232.408203125
Iteration 1000: Loss = -11212.0517578125
Iteration 1100: Loss = -11198.4775390625
Iteration 1200: Loss = -11187.4716796875
Iteration 1300: Loss = -11175.9765625
Iteration 1400: Loss = -11170.52734375
Iteration 1500: Loss = -11166.4404296875
Iteration 1600: Loss = -11163.1103515625
Iteration 1700: Loss = -11160.169921875
Iteration 1800: Loss = -11157.3828125
Iteration 1900: Loss = -11154.861328125
Iteration 2000: Loss = -11152.794921875
Iteration 2100: Loss = -11151.119140625
Iteration 2200: Loss = -11149.720703125
Iteration 2300: Loss = -11148.5283203125
Iteration 2400: Loss = -11147.4921875
Iteration 2500: Loss = -11146.5830078125
Iteration 2600: Loss = -11145.77734375
Iteration 2700: Loss = -11145.056640625
Iteration 2800: Loss = -11144.412109375
Iteration 2900: Loss = -11143.8271484375
Iteration 3000: Loss = -11143.2998046875
Iteration 3100: Loss = -11142.8203125
Iteration 3200: Loss = -11142.3828125
Iteration 3300: Loss = -11141.982421875
Iteration 3400: Loss = -11141.6201171875
Iteration 3500: Loss = -11141.2861328125
Iteration 3600: Loss = -11140.98046875
Iteration 3700: Loss = -11140.7021484375
Iteration 3800: Loss = -11140.4462890625
Iteration 3900: Loss = -11140.2099609375
Iteration 4000: Loss = -11139.994140625
Iteration 4100: Loss = -11139.794921875
Iteration 4200: Loss = -11139.6123046875
Iteration 4300: Loss = -11139.44140625
Iteration 4400: Loss = -11139.2841796875
Iteration 4500: Loss = -11139.1396484375
Iteration 4600: Loss = -11139.00390625
Iteration 4700: Loss = -11138.880859375
Iteration 4800: Loss = -11138.7646484375
Iteration 4900: Loss = -11138.654296875
Iteration 5000: Loss = -11138.5537109375
Iteration 5100: Loss = -11138.4619140625
Iteration 5200: Loss = -11138.3720703125
Iteration 5300: Loss = -11138.2890625
Iteration 5400: Loss = -11138.2119140625
Iteration 5500: Loss = -11138.1396484375
Iteration 5600: Loss = -11138.0732421875
Iteration 5700: Loss = -11138.0068359375
Iteration 5800: Loss = -11137.9462890625
Iteration 5900: Loss = -11137.890625
Iteration 6000: Loss = -11137.8359375
Iteration 6100: Loss = -11137.787109375
Iteration 6200: Loss = -11137.73828125
Iteration 6300: Loss = -11137.6923828125
Iteration 6400: Loss = -11137.6494140625
Iteration 6500: Loss = -11137.609375
Iteration 6600: Loss = -11137.5732421875
Iteration 6700: Loss = -11137.5361328125
Iteration 6800: Loss = -11137.5009765625
Iteration 6900: Loss = -11137.466796875
Iteration 7000: Loss = -11137.435546875
Iteration 7100: Loss = -11137.40625
Iteration 7200: Loss = -11137.3759765625
Iteration 7300: Loss = -11137.3505859375
Iteration 7400: Loss = -11137.326171875
Iteration 7500: Loss = -11137.30078125
Iteration 7600: Loss = -11137.27734375
Iteration 7700: Loss = -11137.2529296875
Iteration 7800: Loss = -11137.234375
Iteration 7900: Loss = -11137.2119140625
Iteration 8000: Loss = -11137.1904296875
Iteration 8100: Loss = -11137.1728515625
Iteration 8200: Loss = -11137.1513671875
Iteration 8300: Loss = -11137.1337890625
Iteration 8400: Loss = -11137.115234375
Iteration 8500: Loss = -11137.09765625
Iteration 8600: Loss = -11137.0791015625
Iteration 8700: Loss = -11137.05859375
Iteration 8800: Loss = -11137.037109375
Iteration 8900: Loss = -11137.017578125
Iteration 9000: Loss = -11136.9970703125
Iteration 9100: Loss = -11136.9775390625
Iteration 9200: Loss = -11136.9580078125
Iteration 9300: Loss = -11136.94140625
Iteration 9400: Loss = -11136.923828125
Iteration 9500: Loss = -11136.91015625
Iteration 9600: Loss = -11136.8955078125
Iteration 9700: Loss = -11136.8828125
Iteration 9800: Loss = -11136.8701171875
Iteration 9900: Loss = -11136.8583984375
Iteration 10000: Loss = -11136.84765625
Iteration 10100: Loss = -11136.8369140625
Iteration 10200: Loss = -11136.826171875
Iteration 10300: Loss = -11136.814453125
Iteration 10400: Loss = -11136.8056640625
Iteration 10500: Loss = -11136.794921875
Iteration 10600: Loss = -11136.7841796875
Iteration 10700: Loss = -11136.775390625
Iteration 10800: Loss = -11136.7666015625
Iteration 10900: Loss = -11136.7578125
Iteration 11000: Loss = -11136.7490234375
Iteration 11100: Loss = -11136.7392578125
Iteration 11200: Loss = -11136.73046875
Iteration 11300: Loss = -11136.72265625
Iteration 11400: Loss = -11136.7119140625
Iteration 11500: Loss = -11136.7041015625
Iteration 11600: Loss = -11136.6943359375
Iteration 11700: Loss = -11136.685546875
Iteration 11800: Loss = -11136.6708984375
Iteration 11900: Loss = -11136.662109375
Iteration 12000: Loss = -11136.6484375
Iteration 12100: Loss = -11136.63671875
Iteration 12200: Loss = -11136.62109375
Iteration 12300: Loss = -11136.6025390625
Iteration 12400: Loss = -11136.58203125
Iteration 12500: Loss = -11136.556640625
Iteration 12600: Loss = -11136.5283203125
Iteration 12700: Loss = -11136.4931640625
Iteration 12800: Loss = -11136.4482421875
Iteration 12900: Loss = -11136.3916015625
Iteration 13000: Loss = -11136.3125
Iteration 13100: Loss = -11136.2021484375
Iteration 13200: Loss = -11136.0341796875
Iteration 13300: Loss = -11135.8330078125
Iteration 13400: Loss = -11135.7158203125
Iteration 13500: Loss = -11135.6708984375
Iteration 13600: Loss = -11135.6337890625
Iteration 13700: Loss = -11135.59765625
Iteration 13800: Loss = -11135.5556640625
Iteration 13900: Loss = -11135.513671875
Iteration 14000: Loss = -11135.46484375
Iteration 14100: Loss = -11135.20703125
Iteration 14200: Loss = -11134.169921875
Iteration 14300: Loss = -11134.01171875
Iteration 14400: Loss = -11133.9091796875
Iteration 14500: Loss = -11133.87109375
Iteration 14600: Loss = -11133.81640625
Iteration 14700: Loss = -11133.78515625
Iteration 14800: Loss = -11133.7158203125
Iteration 14900: Loss = -11133.6162109375
Iteration 15000: Loss = -11133.60546875
Iteration 15100: Loss = -11133.5986328125
Iteration 15200: Loss = -11133.5908203125
Iteration 15300: Loss = -11133.58984375
Iteration 15400: Loss = -11133.5849609375
Iteration 15500: Loss = -11133.576171875
Iteration 15600: Loss = -11133.5654296875
Iteration 15700: Loss = -11133.5595703125
Iteration 15800: Loss = -11133.556640625
Iteration 15900: Loss = -11133.5556640625
Iteration 16000: Loss = -11133.5517578125
Iteration 16100: Loss = -11133.552734375
1
Iteration 16200: Loss = -11133.5478515625
Iteration 16300: Loss = -11133.5458984375
Iteration 16400: Loss = -11133.5439453125
Iteration 16500: Loss = -11133.541015625
Iteration 16600: Loss = -11133.541015625
Iteration 16700: Loss = -11133.5390625
Iteration 16800: Loss = -11133.5390625
Iteration 16900: Loss = -11133.537109375
Iteration 17000: Loss = -11133.53515625
Iteration 17100: Loss = -11133.5361328125
1
Iteration 17200: Loss = -11133.533203125
Iteration 17300: Loss = -11133.533203125
Iteration 17400: Loss = -11133.53125
Iteration 17500: Loss = -11133.53125
Iteration 17600: Loss = -11133.5322265625
1
Iteration 17700: Loss = -11133.53125
Iteration 17800: Loss = -11133.53125
Iteration 17900: Loss = -11133.5341796875
1
Iteration 18000: Loss = -11133.5302734375
Iteration 18100: Loss = -11133.53125
1
Iteration 18200: Loss = -11133.53125
2
Iteration 18300: Loss = -11133.53125
3
Iteration 18400: Loss = -11133.5322265625
4
Iteration 18500: Loss = -11133.53125
5
Iteration 18600: Loss = -11133.5302734375
Iteration 18700: Loss = -11133.53125
1
Iteration 18800: Loss = -11133.53125
2
Iteration 18900: Loss = -11133.53125
3
Iteration 19000: Loss = -11133.529296875
Iteration 19100: Loss = -11133.5302734375
1
Iteration 19200: Loss = -11133.5302734375
2
Iteration 19300: Loss = -11133.53125
3
Iteration 19400: Loss = -11133.5302734375
4
Iteration 19500: Loss = -11133.5302734375
5
Iteration 19600: Loss = -11133.529296875
Iteration 19700: Loss = -11133.53125
1
Iteration 19800: Loss = -11133.53125
2
Iteration 19900: Loss = -11133.5302734375
3
Iteration 20000: Loss = -11133.529296875
Iteration 20100: Loss = -11133.5283203125
Iteration 20200: Loss = -11133.529296875
1
Iteration 20300: Loss = -11133.53125
2
Iteration 20400: Loss = -11133.5283203125
Iteration 20500: Loss = -11133.5283203125
Iteration 20600: Loss = -11133.5302734375
1
Iteration 20700: Loss = -11133.5302734375
2
Iteration 20800: Loss = -11133.529296875
3
Iteration 20900: Loss = -11133.5302734375
4
Iteration 21000: Loss = -11133.529296875
5
Iteration 21100: Loss = -11133.5302734375
6
Iteration 21200: Loss = -11133.53125
7
Iteration 21300: Loss = -11133.529296875
8
Iteration 21400: Loss = -11133.5302734375
9
Iteration 21500: Loss = -11133.5302734375
10
Iteration 21600: Loss = -11133.5302734375
11
Iteration 21700: Loss = -11133.529296875
12
Iteration 21800: Loss = -11133.5283203125
Iteration 21900: Loss = -11133.5283203125
Iteration 22000: Loss = -11133.529296875
1
Iteration 22100: Loss = -11133.529296875
2
Iteration 22200: Loss = -11133.529296875
3
Iteration 22300: Loss = -11133.529296875
4
Iteration 22400: Loss = -11133.5283203125
Iteration 22500: Loss = -11133.529296875
1
Iteration 22600: Loss = -11133.5302734375
2
Iteration 22700: Loss = -11133.5302734375
3
Iteration 22800: Loss = -11133.529296875
4
Iteration 22900: Loss = -11133.529296875
5
Iteration 23000: Loss = -11133.529296875
6
Iteration 23100: Loss = -11133.529296875
7
Iteration 23200: Loss = -11133.529296875
8
Iteration 23300: Loss = -11133.529296875
9
Iteration 23400: Loss = -11133.5283203125
Iteration 23500: Loss = -11133.529296875
1
Iteration 23600: Loss = -11133.529296875
2
Iteration 23700: Loss = -11133.5302734375
3
Iteration 23800: Loss = -11133.529296875
4
Iteration 23900: Loss = -11133.5302734375
5
Iteration 24000: Loss = -11133.5302734375
6
Iteration 24100: Loss = -11133.5283203125
Iteration 24200: Loss = -11133.529296875
1
Iteration 24300: Loss = -11133.5283203125
Iteration 24400: Loss = -11133.52734375
Iteration 24500: Loss = -11133.5283203125
1
Iteration 24600: Loss = -11133.5283203125
2
Iteration 24700: Loss = -11133.5283203125
3
Iteration 24800: Loss = -11133.5302734375
4
Iteration 24900: Loss = -11133.533203125
5
Iteration 25000: Loss = -11133.529296875
6
Iteration 25100: Loss = -11133.5283203125
7
Iteration 25200: Loss = -11133.52734375
Iteration 25300: Loss = -11133.5283203125
1
Iteration 25400: Loss = -11133.5283203125
2
Iteration 25500: Loss = -11133.529296875
3
Iteration 25600: Loss = -11133.5283203125
4
Iteration 25700: Loss = -11133.5322265625
5
Iteration 25800: Loss = -11133.529296875
6
Iteration 25900: Loss = -11133.529296875
7
Iteration 26000: Loss = -11133.529296875
8
Iteration 26100: Loss = -11133.52734375
Iteration 26200: Loss = -11133.5322265625
1
Iteration 26300: Loss = -11133.5283203125
2
Iteration 26400: Loss = -11133.53125
3
Iteration 26500: Loss = -11133.529296875
4
Iteration 26600: Loss = -11133.5322265625
5
Iteration 26700: Loss = -11133.529296875
6
Iteration 26800: Loss = -11133.529296875
7
Iteration 26900: Loss = -11133.5283203125
8
Iteration 27000: Loss = -11133.529296875
9
Iteration 27100: Loss = -11133.5302734375
10
Iteration 27200: Loss = -11133.5283203125
11
Iteration 27300: Loss = -11133.529296875
12
Iteration 27400: Loss = -11133.5283203125
13
Iteration 27500: Loss = -11133.5283203125
14
Iteration 27600: Loss = -11133.5283203125
15
Stopping early at iteration 27600 due to no improvement.
pi: tensor([[9.9999e-01, 5.8879e-06],
        [5.7032e-01, 4.2968e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9359, 0.0641], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1664, 0.1184],
         [0.9604, 0.0593]],

        [[0.9849, 0.2744],
         [0.0291, 0.0722]],

        [[0.8999, 0.1329],
         [0.9591, 0.6325]],

        [[0.2284, 0.2195],
         [0.0580, 0.1979]],

        [[0.0102, 0.2215],
         [0.0437, 0.0140]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017622767853717912
Average Adjusted Rand Index: 0.002076227299044664
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22920.42578125
Iteration 100: Loss = -16369.533203125
Iteration 200: Loss = -12263.66796875
Iteration 300: Loss = -11487.013671875
Iteration 400: Loss = -11325.3818359375
Iteration 500: Loss = -11259.798828125
Iteration 600: Loss = -11224.3916015625
Iteration 700: Loss = -11202.908203125
Iteration 800: Loss = -11188.5732421875
Iteration 900: Loss = -11178.4248046875
Iteration 1000: Loss = -11170.8291015625
Iteration 1100: Loss = -11164.912109375
Iteration 1200: Loss = -11160.03125
Iteration 1300: Loss = -11155.8759765625
Iteration 1400: Loss = -11152.43359375
Iteration 1500: Loss = -11149.7685546875
Iteration 1600: Loss = -11147.650390625
Iteration 1700: Loss = -11145.908203125
Iteration 1800: Loss = -11144.443359375
Iteration 1900: Loss = -11143.19140625
Iteration 2000: Loss = -11142.09375
Iteration 2100: Loss = -11141.0849609375
Iteration 2200: Loss = -11139.9609375
Iteration 2300: Loss = -11137.5
Iteration 2400: Loss = -11135.056640625
Iteration 2500: Loss = -11133.3984375
Iteration 2600: Loss = -11132.2578125
Iteration 2700: Loss = -11131.5732421875
Iteration 2800: Loss = -11131.076171875
Iteration 2900: Loss = -11130.6630859375
Iteration 3000: Loss = -11130.30078125
Iteration 3100: Loss = -11129.9755859375
Iteration 3200: Loss = -11129.677734375
Iteration 3300: Loss = -11129.400390625
Iteration 3400: Loss = -11129.1376953125
Iteration 3500: Loss = -11128.8837890625
Iteration 3600: Loss = -11128.630859375
Iteration 3700: Loss = -11128.369140625
Iteration 3800: Loss = -11128.0859375
Iteration 3900: Loss = -11127.7724609375
Iteration 4000: Loss = -11127.392578125
Iteration 4100: Loss = -11127.0263671875
Iteration 4200: Loss = -11126.69921875
Iteration 4300: Loss = -11126.3955078125
Iteration 4400: Loss = -11126.072265625
Iteration 4500: Loss = -11125.71484375
Iteration 4600: Loss = -11125.1845703125
Iteration 4700: Loss = -11114.3974609375
Iteration 4800: Loss = -11106.826171875
Iteration 4900: Loss = -11103.4775390625
Iteration 5000: Loss = -11101.2177734375
Iteration 5100: Loss = -11096.7138671875
Iteration 5200: Loss = -11095.9775390625
Iteration 5300: Loss = -11093.3193359375
Iteration 5400: Loss = -11092.259765625
Iteration 5500: Loss = -11089.849609375
Iteration 5600: Loss = -11089.75390625
Iteration 5700: Loss = -11089.3388671875
Iteration 5800: Loss = -11089.287109375
Iteration 5900: Loss = -11089.2421875
Iteration 6000: Loss = -11089.19921875
Iteration 6100: Loss = -11089.1337890625
Iteration 6200: Loss = -11086.353515625
Iteration 6300: Loss = -11086.1982421875
Iteration 6400: Loss = -11086.1357421875
Iteration 6500: Loss = -11086.1064453125
Iteration 6600: Loss = -11086.080078125
Iteration 6700: Loss = -11086.0546875
Iteration 6800: Loss = -11086.03125
Iteration 6900: Loss = -11086.0087890625
Iteration 7000: Loss = -11085.9892578125
Iteration 7100: Loss = -11085.9697265625
Iteration 7200: Loss = -11085.94921875
Iteration 7300: Loss = -11085.9267578125
Iteration 7400: Loss = -11085.8720703125
Iteration 7500: Loss = -11085.64453125
Iteration 7600: Loss = -11084.220703125
Iteration 7700: Loss = -11084.1572265625
Iteration 7800: Loss = -11084.13671875
Iteration 7900: Loss = -11084.119140625
Iteration 8000: Loss = -11084.10546875
Iteration 8100: Loss = -11084.091796875
Iteration 8200: Loss = -11084.080078125
Iteration 8300: Loss = -11084.06640625
Iteration 8400: Loss = -11084.0537109375
Iteration 8500: Loss = -11084.0439453125
Iteration 8600: Loss = -11084.0361328125
Iteration 8700: Loss = -11084.0263671875
Iteration 8800: Loss = -11084.01953125
Iteration 8900: Loss = -11084.0126953125
Iteration 9000: Loss = -11084.0068359375
Iteration 9100: Loss = -11083.9990234375
Iteration 9200: Loss = -11083.9931640625
Iteration 9300: Loss = -11083.986328125
Iteration 9400: Loss = -11083.9794921875
Iteration 9500: Loss = -11083.16015625
Iteration 9600: Loss = -11083.1318359375
Iteration 9700: Loss = -11082.8837890625
Iteration 9800: Loss = -11080.61328125
Iteration 9900: Loss = -11080.5078125
Iteration 10000: Loss = -11080.5029296875
Iteration 10100: Loss = -11080.498046875
Iteration 10200: Loss = -11080.4892578125
Iteration 10300: Loss = -11080.484375
Iteration 10400: Loss = -11080.4814453125
Iteration 10500: Loss = -11080.478515625
Iteration 10600: Loss = -11080.4765625
Iteration 10700: Loss = -11080.47265625
Iteration 10800: Loss = -11080.46875
Iteration 10900: Loss = -11080.46875
Iteration 11000: Loss = -11080.4658203125
Iteration 11100: Loss = -11080.462890625
Iteration 11200: Loss = -11080.4609375
Iteration 11300: Loss = -11080.4599609375
Iteration 11400: Loss = -11080.45703125
Iteration 11500: Loss = -11080.4560546875
Iteration 11600: Loss = -11080.453125
Iteration 11700: Loss = -11080.396484375
Iteration 11800: Loss = -11079.3427734375
Iteration 11900: Loss = -11079.341796875
Iteration 12000: Loss = -11079.33984375
Iteration 12100: Loss = -11079.3388671875
Iteration 12200: Loss = -11079.3359375
Iteration 12300: Loss = -11079.3359375
Iteration 12400: Loss = -11079.333984375
Iteration 12500: Loss = -11079.3349609375
1
Iteration 12600: Loss = -11079.3330078125
Iteration 12700: Loss = -11079.33203125
Iteration 12800: Loss = -11079.330078125
Iteration 12900: Loss = -11079.330078125
Iteration 13000: Loss = -11079.3291015625
Iteration 13100: Loss = -11079.328125
Iteration 13200: Loss = -11078.6787109375
Iteration 13300: Loss = -11078.6630859375
Iteration 13400: Loss = -11078.6611328125
Iteration 13500: Loss = -11078.6572265625
Iteration 13600: Loss = -11078.65625
Iteration 13700: Loss = -11078.6572265625
1
Iteration 13800: Loss = -11078.6533203125
Iteration 13900: Loss = -11078.6533203125
Iteration 14000: Loss = -11078.6533203125
Iteration 14100: Loss = -11078.6533203125
Iteration 14200: Loss = -11078.65234375
Iteration 14300: Loss = -11078.65234375
Iteration 14400: Loss = -11078.576171875
Iteration 14500: Loss = -11078.1669921875
Iteration 14600: Loss = -11078.1669921875
Iteration 14700: Loss = -11078.1669921875
Iteration 14800: Loss = -11078.1689453125
1
Iteration 14900: Loss = -11078.1650390625
Iteration 15000: Loss = -11078.166015625
1
Iteration 15100: Loss = -11078.1650390625
Iteration 15200: Loss = -11078.1640625
Iteration 15300: Loss = -11078.1650390625
1
Iteration 15400: Loss = -11078.1640625
Iteration 15500: Loss = -11078.1640625
Iteration 15600: Loss = -11078.1640625
Iteration 15700: Loss = -11078.162109375
Iteration 15800: Loss = -11078.1630859375
1
Iteration 15900: Loss = -11078.162109375
Iteration 16000: Loss = -11078.1416015625
Iteration 16100: Loss = -11078.1416015625
Iteration 16200: Loss = -11078.1416015625
Iteration 16300: Loss = -11078.140625
Iteration 16400: Loss = -11078.1396484375
Iteration 16500: Loss = -11078.1416015625
1
Iteration 16600: Loss = -11078.1396484375
Iteration 16700: Loss = -11078.140625
1
Iteration 16800: Loss = -11078.140625
2
Iteration 16900: Loss = -11078.138671875
Iteration 17000: Loss = -11078.1376953125
Iteration 17100: Loss = -11078.13671875
Iteration 17200: Loss = -11078.1376953125
1
Iteration 17300: Loss = -11078.1376953125
2
Iteration 17400: Loss = -11078.1376953125
3
Iteration 17500: Loss = -11078.13671875
Iteration 17600: Loss = -11078.13671875
Iteration 17700: Loss = -11078.125
Iteration 17800: Loss = -11078.1259765625
1
Iteration 17900: Loss = -11078.1240234375
Iteration 18000: Loss = -11078.125
1
Iteration 18100: Loss = -11078.125
2
Iteration 18200: Loss = -11078.125
3
Iteration 18300: Loss = -11078.123046875
Iteration 18400: Loss = -11078.123046875
Iteration 18500: Loss = -11078.123046875
Iteration 18600: Loss = -11078.1240234375
1
Iteration 18700: Loss = -11078.111328125
Iteration 18800: Loss = -11077.8447265625
Iteration 18900: Loss = -11077.8466796875
1
Iteration 19000: Loss = -11077.8447265625
Iteration 19100: Loss = -11077.8466796875
1
Iteration 19200: Loss = -11077.8447265625
Iteration 19300: Loss = -11077.845703125
1
Iteration 19400: Loss = -11077.8447265625
Iteration 19500: Loss = -11077.849609375
1
Iteration 19600: Loss = -11077.8447265625
Iteration 19700: Loss = -11077.8447265625
Iteration 19800: Loss = -11077.845703125
1
Iteration 19900: Loss = -11077.8447265625
Iteration 20000: Loss = -11077.8447265625
Iteration 20100: Loss = -11077.845703125
1
Iteration 20200: Loss = -11077.8447265625
Iteration 20300: Loss = -11077.845703125
1
Iteration 20400: Loss = -11077.84375
Iteration 20500: Loss = -11077.8447265625
1
Iteration 20600: Loss = -11077.84375
Iteration 20700: Loss = -11077.8447265625
1
Iteration 20800: Loss = -11077.845703125
2
Iteration 20900: Loss = -11077.841796875
Iteration 21000: Loss = -11077.8408203125
Iteration 21100: Loss = -11077.83984375
Iteration 21200: Loss = -11077.841796875
1
Iteration 21300: Loss = -11077.841796875
2
Iteration 21400: Loss = -11077.841796875
3
Iteration 21500: Loss = -11077.8408203125
4
Iteration 21600: Loss = -11077.8408203125
5
Iteration 21700: Loss = -11077.8408203125
6
Iteration 21800: Loss = -11077.8408203125
7
Iteration 21900: Loss = -11077.841796875
8
Iteration 22000: Loss = -11077.841796875
9
Iteration 22100: Loss = -11077.841796875
10
Iteration 22200: Loss = -11077.8408203125
11
Iteration 22300: Loss = -11077.841796875
12
Iteration 22400: Loss = -11077.8408203125
13
Iteration 22500: Loss = -11077.8408203125
14
Iteration 22600: Loss = -11077.8388671875
Iteration 22700: Loss = -11077.8388671875
Iteration 22800: Loss = -11077.8388671875
Iteration 22900: Loss = -11077.8388671875
Iteration 23000: Loss = -11077.837890625
Iteration 23100: Loss = -11077.8388671875
1
Iteration 23200: Loss = -11077.837890625
Iteration 23300: Loss = -11077.837890625
Iteration 23400: Loss = -11077.8388671875
1
Iteration 23500: Loss = -11077.83984375
2
Iteration 23600: Loss = -11077.8388671875
3
Iteration 23700: Loss = -11077.8388671875
4
Iteration 23800: Loss = -11077.8388671875
5
Iteration 23900: Loss = -11077.837890625
Iteration 24000: Loss = -11077.83984375
1
Iteration 24100: Loss = -11077.8388671875
2
Iteration 24200: Loss = -11077.8388671875
3
Iteration 24300: Loss = -11077.8388671875
4
Iteration 24400: Loss = -11077.8388671875
5
Iteration 24500: Loss = -11077.8388671875
6
Iteration 24600: Loss = -11077.83984375
7
Iteration 24700: Loss = -11077.83984375
8
Iteration 24800: Loss = -11077.8388671875
9
Iteration 24900: Loss = -11077.8388671875
10
Iteration 25000: Loss = -11077.837890625
Iteration 25100: Loss = -11077.8388671875
1
Iteration 25200: Loss = -11077.8388671875
2
Iteration 25300: Loss = -11077.837890625
Iteration 25400: Loss = -11077.8388671875
1
Iteration 25500: Loss = -11077.83984375
2
Iteration 25600: Loss = -11077.8388671875
3
Iteration 25700: Loss = -11077.83984375
4
Iteration 25800: Loss = -11077.8388671875
5
Iteration 25900: Loss = -11077.8388671875
6
Iteration 26000: Loss = -11077.8388671875
7
Iteration 26100: Loss = -11077.8388671875
8
Iteration 26200: Loss = -11077.841796875
9
Iteration 26300: Loss = -11077.8388671875
10
Iteration 26400: Loss = -11077.8388671875
11
Iteration 26500: Loss = -11077.8388671875
12
Iteration 26600: Loss = -11077.83984375
13
Iteration 26700: Loss = -11077.8388671875
14
Iteration 26800: Loss = -11077.8388671875
15
Stopping early at iteration 26800 due to no improvement.
pi: tensor([[0.5683, 0.4317],
        [0.4682, 0.5318]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 5.3593e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1760, 0.1619],
         [0.1291, 0.2696]],

        [[0.0310, 0.1083],
         [0.0104, 0.7941]],

        [[0.7337, 0.1029],
         [0.9446, 0.2757]],

        [[0.6532, 0.1139],
         [0.9527, 0.9924]],

        [[0.0395, 0.1055],
         [0.0382, 0.9117]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080123577576726
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369772540766506
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 14
Adjusted Rand Index: 0.5135353535353535
Global Adjusted Rand Index: 0.15184795323620875
Average Adjusted Rand Index: 0.5661292354981777
[-0.0017622767853717912, 0.15184795323620875] [0.002076227299044664, 0.5661292354981777] [11133.5283203125, 11077.8388671875]
-------------------------------------
This iteration is 11
True Objective function: Loss = -10918.739822650363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -60109.0703125
Iteration 100: Loss = -45079.59765625
Iteration 200: Loss = -33132.0390625
Iteration 300: Loss = -24088.67578125
Iteration 400: Loss = -17881.681640625
Iteration 500: Loss = -14205.31640625
Iteration 600: Loss = -12395.7412109375
Iteration 700: Loss = -11597.31640625
Iteration 800: Loss = -11273.1845703125
Iteration 900: Loss = -11121.205078125
Iteration 1000: Loss = -11061.455078125
Iteration 1100: Loss = -11037.6650390625
Iteration 1200: Loss = -11022.744140625
Iteration 1300: Loss = -11009.005859375
Iteration 1400: Loss = -10995.5791015625
Iteration 1500: Loss = -10985.8017578125
Iteration 1600: Loss = -10980.736328125
Iteration 1700: Loss = -10977.0615234375
Iteration 1800: Loss = -10974.84765625
Iteration 1900: Loss = -10973.09375
Iteration 2000: Loss = -10971.091796875
Iteration 2100: Loss = -10967.974609375
Iteration 2200: Loss = -10963.6005859375
Iteration 2300: Loss = -10960.5419921875
Iteration 2400: Loss = -10958.228515625
Iteration 2500: Loss = -10956.1435546875
Iteration 2600: Loss = -10954.4267578125
Iteration 2700: Loss = -10953.3779296875
Iteration 2800: Loss = -10952.4912109375
Iteration 2900: Loss = -10951.4072265625
Iteration 3000: Loss = -10950.6279296875
Iteration 3100: Loss = -10949.9521484375
Iteration 3200: Loss = -10949.37109375
Iteration 3300: Loss = -10948.474609375
Iteration 3400: Loss = -10943.79296875
Iteration 3500: Loss = -10942.3466796875
Iteration 3600: Loss = -10939.958984375
Iteration 3700: Loss = -10937.662109375
Iteration 3800: Loss = -10933.9150390625
Iteration 3900: Loss = -10930.6728515625
Iteration 4000: Loss = -10927.80078125
Iteration 4100: Loss = -10924.736328125
Iteration 4200: Loss = -10923.232421875
Iteration 4300: Loss = -10920.9072265625
Iteration 4400: Loss = -10919.3701171875
Iteration 4500: Loss = -10916.783203125
Iteration 4600: Loss = -10912.515625
Iteration 4700: Loss = -10904.677734375
Iteration 4800: Loss = -10899.0498046875
Iteration 4900: Loss = -10891.197265625
Iteration 5000: Loss = -10887.7978515625
Iteration 5100: Loss = -10886.7333984375
Iteration 5200: Loss = -10885.6298828125
Iteration 5300: Loss = -10885.041015625
Iteration 5400: Loss = -10883.9189453125
Iteration 5500: Loss = -10882.5693359375
Iteration 5600: Loss = -10882.1396484375
Iteration 5700: Loss = -10882.0107421875
Iteration 5800: Loss = -10881.9130859375
Iteration 5900: Loss = -10881.755859375
Iteration 6000: Loss = -10881.6357421875
Iteration 6100: Loss = -10881.564453125
Iteration 6200: Loss = -10881.5029296875
Iteration 6300: Loss = -10878.3701171875
Iteration 6400: Loss = -10877.548828125
Iteration 6500: Loss = -10877.4921875
Iteration 6600: Loss = -10877.4599609375
Iteration 6700: Loss = -10877.4140625
Iteration 6800: Loss = -10877.380859375
Iteration 6900: Loss = -10877.3681640625
Iteration 7000: Loss = -10877.3583984375
Iteration 7100: Loss = -10877.3505859375
Iteration 7200: Loss = -10877.34375
Iteration 7300: Loss = -10877.337890625
Iteration 7400: Loss = -10877.3310546875
Iteration 7500: Loss = -10877.322265625
Iteration 7600: Loss = -10877.314453125
Iteration 7700: Loss = -10877.3056640625
Iteration 7800: Loss = -10877.294921875
Iteration 7900: Loss = -10877.26953125
Iteration 8000: Loss = -10877.1240234375
Iteration 8100: Loss = -10877.109375
Iteration 8200: Loss = -10877.1044921875
Iteration 8300: Loss = -10877.09765625
Iteration 8400: Loss = -10877.0654296875
Iteration 8500: Loss = -10877.0546875
Iteration 8600: Loss = -10877.052734375
Iteration 8700: Loss = -10877.05078125
Iteration 8800: Loss = -10877.0498046875
Iteration 8900: Loss = -10877.046875
Iteration 9000: Loss = -10877.044921875
Iteration 9100: Loss = -10877.0068359375
Iteration 9200: Loss = -10877.00390625
Iteration 9300: Loss = -10876.9873046875
Iteration 9400: Loss = -10876.9853515625
Iteration 9500: Loss = -10876.984375
Iteration 9600: Loss = -10876.9833984375
Iteration 9700: Loss = -10876.9833984375
Iteration 9800: Loss = -10876.9814453125
Iteration 9900: Loss = -10876.9814453125
Iteration 10000: Loss = -10876.98046875
Iteration 10100: Loss = -10876.978515625
Iteration 10200: Loss = -10876.978515625
Iteration 10300: Loss = -10876.9775390625
Iteration 10400: Loss = -10876.9775390625
Iteration 10500: Loss = -10876.958984375
Iteration 10600: Loss = -10876.9599609375
1
Iteration 10700: Loss = -10876.958984375
Iteration 10800: Loss = -10876.958984375
Iteration 10900: Loss = -10876.9580078125
Iteration 11000: Loss = -10876.9541015625
Iteration 11100: Loss = -10876.94140625
Iteration 11200: Loss = -10876.94140625
Iteration 11300: Loss = -10876.8017578125
Iteration 11400: Loss = -10876.794921875
Iteration 11500: Loss = -10876.794921875
Iteration 11600: Loss = -10876.7939453125
Iteration 11700: Loss = -10876.79296875
Iteration 11800: Loss = -10876.791015625
Iteration 11900: Loss = -10876.791015625
Iteration 12000: Loss = -10876.791015625
Iteration 12100: Loss = -10876.791015625
Iteration 12200: Loss = -10876.7919921875
1
Iteration 12300: Loss = -10876.791015625
Iteration 12400: Loss = -10876.791015625
Iteration 12500: Loss = -10876.7900390625
Iteration 12600: Loss = -10876.7900390625
Iteration 12700: Loss = -10876.7900390625
Iteration 12800: Loss = -10876.7890625
Iteration 12900: Loss = -10876.7890625
Iteration 13000: Loss = -10876.783203125
Iteration 13100: Loss = -10876.7783203125
Iteration 13200: Loss = -10876.7783203125
Iteration 13300: Loss = -10876.7783203125
Iteration 13400: Loss = -10876.7783203125
Iteration 13500: Loss = -10876.779296875
1
Iteration 13600: Loss = -10876.779296875
2
Iteration 13700: Loss = -10876.7783203125
Iteration 13800: Loss = -10876.7783203125
Iteration 13900: Loss = -10876.779296875
1
Iteration 14000: Loss = -10876.7783203125
Iteration 14100: Loss = -10876.7783203125
Iteration 14200: Loss = -10876.7763671875
Iteration 14300: Loss = -10876.7783203125
1
Iteration 14400: Loss = -10876.7783203125
2
Iteration 14500: Loss = -10876.77734375
3
Iteration 14600: Loss = -10876.7783203125
4
Iteration 14700: Loss = -10876.7763671875
Iteration 14800: Loss = -10876.7744140625
Iteration 14900: Loss = -10876.7734375
Iteration 15000: Loss = -10876.7744140625
1
Iteration 15100: Loss = -10876.7744140625
2
Iteration 15200: Loss = -10876.7724609375
Iteration 15300: Loss = -10876.771484375
Iteration 15400: Loss = -10876.771484375
Iteration 15500: Loss = -10876.771484375
Iteration 15600: Loss = -10876.7705078125
Iteration 15700: Loss = -10876.771484375
1
Iteration 15800: Loss = -10876.771484375
2
Iteration 15900: Loss = -10876.7646484375
Iteration 16000: Loss = -10876.7646484375
Iteration 16100: Loss = -10876.763671875
Iteration 16200: Loss = -10876.7646484375
1
Iteration 16300: Loss = -10876.7646484375
2
Iteration 16400: Loss = -10876.7646484375
3
Iteration 16500: Loss = -10876.763671875
Iteration 16600: Loss = -10876.7646484375
1
Iteration 16700: Loss = -10876.7626953125
Iteration 16800: Loss = -10876.7587890625
Iteration 16900: Loss = -10876.7431640625
Iteration 17000: Loss = -10876.740234375
Iteration 17100: Loss = -10876.740234375
Iteration 17200: Loss = -10876.740234375
Iteration 17300: Loss = -10876.7412109375
1
Iteration 17400: Loss = -10876.740234375
Iteration 17500: Loss = -10876.740234375
Iteration 17600: Loss = -10876.7392578125
Iteration 17700: Loss = -10876.740234375
1
Iteration 17800: Loss = -10876.740234375
2
Iteration 17900: Loss = -10876.7412109375
3
Iteration 18000: Loss = -10876.73828125
Iteration 18100: Loss = -10876.7392578125
1
Iteration 18200: Loss = -10876.740234375
2
Iteration 18300: Loss = -10876.740234375
3
Iteration 18400: Loss = -10876.740234375
4
Iteration 18500: Loss = -10876.7392578125
5
Iteration 18600: Loss = -10876.73828125
Iteration 18700: Loss = -10876.7353515625
Iteration 18800: Loss = -10876.736328125
1
Iteration 18900: Loss = -10876.7353515625
Iteration 19000: Loss = -10876.7353515625
Iteration 19100: Loss = -10876.7353515625
Iteration 19200: Loss = -10876.7353515625
Iteration 19300: Loss = -10876.7353515625
Iteration 19400: Loss = -10876.734375
Iteration 19500: Loss = -10876.7353515625
1
Iteration 19600: Loss = -10876.7353515625
2
Iteration 19700: Loss = -10876.734375
Iteration 19800: Loss = -10876.734375
Iteration 19900: Loss = -10876.736328125
1
Iteration 20000: Loss = -10876.734375
Iteration 20100: Loss = -10876.7353515625
1
Iteration 20200: Loss = -10876.734375
Iteration 20300: Loss = -10876.7353515625
1
Iteration 20400: Loss = -10876.734375
Iteration 20500: Loss = -10876.7333984375
Iteration 20600: Loss = -10876.734375
1
Iteration 20700: Loss = -10876.7353515625
2
Iteration 20800: Loss = -10876.7353515625
3
Iteration 20900: Loss = -10876.7353515625
4
Iteration 21000: Loss = -10876.7265625
Iteration 21100: Loss = -10876.728515625
1
Iteration 21200: Loss = -10876.728515625
2
Iteration 21300: Loss = -10876.728515625
3
Iteration 21400: Loss = -10876.7275390625
4
Iteration 21500: Loss = -10876.7275390625
5
Iteration 21600: Loss = -10876.7275390625
6
Iteration 21700: Loss = -10876.7275390625
7
Iteration 21800: Loss = -10876.728515625
8
Iteration 21900: Loss = -10876.7265625
Iteration 22000: Loss = -10876.7275390625
1
Iteration 22100: Loss = -10876.728515625
2
Iteration 22200: Loss = -10876.7265625
Iteration 22300: Loss = -10876.7275390625
1
Iteration 22400: Loss = -10876.728515625
2
Iteration 22500: Loss = -10876.728515625
3
Iteration 22600: Loss = -10876.728515625
4
Iteration 22700: Loss = -10876.7275390625
5
Iteration 22800: Loss = -10876.7275390625
6
Iteration 22900: Loss = -10876.7265625
Iteration 23000: Loss = -10876.728515625
1
Iteration 23100: Loss = -10876.7275390625
2
Iteration 23200: Loss = -10876.728515625
3
Iteration 23300: Loss = -10876.7275390625
4
Iteration 23400: Loss = -10876.7275390625
5
Iteration 23500: Loss = -10876.7265625
Iteration 23600: Loss = -10876.7275390625
1
Iteration 23700: Loss = -10876.7275390625
2
Iteration 23800: Loss = -10876.728515625
3
Iteration 23900: Loss = -10876.728515625
4
Iteration 24000: Loss = -10876.7265625
Iteration 24100: Loss = -10876.7265625
Iteration 24200: Loss = -10876.728515625
1
Iteration 24300: Loss = -10876.7265625
Iteration 24400: Loss = -10876.728515625
1
Iteration 24500: Loss = -10876.7275390625
2
Iteration 24600: Loss = -10876.7275390625
3
Iteration 24700: Loss = -10876.728515625
4
Iteration 24800: Loss = -10876.7275390625
5
Iteration 24900: Loss = -10876.728515625
6
Iteration 25000: Loss = -10876.7275390625
7
Iteration 25100: Loss = -10876.7265625
Iteration 25200: Loss = -10876.728515625
1
Iteration 25300: Loss = -10876.73046875
2
Iteration 25400: Loss = -10876.7265625
Iteration 25500: Loss = -10876.73046875
1
Iteration 25600: Loss = -10876.7265625
Iteration 25700: Loss = -10876.7275390625
1
Iteration 25800: Loss = -10876.7275390625
2
Iteration 25900: Loss = -10876.7275390625
3
Iteration 26000: Loss = -10876.7275390625
4
Iteration 26100: Loss = -10876.7275390625
5
Iteration 26200: Loss = -10876.7275390625
6
Iteration 26300: Loss = -10876.73046875
7
Iteration 26400: Loss = -10876.728515625
8
Iteration 26500: Loss = -10876.728515625
9
Iteration 26600: Loss = -10876.7275390625
10
Iteration 26700: Loss = -10876.7275390625
11
Iteration 26800: Loss = -10876.7265625
Iteration 26900: Loss = -10876.7275390625
1
Iteration 27000: Loss = -10876.7255859375
Iteration 27100: Loss = -10876.7294921875
1
Iteration 27200: Loss = -10876.7265625
2
Iteration 27300: Loss = -10876.7265625
3
Iteration 27400: Loss = -10876.7265625
4
Iteration 27500: Loss = -10876.7265625
5
Iteration 27600: Loss = -10876.7275390625
6
Iteration 27700: Loss = -10876.7275390625
7
Iteration 27800: Loss = -10876.7275390625
8
Iteration 27900: Loss = -10876.7275390625
9
Iteration 28000: Loss = -10876.7265625
10
Iteration 28100: Loss = -10876.7275390625
11
Iteration 28200: Loss = -10876.7275390625
12
Iteration 28300: Loss = -10876.7265625
13
Iteration 28400: Loss = -10876.7275390625
14
Iteration 28500: Loss = -10876.7275390625
15
Stopping early at iteration 28500 due to no improvement.
pi: tensor([[0.3950, 0.6050],
        [0.6543, 0.3457]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4741, 0.5259], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2233, 0.1009],
         [0.9871, 0.2206]],

        [[0.8189, 0.0923],
         [0.0333, 0.1604]],

        [[0.2136, 0.0998],
         [0.7149, 0.0261]],

        [[0.0326, 0.1028],
         [0.2034, 0.1320]],

        [[0.5964, 0.1002],
         [0.9837, 0.0073]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080650161849864
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6691025770992097
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.6046287815855185
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 12
Adjusted Rand Index: 0.5733494225133597
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 20
Adjusted Rand Index: 0.353778413555379
Global Adjusted Rand Index: 0.008013218243172108
Average Adjusted Rand Index: 0.6017848421876907
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27096.576171875
Iteration 100: Loss = -16757.05859375
Iteration 200: Loss = -12247.60546875
Iteration 300: Loss = -11352.05859375
Iteration 400: Loss = -11192.1923828125
Iteration 500: Loss = -11113.248046875
Iteration 600: Loss = -11070.669921875
Iteration 700: Loss = -11045.6044921875
Iteration 800: Loss = -11030.6083984375
Iteration 900: Loss = -11018.2421875
Iteration 1000: Loss = -11009.46875
Iteration 1100: Loss = -11003.1953125
Iteration 1200: Loss = -10999.03515625
Iteration 1300: Loss = -10995.3486328125
Iteration 1400: Loss = -10983.458984375
Iteration 1500: Loss = -10980.734375
Iteration 1600: Loss = -10969.57421875
Iteration 1700: Loss = -10964.857421875
Iteration 1800: Loss = -10963.1044921875
Iteration 1900: Loss = -10959.046875
Iteration 2000: Loss = -10957.8681640625
Iteration 2100: Loss = -10956.9921875
Iteration 2200: Loss = -10956.2578125
Iteration 2300: Loss = -10955.6328125
Iteration 2400: Loss = -10955.0908203125
Iteration 2500: Loss = -10954.6162109375
Iteration 2600: Loss = -10954.19921875
Iteration 2700: Loss = -10953.8271484375
Iteration 2800: Loss = -10953.494140625
Iteration 2900: Loss = -10953.1982421875
Iteration 3000: Loss = -10952.935546875
Iteration 3100: Loss = -10952.697265625
Iteration 3200: Loss = -10952.4833984375
Iteration 3300: Loss = -10952.287109375
Iteration 3400: Loss = -10952.1083984375
Iteration 3500: Loss = -10951.943359375
Iteration 3600: Loss = -10951.791015625
Iteration 3700: Loss = -10951.650390625
Iteration 3800: Loss = -10951.5224609375
Iteration 3900: Loss = -10951.404296875
Iteration 4000: Loss = -10951.2958984375
Iteration 4100: Loss = -10951.193359375
Iteration 4200: Loss = -10951.099609375
Iteration 4300: Loss = -10951.01171875
Iteration 4400: Loss = -10950.9296875
Iteration 4500: Loss = -10950.8544921875
Iteration 4600: Loss = -10950.7822265625
Iteration 4700: Loss = -10950.716796875
Iteration 4800: Loss = -10950.6533203125
Iteration 4900: Loss = -10950.5947265625
Iteration 5000: Loss = -10950.5400390625
Iteration 5100: Loss = -10950.48828125
Iteration 5200: Loss = -10950.44140625
Iteration 5300: Loss = -10950.39453125
Iteration 5400: Loss = -10950.3515625
Iteration 5500: Loss = -10950.3115234375
Iteration 5600: Loss = -10950.2724609375
Iteration 5700: Loss = -10950.236328125
Iteration 5800: Loss = -10950.201171875
Iteration 5900: Loss = -10950.169921875
Iteration 6000: Loss = -10950.1396484375
Iteration 6100: Loss = -10950.111328125
Iteration 6200: Loss = -10950.0869140625
Iteration 6300: Loss = -10950.056640625
Iteration 6400: Loss = -10950.03515625
Iteration 6500: Loss = -10950.0126953125
Iteration 6600: Loss = -10949.9892578125
Iteration 6700: Loss = -10949.96875
Iteration 6800: Loss = -10949.9482421875
Iteration 6900: Loss = -10949.9296875
Iteration 7000: Loss = -10949.912109375
Iteration 7100: Loss = -10949.89453125
Iteration 7200: Loss = -10949.8779296875
Iteration 7300: Loss = -10949.8642578125
Iteration 7400: Loss = -10949.849609375
Iteration 7500: Loss = -10949.8349609375
Iteration 7600: Loss = -10949.8203125
Iteration 7700: Loss = -10949.80859375
Iteration 7800: Loss = -10949.7978515625
Iteration 7900: Loss = -10949.787109375
Iteration 8000: Loss = -10949.7763671875
Iteration 8100: Loss = -10949.765625
Iteration 8200: Loss = -10949.7568359375
Iteration 8300: Loss = -10949.7451171875
Iteration 8400: Loss = -10949.736328125
Iteration 8500: Loss = -10949.7294921875
Iteration 8600: Loss = -10949.720703125
Iteration 8700: Loss = -10949.7138671875
Iteration 8800: Loss = -10949.7060546875
Iteration 8900: Loss = -10949.7001953125
Iteration 9000: Loss = -10949.69140625
Iteration 9100: Loss = -10949.6845703125
Iteration 9200: Loss = -10949.6806640625
Iteration 9300: Loss = -10949.673828125
Iteration 9400: Loss = -10949.6689453125
Iteration 9500: Loss = -10949.6630859375
Iteration 9600: Loss = -10949.658203125
Iteration 9700: Loss = -10949.65234375
Iteration 9800: Loss = -10949.6474609375
Iteration 9900: Loss = -10949.6435546875
Iteration 10000: Loss = -10949.6396484375
Iteration 10100: Loss = -10949.6337890625
Iteration 10200: Loss = -10949.6298828125
Iteration 10300: Loss = -10949.6259765625
Iteration 10400: Loss = -10949.6240234375
Iteration 10500: Loss = -10949.6201171875
Iteration 10600: Loss = -10949.6171875
Iteration 10700: Loss = -10949.615234375
Iteration 10800: Loss = -10949.611328125
Iteration 10900: Loss = -10949.609375
Iteration 11000: Loss = -10949.6064453125
Iteration 11100: Loss = -10949.6044921875
Iteration 11200: Loss = -10949.6025390625
Iteration 11300: Loss = -10949.599609375
Iteration 11400: Loss = -10949.5966796875
Iteration 11500: Loss = -10949.5947265625
Iteration 11600: Loss = -10949.5947265625
Iteration 11700: Loss = -10949.5927734375
Iteration 11800: Loss = -10949.58984375
Iteration 11900: Loss = -10949.5810546875
Iteration 12000: Loss = -10949.57421875
Iteration 12100: Loss = -10949.56640625
Iteration 12200: Loss = -10949.55859375
Iteration 12300: Loss = -10949.548828125
Iteration 12400: Loss = -10949.5400390625
Iteration 12500: Loss = -10949.5341796875
Iteration 12600: Loss = -10949.52734375
Iteration 12700: Loss = -10949.521484375
Iteration 12800: Loss = -10949.51171875
Iteration 12900: Loss = -10949.50390625
Iteration 13000: Loss = -10949.4951171875
Iteration 13100: Loss = -10949.484375
Iteration 13200: Loss = -10949.47265625
Iteration 13300: Loss = -10949.458984375
Iteration 13400: Loss = -10949.4462890625
Iteration 13500: Loss = -10949.427734375
Iteration 13600: Loss = -10949.40234375
Iteration 13700: Loss = -10949.36328125
Iteration 13800: Loss = -10949.3408203125
Iteration 13900: Loss = -10949.3076171875
Iteration 14000: Loss = -10949.2890625
Iteration 14100: Loss = -10949.2041015625
Iteration 14200: Loss = -10949.083984375
Iteration 14300: Loss = -10948.9423828125
Iteration 14400: Loss = -10948.8115234375
Iteration 14500: Loss = -10948.7646484375
Iteration 14600: Loss = -10948.7353515625
Iteration 14700: Loss = -10948.7236328125
Iteration 14800: Loss = -10948.716796875
Iteration 14900: Loss = -10948.7060546875
Iteration 15000: Loss = -10948.705078125
Iteration 15100: Loss = -10948.7041015625
Iteration 15200: Loss = -10948.6982421875
Iteration 15300: Loss = -10948.4833984375
Iteration 15400: Loss = -10948.3291015625
Iteration 15500: Loss = -10948.3291015625
Iteration 15600: Loss = -10948.3271484375
Iteration 15700: Loss = -10948.3271484375
Iteration 15800: Loss = -10948.3271484375
Iteration 15900: Loss = -10948.3271484375
Iteration 16000: Loss = -10948.326171875
Iteration 16100: Loss = -10948.3271484375
1
Iteration 16200: Loss = -10948.326171875
Iteration 16300: Loss = -10948.3271484375
1
Iteration 16400: Loss = -10948.326171875
Iteration 16500: Loss = -10948.326171875
Iteration 16600: Loss = -10948.3271484375
1
Iteration 16700: Loss = -10948.3251953125
Iteration 16800: Loss = -10948.326171875
1
Iteration 16900: Loss = -10948.3251953125
Iteration 17000: Loss = -10948.326171875
1
Iteration 17100: Loss = -10948.326171875
2
Iteration 17200: Loss = -10948.326171875
3
Iteration 17300: Loss = -10948.3251953125
Iteration 17400: Loss = -10948.326171875
1
Iteration 17500: Loss = -10948.326171875
2
Iteration 17600: Loss = -10948.3251953125
Iteration 17700: Loss = -10948.326171875
1
Iteration 17800: Loss = -10948.326171875
2
Iteration 17900: Loss = -10948.326171875
3
Iteration 18000: Loss = -10948.3271484375
4
Iteration 18100: Loss = -10948.326171875
5
Iteration 18200: Loss = -10948.3271484375
6
Iteration 18300: Loss = -10948.32421875
Iteration 18400: Loss = -10948.326171875
1
Iteration 18500: Loss = -10948.326171875
2
Iteration 18600: Loss = -10948.3271484375
3
Iteration 18700: Loss = -10948.326171875
4
Iteration 18800: Loss = -10948.3251953125
5
Iteration 18900: Loss = -10948.326171875
6
Iteration 19000: Loss = -10948.3271484375
7
Iteration 19100: Loss = -10948.3251953125
8
Iteration 19200: Loss = -10948.326171875
9
Iteration 19300: Loss = -10948.3251953125
10
Iteration 19400: Loss = -10948.328125
11
Iteration 19500: Loss = -10948.3251953125
12
Iteration 19600: Loss = -10948.3251953125
13
Iteration 19700: Loss = -10948.3251953125
14
Iteration 19800: Loss = -10948.3251953125
15
Stopping early at iteration 19800 due to no improvement.
pi: tensor([[6.2094e-01, 3.7906e-01],
        [1.6931e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2399, 0.7601], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1954, 0.1717],
         [0.0084, 0.1583]],

        [[0.0223, 0.1829],
         [0.9054, 0.3489]],

        [[0.9523, 0.1709],
         [0.9919, 0.8176]],

        [[0.9280, 0.1638],
         [0.0412, 0.2303]],

        [[0.9711, 0.1477],
         [0.0641, 0.1591]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00032091907810639935
Average Adjusted Rand Index: -0.0005784021178947197
[0.008013218243172108, 0.00032091907810639935] [0.6017848421876907, -0.0005784021178947197] [10876.7275390625, 10948.3251953125]
-------------------------------------
This iteration is 12
True Objective function: Loss = -10942.982276960178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24680.681640625
Iteration 100: Loss = -16025.875
Iteration 200: Loss = -11917.4931640625
Iteration 300: Loss = -11448.66796875
Iteration 400: Loss = -11333.6904296875
Iteration 500: Loss = -11274.994140625
Iteration 600: Loss = -11240.8359375
Iteration 700: Loss = -11217.1357421875
Iteration 800: Loss = -11200.51171875
Iteration 900: Loss = -11189.634765625
Iteration 1000: Loss = -11180.6064453125
Iteration 1100: Loss = -11171.822265625
Iteration 1200: Loss = -11162.353515625
Iteration 1300: Loss = -11157.1220703125
Iteration 1400: Loss = -11151.4580078125
Iteration 1500: Loss = -11147.6220703125
Iteration 1600: Loss = -11143.171875
Iteration 1700: Loss = -11138.392578125
Iteration 1800: Loss = -11134.9228515625
Iteration 1900: Loss = -11131.6123046875
Iteration 2000: Loss = -11126.55859375
Iteration 2100: Loss = -11123.283203125
Iteration 2200: Loss = -11119.8095703125
Iteration 2300: Loss = -11116.7626953125
Iteration 2400: Loss = -11113.8525390625
Iteration 2500: Loss = -11110.16015625
Iteration 2600: Loss = -11106.66796875
Iteration 2700: Loss = -11103.853515625
Iteration 2800: Loss = -11102.056640625
Iteration 2900: Loss = -11100.7841796875
Iteration 3000: Loss = -11098.4423828125
Iteration 3100: Loss = -11095.16796875
Iteration 3200: Loss = -11092.1474609375
Iteration 3300: Loss = -11088.1708984375
Iteration 3400: Loss = -11087.0234375
Iteration 3500: Loss = -11086.4384765625
Iteration 3600: Loss = -11085.9814453125
Iteration 3700: Loss = -11085.4228515625
Iteration 3800: Loss = -11082.9716796875
Iteration 3900: Loss = -11082.0556640625
Iteration 4000: Loss = -11081.6611328125
Iteration 4100: Loss = -11081.38671875
Iteration 4200: Loss = -11081.1630859375
Iteration 4300: Loss = -11080.9892578125
Iteration 4400: Loss = -11080.841796875
Iteration 4500: Loss = -11080.7109375
Iteration 4600: Loss = -11080.556640625
Iteration 4700: Loss = -11076.9501953125
Iteration 4800: Loss = -11075.984375
Iteration 4900: Loss = -11075.716796875
Iteration 5000: Loss = -11075.5234375
Iteration 5100: Loss = -11075.3642578125
Iteration 5200: Loss = -11075.23046875
Iteration 5300: Loss = -11075.115234375
Iteration 5400: Loss = -11075.01171875
Iteration 5500: Loss = -11074.91796875
Iteration 5600: Loss = -11074.8349609375
Iteration 5700: Loss = -11074.759765625
Iteration 5800: Loss = -11074.69140625
Iteration 5900: Loss = -11074.6279296875
Iteration 6000: Loss = -11074.568359375
Iteration 6100: Loss = -11074.513671875
Iteration 6200: Loss = -11074.4619140625
Iteration 6300: Loss = -11074.4130859375
Iteration 6400: Loss = -11074.365234375
Iteration 6500: Loss = -11074.318359375
Iteration 6600: Loss = -11074.271484375
Iteration 6700: Loss = -11074.2236328125
Iteration 6800: Loss = -11074.1689453125
Iteration 6900: Loss = -11074.03515625
Iteration 7000: Loss = -11071.96875
Iteration 7100: Loss = -11070.2939453125
Iteration 7200: Loss = -11070.076171875
Iteration 7300: Loss = -11069.962890625
Iteration 7400: Loss = -11069.810546875
Iteration 7500: Loss = -11069.6201171875
Iteration 7600: Loss = -11069.57421875
Iteration 7700: Loss = -11069.5380859375
Iteration 7800: Loss = -11069.5048828125
Iteration 7900: Loss = -11069.474609375
Iteration 8000: Loss = -11069.44921875
Iteration 8100: Loss = -11069.42578125
Iteration 8200: Loss = -11069.404296875
Iteration 8300: Loss = -11069.3837890625
Iteration 8400: Loss = -11069.365234375
Iteration 8500: Loss = -11069.3486328125
Iteration 8600: Loss = -11069.3330078125
Iteration 8700: Loss = -11069.318359375
Iteration 8800: Loss = -11069.3046875
Iteration 8900: Loss = -11069.2900390625
Iteration 9000: Loss = -11069.279296875
Iteration 9100: Loss = -11069.2666015625
Iteration 9200: Loss = -11069.2568359375
Iteration 9300: Loss = -11069.244140625
Iteration 9400: Loss = -11069.2353515625
Iteration 9500: Loss = -11069.2255859375
Iteration 9600: Loss = -11069.2158203125
Iteration 9700: Loss = -11069.2080078125
Iteration 9800: Loss = -11069.201171875
Iteration 9900: Loss = -11069.193359375
Iteration 10000: Loss = -11069.1865234375
Iteration 10100: Loss = -11069.1796875
Iteration 10200: Loss = -11069.173828125
Iteration 10300: Loss = -11069.16796875
Iteration 10400: Loss = -11069.162109375
Iteration 10500: Loss = -11069.1552734375
Iteration 10600: Loss = -11069.150390625
Iteration 10700: Loss = -11069.146484375
Iteration 10800: Loss = -11069.142578125
Iteration 10900: Loss = -11069.1376953125
Iteration 11000: Loss = -11069.1357421875
Iteration 11100: Loss = -11069.130859375
Iteration 11200: Loss = -11069.126953125
Iteration 11300: Loss = -11069.123046875
Iteration 11400: Loss = -11069.12109375
Iteration 11500: Loss = -11069.1181640625
Iteration 11600: Loss = -11069.1162109375
Iteration 11700: Loss = -11069.11328125
Iteration 11800: Loss = -11069.111328125
Iteration 11900: Loss = -11069.109375
Iteration 12000: Loss = -11069.1083984375
Iteration 12100: Loss = -11069.1064453125
Iteration 12200: Loss = -11069.10546875
Iteration 12300: Loss = -11069.1025390625
Iteration 12400: Loss = -11069.1044921875
1
Iteration 12500: Loss = -11069.1005859375
Iteration 12600: Loss = -11069.099609375
Iteration 12700: Loss = -11069.09765625
Iteration 12800: Loss = -11069.0966796875
Iteration 12900: Loss = -11069.095703125
Iteration 13000: Loss = -11069.095703125
Iteration 13100: Loss = -11069.09375
Iteration 13200: Loss = -11069.09375
Iteration 13300: Loss = -11069.091796875
Iteration 13400: Loss = -11069.0908203125
Iteration 13500: Loss = -11069.0908203125
Iteration 13600: Loss = -11069.08984375
Iteration 13700: Loss = -11069.087890625
Iteration 13800: Loss = -11069.087890625
Iteration 13900: Loss = -11069.0869140625
Iteration 14000: Loss = -11069.0859375
Iteration 14100: Loss = -11069.0859375
Iteration 14200: Loss = -11069.0849609375
Iteration 14300: Loss = -11069.083984375
Iteration 14400: Loss = -11069.083984375
Iteration 14500: Loss = -11069.083984375
Iteration 14600: Loss = -11069.0830078125
Iteration 14700: Loss = -11069.0830078125
Iteration 14800: Loss = -11069.0830078125
Iteration 14900: Loss = -11069.0810546875
Iteration 15000: Loss = -11069.08203125
1
Iteration 15100: Loss = -11069.08203125
2
Iteration 15200: Loss = -11069.08203125
3
Iteration 15300: Loss = -11069.0810546875
Iteration 15400: Loss = -11069.0810546875
Iteration 15500: Loss = -11069.0791015625
Iteration 15600: Loss = -11069.0791015625
Iteration 15700: Loss = -11069.080078125
1
Iteration 15800: Loss = -11069.078125
Iteration 15900: Loss = -11069.0791015625
1
Iteration 16000: Loss = -11069.0791015625
2
Iteration 16100: Loss = -11069.078125
Iteration 16200: Loss = -11069.078125
Iteration 16300: Loss = -11069.0771484375
Iteration 16400: Loss = -11069.078125
1
Iteration 16500: Loss = -11069.078125
2
Iteration 16600: Loss = -11069.078125
3
Iteration 16700: Loss = -11069.080078125
4
Iteration 16800: Loss = -11069.0771484375
Iteration 16900: Loss = -11069.076171875
Iteration 17000: Loss = -11069.0771484375
1
Iteration 17100: Loss = -11069.0771484375
2
Iteration 17200: Loss = -11069.076171875
Iteration 17300: Loss = -11069.076171875
Iteration 17400: Loss = -11069.080078125
1
Iteration 17500: Loss = -11069.076171875
Iteration 17600: Loss = -11069.076171875
Iteration 17700: Loss = -11069.0771484375
1
Iteration 17800: Loss = -11069.076171875
Iteration 17900: Loss = -11069.07421875
Iteration 18000: Loss = -11069.0791015625
1
Iteration 18100: Loss = -11069.0751953125
2
Iteration 18200: Loss = -11069.0751953125
3
Iteration 18300: Loss = -11069.07421875
Iteration 18400: Loss = -11069.076171875
1
Iteration 18500: Loss = -11069.07421875
Iteration 18600: Loss = -11069.0751953125
1
Iteration 18700: Loss = -11069.0751953125
2
Iteration 18800: Loss = -11069.0751953125
3
Iteration 18900: Loss = -11069.076171875
4
Iteration 19000: Loss = -11069.0751953125
5
Iteration 19100: Loss = -11069.076171875
6
Iteration 19200: Loss = -11069.07421875
Iteration 19300: Loss = -11069.07421875
Iteration 19400: Loss = -11069.076171875
1
Iteration 19500: Loss = -11069.07421875
Iteration 19600: Loss = -11069.07421875
Iteration 19700: Loss = -11069.0751953125
1
Iteration 19800: Loss = -11069.07421875
Iteration 19900: Loss = -11069.0751953125
1
Iteration 20000: Loss = -11069.0751953125
2
Iteration 20100: Loss = -11069.07421875
Iteration 20200: Loss = -11069.07421875
Iteration 20300: Loss = -11069.0751953125
1
Iteration 20400: Loss = -11069.0751953125
2
Iteration 20500: Loss = -11069.07421875
Iteration 20600: Loss = -11069.076171875
1
Iteration 20700: Loss = -11069.0751953125
2
Iteration 20800: Loss = -11069.0751953125
3
Iteration 20900: Loss = -11069.07421875
Iteration 21000: Loss = -11069.0751953125
1
Iteration 21100: Loss = -11069.076171875
2
Iteration 21200: Loss = -11069.07421875
Iteration 21300: Loss = -11069.0732421875
Iteration 21400: Loss = -11069.0751953125
1
Iteration 21500: Loss = -11069.07421875
2
Iteration 21600: Loss = -11069.07421875
3
Iteration 21700: Loss = -11069.07421875
4
Iteration 21800: Loss = -11069.0751953125
5
Iteration 21900: Loss = -11069.07421875
6
Iteration 22000: Loss = -11069.07421875
7
Iteration 22100: Loss = -11069.0751953125
8
Iteration 22200: Loss = -11069.0751953125
9
Iteration 22300: Loss = -11069.07421875
10
Iteration 22400: Loss = -11069.0732421875
Iteration 22500: Loss = -11069.0732421875
Iteration 22600: Loss = -11069.07421875
1
Iteration 22700: Loss = -11069.07421875
2
Iteration 22800: Loss = -11069.07421875
3
Iteration 22900: Loss = -11069.0751953125
4
Iteration 23000: Loss = -11069.07421875
5
Iteration 23100: Loss = -11069.07421875
6
Iteration 23200: Loss = -11069.0732421875
Iteration 23300: Loss = -11069.07421875
1
Iteration 23400: Loss = -11069.07421875
2
Iteration 23500: Loss = -11069.0751953125
3
Iteration 23600: Loss = -11069.07421875
4
Iteration 23700: Loss = -11069.0732421875
Iteration 23800: Loss = -11069.07421875
1
Iteration 23900: Loss = -11069.0751953125
2
Iteration 24000: Loss = -11069.07421875
3
Iteration 24100: Loss = -11069.0732421875
Iteration 24200: Loss = -11069.0732421875
Iteration 24300: Loss = -11069.0732421875
Iteration 24400: Loss = -11069.07421875
1
Iteration 24500: Loss = -11069.07421875
2
Iteration 24600: Loss = -11069.07421875
3
Iteration 24700: Loss = -11069.07421875
4
Iteration 24800: Loss = -11069.07421875
5
Iteration 24900: Loss = -11069.0732421875
Iteration 25000: Loss = -11069.07421875
1
Iteration 25100: Loss = -11069.0732421875
Iteration 25200: Loss = -11069.07421875
1
Iteration 25300: Loss = -11069.07421875
2
Iteration 25400: Loss = -11069.0732421875
Iteration 25500: Loss = -11069.07421875
1
Iteration 25600: Loss = -11069.0732421875
Iteration 25700: Loss = -11069.0751953125
1
Iteration 25800: Loss = -11069.0751953125
2
Iteration 25900: Loss = -11069.07421875
3
Iteration 26000: Loss = -11069.07421875
4
Iteration 26100: Loss = -11069.07421875
5
Iteration 26200: Loss = -11069.0732421875
Iteration 26300: Loss = -11068.3857421875
Iteration 26400: Loss = -11068.2705078125
Iteration 26500: Loss = -11068.271484375
1
Iteration 26600: Loss = -11068.26953125
Iteration 26700: Loss = -11068.2451171875
Iteration 26800: Loss = -11068.244140625
Iteration 26900: Loss = -11068.24609375
1
Iteration 27000: Loss = -11068.23828125
Iteration 27100: Loss = -11068.2275390625
Iteration 27200: Loss = -11068.23046875
1
Iteration 27300: Loss = -11068.2255859375
Iteration 27400: Loss = -11068.2275390625
1
Iteration 27500: Loss = -11068.228515625
2
Iteration 27600: Loss = -11068.228515625
3
Iteration 27700: Loss = -11068.2275390625
4
Iteration 27800: Loss = -11068.2275390625
5
Iteration 27900: Loss = -11068.2275390625
6
Iteration 28000: Loss = -11068.2255859375
Iteration 28100: Loss = -11068.2265625
1
Iteration 28200: Loss = -11068.2275390625
2
Iteration 28300: Loss = -11068.2265625
3
Iteration 28400: Loss = -11068.2265625
4
Iteration 28500: Loss = -11068.224609375
Iteration 28600: Loss = -11068.2265625
1
Iteration 28700: Loss = -11068.2255859375
2
Iteration 28800: Loss = -11068.2265625
3
Iteration 28900: Loss = -11068.2255859375
4
Iteration 29000: Loss = -11068.2265625
5
Iteration 29100: Loss = -11068.2265625
6
Iteration 29200: Loss = -11068.224609375
Iteration 29300: Loss = -11068.2265625
1
Iteration 29400: Loss = -11068.224609375
Iteration 29500: Loss = -11068.2255859375
1
Iteration 29600: Loss = -11068.2265625
2
Iteration 29700: Loss = -11068.224609375
Iteration 29800: Loss = -11068.2265625
1
Iteration 29900: Loss = -11068.2265625
2
pi: tensor([[6.4538e-01, 3.5462e-01],
        [1.2979e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0267, 0.9733], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3647, 0.0579],
         [0.9454, 0.1656]],

        [[0.8559, 0.1335],
         [0.4136, 0.0424]],

        [[0.3323, 0.2388],
         [0.2568, 0.7967]],

        [[0.8224, 0.2204],
         [0.1382, 0.1726]],

        [[0.0148, 0.1620],
         [0.0343, 0.0821]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0013742166364525297
Average Adjusted Rand Index: -0.0011522418127903108
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31939.27734375
Iteration 100: Loss = -19123.083984375
Iteration 200: Loss = -12773.9814453125
Iteration 300: Loss = -11779.4130859375
Iteration 400: Loss = -11541.6865234375
Iteration 500: Loss = -11409.7666015625
Iteration 600: Loss = -11338.5625
Iteration 700: Loss = -11290.3818359375
Iteration 800: Loss = -11255.37890625
Iteration 900: Loss = -11219.9560546875
Iteration 1000: Loss = -11196.765625
Iteration 1100: Loss = -11177.7685546875
Iteration 1200: Loss = -11161.66015625
Iteration 1300: Loss = -11144.6982421875
Iteration 1400: Loss = -11137.619140625
Iteration 1500: Loss = -11131.376953125
Iteration 1600: Loss = -11122.306640625
Iteration 1700: Loss = -11117.9765625
Iteration 1800: Loss = -11115.4287109375
Iteration 1900: Loss = -11113.080078125
Iteration 2000: Loss = -11111.5166015625
Iteration 2100: Loss = -11110.265625
Iteration 2200: Loss = -11109.1796875
Iteration 2300: Loss = -11108.1181640625
Iteration 2400: Loss = -11106.685546875
Iteration 2500: Loss = -11104.71875
Iteration 2600: Loss = -11103.783203125
Iteration 2700: Loss = -11103.134765625
Iteration 2800: Loss = -11102.55859375
Iteration 2900: Loss = -11101.9208984375
Iteration 3000: Loss = -11099.361328125
Iteration 3100: Loss = -11096.318359375
Iteration 3200: Loss = -11090.8837890625
Iteration 3300: Loss = -11089.861328125
Iteration 3400: Loss = -11089.3623046875
Iteration 3500: Loss = -11088.998046875
Iteration 3600: Loss = -11088.69921875
Iteration 3700: Loss = -11088.44140625
Iteration 3800: Loss = -11088.212890625
Iteration 3900: Loss = -11088.0087890625
Iteration 4000: Loss = -11087.8212890625
Iteration 4100: Loss = -11087.6513671875
Iteration 4200: Loss = -11087.48828125
Iteration 4300: Loss = -11087.3330078125
Iteration 4400: Loss = -11087.1552734375
Iteration 4500: Loss = -11085.328125
Iteration 4600: Loss = -11081.4697265625
Iteration 4700: Loss = -11080.4443359375
Iteration 4800: Loss = -11080.10546875
Iteration 4900: Loss = -11079.8818359375
Iteration 5000: Loss = -11079.712890625
Iteration 5100: Loss = -11079.564453125
Iteration 5200: Loss = -11079.4365234375
Iteration 5300: Loss = -11079.33203125
Iteration 5400: Loss = -11079.2392578125
Iteration 5500: Loss = -11079.1572265625
Iteration 5600: Loss = -11079.0810546875
Iteration 5700: Loss = -11079.0126953125
Iteration 5800: Loss = -11078.951171875
Iteration 5900: Loss = -11078.89453125
Iteration 6000: Loss = -11078.841796875
Iteration 6100: Loss = -11078.7919921875
Iteration 6200: Loss = -11078.74609375
Iteration 6300: Loss = -11078.7060546875
Iteration 6400: Loss = -11078.6669921875
Iteration 6500: Loss = -11078.62890625
Iteration 6600: Loss = -11078.595703125
Iteration 6700: Loss = -11078.5615234375
Iteration 6800: Loss = -11078.5302734375
Iteration 6900: Loss = -11078.498046875
Iteration 7000: Loss = -11078.4677734375
Iteration 7100: Loss = -11078.4375
Iteration 7200: Loss = -11078.4072265625
Iteration 7300: Loss = -11078.3671875
Iteration 7400: Loss = -11074.8330078125
Iteration 7500: Loss = -11073.5498046875
Iteration 7600: Loss = -11071.13671875
Iteration 7700: Loss = -11070.712890625
Iteration 7800: Loss = -11070.470703125
Iteration 7900: Loss = -11070.3095703125
Iteration 8000: Loss = -11070.189453125
Iteration 8100: Loss = -11070.0966796875
Iteration 8200: Loss = -11070.0244140625
Iteration 8300: Loss = -11069.962890625
Iteration 8400: Loss = -11069.9111328125
Iteration 8500: Loss = -11069.8662109375
Iteration 8600: Loss = -11069.828125
Iteration 8700: Loss = -11069.7939453125
Iteration 8800: Loss = -11069.765625
Iteration 8900: Loss = -11069.736328125
Iteration 9000: Loss = -11069.7109375
Iteration 9100: Loss = -11069.685546875
Iteration 9200: Loss = -11069.6650390625
Iteration 9300: Loss = -11069.6435546875
Iteration 9400: Loss = -11069.6240234375
Iteration 9500: Loss = -11069.599609375
Iteration 9600: Loss = -11069.57421875
Iteration 9700: Loss = -11069.533203125
Iteration 9800: Loss = -11069.462890625
Iteration 9900: Loss = -11069.359375
Iteration 10000: Loss = -11069.3056640625
Iteration 10100: Loss = -11069.287109375
Iteration 10200: Loss = -11069.2724609375
Iteration 10300: Loss = -11069.2587890625
Iteration 10400: Loss = -11069.2451171875
Iteration 10500: Loss = -11069.236328125
Iteration 10600: Loss = -11069.2265625
Iteration 10700: Loss = -11069.216796875
Iteration 10800: Loss = -11069.2099609375
Iteration 10900: Loss = -11069.2001953125
Iteration 11000: Loss = -11069.1953125
Iteration 11100: Loss = -11069.1884765625
Iteration 11200: Loss = -11069.1826171875
Iteration 11300: Loss = -11069.17578125
Iteration 11400: Loss = -11069.1708984375
Iteration 11500: Loss = -11069.166015625
Iteration 11600: Loss = -11069.1611328125
Iteration 11700: Loss = -11069.1572265625
Iteration 11800: Loss = -11069.154296875
Iteration 11900: Loss = -11069.1484375
Iteration 12000: Loss = -11069.142578125
Iteration 12100: Loss = -11069.1376953125
Iteration 12200: Loss = -11069.130859375
Iteration 12300: Loss = -11069.126953125
Iteration 12400: Loss = -11069.123046875
Iteration 12500: Loss = -11069.12109375
Iteration 12600: Loss = -11069.1162109375
Iteration 12700: Loss = -11069.1162109375
Iteration 12800: Loss = -11069.11328125
Iteration 12900: Loss = -11069.1103515625
Iteration 13000: Loss = -11069.109375
Iteration 13100: Loss = -11069.107421875
Iteration 13200: Loss = -11069.1044921875
Iteration 13300: Loss = -11069.1044921875
Iteration 13400: Loss = -11069.1025390625
Iteration 13500: Loss = -11069.1005859375
Iteration 13600: Loss = -11069.0986328125
Iteration 13700: Loss = -11069.09765625
Iteration 13800: Loss = -11069.09765625
Iteration 13900: Loss = -11069.095703125
Iteration 14000: Loss = -11069.095703125
Iteration 14100: Loss = -11069.0947265625
Iteration 14200: Loss = -11069.0927734375
Iteration 14300: Loss = -11069.0927734375
Iteration 14400: Loss = -11069.091796875
Iteration 14500: Loss = -11069.08984375
Iteration 14600: Loss = -11069.08984375
Iteration 14700: Loss = -11069.0888671875
Iteration 14800: Loss = -11069.0888671875
Iteration 14900: Loss = -11069.087890625
Iteration 15000: Loss = -11069.0869140625
Iteration 15100: Loss = -11069.0869140625
Iteration 15200: Loss = -11069.0849609375
Iteration 15300: Loss = -11069.0849609375
Iteration 15400: Loss = -11069.0849609375
Iteration 15500: Loss = -11069.0830078125
Iteration 15600: Loss = -11069.0869140625
1
Iteration 15700: Loss = -11069.083984375
2
Iteration 15800: Loss = -11069.0830078125
Iteration 15900: Loss = -11069.08203125
Iteration 16000: Loss = -11069.0810546875
Iteration 16100: Loss = -11069.08203125
1
Iteration 16200: Loss = -11069.08203125
2
Iteration 16300: Loss = -11069.08203125
3
Iteration 16400: Loss = -11069.0810546875
Iteration 16500: Loss = -11069.0791015625
Iteration 16600: Loss = -11069.080078125
1
Iteration 16700: Loss = -11069.0810546875
2
Iteration 16800: Loss = -11069.080078125
3
Iteration 16900: Loss = -11069.080078125
4
Iteration 17000: Loss = -11069.0771484375
Iteration 17100: Loss = -11069.0791015625
1
Iteration 17200: Loss = -11069.0771484375
Iteration 17300: Loss = -11069.0810546875
1
Iteration 17400: Loss = -11069.0771484375
Iteration 17500: Loss = -11069.0791015625
1
Iteration 17600: Loss = -11069.078125
2
Iteration 17700: Loss = -11069.0771484375
Iteration 17800: Loss = -11069.0771484375
Iteration 17900: Loss = -11069.0771484375
Iteration 18000: Loss = -11069.0771484375
Iteration 18100: Loss = -11069.076171875
Iteration 18200: Loss = -11069.0771484375
1
Iteration 18300: Loss = -11069.076171875
Iteration 18400: Loss = -11069.0810546875
1
Iteration 18500: Loss = -11069.0771484375
2
Iteration 18600: Loss = -11069.0771484375
3
Iteration 18700: Loss = -11069.0771484375
4
Iteration 18800: Loss = -11069.076171875
Iteration 18900: Loss = -11069.0751953125
Iteration 19000: Loss = -11069.0751953125
Iteration 19100: Loss = -11069.0751953125
Iteration 19200: Loss = -11069.076171875
1
Iteration 19300: Loss = -11069.0771484375
2
Iteration 19400: Loss = -11069.07421875
Iteration 19500: Loss = -11069.076171875
1
Iteration 19600: Loss = -11069.0751953125
2
Iteration 19700: Loss = -11069.0751953125
3
Iteration 19800: Loss = -11069.0751953125
4
Iteration 19900: Loss = -11069.0751953125
5
Iteration 20000: Loss = -11069.0771484375
6
Iteration 20100: Loss = -11069.07421875
Iteration 20200: Loss = -11069.07421875
Iteration 20300: Loss = -11069.07421875
Iteration 20400: Loss = -11069.0732421875
Iteration 20500: Loss = -11069.0771484375
1
Iteration 20600: Loss = -11069.0751953125
2
Iteration 20700: Loss = -11069.076171875
3
Iteration 20800: Loss = -11069.0751953125
4
Iteration 20900: Loss = -11069.07421875
5
Iteration 21000: Loss = -11069.0732421875
Iteration 21100: Loss = -11069.07421875
1
Iteration 21200: Loss = -11069.0732421875
Iteration 21300: Loss = -11069.0732421875
Iteration 21400: Loss = -11069.0751953125
1
Iteration 21500: Loss = -11069.0732421875
Iteration 21600: Loss = -11069.0732421875
Iteration 21700: Loss = -11069.0732421875
Iteration 21800: Loss = -11069.0751953125
1
Iteration 21900: Loss = -11069.07421875
2
Iteration 22000: Loss = -11069.0732421875
Iteration 22100: Loss = -11069.07421875
1
Iteration 22200: Loss = -11069.07421875
2
Iteration 22300: Loss = -11069.07421875
3
Iteration 22400: Loss = -11069.0732421875
Iteration 22500: Loss = -11069.0751953125
1
Iteration 22600: Loss = -11069.07421875
2
Iteration 22700: Loss = -11069.0732421875
Iteration 22800: Loss = -11069.076171875
1
Iteration 22900: Loss = -11069.0751953125
2
Iteration 23000: Loss = -11069.0751953125
3
Iteration 23100: Loss = -11069.07421875
4
Iteration 23200: Loss = -11069.07421875
5
Iteration 23300: Loss = -11069.0751953125
6
Iteration 23400: Loss = -11069.07421875
7
Iteration 23500: Loss = -11069.0751953125
8
Iteration 23600: Loss = -11069.07421875
9
Iteration 23700: Loss = -11069.07421875
10
Iteration 23800: Loss = -11069.07421875
11
Iteration 23900: Loss = -11069.0732421875
Iteration 24000: Loss = -11069.07421875
1
Iteration 24100: Loss = -11069.07421875
2
Iteration 24200: Loss = -11069.072265625
Iteration 24300: Loss = -11069.07421875
1
Iteration 24400: Loss = -11069.072265625
Iteration 24500: Loss = -11069.072265625
Iteration 24600: Loss = -11069.0751953125
1
Iteration 24700: Loss = -11069.0751953125
2
Iteration 24800: Loss = -11069.07421875
3
Iteration 24900: Loss = -11069.07421875
4
Iteration 25000: Loss = -11069.0751953125
5
Iteration 25100: Loss = -11069.0732421875
6
Iteration 25200: Loss = -11069.07421875
7
Iteration 25300: Loss = -11069.07421875
8
Iteration 25400: Loss = -11068.30078125
Iteration 25500: Loss = -11068.29296875
Iteration 25600: Loss = -11068.2890625
Iteration 25700: Loss = -11068.291015625
1
Iteration 25800: Loss = -11068.2724609375
Iteration 25900: Loss = -11068.2646484375
Iteration 26000: Loss = -11068.251953125
Iteration 26100: Loss = -11068.2294921875
Iteration 26200: Loss = -11068.228515625
Iteration 26300: Loss = -11068.23046875
1
Iteration 26400: Loss = -11068.2294921875
2
Iteration 26500: Loss = -11068.2294921875
3
Iteration 26600: Loss = -11068.2294921875
4
Iteration 26700: Loss = -11068.228515625
Iteration 26800: Loss = -11068.2294921875
1
Iteration 26900: Loss = -11068.2294921875
2
Iteration 27000: Loss = -11068.228515625
Iteration 27100: Loss = -11068.228515625
Iteration 27200: Loss = -11068.2265625
Iteration 27300: Loss = -11068.2275390625
1
Iteration 27400: Loss = -11068.228515625
2
Iteration 27500: Loss = -11068.2275390625
3
Iteration 27600: Loss = -11068.220703125
Iteration 27700: Loss = -11068.2197265625
Iteration 27800: Loss = -11068.21875
Iteration 27900: Loss = -11068.2197265625
1
Iteration 28000: Loss = -11068.2197265625
2
Iteration 28100: Loss = -11068.2177734375
Iteration 28200: Loss = -11068.2177734375
Iteration 28300: Loss = -11068.2177734375
Iteration 28400: Loss = -11068.216796875
Iteration 28500: Loss = -11068.2177734375
1
Iteration 28600: Loss = -11068.21875
2
Iteration 28700: Loss = -11068.2177734375
3
Iteration 28800: Loss = -11068.2177734375
4
Iteration 28900: Loss = -11068.216796875
Iteration 29000: Loss = -11068.2177734375
1
Iteration 29100: Loss = -11068.216796875
Iteration 29200: Loss = -11068.2177734375
1
Iteration 29300: Loss = -11068.216796875
Iteration 29400: Loss = -11068.216796875
Iteration 29500: Loss = -11068.216796875
Iteration 29600: Loss = -11068.2177734375
1
Iteration 29700: Loss = -11068.216796875
Iteration 29800: Loss = -11068.216796875
Iteration 29900: Loss = -11068.2177734375
1
pi: tensor([[1.0000e+00, 1.8480e-06],
        [3.5558e-01, 6.4442e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9729, 0.0271], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1656, 0.0582],
         [0.1200, 0.3554]],

        [[0.9759, 0.1333],
         [0.2383, 0.0554]],

        [[0.9313, 0.2380],
         [0.5034, 0.0257]],

        [[0.6569, 0.2201],
         [0.3419, 0.4776]],

        [[0.9122, 0.1621],
         [0.8988, 0.6931]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0013742166364525297
Average Adjusted Rand Index: -0.0011522418127903108
[0.0013742166364525297, 0.0013742166364525297] [-0.0011522418127903108, -0.0011522418127903108] [11068.2265625, 11068.2177734375]
-------------------------------------
This iteration is 13
True Objective function: Loss = -10801.847408716018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28650.6796875
Iteration 100: Loss = -19156.51953125
Iteration 200: Loss = -13428.2587890625
Iteration 300: Loss = -11851.1865234375
Iteration 400: Loss = -11405.26171875
Iteration 500: Loss = -11192.5849609375
Iteration 600: Loss = -11078.509765625
Iteration 700: Loss = -11024.9970703125
Iteration 800: Loss = -10989.0068359375
Iteration 900: Loss = -10968.1572265625
Iteration 1000: Loss = -10958.5595703125
Iteration 1100: Loss = -10948.6611328125
Iteration 1200: Loss = -10943.16015625
Iteration 1300: Loss = -10939.3564453125
Iteration 1400: Loss = -10936.400390625
Iteration 1500: Loss = -10934.029296875
Iteration 1600: Loss = -10932.0830078125
Iteration 1700: Loss = -10930.4560546875
Iteration 1800: Loss = -10929.0791015625
Iteration 1900: Loss = -10927.9296875
Iteration 2000: Loss = -10926.955078125
Iteration 2100: Loss = -10926.1142578125
Iteration 2200: Loss = -10925.384765625
Iteration 2300: Loss = -10924.7451171875
Iteration 2400: Loss = -10924.1796875
Iteration 2500: Loss = -10923.6767578125
Iteration 2600: Loss = -10923.2333984375
Iteration 2700: Loss = -10922.833984375
Iteration 2800: Loss = -10922.474609375
Iteration 2900: Loss = -10922.1513671875
Iteration 3000: Loss = -10921.86328125
Iteration 3100: Loss = -10921.6005859375
Iteration 3200: Loss = -10921.3623046875
Iteration 3300: Loss = -10921.146484375
Iteration 3400: Loss = -10920.947265625
Iteration 3500: Loss = -10920.767578125
Iteration 3600: Loss = -10920.6005859375
Iteration 3700: Loss = -10920.44921875
Iteration 3800: Loss = -10920.3095703125
Iteration 3900: Loss = -10920.1787109375
Iteration 4000: Loss = -10920.0576171875
Iteration 4100: Loss = -10919.9482421875
Iteration 4200: Loss = -10919.845703125
Iteration 4300: Loss = -10919.7490234375
Iteration 4400: Loss = -10919.658203125
Iteration 4500: Loss = -10919.5751953125
Iteration 4600: Loss = -10919.49609375
Iteration 4700: Loss = -10919.423828125
Iteration 4800: Loss = -10919.353515625
Iteration 4900: Loss = -10919.2890625
Iteration 5000: Loss = -10919.228515625
Iteration 5100: Loss = -10919.1748046875
Iteration 5200: Loss = -10919.1220703125
Iteration 5300: Loss = -10919.0751953125
Iteration 5400: Loss = -10919.0302734375
Iteration 5500: Loss = -10918.9892578125
Iteration 5600: Loss = -10918.947265625
Iteration 5700: Loss = -10918.9111328125
Iteration 5800: Loss = -10918.876953125
Iteration 5900: Loss = -10918.8408203125
Iteration 6000: Loss = -10918.810546875
Iteration 6100: Loss = -10918.7802734375
Iteration 6200: Loss = -10918.7509765625
Iteration 6300: Loss = -10918.7216796875
Iteration 6400: Loss = -10918.6943359375
Iteration 6500: Loss = -10918.669921875
Iteration 6600: Loss = -10918.646484375
Iteration 6700: Loss = -10918.623046875
Iteration 6800: Loss = -10918.6025390625
Iteration 6900: Loss = -10918.583984375
Iteration 7000: Loss = -10918.564453125
Iteration 7100: Loss = -10918.548828125
Iteration 7200: Loss = -10918.5322265625
Iteration 7300: Loss = -10918.5166015625
Iteration 7400: Loss = -10918.50390625
Iteration 7500: Loss = -10918.48828125
Iteration 7600: Loss = -10918.474609375
Iteration 7700: Loss = -10918.4619140625
Iteration 7800: Loss = -10918.451171875
Iteration 7900: Loss = -10918.4404296875
Iteration 8000: Loss = -10918.4287109375
Iteration 8100: Loss = -10918.41796875
Iteration 8200: Loss = -10918.408203125
Iteration 8300: Loss = -10918.400390625
Iteration 8400: Loss = -10918.3916015625
Iteration 8500: Loss = -10918.3828125
Iteration 8600: Loss = -10918.376953125
Iteration 8700: Loss = -10918.3681640625
Iteration 8800: Loss = -10918.3623046875
Iteration 8900: Loss = -10918.35546875
Iteration 9000: Loss = -10918.3486328125
Iteration 9100: Loss = -10918.3427734375
Iteration 9200: Loss = -10918.3369140625
Iteration 9300: Loss = -10918.3310546875
Iteration 9400: Loss = -10918.326171875
Iteration 9500: Loss = -10918.322265625
Iteration 9600: Loss = -10918.3193359375
Iteration 9700: Loss = -10918.3134765625
Iteration 9800: Loss = -10918.3095703125
Iteration 9900: Loss = -10918.3046875
Iteration 10000: Loss = -10918.302734375
Iteration 10100: Loss = -10918.2998046875
Iteration 10200: Loss = -10918.2958984375
Iteration 10300: Loss = -10918.29296875
Iteration 10400: Loss = -10918.2900390625
Iteration 10500: Loss = -10918.28515625
Iteration 10600: Loss = -10918.283203125
Iteration 10700: Loss = -10918.28125
Iteration 10800: Loss = -10918.2783203125
Iteration 10900: Loss = -10918.275390625
Iteration 11000: Loss = -10918.2744140625
Iteration 11100: Loss = -10918.2705078125
Iteration 11200: Loss = -10918.26953125
Iteration 11300: Loss = -10918.267578125
Iteration 11400: Loss = -10918.267578125
Iteration 11500: Loss = -10918.2646484375
Iteration 11600: Loss = -10918.2626953125
Iteration 11700: Loss = -10918.2626953125
Iteration 11800: Loss = -10918.2607421875
Iteration 11900: Loss = -10918.2587890625
Iteration 12000: Loss = -10918.2578125
Iteration 12100: Loss = -10918.2578125
Iteration 12200: Loss = -10918.255859375
Iteration 12300: Loss = -10918.2548828125
Iteration 12400: Loss = -10918.2529296875
Iteration 12500: Loss = -10918.251953125
Iteration 12600: Loss = -10918.25
Iteration 12700: Loss = -10918.2529296875
1
Iteration 12800: Loss = -10918.248046875
Iteration 12900: Loss = -10918.248046875
Iteration 13000: Loss = -10918.25
1
Iteration 13100: Loss = -10918.24609375
Iteration 13200: Loss = -10918.2451171875
Iteration 13300: Loss = -10918.2431640625
Iteration 13400: Loss = -10918.2421875
Iteration 13500: Loss = -10918.2412109375
Iteration 13600: Loss = -10918.240234375
Iteration 13700: Loss = -10918.2373046875
Iteration 13800: Loss = -10918.2373046875
Iteration 13900: Loss = -10918.236328125
Iteration 14000: Loss = -10918.234375
Iteration 14100: Loss = -10918.2333984375
Iteration 14200: Loss = -10918.232421875
Iteration 14300: Loss = -10918.23046875
Iteration 14400: Loss = -10918.2265625
Iteration 14500: Loss = -10918.22265625
Iteration 14600: Loss = -10918.220703125
Iteration 14700: Loss = -10918.220703125
Iteration 14800: Loss = -10918.2177734375
Iteration 14900: Loss = -10918.21484375
Iteration 15000: Loss = -10918.212890625
Iteration 15100: Loss = -10918.212890625
Iteration 15200: Loss = -10918.2109375
Iteration 15300: Loss = -10918.2099609375
Iteration 15400: Loss = -10918.2099609375
Iteration 15500: Loss = -10918.208984375
Iteration 15600: Loss = -10918.2080078125
Iteration 15700: Loss = -10918.20703125
Iteration 15800: Loss = -10918.2080078125
1
Iteration 15900: Loss = -10918.205078125
Iteration 16000: Loss = -10918.20703125
1
Iteration 16100: Loss = -10918.2041015625
Iteration 16200: Loss = -10918.205078125
1
Iteration 16300: Loss = -10918.2060546875
2
Iteration 16400: Loss = -10918.2041015625
Iteration 16500: Loss = -10918.2060546875
1
Iteration 16600: Loss = -10918.2041015625
Iteration 16700: Loss = -10918.2060546875
1
Iteration 16800: Loss = -10918.2041015625
Iteration 16900: Loss = -10918.203125
Iteration 17000: Loss = -10918.2021484375
Iteration 17100: Loss = -10918.203125
1
Iteration 17200: Loss = -10918.201171875
Iteration 17300: Loss = -10918.2001953125
Iteration 17400: Loss = -10918.19921875
Iteration 17500: Loss = -10918.1982421875
Iteration 17600: Loss = -10918.19921875
1
Iteration 17700: Loss = -10918.203125
2
Iteration 17800: Loss = -10918.19921875
3
Iteration 17900: Loss = -10918.1982421875
Iteration 18000: Loss = -10918.2041015625
1
Iteration 18100: Loss = -10918.19921875
2
Iteration 18200: Loss = -10918.1982421875
Iteration 18300: Loss = -10918.1982421875
Iteration 18400: Loss = -10918.1982421875
Iteration 18500: Loss = -10918.19921875
1
Iteration 18600: Loss = -10918.19921875
2
Iteration 18700: Loss = -10918.2001953125
3
Iteration 18800: Loss = -10918.1982421875
Iteration 18900: Loss = -10918.1982421875
Iteration 19000: Loss = -10918.203125
1
Iteration 19100: Loss = -10918.197265625
Iteration 19200: Loss = -10918.19921875
1
Iteration 19300: Loss = -10918.1982421875
2
Iteration 19400: Loss = -10918.1982421875
3
Iteration 19500: Loss = -10918.19921875
4
Iteration 19600: Loss = -10918.19921875
5
Iteration 19700: Loss = -10918.197265625
Iteration 19800: Loss = -10918.197265625
Iteration 19900: Loss = -10918.1962890625
Iteration 20000: Loss = -10918.1962890625
Iteration 20100: Loss = -10918.197265625
1
Iteration 20200: Loss = -10918.197265625
2
Iteration 20300: Loss = -10918.197265625
3
Iteration 20400: Loss = -10918.197265625
4
Iteration 20500: Loss = -10918.1982421875
5
Iteration 20600: Loss = -10918.1982421875
6
Iteration 20700: Loss = -10918.1962890625
Iteration 20800: Loss = -10918.1962890625
Iteration 20900: Loss = -10918.1982421875
1
Iteration 21000: Loss = -10918.197265625
2
Iteration 21100: Loss = -10918.197265625
3
Iteration 21200: Loss = -10918.197265625
4
Iteration 21300: Loss = -10918.1962890625
Iteration 21400: Loss = -10918.1953125
Iteration 21500: Loss = -10918.1962890625
1
Iteration 21600: Loss = -10918.1982421875
2
Iteration 21700: Loss = -10918.197265625
3
Iteration 21800: Loss = -10918.197265625
4
Iteration 21900: Loss = -10918.19140625
Iteration 22000: Loss = -10918.171875
Iteration 22100: Loss = -10918.1435546875
Iteration 22200: Loss = -10918.0908203125
Iteration 22300: Loss = -10918.01953125
Iteration 22400: Loss = -10917.96484375
Iteration 22500: Loss = -10917.91796875
Iteration 22600: Loss = -10917.7568359375
Iteration 22700: Loss = -10917.6728515625
Iteration 22800: Loss = -10917.3447265625
Iteration 22900: Loss = -10917.3359375
Iteration 23000: Loss = -10917.3173828125
Iteration 23100: Loss = -10917.3095703125
Iteration 23200: Loss = -10917.287109375
Iteration 23300: Loss = -10917.2724609375
Iteration 23400: Loss = -10917.26953125
Iteration 23500: Loss = -10917.2705078125
1
Iteration 23600: Loss = -10917.2666015625
Iteration 23700: Loss = -10917.265625
Iteration 23800: Loss = -10917.26171875
Iteration 23900: Loss = -10917.2060546875
Iteration 24000: Loss = -10917.203125
Iteration 24100: Loss = -10917.2021484375
Iteration 24200: Loss = -10917.197265625
Iteration 24300: Loss = -10917.1943359375
Iteration 24400: Loss = -10917.193359375
Iteration 24500: Loss = -10917.1923828125
Iteration 24600: Loss = -10917.193359375
1
Iteration 24700: Loss = -10917.193359375
2
Iteration 24800: Loss = -10917.19140625
Iteration 24900: Loss = -10917.1953125
1
Iteration 25000: Loss = -10917.19140625
Iteration 25100: Loss = -10917.19140625
Iteration 25200: Loss = -10917.19140625
Iteration 25300: Loss = -10917.1904296875
Iteration 25400: Loss = -10917.1923828125
1
Iteration 25500: Loss = -10917.19140625
2
Iteration 25600: Loss = -10917.1923828125
3
Iteration 25700: Loss = -10917.1923828125
4
Iteration 25800: Loss = -10917.1923828125
5
Iteration 25900: Loss = -10917.19140625
6
Iteration 26000: Loss = -10917.1904296875
Iteration 26100: Loss = -10917.1904296875
Iteration 26200: Loss = -10917.19140625
1
Iteration 26300: Loss = -10917.1923828125
2
Iteration 26400: Loss = -10917.1923828125
3
Iteration 26500: Loss = -10917.19140625
4
Iteration 26600: Loss = -10917.1904296875
Iteration 26700: Loss = -10917.1904296875
Iteration 26800: Loss = -10917.1923828125
1
Iteration 26900: Loss = -10917.1904296875
Iteration 27000: Loss = -10917.1904296875
Iteration 27100: Loss = -10917.1923828125
1
Iteration 27200: Loss = -10917.1923828125
2
Iteration 27300: Loss = -10917.1923828125
3
Iteration 27400: Loss = -10917.1904296875
Iteration 27500: Loss = -10917.1923828125
1
Iteration 27600: Loss = -10917.19140625
2
Iteration 27700: Loss = -10917.19140625
3
Iteration 27800: Loss = -10917.1904296875
Iteration 27900: Loss = -10917.19140625
1
Iteration 28000: Loss = -10917.19140625
2
Iteration 28100: Loss = -10917.1923828125
3
Iteration 28200: Loss = -10917.1923828125
4
Iteration 28300: Loss = -10917.1923828125
5
Iteration 28400: Loss = -10917.19140625
6
Iteration 28500: Loss = -10917.19140625
7
Iteration 28600: Loss = -10917.19140625
8
Iteration 28700: Loss = -10917.19140625
9
Iteration 28800: Loss = -10917.1923828125
10
Iteration 28900: Loss = -10917.19140625
11
Iteration 29000: Loss = -10917.19140625
12
Iteration 29100: Loss = -10917.1904296875
Iteration 29200: Loss = -10917.1904296875
Iteration 29300: Loss = -10917.189453125
Iteration 29400: Loss = -10917.189453125
Iteration 29500: Loss = -10917.1904296875
1
Iteration 29600: Loss = -10917.19140625
2
Iteration 29700: Loss = -10917.19140625
3
Iteration 29800: Loss = -10917.189453125
Iteration 29900: Loss = -10917.189453125
pi: tensor([[4.2955e-05, 9.9996e-01],
        [2.7365e-02, 9.7263e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.5206e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2111, 0.1844],
         [0.9459, 0.1602]],

        [[0.9641, 0.1919],
         [0.9808, 0.3028]],

        [[0.8433, 0.2203],
         [0.1594, 0.0430]],

        [[0.9211, 0.2253],
         [0.4769, 0.0537]],

        [[0.0077, 0.0983],
         [0.0192, 0.2063]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35614.69140625
Iteration 100: Loss = -23264.806640625
Iteration 200: Loss = -13682.3232421875
Iteration 300: Loss = -11746.7705078125
Iteration 400: Loss = -11385.060546875
Iteration 500: Loss = -11268.1103515625
Iteration 600: Loss = -11212.115234375
Iteration 700: Loss = -11171.87109375
Iteration 800: Loss = -11144.8837890625
Iteration 900: Loss = -11124.7197265625
Iteration 1000: Loss = -11107.2490234375
Iteration 1100: Loss = -11088.880859375
Iteration 1200: Loss = -11073.5380859375
Iteration 1300: Loss = -11059.8583984375
Iteration 1400: Loss = -11050.0673828125
Iteration 1500: Loss = -11041.8076171875
Iteration 1600: Loss = -11033.90234375
Iteration 1700: Loss = -11022.189453125
Iteration 1800: Loss = -11015.4892578125
Iteration 1900: Loss = -11010.9697265625
Iteration 2000: Loss = -11004.4443359375
Iteration 2100: Loss = -10998.1669921875
Iteration 2200: Loss = -10993.29296875
Iteration 2300: Loss = -10985.806640625
Iteration 2400: Loss = -10980.58984375
Iteration 2500: Loss = -10976.734375
Iteration 2600: Loss = -10971.974609375
Iteration 2700: Loss = -10968.455078125
Iteration 2800: Loss = -10966.013671875
Iteration 2900: Loss = -10963.677734375
Iteration 3000: Loss = -10961.7177734375
Iteration 3100: Loss = -10959.6728515625
Iteration 3200: Loss = -10952.9111328125
Iteration 3300: Loss = -10950.482421875
Iteration 3400: Loss = -10948.6875
Iteration 3500: Loss = -10943.8544921875
Iteration 3600: Loss = -10942.28515625
Iteration 3700: Loss = -10938.0908203125
Iteration 3800: Loss = -10937.134765625
Iteration 3900: Loss = -10936.0703125
Iteration 4000: Loss = -10935.02734375
Iteration 4100: Loss = -10934.42578125
Iteration 4200: Loss = -10933.93359375
Iteration 4300: Loss = -10933.3828125
Iteration 4400: Loss = -10932.572265625
Iteration 4500: Loss = -10931.9189453125
Iteration 4600: Loss = -10931.509765625
Iteration 4700: Loss = -10926.9150390625
Iteration 4800: Loss = -10926.4072265625
Iteration 4900: Loss = -10926.1142578125
Iteration 5000: Loss = -10925.884765625
Iteration 5100: Loss = -10925.697265625
Iteration 5200: Loss = -10925.5322265625
Iteration 5300: Loss = -10925.3876953125
Iteration 5400: Loss = -10925.2607421875
Iteration 5500: Loss = -10925.1474609375
Iteration 5600: Loss = -10925.04296875
Iteration 5700: Loss = -10924.9404296875
Iteration 5800: Loss = -10924.84375
Iteration 5900: Loss = -10924.7626953125
Iteration 6000: Loss = -10924.6884765625
Iteration 6100: Loss = -10924.6201171875
Iteration 6200: Loss = -10924.556640625
Iteration 6300: Loss = -10924.498046875
Iteration 6400: Loss = -10924.443359375
Iteration 6500: Loss = -10924.3896484375
Iteration 6600: Loss = -10924.33984375
Iteration 6700: Loss = -10924.28515625
Iteration 6800: Loss = -10924.2236328125
Iteration 6900: Loss = -10924.138671875
Iteration 7000: Loss = -10924.0654296875
Iteration 7100: Loss = -10924.009765625
Iteration 7200: Loss = -10923.9658203125
Iteration 7300: Loss = -10923.9296875
Iteration 7400: Loss = -10923.8984375
Iteration 7500: Loss = -10923.8671875
Iteration 7600: Loss = -10923.841796875
Iteration 7700: Loss = -10923.81640625
Iteration 7800: Loss = -10923.79296875
Iteration 7900: Loss = -10923.771484375
Iteration 8000: Loss = -10923.7509765625
Iteration 8100: Loss = -10923.7314453125
Iteration 8200: Loss = -10923.712890625
Iteration 8300: Loss = -10923.6953125
Iteration 8400: Loss = -10923.677734375
Iteration 8500: Loss = -10923.66015625
Iteration 8600: Loss = -10923.64453125
Iteration 8700: Loss = -10923.6318359375
Iteration 8800: Loss = -10923.6181640625
Iteration 8900: Loss = -10923.6064453125
Iteration 9000: Loss = -10923.595703125
Iteration 9100: Loss = -10923.5859375
Iteration 9200: Loss = -10923.576171875
Iteration 9300: Loss = -10923.5673828125
Iteration 9400: Loss = -10923.5595703125
Iteration 9500: Loss = -10923.5517578125
Iteration 9600: Loss = -10923.54296875
Iteration 9700: Loss = -10923.537109375
Iteration 9800: Loss = -10923.5302734375
Iteration 9900: Loss = -10923.5244140625
Iteration 10000: Loss = -10923.5166015625
Iteration 10100: Loss = -10923.51171875
Iteration 10200: Loss = -10923.505859375
Iteration 10300: Loss = -10923.5009765625
Iteration 10400: Loss = -10923.4951171875
Iteration 10500: Loss = -10923.4912109375
Iteration 10600: Loss = -10923.4853515625
Iteration 10700: Loss = -10923.48046875
Iteration 10800: Loss = -10923.4765625
Iteration 10900: Loss = -10923.4736328125
Iteration 11000: Loss = -10923.4677734375
Iteration 11100: Loss = -10923.4638671875
Iteration 11200: Loss = -10923.4599609375
Iteration 11300: Loss = -10923.453125
Iteration 11400: Loss = -10923.4501953125
Iteration 11500: Loss = -10923.447265625
Iteration 11600: Loss = -10923.44140625
Iteration 11700: Loss = -10923.439453125
Iteration 11800: Loss = -10923.4365234375
Iteration 11900: Loss = -10918.4765625
Iteration 12000: Loss = -10918.1904296875
Iteration 12100: Loss = -10918.1337890625
Iteration 12200: Loss = -10918.107421875
Iteration 12300: Loss = -10918.08984375
Iteration 12400: Loss = -10918.0771484375
Iteration 12500: Loss = -10918.0693359375
Iteration 12600: Loss = -10918.064453125
Iteration 12700: Loss = -10918.0595703125
Iteration 12800: Loss = -10918.0556640625
Iteration 12900: Loss = -10918.05078125
Iteration 13000: Loss = -10918.0478515625
Iteration 13100: Loss = -10918.0439453125
Iteration 13200: Loss = -10918.04296875
Iteration 13300: Loss = -10918.0390625
Iteration 13400: Loss = -10918.0380859375
Iteration 13500: Loss = -10918.037109375
Iteration 13600: Loss = -10918.0341796875
Iteration 13700: Loss = -10918.0322265625
Iteration 13800: Loss = -10918.0302734375
Iteration 13900: Loss = -10918.0234375
Iteration 14000: Loss = -10917.8330078125
Iteration 14100: Loss = -10917.3828125
Iteration 14200: Loss = -10917.2529296875
Iteration 14300: Loss = -10917.2255859375
Iteration 14400: Loss = -10917.21484375
Iteration 14500: Loss = -10917.2109375
Iteration 14600: Loss = -10917.2080078125
Iteration 14700: Loss = -10917.2041015625
Iteration 14800: Loss = -10917.203125
Iteration 14900: Loss = -10917.2021484375
Iteration 15000: Loss = -10917.2001953125
Iteration 15100: Loss = -10917.1982421875
Iteration 15200: Loss = -10917.19921875
1
Iteration 15300: Loss = -10917.1982421875
Iteration 15400: Loss = -10917.197265625
Iteration 15500: Loss = -10917.1962890625
Iteration 15600: Loss = -10917.1953125
Iteration 15700: Loss = -10917.1953125
Iteration 15800: Loss = -10917.1943359375
Iteration 15900: Loss = -10917.193359375
Iteration 16000: Loss = -10917.1943359375
1
Iteration 16100: Loss = -10917.193359375
Iteration 16200: Loss = -10917.1923828125
Iteration 16300: Loss = -10917.1923828125
Iteration 16400: Loss = -10917.1923828125
Iteration 16500: Loss = -10917.193359375
1
Iteration 16600: Loss = -10917.19140625
Iteration 16700: Loss = -10917.1904296875
Iteration 16800: Loss = -10917.189453125
Iteration 16900: Loss = -10917.19140625
1
Iteration 17000: Loss = -10917.189453125
Iteration 17100: Loss = -10917.1904296875
1
Iteration 17200: Loss = -10917.189453125
Iteration 17300: Loss = -10917.189453125
Iteration 17400: Loss = -10917.1904296875
1
Iteration 17500: Loss = -10917.1904296875
2
Iteration 17600: Loss = -10917.1904296875
3
Iteration 17700: Loss = -10917.1904296875
4
Iteration 17800: Loss = -10917.189453125
Iteration 17900: Loss = -10917.1884765625
Iteration 18000: Loss = -10917.1884765625
Iteration 18100: Loss = -10917.1884765625
Iteration 18200: Loss = -10917.1884765625
Iteration 18300: Loss = -10917.189453125
1
Iteration 18400: Loss = -10917.1884765625
Iteration 18500: Loss = -10917.1875
Iteration 18600: Loss = -10917.1884765625
1
Iteration 18700: Loss = -10917.1884765625
2
Iteration 18800: Loss = -10917.1875
Iteration 18900: Loss = -10917.1875
Iteration 19000: Loss = -10917.1865234375
Iteration 19100: Loss = -10917.1865234375
Iteration 19200: Loss = -10917.1865234375
Iteration 19300: Loss = -10917.1875
1
Iteration 19400: Loss = -10917.1875
2
Iteration 19500: Loss = -10917.1875
3
Iteration 19600: Loss = -10917.1865234375
Iteration 19700: Loss = -10917.189453125
1
Iteration 19800: Loss = -10917.1875
2
Iteration 19900: Loss = -10917.1865234375
Iteration 20000: Loss = -10917.1865234375
Iteration 20100: Loss = -10917.1865234375
Iteration 20200: Loss = -10917.1865234375
Iteration 20300: Loss = -10917.1865234375
Iteration 20400: Loss = -10917.1875
1
Iteration 20500: Loss = -10917.1865234375
Iteration 20600: Loss = -10917.1875
1
Iteration 20700: Loss = -10917.1875
2
Iteration 20800: Loss = -10917.1875
3
Iteration 20900: Loss = -10917.1875
4
Iteration 21000: Loss = -10917.1865234375
Iteration 21100: Loss = -10917.1875
1
Iteration 21200: Loss = -10917.1865234375
Iteration 21300: Loss = -10917.1875
1
Iteration 21400: Loss = -10917.185546875
Iteration 21500: Loss = -10917.1875
1
Iteration 21600: Loss = -10917.185546875
Iteration 21700: Loss = -10917.1875
1
Iteration 21800: Loss = -10917.1875
2
Iteration 21900: Loss = -10917.1865234375
3
Iteration 22000: Loss = -10917.1865234375
4
Iteration 22100: Loss = -10917.1875
5
Iteration 22200: Loss = -10917.1875
6
Iteration 22300: Loss = -10917.1865234375
7
Iteration 22400: Loss = -10917.1865234375
8
Iteration 22500: Loss = -10917.185546875
Iteration 22600: Loss = -10917.1865234375
1
Iteration 22700: Loss = -10917.1875
2
Iteration 22800: Loss = -10917.1865234375
3
Iteration 22900: Loss = -10917.1875
4
Iteration 23000: Loss = -10917.185546875
Iteration 23100: Loss = -10917.1865234375
1
Iteration 23200: Loss = -10917.1875
2
Iteration 23300: Loss = -10917.1865234375
3
Iteration 23400: Loss = -10917.1865234375
4
Iteration 23500: Loss = -10917.1865234375
5
Iteration 23600: Loss = -10917.1875
6
Iteration 23700: Loss = -10917.185546875
Iteration 23800: Loss = -10917.1875
1
Iteration 23900: Loss = -10917.1875
2
Iteration 24000: Loss = -10917.1865234375
3
Iteration 24100: Loss = -10917.1865234375
4
Iteration 24200: Loss = -10917.1875
5
Iteration 24300: Loss = -10917.1884765625
6
Iteration 24400: Loss = -10917.1875
7
Iteration 24500: Loss = -10917.1875
8
Iteration 24600: Loss = -10917.185546875
Iteration 24700: Loss = -10917.1875
1
Iteration 24800: Loss = -10917.1865234375
2
Iteration 24900: Loss = -10917.185546875
Iteration 25000: Loss = -10917.1875
1
Iteration 25100: Loss = -10917.1875
2
Iteration 25200: Loss = -10917.1875
3
Iteration 25300: Loss = -10917.1875
4
Iteration 25400: Loss = -10917.1875
5
Iteration 25500: Loss = -10917.1875
6
Iteration 25600: Loss = -10917.1865234375
7
Iteration 25700: Loss = -10917.1875
8
Iteration 25800: Loss = -10917.185546875
Iteration 25900: Loss = -10917.1875
1
Iteration 26000: Loss = -10917.1875
2
Iteration 26100: Loss = -10917.1865234375
3
Iteration 26200: Loss = -10917.1865234375
4
Iteration 26300: Loss = -10917.1875
5
Iteration 26400: Loss = -10917.1875
6
Iteration 26500: Loss = -10917.1875
7
Iteration 26600: Loss = -10917.1865234375
8
Iteration 26700: Loss = -10917.1865234375
9
Iteration 26800: Loss = -10917.185546875
Iteration 26900: Loss = -10917.185546875
Iteration 27000: Loss = -10917.1875
1
Iteration 27100: Loss = -10917.1865234375
2
Iteration 27200: Loss = -10917.1865234375
3
Iteration 27300: Loss = -10917.1865234375
4
Iteration 27400: Loss = -10917.1875
5
Iteration 27500: Loss = -10917.1875
6
Iteration 27600: Loss = -10917.185546875
Iteration 27700: Loss = -10917.185546875
Iteration 27800: Loss = -10917.1875
1
Iteration 27900: Loss = -10917.1875
2
Iteration 28000: Loss = -10917.1875
3
Iteration 28100: Loss = -10917.1865234375
4
Iteration 28200: Loss = -10917.1875
5
Iteration 28300: Loss = -10917.1875
6
Iteration 28400: Loss = -10917.1875
7
Iteration 28500: Loss = -10917.1875
8
Iteration 28600: Loss = -10917.1875
9
Iteration 28700: Loss = -10917.1875
10
Iteration 28800: Loss = -10917.1865234375
11
Iteration 28900: Loss = -10917.1865234375
12
Iteration 29000: Loss = -10917.185546875
Iteration 29100: Loss = -10917.1865234375
1
Iteration 29200: Loss = -10917.1865234375
2
Iteration 29300: Loss = -10917.1865234375
3
Iteration 29400: Loss = -10917.1865234375
4
Iteration 29500: Loss = -10917.1875
5
Iteration 29600: Loss = -10917.185546875
Iteration 29700: Loss = -10917.185546875
Iteration 29800: Loss = -10917.1865234375
1
Iteration 29900: Loss = -10917.1875
2
pi: tensor([[1.6412e-08, 1.0000e+00],
        [1.0662e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9910, 0.0090], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1661, 0.0549],
         [0.4830, 0.1600]],

        [[0.9829, 0.2238],
         [0.9790, 0.6645]],

        [[0.0089, 0.1892],
         [0.7484, 0.1362]],

        [[0.9885, 0.1573],
         [0.1518, 0.8834]],

        [[0.1522, 0.7343],
         [0.9864, 0.0096]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006574174232012226
Average Adjusted Rand Index: 0.0001633280491905654
[0.0, -0.0006574174232012226] [0.0, 0.0001633280491905654] [10917.189453125, 10917.1884765625]
-------------------------------------
This iteration is 14
True Objective function: Loss = -10755.02711673985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -59151.0
Iteration 100: Loss = -39923.25
Iteration 200: Loss = -23034.046875
Iteration 300: Loss = -15649.46875
Iteration 400: Loss = -12813.4912109375
Iteration 500: Loss = -11722.1650390625
Iteration 600: Loss = -11302.1650390625
Iteration 700: Loss = -11154.0751953125
Iteration 800: Loss = -11073.375
Iteration 900: Loss = -11025.5400390625
Iteration 1000: Loss = -10996.4228515625
Iteration 1100: Loss = -10965.9775390625
Iteration 1200: Loss = -10942.1484375
Iteration 1300: Loss = -10922.119140625
Iteration 1400: Loss = -10910.3203125
Iteration 1500: Loss = -10896.89453125
Iteration 1600: Loss = -10886.080078125
Iteration 1700: Loss = -10879.1318359375
Iteration 1800: Loss = -10873.509765625
Iteration 1900: Loss = -10866.6943359375
Iteration 2000: Loss = -10860.21484375
Iteration 2100: Loss = -10856.8046875
Iteration 2200: Loss = -10854.216796875
Iteration 2300: Loss = -10852.736328125
Iteration 2400: Loss = -10851.69921875
Iteration 2500: Loss = -10850.8720703125
Iteration 2600: Loss = -10850.17578125
Iteration 2700: Loss = -10849.55859375
Iteration 2800: Loss = -10848.7841796875
Iteration 2900: Loss = -10846.357421875
Iteration 3000: Loss = -10844.5390625
Iteration 3100: Loss = -10843.84765625
Iteration 3200: Loss = -10843.34765625
Iteration 3300: Loss = -10842.841796875
Iteration 3400: Loss = -10842.478515625
Iteration 3500: Loss = -10842.154296875
Iteration 3600: Loss = -10841.857421875
Iteration 3700: Loss = -10841.5888671875
Iteration 3800: Loss = -10841.3447265625
Iteration 3900: Loss = -10841.1181640625
Iteration 4000: Loss = -10840.9130859375
Iteration 4100: Loss = -10840.720703125
Iteration 4200: Loss = -10840.5400390625
Iteration 4300: Loss = -10840.3720703125
Iteration 4400: Loss = -10840.19921875
Iteration 4500: Loss = -10837.8818359375
Iteration 4600: Loss = -10835.04296875
Iteration 4700: Loss = -10834.7646484375
Iteration 4800: Loss = -10834.59765625
Iteration 4900: Loss = -10834.46875
Iteration 5000: Loss = -10834.357421875
Iteration 5100: Loss = -10834.2548828125
Iteration 5200: Loss = -10834.1640625
Iteration 5300: Loss = -10834.08203125
Iteration 5400: Loss = -10834.0029296875
Iteration 5500: Loss = -10833.9306640625
Iteration 5600: Loss = -10833.86328125
Iteration 5700: Loss = -10833.798828125
Iteration 5800: Loss = -10833.7412109375
Iteration 5900: Loss = -10833.6845703125
Iteration 6000: Loss = -10833.630859375
Iteration 6100: Loss = -10833.5791015625
Iteration 6200: Loss = -10833.5341796875
Iteration 6300: Loss = -10833.470703125
Iteration 6400: Loss = -10828.4326171875
Iteration 6500: Loss = -10828.28125
Iteration 6600: Loss = -10828.2158203125
Iteration 6700: Loss = -10828.16796875
Iteration 6800: Loss = -10828.1240234375
Iteration 6900: Loss = -10828.0869140625
Iteration 7000: Loss = -10828.0478515625
Iteration 7100: Loss = -10827.9658203125
Iteration 7200: Loss = -10824.736328125
Iteration 7300: Loss = -10824.6181640625
Iteration 7400: Loss = -10824.568359375
Iteration 7500: Loss = -10824.5322265625
Iteration 7600: Loss = -10824.501953125
Iteration 7700: Loss = -10824.4755859375
Iteration 7800: Loss = -10824.453125
Iteration 7900: Loss = -10824.4296875
Iteration 8000: Loss = -10824.4111328125
Iteration 8100: Loss = -10824.3916015625
Iteration 8200: Loss = -10824.373046875
Iteration 8300: Loss = -10824.3564453125
Iteration 8400: Loss = -10824.33984375
Iteration 8500: Loss = -10824.3251953125
Iteration 8600: Loss = -10821.021484375
Iteration 8700: Loss = -10820.7744140625
Iteration 8800: Loss = -10820.6943359375
Iteration 8900: Loss = -10820.642578125
Iteration 9000: Loss = -10820.6015625
Iteration 9100: Loss = -10820.572265625
Iteration 9200: Loss = -10820.546875
Iteration 9300: Loss = -10820.5244140625
Iteration 9400: Loss = -10820.5048828125
Iteration 9500: Loss = -10820.4892578125
Iteration 9600: Loss = -10820.4736328125
Iteration 9700: Loss = -10820.4599609375
Iteration 9800: Loss = -10820.44921875
Iteration 9900: Loss = -10820.435546875
Iteration 10000: Loss = -10820.42578125
Iteration 10100: Loss = -10820.41796875
Iteration 10200: Loss = -10820.408203125
Iteration 10300: Loss = -10820.3984375
Iteration 10400: Loss = -10820.390625
Iteration 10500: Loss = -10820.3857421875
Iteration 10600: Loss = -10820.37890625
Iteration 10700: Loss = -10820.373046875
Iteration 10800: Loss = -10820.3662109375
Iteration 10900: Loss = -10820.3603515625
Iteration 11000: Loss = -10820.3564453125
Iteration 11100: Loss = -10820.3525390625
Iteration 11200: Loss = -10820.345703125
Iteration 11300: Loss = -10820.341796875
Iteration 11400: Loss = -10820.337890625
Iteration 11500: Loss = -10820.333984375
Iteration 11600: Loss = -10820.3310546875
Iteration 11700: Loss = -10820.326171875
Iteration 11800: Loss = -10820.32421875
Iteration 11900: Loss = -10820.3212890625
Iteration 12000: Loss = -10820.3173828125
Iteration 12100: Loss = -10820.314453125
Iteration 12200: Loss = -10820.3134765625
Iteration 12300: Loss = -10820.3115234375
Iteration 12400: Loss = -10820.30859375
Iteration 12500: Loss = -10820.3056640625
Iteration 12600: Loss = -10820.3037109375
Iteration 12700: Loss = -10820.3017578125
Iteration 12800: Loss = -10820.30078125
Iteration 12900: Loss = -10820.298828125
Iteration 13000: Loss = -10820.2958984375
Iteration 13100: Loss = -10820.2919921875
Iteration 13200: Loss = -10820.2900390625
Iteration 13300: Loss = -10820.2890625
Iteration 13400: Loss = -10820.2880859375
Iteration 13500: Loss = -10820.28515625
Iteration 13600: Loss = -10820.2861328125
1
Iteration 13700: Loss = -10820.283203125
Iteration 13800: Loss = -10820.283203125
Iteration 13900: Loss = -10820.2802734375
Iteration 14000: Loss = -10820.279296875
Iteration 14100: Loss = -10820.2802734375
1
Iteration 14200: Loss = -10820.2783203125
Iteration 14300: Loss = -10820.2783203125
Iteration 14400: Loss = -10820.27734375
Iteration 14500: Loss = -10820.2763671875
Iteration 14600: Loss = -10820.2763671875
Iteration 14700: Loss = -10820.2744140625
Iteration 14800: Loss = -10820.2724609375
Iteration 14900: Loss = -10820.275390625
1
Iteration 15000: Loss = -10820.2724609375
Iteration 15100: Loss = -10820.2724609375
Iteration 15200: Loss = -10820.271484375
Iteration 15300: Loss = -10820.271484375
Iteration 15400: Loss = -10820.2705078125
Iteration 15500: Loss = -10820.26953125
Iteration 15600: Loss = -10820.2705078125
1
Iteration 15700: Loss = -10820.267578125
Iteration 15800: Loss = -10820.2666015625
Iteration 15900: Loss = -10820.265625
Iteration 16000: Loss = -10820.259765625
Iteration 16100: Loss = -10820.2587890625
Iteration 16200: Loss = -10820.2578125
Iteration 16300: Loss = -10820.25
Iteration 16400: Loss = -10820.2333984375
Iteration 16500: Loss = -10820.224609375
Iteration 16600: Loss = -10820.2021484375
Iteration 16700: Loss = -10820.1259765625
Iteration 16800: Loss = -10819.697265625
Iteration 16900: Loss = -10819.5908203125
Iteration 17000: Loss = -10819.564453125
Iteration 17100: Loss = -10819.1201171875
Iteration 17200: Loss = -10819.0595703125
Iteration 17300: Loss = -10818.8310546875
Iteration 17400: Loss = -10818.4892578125
Iteration 17500: Loss = -10818.3935546875
Iteration 17600: Loss = -10818.3916015625
Iteration 17700: Loss = -10817.4873046875
Iteration 17800: Loss = -10817.4375
Iteration 17900: Loss = -10817.4267578125
Iteration 18000: Loss = -10817.421875
Iteration 18100: Loss = -10817.4189453125
Iteration 18200: Loss = -10817.4189453125
Iteration 18300: Loss = -10817.416015625
Iteration 18400: Loss = -10817.4140625
Iteration 18500: Loss = -10817.4150390625
1
Iteration 18600: Loss = -10817.416015625
2
Iteration 18700: Loss = -10817.4140625
Iteration 18800: Loss = -10817.4140625
Iteration 18900: Loss = -10817.4130859375
Iteration 19000: Loss = -10817.412109375
Iteration 19100: Loss = -10817.412109375
Iteration 19200: Loss = -10817.4111328125
Iteration 19300: Loss = -10817.412109375
1
Iteration 19400: Loss = -10817.4111328125
Iteration 19500: Loss = -10817.4111328125
Iteration 19600: Loss = -10817.4111328125
Iteration 19700: Loss = -10817.4111328125
Iteration 19800: Loss = -10817.4111328125
Iteration 19900: Loss = -10817.4111328125
Iteration 20000: Loss = -10817.4111328125
Iteration 20100: Loss = -10817.41015625
Iteration 20200: Loss = -10817.41015625
Iteration 20300: Loss = -10817.41015625
Iteration 20400: Loss = -10817.41015625
Iteration 20500: Loss = -10817.4111328125
1
Iteration 20600: Loss = -10817.41015625
Iteration 20700: Loss = -10817.41015625
Iteration 20800: Loss = -10817.41015625
Iteration 20900: Loss = -10817.408203125
Iteration 21000: Loss = -10817.41015625
1
Iteration 21100: Loss = -10817.41015625
2
Iteration 21200: Loss = -10817.412109375
3
Iteration 21300: Loss = -10817.41015625
4
Iteration 21400: Loss = -10817.41015625
5
Iteration 21500: Loss = -10817.408203125
Iteration 21600: Loss = -10817.4091796875
1
Iteration 21700: Loss = -10817.41015625
2
Iteration 21800: Loss = -10817.4091796875
3
Iteration 21900: Loss = -10817.4111328125
4
Iteration 22000: Loss = -10817.4091796875
5
Iteration 22100: Loss = -10817.41015625
6
Iteration 22200: Loss = -10817.4111328125
7
Iteration 22300: Loss = -10817.4091796875
8
Iteration 22400: Loss = -10817.4091796875
9
Iteration 22500: Loss = -10817.4091796875
10
Iteration 22600: Loss = -10817.41015625
11
Iteration 22700: Loss = -10817.4091796875
12
Iteration 22800: Loss = -10817.4091796875
13
Iteration 22900: Loss = -10817.408203125
Iteration 23000: Loss = -10817.4091796875
1
Iteration 23100: Loss = -10817.4091796875
2
Iteration 23200: Loss = -10817.41015625
3
Iteration 23300: Loss = -10817.41015625
4
Iteration 23400: Loss = -10817.4091796875
5
Iteration 23500: Loss = -10817.41015625
6
Iteration 23600: Loss = -10817.41015625
7
Iteration 23700: Loss = -10817.4091796875
8
Iteration 23800: Loss = -10817.4091796875
9
Iteration 23900: Loss = -10817.41015625
10
Iteration 24000: Loss = -10817.41015625
11
Iteration 24100: Loss = -10817.4091796875
12
Iteration 24200: Loss = -10817.41015625
13
Iteration 24300: Loss = -10817.4091796875
14
Iteration 24400: Loss = -10817.4091796875
15
Stopping early at iteration 24400 due to no improvement.
pi: tensor([[1.0000e+00, 7.8298e-07],
        [9.7177e-01, 2.8226e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0013, 0.9987], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1620, 0.1477],
         [0.0302, 0.1484]],

        [[0.9346, 0.0960],
         [0.8311, 0.7274]],

        [[0.0870, 0.8619],
         [0.8984, 0.9310]],

        [[0.7497, 0.1703],
         [0.1030, 0.9931]],

        [[0.0132, 0.8846],
         [0.4622, 0.0221]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001621753445879888
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52924.046875
Iteration 100: Loss = -39689.0
Iteration 200: Loss = -28727.7890625
Iteration 300: Loss = -20051.669921875
Iteration 400: Loss = -14768.1865234375
Iteration 500: Loss = -12317.271484375
Iteration 600: Loss = -11358.5185546875
Iteration 700: Loss = -11038.4140625
Iteration 800: Loss = -10922.361328125
Iteration 900: Loss = -10879.9638671875
Iteration 1000: Loss = -10859.65234375
Iteration 1100: Loss = -10848.2294921875
Iteration 1200: Loss = -10841.47265625
Iteration 1300: Loss = -10838.587890625
Iteration 1400: Loss = -10836.31640625
Iteration 1500: Loss = -10832.623046875
Iteration 1600: Loss = -10826.66015625
Iteration 1700: Loss = -10824.48046875
Iteration 1800: Loss = -10823.6455078125
Iteration 1900: Loss = -10823.166015625
Iteration 2000: Loss = -10822.8251953125
Iteration 2100: Loss = -10822.564453125
Iteration 2200: Loss = -10822.349609375
Iteration 2300: Loss = -10822.1669921875
Iteration 2400: Loss = -10822.0078125
Iteration 2500: Loss = -10821.8720703125
Iteration 2600: Loss = -10821.7490234375
Iteration 2700: Loss = -10821.638671875
Iteration 2800: Loss = -10821.541015625
Iteration 2900: Loss = -10821.4541015625
Iteration 3000: Loss = -10821.376953125
Iteration 3100: Loss = -10821.3017578125
Iteration 3200: Loss = -10821.23828125
Iteration 3300: Loss = -10821.1767578125
Iteration 3400: Loss = -10821.12109375
Iteration 3500: Loss = -10821.0693359375
Iteration 3600: Loss = -10821.0205078125
Iteration 3700: Loss = -10820.9775390625
Iteration 3800: Loss = -10820.9345703125
Iteration 3900: Loss = -10820.8955078125
Iteration 4000: Loss = -10820.8603515625
Iteration 4100: Loss = -10820.8271484375
Iteration 4200: Loss = -10820.7958984375
Iteration 4300: Loss = -10820.765625
Iteration 4400: Loss = -10820.7392578125
Iteration 4500: Loss = -10820.7119140625
Iteration 4600: Loss = -10820.6875
Iteration 4700: Loss = -10820.6640625
Iteration 4800: Loss = -10820.642578125
Iteration 4900: Loss = -10820.6220703125
Iteration 5000: Loss = -10820.6025390625
Iteration 5100: Loss = -10820.583984375
Iteration 5200: Loss = -10820.5673828125
Iteration 5300: Loss = -10820.5498046875
Iteration 5400: Loss = -10820.5341796875
Iteration 5500: Loss = -10820.517578125
Iteration 5600: Loss = -10820.50390625
Iteration 5700: Loss = -10820.4921875
Iteration 5800: Loss = -10820.478515625
Iteration 5900: Loss = -10820.4658203125
Iteration 6000: Loss = -10820.455078125
Iteration 6100: Loss = -10820.443359375
Iteration 6200: Loss = -10820.43359375
Iteration 6300: Loss = -10820.4248046875
Iteration 6400: Loss = -10820.4150390625
Iteration 6500: Loss = -10820.4052734375
Iteration 6600: Loss = -10820.3984375
Iteration 6700: Loss = -10820.390625
Iteration 6800: Loss = -10820.3818359375
Iteration 6900: Loss = -10820.3740234375
Iteration 7000: Loss = -10820.3671875
Iteration 7100: Loss = -10820.361328125
Iteration 7200: Loss = -10820.35546875
Iteration 7300: Loss = -10820.349609375
Iteration 7400: Loss = -10820.3447265625
Iteration 7500: Loss = -10820.3388671875
Iteration 7600: Loss = -10820.3349609375
Iteration 7700: Loss = -10820.330078125
Iteration 7800: Loss = -10820.3251953125
Iteration 7900: Loss = -10820.3212890625
Iteration 8000: Loss = -10820.3173828125
Iteration 8100: Loss = -10820.3134765625
Iteration 8200: Loss = -10820.310546875
Iteration 8300: Loss = -10820.306640625
Iteration 8400: Loss = -10820.3046875
Iteration 8500: Loss = -10820.2998046875
Iteration 8600: Loss = -10820.296875
Iteration 8700: Loss = -10820.29296875
Iteration 8800: Loss = -10820.291015625
Iteration 8900: Loss = -10820.287109375
Iteration 9000: Loss = -10820.2861328125
Iteration 9100: Loss = -10820.283203125
Iteration 9200: Loss = -10820.2802734375
Iteration 9300: Loss = -10820.279296875
Iteration 9400: Loss = -10820.2744140625
Iteration 9500: Loss = -10820.2744140625
Iteration 9600: Loss = -10820.271484375
Iteration 9700: Loss = -10820.2705078125
Iteration 9800: Loss = -10820.2685546875
Iteration 9900: Loss = -10820.265625
Iteration 10000: Loss = -10820.2666015625
1
Iteration 10100: Loss = -10820.263671875
Iteration 10200: Loss = -10820.263671875
Iteration 10300: Loss = -10820.2626953125
Iteration 10400: Loss = -10820.26171875
Iteration 10500: Loss = -10820.2607421875
Iteration 10600: Loss = -10820.2587890625
Iteration 10700: Loss = -10820.2578125
Iteration 10800: Loss = -10820.2568359375
Iteration 10900: Loss = -10820.2529296875
Iteration 11000: Loss = -10820.2529296875
Iteration 11100: Loss = -10820.251953125
Iteration 11200: Loss = -10820.2509765625
Iteration 11300: Loss = -10820.25
Iteration 11400: Loss = -10820.2470703125
Iteration 11500: Loss = -10820.2490234375
1
Iteration 11600: Loss = -10820.24609375
Iteration 11700: Loss = -10820.24609375
Iteration 11800: Loss = -10820.2431640625
Iteration 11900: Loss = -10820.244140625
1
Iteration 12000: Loss = -10820.2412109375
Iteration 12100: Loss = -10820.23828125
Iteration 12200: Loss = -10820.232421875
Iteration 12300: Loss = -10820.1630859375
Iteration 12400: Loss = -10818.685546875
Iteration 12500: Loss = -10818.5205078125
Iteration 12600: Loss = -10818.45703125
Iteration 12700: Loss = -10818.421875
Iteration 12800: Loss = -10818.3994140625
Iteration 12900: Loss = -10818.3837890625
Iteration 13000: Loss = -10818.37109375
Iteration 13100: Loss = -10818.361328125
Iteration 13200: Loss = -10818.353515625
Iteration 13300: Loss = -10818.34765625
Iteration 13400: Loss = -10818.337890625
Iteration 13500: Loss = -10818.3330078125
Iteration 13600: Loss = -10818.3310546875
Iteration 13700: Loss = -10818.328125
Iteration 13800: Loss = -10818.3251953125
Iteration 13900: Loss = -10818.3232421875
Iteration 14000: Loss = -10818.322265625
Iteration 14100: Loss = -10818.3212890625
Iteration 14200: Loss = -10818.318359375
Iteration 14300: Loss = -10818.3173828125
Iteration 14400: Loss = -10818.3154296875
Iteration 14500: Loss = -10818.3154296875
Iteration 14600: Loss = -10818.3154296875
Iteration 14700: Loss = -10818.3134765625
Iteration 14800: Loss = -10818.3125
Iteration 14900: Loss = -10818.3125
Iteration 15000: Loss = -10818.3134765625
1
Iteration 15100: Loss = -10818.3115234375
Iteration 15200: Loss = -10818.3095703125
Iteration 15300: Loss = -10818.310546875
1
Iteration 15400: Loss = -10818.30859375
Iteration 15500: Loss = -10818.3095703125
1
Iteration 15600: Loss = -10818.3076171875
Iteration 15700: Loss = -10818.30859375
1
Iteration 15800: Loss = -10818.3076171875
Iteration 15900: Loss = -10818.306640625
Iteration 16000: Loss = -10818.3076171875
1
Iteration 16100: Loss = -10818.3056640625
Iteration 16200: Loss = -10818.3056640625
Iteration 16300: Loss = -10818.3056640625
Iteration 16400: Loss = -10818.306640625
1
Iteration 16500: Loss = -10818.3056640625
Iteration 16600: Loss = -10818.3046875
Iteration 16700: Loss = -10818.3037109375
Iteration 16800: Loss = -10818.3046875
1
Iteration 16900: Loss = -10818.3037109375
Iteration 17000: Loss = -10818.3037109375
Iteration 17100: Loss = -10818.302734375
Iteration 17200: Loss = -10818.3046875
1
Iteration 17300: Loss = -10818.3037109375
2
Iteration 17400: Loss = -10818.302734375
Iteration 17500: Loss = -10818.302734375
Iteration 17600: Loss = -10818.302734375
Iteration 17700: Loss = -10818.3017578125
Iteration 17800: Loss = -10818.3017578125
Iteration 17900: Loss = -10818.3017578125
Iteration 18000: Loss = -10818.302734375
1
Iteration 18100: Loss = -10818.3046875
2
Iteration 18200: Loss = -10818.30078125
Iteration 18300: Loss = -10818.30078125
Iteration 18400: Loss = -10818.30078125
Iteration 18500: Loss = -10818.30078125
Iteration 18600: Loss = -10818.30078125
Iteration 18700: Loss = -10818.298828125
Iteration 18800: Loss = -10818.30078125
1
Iteration 18900: Loss = -10818.30078125
2
Iteration 19000: Loss = -10818.298828125
Iteration 19100: Loss = -10818.298828125
Iteration 19200: Loss = -10818.298828125
Iteration 19300: Loss = -10818.298828125
Iteration 19400: Loss = -10818.2978515625
Iteration 19500: Loss = -10818.2978515625
Iteration 19600: Loss = -10818.298828125
1
Iteration 19700: Loss = -10818.296875
Iteration 19800: Loss = -10818.298828125
1
Iteration 19900: Loss = -10818.296875
Iteration 20000: Loss = -10818.296875
Iteration 20100: Loss = -10818.298828125
1
Iteration 20200: Loss = -10818.296875
Iteration 20300: Loss = -10818.2978515625
1
Iteration 20400: Loss = -10818.296875
Iteration 20500: Loss = -10818.2958984375
Iteration 20600: Loss = -10818.296875
1
Iteration 20700: Loss = -10818.2978515625
2
Iteration 20800: Loss = -10818.2958984375
Iteration 20900: Loss = -10818.2958984375
Iteration 21000: Loss = -10818.296875
1
Iteration 21100: Loss = -10818.296875
2
Iteration 21200: Loss = -10818.2890625
Iteration 21300: Loss = -10818.2900390625
1
Iteration 21400: Loss = -10818.2880859375
Iteration 21500: Loss = -10818.2880859375
Iteration 21600: Loss = -10818.2861328125
Iteration 21700: Loss = -10818.28515625
Iteration 21800: Loss = -10818.2841796875
Iteration 21900: Loss = -10818.2861328125
1
Iteration 22000: Loss = -10818.283203125
Iteration 22100: Loss = -10818.2822265625
Iteration 22200: Loss = -10818.279296875
Iteration 22300: Loss = -10818.2763671875
Iteration 22400: Loss = -10818.275390625
Iteration 22500: Loss = -10818.271484375
Iteration 22600: Loss = -10818.271484375
Iteration 22700: Loss = -10818.2685546875
Iteration 22800: Loss = -10818.2666015625
Iteration 22900: Loss = -10818.2666015625
Iteration 23000: Loss = -10818.263671875
Iteration 23100: Loss = -10818.255859375
Iteration 23200: Loss = -10818.255859375
Iteration 23300: Loss = -10818.25390625
Iteration 23400: Loss = -10818.255859375
1
Iteration 23500: Loss = -10818.255859375
2
Iteration 23600: Loss = -10818.2548828125
3
Iteration 23700: Loss = -10818.25390625
Iteration 23800: Loss = -10818.25390625
Iteration 23900: Loss = -10818.24609375
Iteration 24000: Loss = -10818.24609375
Iteration 24100: Loss = -10818.2451171875
Iteration 24200: Loss = -10818.24609375
1
Iteration 24300: Loss = -10818.2451171875
Iteration 24400: Loss = -10818.2451171875
Iteration 24500: Loss = -10818.2451171875
Iteration 24600: Loss = -10818.2431640625
Iteration 24700: Loss = -10818.244140625
1
Iteration 24800: Loss = -10818.2451171875
2
Iteration 24900: Loss = -10818.244140625
3
Iteration 25000: Loss = -10818.24609375
4
Iteration 25100: Loss = -10818.244140625
5
Iteration 25200: Loss = -10818.244140625
6
Iteration 25300: Loss = -10818.244140625
7
Iteration 25400: Loss = -10818.244140625
8
Iteration 25500: Loss = -10818.244140625
9
Iteration 25600: Loss = -10818.244140625
10
Iteration 25700: Loss = -10818.2451171875
11
Iteration 25800: Loss = -10818.2431640625
Iteration 25900: Loss = -10818.2431640625
Iteration 26000: Loss = -10818.2412109375
Iteration 26100: Loss = -10818.244140625
1
Iteration 26200: Loss = -10818.2431640625
2
Iteration 26300: Loss = -10818.2431640625
3
Iteration 26400: Loss = -10818.244140625
4
Iteration 26500: Loss = -10818.2431640625
5
Iteration 26600: Loss = -10818.2421875
6
Iteration 26700: Loss = -10818.2421875
7
Iteration 26800: Loss = -10818.2431640625
8
Iteration 26900: Loss = -10818.2421875
9
Iteration 27000: Loss = -10818.2431640625
10
Iteration 27100: Loss = -10818.2431640625
11
Iteration 27200: Loss = -10818.244140625
12
Iteration 27300: Loss = -10818.244140625
13
Iteration 27400: Loss = -10818.2412109375
Iteration 27500: Loss = -10818.2421875
1
Iteration 27600: Loss = -10818.2421875
2
Iteration 27700: Loss = -10818.2431640625
3
Iteration 27800: Loss = -10818.2421875
4
Iteration 27900: Loss = -10818.2431640625
5
Iteration 28000: Loss = -10818.2421875
6
Iteration 28100: Loss = -10818.2421875
7
Iteration 28200: Loss = -10818.2431640625
8
Iteration 28300: Loss = -10818.2431640625
9
Iteration 28400: Loss = -10818.2421875
10
Iteration 28500: Loss = -10818.2421875
11
Iteration 28600: Loss = -10818.2421875
12
Iteration 28700: Loss = -10818.2431640625
13
Iteration 28800: Loss = -10818.244140625
14
Iteration 28900: Loss = -10818.244140625
15
Stopping early at iteration 28900 due to no improvement.
pi: tensor([[9.9999e-01, 8.8799e-06],
        [1.2966e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0103, 0.9897], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0729, 0.0843],
         [0.0418, 0.1581]],

        [[0.2464, 0.2478],
         [0.9931, 0.0406]],

        [[0.5412, 0.2208],
         [0.9799, 0.9877]],

        [[0.0247, 0.1901],
         [0.1015, 0.7561]],

        [[0.6118, 0.1813],
         [0.0074, 0.9523]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.0030485335024604443
Average Adjusted Rand Index: -0.003276805219792214
[-0.001621753445879888, -0.0030485335024604443] [0.0, -0.003276805219792214] [10817.4091796875, 10818.244140625]
-------------------------------------
This iteration is 15
True Objective function: Loss = -10918.331578477699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17997.716796875
Iteration 100: Loss = -13846.1494140625
Iteration 200: Loss = -11666.18359375
Iteration 300: Loss = -11131.7900390625
Iteration 400: Loss = -11071.6064453125
Iteration 500: Loss = -11049.6884765625
Iteration 600: Loss = -11035.2548828125
Iteration 700: Loss = -11028.5234375
Iteration 800: Loss = -11023.1494140625
Iteration 900: Loss = -11019.4013671875
Iteration 1000: Loss = -11017.77734375
Iteration 1100: Loss = -11016.9208984375
Iteration 1200: Loss = -11016.33984375
Iteration 1300: Loss = -11015.9052734375
Iteration 1400: Loss = -11015.568359375
Iteration 1500: Loss = -11015.30078125
Iteration 1600: Loss = -11015.083984375
Iteration 1700: Loss = -11014.9052734375
Iteration 1800: Loss = -11014.755859375
Iteration 1900: Loss = -11014.6279296875
Iteration 2000: Loss = -11014.51953125
Iteration 2100: Loss = -11014.423828125
Iteration 2200: Loss = -11014.3408203125
Iteration 2300: Loss = -11014.2646484375
Iteration 2400: Loss = -11014.1953125
Iteration 2500: Loss = -11014.1298828125
Iteration 2600: Loss = -11014.072265625
Iteration 2700: Loss = -11014.01953125
Iteration 2800: Loss = -11013.9755859375
Iteration 2900: Loss = -11013.93359375
Iteration 3000: Loss = -11013.89453125
Iteration 3100: Loss = -11013.8603515625
Iteration 3200: Loss = -11013.826171875
Iteration 3300: Loss = -11013.796875
Iteration 3400: Loss = -11013.7685546875
Iteration 3500: Loss = -11013.7421875
Iteration 3600: Loss = -11013.7158203125
Iteration 3700: Loss = -11013.69140625
Iteration 3800: Loss = -11013.6669921875
Iteration 3900: Loss = -11013.6435546875
Iteration 4000: Loss = -11013.6201171875
Iteration 4100: Loss = -11013.59765625
Iteration 4200: Loss = -11013.5732421875
Iteration 4300: Loss = -11013.5478515625
Iteration 4400: Loss = -11013.51953125
Iteration 4500: Loss = -11013.4873046875
Iteration 4600: Loss = -11013.4404296875
Iteration 4700: Loss = -11013.3779296875
Iteration 4800: Loss = -11013.283203125
Iteration 4900: Loss = -11013.1279296875
Iteration 5000: Loss = -11012.9296875
Iteration 5100: Loss = -11012.7763671875
Iteration 5200: Loss = -11012.6728515625
Iteration 5300: Loss = -11012.5693359375
Iteration 5400: Loss = -11012.3896484375
Iteration 5500: Loss = -11011.267578125
Iteration 5600: Loss = -11009.7802734375
Iteration 5700: Loss = -11009.623046875
Iteration 5800: Loss = -11009.5419921875
Iteration 5900: Loss = -11009.4306640625
Iteration 6000: Loss = -11008.7470703125
Iteration 6100: Loss = -11008.572265625
Iteration 6200: Loss = -11008.478515625
Iteration 6300: Loss = -11008.408203125
Iteration 6400: Loss = -11008.357421875
Iteration 6500: Loss = -11008.3173828125
Iteration 6600: Loss = -11008.28125
Iteration 6700: Loss = -11008.2470703125
Iteration 6800: Loss = -11008.2158203125
Iteration 6900: Loss = -11008.18359375
Iteration 7000: Loss = -11008.15625
Iteration 7100: Loss = -11008.12890625
Iteration 7200: Loss = -11008.1025390625
Iteration 7300: Loss = -11008.078125
Iteration 7400: Loss = -11008.056640625
Iteration 7500: Loss = -11008.037109375
Iteration 7600: Loss = -11008.017578125
Iteration 7700: Loss = -11007.9990234375
Iteration 7800: Loss = -11007.982421875
Iteration 7900: Loss = -11007.966796875
Iteration 8000: Loss = -11007.953125
Iteration 8100: Loss = -11007.9384765625
Iteration 8200: Loss = -11007.9267578125
Iteration 8300: Loss = -11007.9140625
Iteration 8400: Loss = -11007.9033203125
Iteration 8500: Loss = -11007.89453125
Iteration 8600: Loss = -11007.8876953125
Iteration 8700: Loss = -11007.8798828125
Iteration 8800: Loss = -11007.8740234375
Iteration 8900: Loss = -11007.8681640625
Iteration 9000: Loss = -11007.86328125
Iteration 9100: Loss = -11007.8603515625
Iteration 9200: Loss = -11007.85546875
Iteration 9300: Loss = -11007.8525390625
Iteration 9400: Loss = -11007.849609375
Iteration 9500: Loss = -11007.84765625
Iteration 9600: Loss = -11007.84375
Iteration 9700: Loss = -11007.8427734375
Iteration 9800: Loss = -11007.83984375
Iteration 9900: Loss = -11007.8388671875
Iteration 10000: Loss = -11007.8359375
Iteration 10100: Loss = -11007.8349609375
Iteration 10200: Loss = -11007.833984375
Iteration 10300: Loss = -11007.83203125
Iteration 10400: Loss = -11007.830078125
Iteration 10500: Loss = -11007.8310546875
1
Iteration 10600: Loss = -11007.8291015625
Iteration 10700: Loss = -11007.828125
Iteration 10800: Loss = -11007.8271484375
Iteration 10900: Loss = -11007.826171875
Iteration 11000: Loss = -11007.8251953125
Iteration 11100: Loss = -11007.82421875
Iteration 11200: Loss = -11007.8251953125
1
Iteration 11300: Loss = -11007.8232421875
Iteration 11400: Loss = -11007.822265625
Iteration 11500: Loss = -11007.822265625
Iteration 11600: Loss = -11007.822265625
Iteration 11700: Loss = -11007.8212890625
Iteration 11800: Loss = -11007.8212890625
Iteration 11900: Loss = -11007.8203125
Iteration 12000: Loss = -11007.8203125
Iteration 12100: Loss = -11007.8212890625
1
Iteration 12200: Loss = -11007.8203125
Iteration 12300: Loss = -11007.8193359375
Iteration 12400: Loss = -11007.8203125
1
Iteration 12500: Loss = -11007.818359375
Iteration 12600: Loss = -11007.8203125
1
Iteration 12700: Loss = -11007.8193359375
2
Iteration 12800: Loss = -11007.8173828125
Iteration 12900: Loss = -11007.818359375
1
Iteration 13000: Loss = -11007.8193359375
2
Iteration 13100: Loss = -11007.8173828125
Iteration 13200: Loss = -11007.8173828125
Iteration 13300: Loss = -11007.81640625
Iteration 13400: Loss = -11007.81640625
Iteration 13500: Loss = -11007.8173828125
1
Iteration 13600: Loss = -11007.81640625
Iteration 13700: Loss = -11007.81640625
Iteration 13800: Loss = -11007.81640625
Iteration 13900: Loss = -11007.8173828125
1
Iteration 14000: Loss = -11007.81640625
Iteration 14100: Loss = -11007.81640625
Iteration 14200: Loss = -11007.8154296875
Iteration 14300: Loss = -11007.8173828125
1
Iteration 14400: Loss = -11007.8154296875
Iteration 14500: Loss = -11007.81640625
1
Iteration 14600: Loss = -11007.8154296875
Iteration 14700: Loss = -11007.8154296875
Iteration 14800: Loss = -11007.8154296875
Iteration 14900: Loss = -11007.81640625
1
Iteration 15000: Loss = -11007.8154296875
Iteration 15100: Loss = -11007.8154296875
Iteration 15200: Loss = -11007.8154296875
Iteration 15300: Loss = -11007.81640625
1
Iteration 15400: Loss = -11007.8154296875
Iteration 15500: Loss = -11007.8154296875
Iteration 15600: Loss = -11007.8154296875
Iteration 15700: Loss = -11007.81640625
1
Iteration 15800: Loss = -11007.814453125
Iteration 15900: Loss = -11007.814453125
Iteration 16000: Loss = -11007.814453125
Iteration 16100: Loss = -11007.8154296875
1
Iteration 16200: Loss = -11007.81640625
2
Iteration 16300: Loss = -11007.814453125
Iteration 16400: Loss = -11007.8134765625
Iteration 16500: Loss = -11007.8154296875
1
Iteration 16600: Loss = -11007.814453125
2
Iteration 16700: Loss = -11007.814453125
3
Iteration 16800: Loss = -11007.8154296875
4
Iteration 16900: Loss = -11007.8154296875
5
Iteration 17000: Loss = -11007.8154296875
6
Iteration 17100: Loss = -11007.8154296875
7
Iteration 17200: Loss = -11007.8134765625
Iteration 17300: Loss = -11007.814453125
1
Iteration 17400: Loss = -11007.8134765625
Iteration 17500: Loss = -11007.8154296875
1
Iteration 17600: Loss = -11007.8154296875
2
Iteration 17700: Loss = -11007.814453125
3
Iteration 17800: Loss = -11007.814453125
4
Iteration 17900: Loss = -11007.814453125
5
Iteration 18000: Loss = -11007.8154296875
6
Iteration 18100: Loss = -11007.8154296875
7
Iteration 18200: Loss = -11007.8154296875
8
Iteration 18300: Loss = -11007.814453125
9
Iteration 18400: Loss = -11007.8134765625
Iteration 18500: Loss = -11007.814453125
1
Iteration 18600: Loss = -11007.814453125
2
Iteration 18700: Loss = -11007.8134765625
Iteration 18800: Loss = -11007.8154296875
1
Iteration 18900: Loss = -11007.8134765625
Iteration 19000: Loss = -11007.8154296875
1
Iteration 19100: Loss = -11007.814453125
2
Iteration 19200: Loss = -11007.8134765625
Iteration 19300: Loss = -11007.814453125
1
Iteration 19400: Loss = -11007.814453125
2
Iteration 19500: Loss = -11007.8134765625
Iteration 19600: Loss = -11007.8134765625
Iteration 19700: Loss = -11007.8134765625
Iteration 19800: Loss = -11007.8154296875
1
Iteration 19900: Loss = -11007.8134765625
Iteration 20000: Loss = -11007.8134765625
Iteration 20100: Loss = -11007.8134765625
Iteration 20200: Loss = -11007.8134765625
Iteration 20300: Loss = -11007.8134765625
Iteration 20400: Loss = -11007.814453125
1
Iteration 20500: Loss = -11007.8134765625
Iteration 20600: Loss = -11007.814453125
1
Iteration 20700: Loss = -11007.814453125
2
Iteration 20800: Loss = -11007.814453125
3
Iteration 20900: Loss = -11007.8154296875
4
Iteration 21000: Loss = -11007.814453125
5
Iteration 21100: Loss = -11007.8134765625
Iteration 21200: Loss = -11007.8154296875
1
Iteration 21300: Loss = -11007.814453125
2
Iteration 21400: Loss = -11007.8134765625
Iteration 21500: Loss = -11007.8154296875
1
Iteration 21600: Loss = -11007.8154296875
2
Iteration 21700: Loss = -11007.8154296875
3
Iteration 21800: Loss = -11007.8154296875
4
Iteration 21900: Loss = -11007.8134765625
Iteration 22000: Loss = -11007.814453125
1
Iteration 22100: Loss = -11007.814453125
2
Iteration 22200: Loss = -11007.8134765625
Iteration 22300: Loss = -11007.8134765625
Iteration 22400: Loss = -11007.814453125
1
Iteration 22500: Loss = -11007.814453125
2
Iteration 22600: Loss = -11007.814453125
3
Iteration 22700: Loss = -11007.814453125
4
Iteration 22800: Loss = -11007.814453125
5
Iteration 22900: Loss = -11007.8154296875
6
Iteration 23000: Loss = -11007.8154296875
7
Iteration 23100: Loss = -11007.8154296875
8
Iteration 23200: Loss = -11007.8134765625
Iteration 23300: Loss = -11007.814453125
1
Iteration 23400: Loss = -11007.814453125
2
Iteration 23500: Loss = -11007.814453125
3
Iteration 23600: Loss = -11007.814453125
4
Iteration 23700: Loss = -11007.814453125
5
Iteration 23800: Loss = -11007.81640625
6
Iteration 23900: Loss = -11007.8134765625
Iteration 24000: Loss = -11007.814453125
1
Iteration 24100: Loss = -11007.814453125
2
Iteration 24200: Loss = -11007.8154296875
3
Iteration 24300: Loss = -11007.8154296875
4
Iteration 24400: Loss = -11007.8154296875
5
Iteration 24500: Loss = -11007.8154296875
6
Iteration 24600: Loss = -11007.8134765625
Iteration 24700: Loss = -11007.814453125
1
Iteration 24800: Loss = -11007.8154296875
2
Iteration 24900: Loss = -11007.8154296875
3
Iteration 25000: Loss = -11007.814453125
4
Iteration 25100: Loss = -11007.8154296875
5
Iteration 25200: Loss = -11007.814453125
6
Iteration 25300: Loss = -11007.814453125
7
Iteration 25400: Loss = -11007.8154296875
8
Iteration 25500: Loss = -11007.814453125
9
Iteration 25600: Loss = -11007.814453125
10
Iteration 25700: Loss = -11007.814453125
11
Iteration 25800: Loss = -11007.814453125
12
Iteration 25900: Loss = -11007.8134765625
Iteration 26000: Loss = -11007.814453125
1
Iteration 26100: Loss = -11007.8154296875
2
Iteration 26200: Loss = -11007.8154296875
3
Iteration 26300: Loss = -11007.814453125
4
Iteration 26400: Loss = -11007.8154296875
5
Iteration 26500: Loss = -11007.814453125
6
Iteration 26600: Loss = -11007.814453125
7
Iteration 26700: Loss = -11007.8134765625
Iteration 26800: Loss = -11007.8154296875
1
Iteration 26900: Loss = -11007.814453125
2
Iteration 27000: Loss = -11007.814453125
3
Iteration 27100: Loss = -11007.814453125
4
Iteration 27200: Loss = -11007.814453125
5
Iteration 27300: Loss = -11007.8154296875
6
Iteration 27400: Loss = -11007.814453125
7
Iteration 27500: Loss = -11007.8134765625
Iteration 27600: Loss = -11007.8134765625
Iteration 27700: Loss = -11007.8154296875
1
Iteration 27800: Loss = -11007.8134765625
Iteration 27900: Loss = -11007.8154296875
1
Iteration 28000: Loss = -11007.8154296875
2
Iteration 28100: Loss = -11007.8134765625
Iteration 28200: Loss = -11007.8134765625
Iteration 28300: Loss = -11007.814453125
1
Iteration 28400: Loss = -11007.814453125
2
Iteration 28500: Loss = -11007.8134765625
Iteration 28600: Loss = -11007.8134765625
Iteration 28700: Loss = -11007.814453125
1
Iteration 28800: Loss = -11007.814453125
2
Iteration 28900: Loss = -11007.814453125
3
Iteration 29000: Loss = -11007.8154296875
4
Iteration 29100: Loss = -11007.814453125
5
Iteration 29200: Loss = -11007.814453125
6
Iteration 29300: Loss = -11007.8134765625
Iteration 29400: Loss = -11007.8154296875
1
Iteration 29500: Loss = -11007.8134765625
Iteration 29600: Loss = -11007.814453125
1
Iteration 29700: Loss = -11007.8134765625
Iteration 29800: Loss = -11007.814453125
1
Iteration 29900: Loss = -11007.814453125
2
pi: tensor([[1.0000e+00, 1.1183e-06],
        [2.1945e-01, 7.8055e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9426, 0.0574], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1591, 0.2380],
         [0.8519, 0.0210]],

        [[0.5334, 0.2096],
         [0.2132, 0.7146]],

        [[0.3543, 0.2141],
         [0.9928, 0.0090]],

        [[0.6049, 0.2450],
         [0.8237, 0.8164]],

        [[0.4421, 0.1356],
         [0.8791, 0.9927]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.016449682236070833
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.012298440577296121
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
Global Adjusted Rand Index: -0.0010057635473506878
Average Adjusted Rand Index: -0.006947483394617005
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41246.12109375
Iteration 100: Loss = -23430.115234375
Iteration 200: Loss = -13880.501953125
Iteration 300: Loss = -12054.994140625
Iteration 400: Loss = -11642.2490234375
Iteration 500: Loss = -11448.4765625
Iteration 600: Loss = -11335.83203125
Iteration 700: Loss = -11261.5732421875
Iteration 800: Loss = -11215.771484375
Iteration 900: Loss = -11169.421875
Iteration 1000: Loss = -11138.6494140625
Iteration 1100: Loss = -11114.5556640625
Iteration 1200: Loss = -11103.0771484375
Iteration 1300: Loss = -11086.5859375
Iteration 1400: Loss = -11073.16796875
Iteration 1500: Loss = -11068.7880859375
Iteration 1600: Loss = -11059.73828125
Iteration 1700: Loss = -11053.0322265625
Iteration 1800: Loss = -11049.6474609375
Iteration 1900: Loss = -11045.37109375
Iteration 2000: Loss = -11040.51953125
Iteration 2100: Loss = -11037.3759765625
Iteration 2200: Loss = -11033.935546875
Iteration 2300: Loss = -11032.759765625
Iteration 2400: Loss = -11031.81640625
Iteration 2500: Loss = -11031.009765625
Iteration 2600: Loss = -11030.3056640625
Iteration 2700: Loss = -11029.6796875
Iteration 2800: Loss = -11029.115234375
Iteration 2900: Loss = -11028.58984375
Iteration 3000: Loss = -11027.345703125
Iteration 3100: Loss = -11024.6669921875
Iteration 3200: Loss = -11024.1337890625
Iteration 3300: Loss = -11023.708984375
Iteration 3400: Loss = -11023.3369140625
Iteration 3500: Loss = -11023.0
Iteration 3600: Loss = -11022.6875
Iteration 3700: Loss = -11022.3994140625
Iteration 3800: Loss = -11022.1279296875
Iteration 3900: Loss = -11021.8779296875
Iteration 4000: Loss = -11021.6416015625
Iteration 4100: Loss = -11021.4228515625
Iteration 4200: Loss = -11021.220703125
Iteration 4300: Loss = -11021.0322265625
Iteration 4400: Loss = -11020.859375
Iteration 4500: Loss = -11020.6982421875
Iteration 4600: Loss = -11020.5458984375
Iteration 4700: Loss = -11020.4052734375
Iteration 4800: Loss = -11020.271484375
Iteration 4900: Loss = -11020.1435546875
Iteration 5000: Loss = -11020.02734375
Iteration 5100: Loss = -11019.916015625
Iteration 5200: Loss = -11019.8095703125
Iteration 5300: Loss = -11019.708984375
Iteration 5400: Loss = -11019.6142578125
Iteration 5500: Loss = -11019.51953125
Iteration 5600: Loss = -11019.4296875
Iteration 5700: Loss = -11019.33984375
Iteration 5800: Loss = -11019.248046875
Iteration 5900: Loss = -11019.154296875
Iteration 6000: Loss = -11014.5751953125
Iteration 6100: Loss = -11013.927734375
Iteration 6200: Loss = -11013.734375
Iteration 6300: Loss = -11013.6005859375
Iteration 6400: Loss = -11013.49609375
Iteration 6500: Loss = -11013.4072265625
Iteration 6600: Loss = -11013.326171875
Iteration 6700: Loss = -11013.2578125
Iteration 6800: Loss = -11013.193359375
Iteration 6900: Loss = -11013.130859375
Iteration 7000: Loss = -11013.0751953125
Iteration 7100: Loss = -11013.021484375
Iteration 7200: Loss = -11012.9697265625
Iteration 7300: Loss = -11012.921875
Iteration 7400: Loss = -11012.873046875
Iteration 7500: Loss = -11012.828125
Iteration 7600: Loss = -11012.7841796875
Iteration 7700: Loss = -11012.7421875
Iteration 7800: Loss = -11012.701171875
Iteration 7900: Loss = -11012.6591796875
Iteration 8000: Loss = -11012.6201171875
Iteration 8100: Loss = -11012.5810546875
Iteration 8200: Loss = -11012.54296875
Iteration 8300: Loss = -11012.5048828125
Iteration 8400: Loss = -11012.4677734375
Iteration 8500: Loss = -11012.4296875
Iteration 8600: Loss = -11012.3935546875
Iteration 8700: Loss = -11012.357421875
Iteration 8800: Loss = -11012.3251953125
Iteration 8900: Loss = -11012.2919921875
Iteration 9000: Loss = -11012.26171875
Iteration 9100: Loss = -11012.2314453125
Iteration 9200: Loss = -11012.20703125
Iteration 9300: Loss = -11012.1806640625
Iteration 9400: Loss = -11012.1591796875
Iteration 9500: Loss = -11012.13671875
Iteration 9600: Loss = -11012.1142578125
Iteration 9700: Loss = -11012.0947265625
Iteration 9800: Loss = -11012.0771484375
Iteration 9900: Loss = -11012.0615234375
Iteration 10000: Loss = -11012.0478515625
Iteration 10100: Loss = -11012.033203125
Iteration 10200: Loss = -11012.01953125
Iteration 10300: Loss = -11012.005859375
Iteration 10400: Loss = -11011.9931640625
Iteration 10500: Loss = -11011.9814453125
Iteration 10600: Loss = -11011.9697265625
Iteration 10700: Loss = -11011.958984375
Iteration 10800: Loss = -11011.953125
Iteration 10900: Loss = -11011.9404296875
Iteration 11000: Loss = -11011.9130859375
Iteration 11100: Loss = -11011.8837890625
Iteration 11200: Loss = -11011.8740234375
Iteration 11300: Loss = -11011.8583984375
Iteration 11400: Loss = -11011.82421875
Iteration 11500: Loss = -11011.798828125
Iteration 11600: Loss = -11011.7900390625
Iteration 11700: Loss = -11011.783203125
Iteration 11800: Loss = -11011.7783203125
Iteration 11900: Loss = -11011.7734375
Iteration 12000: Loss = -11011.7685546875
Iteration 12100: Loss = -11011.76171875
Iteration 12200: Loss = -11011.7548828125
Iteration 12300: Loss = -11011.7421875
Iteration 12400: Loss = -11011.7373046875
Iteration 12500: Loss = -11011.7333984375
Iteration 12600: Loss = -11011.7294921875
Iteration 12700: Loss = -11011.7236328125
Iteration 12800: Loss = -11011.712890625
Iteration 12900: Loss = -11011.6943359375
Iteration 13000: Loss = -11011.64453125
Iteration 13100: Loss = -11011.560546875
Iteration 13200: Loss = -11011.1943359375
Iteration 13300: Loss = -11010.953125
Iteration 13400: Loss = -11010.8876953125
Iteration 13500: Loss = -11010.8466796875
Iteration 13600: Loss = -11010.8154296875
Iteration 13700: Loss = -11010.802734375
Iteration 13800: Loss = -11010.7958984375
Iteration 13900: Loss = -11010.7880859375
Iteration 14000: Loss = -11010.7822265625
Iteration 14100: Loss = -11010.77734375
Iteration 14200: Loss = -11010.7763671875
Iteration 14300: Loss = -11010.7724609375
Iteration 14400: Loss = -11010.771484375
Iteration 14500: Loss = -11010.7705078125
Iteration 14600: Loss = -11010.7685546875
Iteration 14700: Loss = -11010.7666015625
Iteration 14800: Loss = -11010.765625
Iteration 14900: Loss = -11010.7646484375
Iteration 15000: Loss = -11010.76171875
Iteration 15100: Loss = -11010.7626953125
1
Iteration 15200: Loss = -11010.759765625
Iteration 15300: Loss = -11010.7587890625
Iteration 15400: Loss = -11010.7578125
Iteration 15500: Loss = -11010.7568359375
Iteration 15600: Loss = -11010.7568359375
Iteration 15700: Loss = -11010.7568359375
Iteration 15800: Loss = -11010.755859375
Iteration 15900: Loss = -11010.755859375
Iteration 16000: Loss = -11010.755859375
Iteration 16100: Loss = -11010.7548828125
Iteration 16200: Loss = -11010.7568359375
1
Iteration 16300: Loss = -11010.755859375
2
Iteration 16400: Loss = -11010.7548828125
Iteration 16500: Loss = -11010.7548828125
Iteration 16600: Loss = -11010.75390625
Iteration 16700: Loss = -11010.75390625
Iteration 16800: Loss = -11010.75390625
Iteration 16900: Loss = -11010.751953125
Iteration 17000: Loss = -11010.7548828125
1
Iteration 17100: Loss = -11010.75390625
2
Iteration 17200: Loss = -11010.7548828125
3
Iteration 17300: Loss = -11010.7529296875
4
Iteration 17400: Loss = -11010.7509765625
Iteration 17500: Loss = -11010.751953125
1
Iteration 17600: Loss = -11010.751953125
2
Iteration 17700: Loss = -11010.751953125
3
Iteration 17800: Loss = -11010.7509765625
Iteration 17900: Loss = -11010.751953125
1
Iteration 18000: Loss = -11010.751953125
2
Iteration 18100: Loss = -11010.751953125
3
Iteration 18200: Loss = -11010.751953125
4
Iteration 18300: Loss = -11010.751953125
5
Iteration 18400: Loss = -11010.7509765625
Iteration 18500: Loss = -11010.751953125
1
Iteration 18600: Loss = -11010.7509765625
Iteration 18700: Loss = -11010.751953125
1
Iteration 18800: Loss = -11010.751953125
2
Iteration 18900: Loss = -11010.751953125
3
Iteration 19000: Loss = -11010.75
Iteration 19100: Loss = -11010.751953125
1
Iteration 19200: Loss = -11010.75
Iteration 19300: Loss = -11010.75
Iteration 19400: Loss = -11010.748046875
Iteration 19500: Loss = -11010.7490234375
1
Iteration 19600: Loss = -11010.7470703125
Iteration 19700: Loss = -11010.7490234375
1
Iteration 19800: Loss = -11010.7490234375
2
Iteration 19900: Loss = -11010.7470703125
Iteration 20000: Loss = -11010.7490234375
1
Iteration 20100: Loss = -11010.7490234375
2
Iteration 20200: Loss = -11010.748046875
3
Iteration 20300: Loss = -11010.7490234375
4
Iteration 20400: Loss = -11010.748046875
5
Iteration 20500: Loss = -11010.7470703125
Iteration 20600: Loss = -11010.7470703125
Iteration 20700: Loss = -11010.7490234375
1
Iteration 20800: Loss = -11010.7470703125
Iteration 20900: Loss = -11010.75
1
Iteration 21000: Loss = -11010.7470703125
Iteration 21100: Loss = -11010.7490234375
1
Iteration 21200: Loss = -11010.748046875
2
Iteration 21300: Loss = -11010.7470703125
Iteration 21400: Loss = -11010.7470703125
Iteration 21500: Loss = -11010.7470703125
Iteration 21600: Loss = -11010.748046875
1
Iteration 21700: Loss = -11010.748046875
2
Iteration 21800: Loss = -11010.748046875
3
Iteration 21900: Loss = -11010.7470703125
Iteration 22000: Loss = -11010.748046875
1
Iteration 22100: Loss = -11010.748046875
2
Iteration 22200: Loss = -11010.7470703125
Iteration 22300: Loss = -11010.748046875
1
Iteration 22400: Loss = -11010.75
2
Iteration 22500: Loss = -11010.7490234375
3
Iteration 22600: Loss = -11010.7470703125
Iteration 22700: Loss = -11010.6494140625
Iteration 22800: Loss = -11010.6474609375
Iteration 22900: Loss = -11010.2822265625
Iteration 23000: Loss = -11010.1806640625
Iteration 23100: Loss = -11009.9697265625
Iteration 23200: Loss = -11009.6796875
Iteration 23300: Loss = -11009.6298828125
Iteration 23400: Loss = -11009.6201171875
Iteration 23500: Loss = -11009.619140625
Iteration 23600: Loss = -11009.443359375
Iteration 23700: Loss = -11009.44140625
Iteration 23800: Loss = -11009.4208984375
Iteration 23900: Loss = -11009.3984375
Iteration 24000: Loss = -11009.3916015625
Iteration 24100: Loss = -11009.34765625
Iteration 24200: Loss = -11009.185546875
Iteration 24300: Loss = -11009.166015625
Iteration 24400: Loss = -11009.13671875
Iteration 24500: Loss = -11009.12890625
Iteration 24600: Loss = -11009.1162109375
Iteration 24700: Loss = -11009.109375
Iteration 24800: Loss = -11009.0478515625
Iteration 24900: Loss = -11009.0322265625
Iteration 25000: Loss = -11009.025390625
Iteration 25100: Loss = -11009.005859375
Iteration 25200: Loss = -11009.005859375
Iteration 25300: Loss = -11009.0009765625
Iteration 25400: Loss = -11009.0009765625
Iteration 25500: Loss = -11008.99609375
Iteration 25600: Loss = -11008.9951171875
Iteration 25700: Loss = -11008.9921875
Iteration 25800: Loss = -11008.9892578125
Iteration 25900: Loss = -11008.9892578125
Iteration 26000: Loss = -11008.990234375
1
Iteration 26100: Loss = -11008.990234375
2
Iteration 26200: Loss = -11008.990234375
3
Iteration 26300: Loss = -11008.9892578125
Iteration 26400: Loss = -11008.98828125
Iteration 26500: Loss = -11008.98828125
Iteration 26600: Loss = -11008.9892578125
1
Iteration 26700: Loss = -11008.9892578125
2
Iteration 26800: Loss = -11008.990234375
3
Iteration 26900: Loss = -11008.98828125
Iteration 27000: Loss = -11008.9892578125
1
Iteration 27100: Loss = -11008.9892578125
2
Iteration 27200: Loss = -11008.98828125
Iteration 27300: Loss = -11008.98828125
Iteration 27400: Loss = -11008.9892578125
1
Iteration 27500: Loss = -11008.9873046875
Iteration 27600: Loss = -11008.9873046875
Iteration 27700: Loss = -11008.9853515625
Iteration 27800: Loss = -11008.9853515625
Iteration 27900: Loss = -11008.9853515625
Iteration 28000: Loss = -11008.9853515625
Iteration 28100: Loss = -11008.9833984375
Iteration 28200: Loss = -11008.9833984375
Iteration 28300: Loss = -11008.984375
1
Iteration 28400: Loss = -11008.984375
2
Iteration 28500: Loss = -11008.9853515625
3
Iteration 28600: Loss = -11008.984375
4
Iteration 28700: Loss = -11008.984375
5
Iteration 28800: Loss = -11008.9853515625
6
Iteration 28900: Loss = -11008.9892578125
7
Iteration 29000: Loss = -11008.986328125
8
Iteration 29100: Loss = -11008.9853515625
9
Iteration 29200: Loss = -11008.9873046875
10
Iteration 29300: Loss = -11008.9833984375
Iteration 29400: Loss = -11008.96484375
Iteration 29500: Loss = -11008.9638671875
Iteration 29600: Loss = -11008.96484375
1
Iteration 29700: Loss = -11008.96484375
2
Iteration 29800: Loss = -11008.9638671875
Iteration 29900: Loss = -11008.9638671875
pi: tensor([[9.7188e-01, 2.8121e-02],
        [9.9999e-01, 6.9720e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9041, 0.0959], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.2113],
         [0.9635, 0.3892]],

        [[0.9120, 0.2313],
         [0.0541, 0.0745]],

        [[0.2020, 0.2376],
         [0.4023, 0.1823]],

        [[0.0263, 0.0763],
         [0.4591, 0.5744]],

        [[0.0469, 0.2556],
         [0.0528, 0.9135]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.01737152632365585
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0012944666934790188
Average Adjusted Rand Index: -0.006321829167412319
[-0.0010057635473506878, -0.0012944666934790188] [-0.006947483394617005, -0.006321829167412319] [11007.814453125, 11008.962890625]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11049.108356141836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44942.7578125
Iteration 100: Loss = -28991.82421875
Iteration 200: Loss = -16015.9931640625
Iteration 300: Loss = -12524.369140625
Iteration 400: Loss = -11825.3388671875
Iteration 500: Loss = -11560.9599609375
Iteration 600: Loss = -11429.1533203125
Iteration 700: Loss = -11338.28125
Iteration 800: Loss = -11283.34375
Iteration 900: Loss = -11249.365234375
Iteration 1000: Loss = -11223.537109375
Iteration 1100: Loss = -11200.044921875
Iteration 1200: Loss = -11182.8935546875
Iteration 1300: Loss = -11167.4716796875
Iteration 1400: Loss = -11152.4267578125
Iteration 1500: Loss = -11142.5693359375
Iteration 1600: Loss = -11135.0751953125
Iteration 1700: Loss = -11124.0234375
Iteration 1800: Loss = -11118.43359375
Iteration 1900: Loss = -11114.0107421875
Iteration 2000: Loss = -11110.16796875
Iteration 2100: Loss = -11106.140625
Iteration 2200: Loss = -11103.2314453125
Iteration 2300: Loss = -11100.9931640625
Iteration 2400: Loss = -11099.2578125
Iteration 2500: Loss = -11097.771484375
Iteration 2600: Loss = -11096.4580078125
Iteration 2700: Loss = -11095.3095703125
Iteration 2800: Loss = -11094.29296875
Iteration 2900: Loss = -11093.380859375
Iteration 3000: Loss = -11092.5595703125
Iteration 3100: Loss = -11091.8134765625
Iteration 3200: Loss = -11091.1357421875
Iteration 3300: Loss = -11090.5107421875
Iteration 3400: Loss = -11089.9375
Iteration 3500: Loss = -11089.4013671875
Iteration 3600: Loss = -11088.8974609375
Iteration 3700: Loss = -11088.4228515625
Iteration 3800: Loss = -11087.9755859375
Iteration 3900: Loss = -11087.5732421875
Iteration 4000: Loss = -11087.2060546875
Iteration 4100: Loss = -11086.8720703125
Iteration 4200: Loss = -11086.5615234375
Iteration 4300: Loss = -11086.2744140625
Iteration 4400: Loss = -11086.01171875
Iteration 4500: Loss = -11085.765625
Iteration 4600: Loss = -11085.537109375
Iteration 4700: Loss = -11085.326171875
Iteration 4800: Loss = -11085.1298828125
Iteration 4900: Loss = -11084.943359375
Iteration 5000: Loss = -11084.7724609375
Iteration 5100: Loss = -11084.611328125
Iteration 5200: Loss = -11084.4599609375
Iteration 5300: Loss = -11084.3173828125
Iteration 5400: Loss = -11084.1845703125
Iteration 5500: Loss = -11084.0595703125
Iteration 5600: Loss = -11083.9404296875
Iteration 5700: Loss = -11083.830078125
Iteration 5800: Loss = -11083.7255859375
Iteration 5900: Loss = -11083.625
Iteration 6000: Loss = -11083.53125
Iteration 6100: Loss = -11083.4423828125
Iteration 6200: Loss = -11083.095703125
Iteration 6300: Loss = -11080.4912109375
Iteration 6400: Loss = -11080.31640625
Iteration 6500: Loss = -11080.2060546875
Iteration 6600: Loss = -11080.1123046875
Iteration 6700: Loss = -11080.0322265625
Iteration 6800: Loss = -11079.9580078125
Iteration 6900: Loss = -11079.8916015625
Iteration 7000: Loss = -11079.8291015625
Iteration 7100: Loss = -11079.7705078125
Iteration 7200: Loss = -11079.7177734375
Iteration 7300: Loss = -11079.6650390625
Iteration 7400: Loss = -11079.6181640625
Iteration 7500: Loss = -11079.576171875
Iteration 7600: Loss = -11079.5341796875
Iteration 7700: Loss = -11079.4951171875
Iteration 7800: Loss = -11079.4580078125
Iteration 7900: Loss = -11079.42578125
Iteration 8000: Loss = -11079.3935546875
Iteration 8100: Loss = -11079.3623046875
Iteration 8200: Loss = -11079.333984375
Iteration 8300: Loss = -11079.3056640625
Iteration 8400: Loss = -11079.2802734375
Iteration 8500: Loss = -11079.2587890625
Iteration 8600: Loss = -11079.234375
Iteration 8700: Loss = -11079.212890625
Iteration 8800: Loss = -11079.19140625
Iteration 8900: Loss = -11079.171875
Iteration 9000: Loss = -11079.15234375
Iteration 9100: Loss = -11079.134765625
Iteration 9200: Loss = -11079.1181640625
Iteration 9300: Loss = -11079.1025390625
Iteration 9400: Loss = -11079.0859375
Iteration 9500: Loss = -11079.0712890625
Iteration 9600: Loss = -11079.0595703125
Iteration 9700: Loss = -11079.0458984375
Iteration 9800: Loss = -11079.03125
Iteration 9900: Loss = -11079.0166015625
Iteration 10000: Loss = -11074.060546875
Iteration 10100: Loss = -11073.900390625
Iteration 10200: Loss = -11073.849609375
Iteration 10300: Loss = -11073.8203125
Iteration 10400: Loss = -11073.7998046875
Iteration 10500: Loss = -11073.783203125
Iteration 10600: Loss = -11073.76953125
Iteration 10700: Loss = -11073.759765625
Iteration 10800: Loss = -11073.748046875
Iteration 10900: Loss = -11073.7392578125
Iteration 11000: Loss = -11073.7333984375
Iteration 11100: Loss = -11073.7236328125
Iteration 11200: Loss = -11073.716796875
Iteration 11300: Loss = -11073.7099609375
Iteration 11400: Loss = -11073.703125
Iteration 11500: Loss = -11073.69921875
Iteration 11600: Loss = -11073.693359375
Iteration 11700: Loss = -11073.6884765625
Iteration 11800: Loss = -11073.68359375
Iteration 11900: Loss = -11073.6787109375
Iteration 12000: Loss = -11073.6748046875
Iteration 12100: Loss = -11073.6708984375
Iteration 12200: Loss = -11073.6669921875
Iteration 12300: Loss = -11073.6640625
Iteration 12400: Loss = -11073.662109375
Iteration 12500: Loss = -11073.6572265625
Iteration 12600: Loss = -11073.65625
Iteration 12700: Loss = -11073.65234375
Iteration 12800: Loss = -11073.6484375
Iteration 12900: Loss = -11073.6474609375
Iteration 13000: Loss = -11073.64453125
Iteration 13100: Loss = -11073.642578125
Iteration 13200: Loss = -11073.640625
Iteration 13300: Loss = -11073.6376953125
Iteration 13400: Loss = -11073.63671875
Iteration 13500: Loss = -11073.6337890625
Iteration 13600: Loss = -11073.6328125
Iteration 13700: Loss = -11073.6328125
Iteration 13800: Loss = -11073.6279296875
Iteration 13900: Loss = -11073.6279296875
Iteration 14000: Loss = -11073.6279296875
Iteration 14100: Loss = -11073.625
Iteration 14200: Loss = -11073.623046875
Iteration 14300: Loss = -11073.6240234375
1
Iteration 14400: Loss = -11073.6220703125
Iteration 14500: Loss = -11073.6220703125
Iteration 14600: Loss = -11073.619140625
Iteration 14700: Loss = -11073.6171875
Iteration 14800: Loss = -11073.6171875
Iteration 14900: Loss = -11073.615234375
Iteration 15000: Loss = -11073.6142578125
Iteration 15100: Loss = -11073.615234375
1
Iteration 15200: Loss = -11073.6123046875
Iteration 15300: Loss = -11073.611328125
Iteration 15400: Loss = -11073.611328125
Iteration 15500: Loss = -11073.6103515625
Iteration 15600: Loss = -11073.6083984375
Iteration 15700: Loss = -11073.6103515625
1
Iteration 15800: Loss = -11073.6083984375
Iteration 15900: Loss = -11073.607421875
Iteration 16000: Loss = -11073.607421875
Iteration 16100: Loss = -11073.6064453125
Iteration 16200: Loss = -11073.560546875
Iteration 16300: Loss = -11073.546875
Iteration 16400: Loss = -11073.5439453125
Iteration 16500: Loss = -11073.4921875
Iteration 16600: Loss = -11073.490234375
Iteration 16700: Loss = -11073.4921875
1
Iteration 16800: Loss = -11073.4638671875
Iteration 16900: Loss = -11073.4599609375
Iteration 17000: Loss = -11073.4599609375
Iteration 17100: Loss = -11073.4521484375
Iteration 17200: Loss = -11073.4306640625
Iteration 17300: Loss = -11073.4140625
Iteration 17400: Loss = -11073.3974609375
Iteration 17500: Loss = -11073.3798828125
Iteration 17600: Loss = -11073.36328125
Iteration 17700: Loss = -11073.361328125
Iteration 17800: Loss = -11073.3515625
Iteration 17900: Loss = -11073.33984375
Iteration 18000: Loss = -11073.333984375
Iteration 18100: Loss = -11073.3115234375
Iteration 18200: Loss = -11073.3017578125
Iteration 18300: Loss = -11073.29296875
Iteration 18400: Loss = -11073.283203125
Iteration 18500: Loss = -11073.2802734375
Iteration 18600: Loss = -11073.271484375
Iteration 18700: Loss = -11073.2666015625
Iteration 18800: Loss = -11073.2626953125
Iteration 18900: Loss = -11073.2548828125
Iteration 19000: Loss = -11073.2490234375
Iteration 19100: Loss = -11073.24609375
Iteration 19200: Loss = -11073.2392578125
Iteration 19300: Loss = -11073.234375
Iteration 19400: Loss = -11073.23046875
Iteration 19500: Loss = -11073.2275390625
Iteration 19600: Loss = -11073.224609375
Iteration 19700: Loss = -11073.2216796875
Iteration 19800: Loss = -11073.21875
Iteration 19900: Loss = -11073.2197265625
1
Iteration 20000: Loss = -11073.2177734375
Iteration 20100: Loss = -11073.216796875
Iteration 20200: Loss = -11073.216796875
Iteration 20300: Loss = -11073.2138671875
Iteration 20400: Loss = -11073.212890625
Iteration 20500: Loss = -11073.212890625
Iteration 20600: Loss = -11073.2119140625
Iteration 20700: Loss = -11073.2099609375
Iteration 20800: Loss = -11073.208984375
Iteration 20900: Loss = -11073.2099609375
1
Iteration 21000: Loss = -11073.2099609375
2
Iteration 21100: Loss = -11073.208984375
Iteration 21200: Loss = -11073.208984375
Iteration 21300: Loss = -11073.2080078125
Iteration 21400: Loss = -11073.2109375
1
Iteration 21500: Loss = -11073.2099609375
2
Iteration 21600: Loss = -11073.208984375
3
Iteration 21700: Loss = -11073.208984375
4
Iteration 21800: Loss = -11073.208984375
5
Iteration 21900: Loss = -11073.208984375
6
Iteration 22000: Loss = -11073.2099609375
7
Iteration 22100: Loss = -11073.208984375
8
Iteration 22200: Loss = -11073.2080078125
Iteration 22300: Loss = -11073.20703125
Iteration 22400: Loss = -11073.2060546875
Iteration 22500: Loss = -11073.2041015625
Iteration 22600: Loss = -11073.203125
Iteration 22700: Loss = -11073.2001953125
Iteration 22800: Loss = -11073.203125
1
Iteration 22900: Loss = -11073.203125
2
Iteration 23000: Loss = -11073.201171875
3
Iteration 23100: Loss = -11073.1982421875
Iteration 23200: Loss = -11073.19921875
1
Iteration 23300: Loss = -11073.1982421875
Iteration 23400: Loss = -11073.1982421875
Iteration 23500: Loss = -11073.19921875
1
Iteration 23600: Loss = -11073.17578125
Iteration 23700: Loss = -11073.1435546875
Iteration 23800: Loss = -11073.130859375
Iteration 23900: Loss = -11073.1259765625
Iteration 24000: Loss = -11073.1025390625
Iteration 24100: Loss = -11073.06640625
Iteration 24200: Loss = -11073.0
Iteration 24300: Loss = -11072.966796875
Iteration 24400: Loss = -11072.876953125
Iteration 24500: Loss = -11072.8349609375
Iteration 24600: Loss = -11072.8095703125
Iteration 24700: Loss = -11072.796875
Iteration 24800: Loss = -11072.7783203125
Iteration 24900: Loss = -11072.76953125
Iteration 25000: Loss = -11072.7587890625
Iteration 25100: Loss = -11072.74609375
Iteration 25200: Loss = -11072.71875
Iteration 25300: Loss = -11072.697265625
Iteration 25400: Loss = -11072.6923828125
Iteration 25500: Loss = -11072.68359375
Iteration 25600: Loss = -11072.6826171875
Iteration 25700: Loss = -11072.671875
Iteration 25800: Loss = -11072.6640625
Iteration 25900: Loss = -11072.66015625
Iteration 26000: Loss = -11072.658203125
Iteration 26100: Loss = -11072.642578125
Iteration 26200: Loss = -11072.6376953125
Iteration 26300: Loss = -11072.6328125
Iteration 26400: Loss = -11072.630859375
Iteration 26500: Loss = -11072.625
Iteration 26600: Loss = -11072.6162109375
Iteration 26700: Loss = -11072.611328125
Iteration 26800: Loss = -11072.6015625
Iteration 26900: Loss = -11072.4931640625
Iteration 27000: Loss = -11072.326171875
Iteration 27100: Loss = -11072.275390625
Iteration 27200: Loss = -11072.2705078125
Iteration 27300: Loss = -11072.26953125
Iteration 27400: Loss = -11072.26953125
Iteration 27500: Loss = -11072.26953125
Iteration 27600: Loss = -11072.2685546875
Iteration 27700: Loss = -11072.2705078125
1
Iteration 27800: Loss = -11072.26953125
2
Iteration 27900: Loss = -11072.2705078125
3
Iteration 28000: Loss = -11072.26953125
4
Iteration 28100: Loss = -11072.26953125
5
Iteration 28200: Loss = -11072.26953125
6
Iteration 28300: Loss = -11072.2685546875
Iteration 28400: Loss = -11072.26953125
1
Iteration 28500: Loss = -11072.2685546875
Iteration 28600: Loss = -11072.267578125
Iteration 28700: Loss = -11072.26953125
1
Iteration 28800: Loss = -11072.2685546875
2
Iteration 28900: Loss = -11072.26953125
3
Iteration 29000: Loss = -11072.26953125
4
Iteration 29100: Loss = -11072.2685546875
5
Iteration 29200: Loss = -11072.2705078125
6
Iteration 29300: Loss = -11072.2705078125
7
Iteration 29400: Loss = -11072.26953125
8
Iteration 29500: Loss = -11072.26953125
9
Iteration 29600: Loss = -11072.26953125
10
Iteration 29700: Loss = -11072.26953125
11
Iteration 29800: Loss = -11072.2685546875
12
Iteration 29900: Loss = -11072.26953125
13
pi: tensor([[9.8373e-01, 1.6269e-02],
        [9.9999e-01, 5.0252e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 4.0153e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1651, 0.1710],
         [0.9133, 0.1991]],

        [[0.5944, 0.0704],
         [0.9646, 0.9930]],

        [[0.2385, 0.1901],
         [0.0645, 0.0300]],

        [[0.9526, 0.2258],
         [0.0225, 0.2066]],

        [[0.7092, 0.1083],
         [0.3899, 0.0320]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00012598601017541888
Average Adjusted Rand Index: 0.00033344555058593435
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36291.953125
Iteration 100: Loss = -22614.900390625
Iteration 200: Loss = -14117.0849609375
Iteration 300: Loss = -11991.330078125
Iteration 400: Loss = -11526.2587890625
Iteration 500: Loss = -11330.0966796875
Iteration 600: Loss = -11228.7158203125
Iteration 700: Loss = -11188.8310546875
Iteration 800: Loss = -11164.19921875
Iteration 900: Loss = -11147.126953125
Iteration 1000: Loss = -11134.6513671875
Iteration 1100: Loss = -11125.1953125
Iteration 1200: Loss = -11117.81640625
Iteration 1300: Loss = -11111.931640625
Iteration 1400: Loss = -11107.150390625
Iteration 1500: Loss = -11103.2060546875
Iteration 1600: Loss = -11099.90625
Iteration 1700: Loss = -11097.1171875
Iteration 1800: Loss = -11094.7333984375
Iteration 1900: Loss = -11092.68359375
Iteration 2000: Loss = -11090.9013671875
Iteration 2100: Loss = -11089.34765625
Iteration 2200: Loss = -11087.978515625
Iteration 2300: Loss = -11086.7685546875
Iteration 2400: Loss = -11085.6943359375
Iteration 2500: Loss = -11084.7353515625
Iteration 2600: Loss = -11083.8740234375
Iteration 2700: Loss = -11083.1005859375
Iteration 2800: Loss = -11082.40234375
Iteration 2900: Loss = -11081.76953125
Iteration 3000: Loss = -11081.193359375
Iteration 3100: Loss = -11080.6708984375
Iteration 3200: Loss = -11080.1923828125
Iteration 3300: Loss = -11079.7548828125
Iteration 3400: Loss = -11079.353515625
Iteration 3500: Loss = -11078.984375
Iteration 3600: Loss = -11078.64453125
Iteration 3700: Loss = -11078.330078125
Iteration 3800: Loss = -11078.0400390625
Iteration 3900: Loss = -11077.7724609375
Iteration 4000: Loss = -11077.5234375
Iteration 4100: Loss = -11077.29296875
Iteration 4200: Loss = -11077.0791015625
Iteration 4300: Loss = -11076.8798828125
Iteration 4400: Loss = -11076.6923828125
Iteration 4500: Loss = -11076.51953125
Iteration 4600: Loss = -11076.357421875
Iteration 4700: Loss = -11076.208984375
Iteration 4800: Loss = -11076.06640625
Iteration 4900: Loss = -11075.935546875
Iteration 5000: Loss = -11075.810546875
Iteration 5100: Loss = -11075.6943359375
Iteration 5200: Loss = -11075.5849609375
Iteration 5300: Loss = -11075.4833984375
Iteration 5400: Loss = -11075.3876953125
Iteration 5500: Loss = -11075.2978515625
Iteration 5600: Loss = -11075.2119140625
Iteration 5700: Loss = -11075.1328125
Iteration 5800: Loss = -11075.0576171875
Iteration 5900: Loss = -11074.9873046875
Iteration 6000: Loss = -11074.919921875
Iteration 6100: Loss = -11074.8564453125
Iteration 6200: Loss = -11074.7978515625
Iteration 6300: Loss = -11074.7431640625
Iteration 6400: Loss = -11074.6884765625
Iteration 6500: Loss = -11074.6396484375
Iteration 6600: Loss = -11074.5927734375
Iteration 6700: Loss = -11074.546875
Iteration 6800: Loss = -11074.5068359375
Iteration 6900: Loss = -11074.466796875
Iteration 7000: Loss = -11074.4296875
Iteration 7100: Loss = -11074.39453125
Iteration 7200: Loss = -11074.3603515625
Iteration 7300: Loss = -11074.330078125
Iteration 7400: Loss = -11074.2998046875
Iteration 7500: Loss = -11074.2724609375
Iteration 7600: Loss = -11074.2451171875
Iteration 7700: Loss = -11074.2216796875
Iteration 7800: Loss = -11074.1962890625
Iteration 7900: Loss = -11074.173828125
Iteration 8000: Loss = -11074.1533203125
Iteration 8100: Loss = -11074.1318359375
Iteration 8200: Loss = -11074.11328125
Iteration 8300: Loss = -11074.0966796875
Iteration 8400: Loss = -11074.0791015625
Iteration 8500: Loss = -11074.0634765625
Iteration 8600: Loss = -11074.048828125
Iteration 8700: Loss = -11074.0341796875
Iteration 8800: Loss = -11074.021484375
Iteration 8900: Loss = -11074.0078125
Iteration 9000: Loss = -11073.9951171875
Iteration 9100: Loss = -11073.9853515625
Iteration 9200: Loss = -11073.9755859375
Iteration 9300: Loss = -11073.9658203125
Iteration 9400: Loss = -11073.9560546875
Iteration 9500: Loss = -11073.947265625
Iteration 9600: Loss = -11073.9375
Iteration 9700: Loss = -11073.9296875
Iteration 9800: Loss = -11073.923828125
Iteration 9900: Loss = -11073.916015625
Iteration 10000: Loss = -11073.91015625
Iteration 10100: Loss = -11073.9033203125
Iteration 10200: Loss = -11073.8984375
Iteration 10300: Loss = -11073.892578125
Iteration 10400: Loss = -11073.8876953125
Iteration 10500: Loss = -11073.8818359375
Iteration 10600: Loss = -11073.8759765625
Iteration 10700: Loss = -11073.873046875
Iteration 10800: Loss = -11073.869140625
Iteration 10900: Loss = -11073.8642578125
Iteration 11000: Loss = -11073.861328125
Iteration 11100: Loss = -11073.857421875
Iteration 11200: Loss = -11073.85546875
Iteration 11300: Loss = -11073.8525390625
Iteration 11400: Loss = -11073.84765625
Iteration 11500: Loss = -11073.845703125
Iteration 11600: Loss = -11073.8427734375
Iteration 11700: Loss = -11073.83984375
Iteration 11800: Loss = -11073.8369140625
Iteration 11900: Loss = -11073.833984375
Iteration 12000: Loss = -11073.8310546875
Iteration 12100: Loss = -11073.8271484375
Iteration 12200: Loss = -11073.826171875
Iteration 12300: Loss = -11073.8232421875
Iteration 12400: Loss = -11073.8212890625
Iteration 12500: Loss = -11073.818359375
Iteration 12600: Loss = -11073.81640625
Iteration 12700: Loss = -11073.81640625
Iteration 12800: Loss = -11073.8154296875
Iteration 12900: Loss = -11073.814453125
Iteration 13000: Loss = -11073.8125
Iteration 13100: Loss = -11073.8125
Iteration 13200: Loss = -11073.810546875
Iteration 13300: Loss = -11073.8095703125
Iteration 13400: Loss = -11073.8095703125
Iteration 13500: Loss = -11073.80859375
Iteration 13600: Loss = -11073.80859375
Iteration 13700: Loss = -11073.8095703125
1
Iteration 13800: Loss = -11073.8076171875
Iteration 13900: Loss = -11073.806640625
Iteration 14000: Loss = -11073.8076171875
1
Iteration 14100: Loss = -11073.8046875
Iteration 14200: Loss = -11073.8037109375
Iteration 14300: Loss = -11073.8046875
1
Iteration 14400: Loss = -11073.8037109375
Iteration 14500: Loss = -11073.8037109375
Iteration 14600: Loss = -11073.8017578125
Iteration 14700: Loss = -11073.802734375
1
Iteration 14800: Loss = -11073.802734375
2
Iteration 14900: Loss = -11073.8017578125
Iteration 15000: Loss = -11073.798828125
Iteration 15100: Loss = -11073.8017578125
1
Iteration 15200: Loss = -11073.798828125
Iteration 15300: Loss = -11073.798828125
Iteration 15400: Loss = -11073.796875
Iteration 15500: Loss = -11073.7978515625
1
Iteration 15600: Loss = -11073.7958984375
Iteration 15700: Loss = -11073.7958984375
Iteration 15800: Loss = -11073.7939453125
Iteration 15900: Loss = -11073.79296875
Iteration 16000: Loss = -11073.794921875
1
Iteration 16100: Loss = -11073.79296875
Iteration 16200: Loss = -11073.7939453125
1
Iteration 16300: Loss = -11073.79296875
Iteration 16400: Loss = -11073.79296875
Iteration 16500: Loss = -11073.791015625
Iteration 16600: Loss = -11073.7919921875
1
Iteration 16700: Loss = -11073.7919921875
2
Iteration 16800: Loss = -11073.791015625
Iteration 16900: Loss = -11073.791015625
Iteration 17000: Loss = -11073.791015625
Iteration 17100: Loss = -11073.791015625
Iteration 17200: Loss = -11073.7890625
Iteration 17300: Loss = -11073.7900390625
1
Iteration 17400: Loss = -11073.787109375
Iteration 17500: Loss = -11073.765625
Iteration 17600: Loss = -11073.7626953125
Iteration 17700: Loss = -11073.7578125
Iteration 17800: Loss = -11073.7587890625
1
Iteration 17900: Loss = -11073.7548828125
Iteration 18000: Loss = -11073.7529296875
Iteration 18100: Loss = -11073.751953125
Iteration 18200: Loss = -11073.75390625
1
Iteration 18300: Loss = -11073.7509765625
Iteration 18400: Loss = -11073.7490234375
Iteration 18500: Loss = -11073.748046875
Iteration 18600: Loss = -11073.744140625
Iteration 18700: Loss = -11073.7216796875
Iteration 18800: Loss = -11073.7236328125
1
Iteration 18900: Loss = -11073.712890625
Iteration 19000: Loss = -11073.6953125
Iteration 19100: Loss = -11073.6845703125
Iteration 19200: Loss = -11073.67578125
Iteration 19300: Loss = -11073.6728515625
Iteration 19400: Loss = -11073.65234375
Iteration 19500: Loss = -11073.6376953125
Iteration 19600: Loss = -11073.630859375
Iteration 19700: Loss = -11073.6220703125
Iteration 19800: Loss = -11073.609375
Iteration 19900: Loss = -11073.603515625
Iteration 20000: Loss = -11073.59765625
Iteration 20100: Loss = -11073.5908203125
Iteration 20200: Loss = -11073.583984375
Iteration 20300: Loss = -11073.5634765625
Iteration 20400: Loss = -11073.5595703125
Iteration 20500: Loss = -11073.5517578125
Iteration 20600: Loss = -11073.5380859375
Iteration 20700: Loss = -11073.533203125
Iteration 20800: Loss = -11073.529296875
Iteration 20900: Loss = -11073.52734375
Iteration 21000: Loss = -11073.525390625
Iteration 21100: Loss = -11073.521484375
Iteration 21200: Loss = -11073.5185546875
Iteration 21300: Loss = -11073.517578125
Iteration 21400: Loss = -11073.5146484375
Iteration 21500: Loss = -11073.5087890625
Iteration 21600: Loss = -11073.5107421875
1
Iteration 21700: Loss = -11073.509765625
2
Iteration 21800: Loss = -11073.509765625
3
Iteration 21900: Loss = -11073.509765625
4
Iteration 22000: Loss = -11073.5078125
Iteration 22100: Loss = -11073.509765625
1
Iteration 22200: Loss = -11073.5107421875
2
Iteration 22300: Loss = -11073.509765625
3
Iteration 22400: Loss = -11073.5126953125
4
Iteration 22500: Loss = -11073.509765625
5
Iteration 22600: Loss = -11073.509765625
6
Iteration 22700: Loss = -11073.509765625
7
Iteration 22800: Loss = -11073.509765625
8
Iteration 22900: Loss = -11073.509765625
9
Iteration 23000: Loss = -11073.509765625
10
Iteration 23100: Loss = -11073.5087890625
11
Iteration 23200: Loss = -11073.509765625
12
Iteration 23300: Loss = -11073.509765625
13
Iteration 23400: Loss = -11073.5107421875
14
Iteration 23500: Loss = -11073.509765625
15
Stopping early at iteration 23500 due to no improvement.
pi: tensor([[7.4269e-04, 9.9926e-01],
        [6.8970e-03, 9.9310e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([7.7194e-05, 9.9992e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2817, 0.1525],
         [0.9011, 0.1643]],

        [[0.0172, 0.2826],
         [0.8503, 0.7626]],

        [[0.9825, 0.1638],
         [0.0509, 0.0570]],

        [[0.6857, 0.2395],
         [0.0098, 0.7160]],

        [[0.4681, 0.1282],
         [0.8910, 0.9785]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0001300831161973837
Average Adjusted Rand Index: -0.0003077958928485548
[-0.00012598601017541888, 0.0001300831161973837] [0.00033344555058593435, -0.0003077958928485548] [11072.2685546875, 11073.509765625]
-------------------------------------
This iteration is 17
True Objective function: Loss = -10769.744808082021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43244.8984375
Iteration 100: Loss = -23413.765625
Iteration 200: Loss = -13608.7431640625
Iteration 300: Loss = -11692.4140625
Iteration 400: Loss = -11279.8564453125
Iteration 500: Loss = -11090.611328125
Iteration 600: Loss = -10999.490234375
Iteration 700: Loss = -10951.0322265625
Iteration 800: Loss = -10919.1923828125
Iteration 900: Loss = -10893.791015625
Iteration 1000: Loss = -10876.8232421875
Iteration 1100: Loss = -10866.2275390625
Iteration 1200: Loss = -10858.263671875
Iteration 1300: Loss = -10852.005859375
Iteration 1400: Loss = -10846.98046875
Iteration 1500: Loss = -10842.9375
Iteration 1600: Loss = -10839.58984375
Iteration 1700: Loss = -10836.779296875
Iteration 1800: Loss = -10834.39453125
Iteration 1900: Loss = -10832.3505859375
Iteration 2000: Loss = -10830.58203125
Iteration 2100: Loss = -10829.041015625
Iteration 2200: Loss = -10827.6904296875
Iteration 2300: Loss = -10826.4990234375
Iteration 2400: Loss = -10825.4404296875
Iteration 2500: Loss = -10824.4951171875
Iteration 2600: Loss = -10823.650390625
Iteration 2700: Loss = -10822.8876953125
Iteration 2800: Loss = -10822.1982421875
Iteration 2900: Loss = -10821.576171875
Iteration 3000: Loss = -10821.0087890625
Iteration 3100: Loss = -10820.4912109375
Iteration 3200: Loss = -10820.017578125
Iteration 3300: Loss = -10819.58203125
Iteration 3400: Loss = -10819.1826171875
Iteration 3500: Loss = -10818.8154296875
Iteration 3600: Loss = -10818.4765625
Iteration 3700: Loss = -10818.1630859375
Iteration 3800: Loss = -10817.8759765625
Iteration 3900: Loss = -10817.60546875
Iteration 4000: Loss = -10817.3544921875
Iteration 4100: Loss = -10817.123046875
Iteration 4200: Loss = -10816.9052734375
Iteration 4300: Loss = -10816.705078125
Iteration 4400: Loss = -10816.5146484375
Iteration 4500: Loss = -10816.33984375
Iteration 4600: Loss = -10816.17578125
Iteration 4700: Loss = -10816.0224609375
Iteration 4800: Loss = -10815.8759765625
Iteration 4900: Loss = -10815.7412109375
Iteration 5000: Loss = -10815.615234375
Iteration 5100: Loss = -10815.49609375
Iteration 5200: Loss = -10815.3857421875
Iteration 5300: Loss = -10815.28125
Iteration 5400: Loss = -10815.1845703125
Iteration 5500: Loss = -10815.09375
Iteration 5600: Loss = -10815.009765625
Iteration 5700: Loss = -10814.9306640625
Iteration 5800: Loss = -10814.85546875
Iteration 5900: Loss = -10814.78515625
Iteration 6000: Loss = -10814.7197265625
Iteration 6100: Loss = -10814.658203125
Iteration 6200: Loss = -10814.599609375
Iteration 6300: Loss = -10814.5458984375
Iteration 6400: Loss = -10814.494140625
Iteration 6500: Loss = -10814.4482421875
Iteration 6600: Loss = -10814.4013671875
Iteration 6700: Loss = -10814.357421875
Iteration 6800: Loss = -10814.3173828125
Iteration 6900: Loss = -10814.279296875
Iteration 7000: Loss = -10814.2431640625
Iteration 7100: Loss = -10814.2099609375
Iteration 7200: Loss = -10814.1787109375
Iteration 7300: Loss = -10814.146484375
Iteration 7400: Loss = -10814.119140625
Iteration 7500: Loss = -10814.091796875
Iteration 7600: Loss = -10814.0654296875
Iteration 7700: Loss = -10814.0419921875
Iteration 7800: Loss = -10814.0185546875
Iteration 7900: Loss = -10813.9970703125
Iteration 8000: Loss = -10813.9755859375
Iteration 8100: Loss = -10813.955078125
Iteration 8200: Loss = -10813.9365234375
Iteration 8300: Loss = -10813.9189453125
Iteration 8400: Loss = -10813.9033203125
Iteration 8500: Loss = -10813.8857421875
Iteration 8600: Loss = -10813.8720703125
Iteration 8700: Loss = -10813.8564453125
Iteration 8800: Loss = -10813.84375
Iteration 8900: Loss = -10813.83203125
Iteration 9000: Loss = -10813.8173828125
Iteration 9100: Loss = -10813.8076171875
Iteration 9200: Loss = -10813.796875
Iteration 9300: Loss = -10813.78515625
Iteration 9400: Loss = -10813.775390625
Iteration 9500: Loss = -10813.765625
Iteration 9600: Loss = -10813.7568359375
Iteration 9700: Loss = -10813.7470703125
Iteration 9800: Loss = -10813.7392578125
Iteration 9900: Loss = -10813.7314453125
Iteration 10000: Loss = -10813.7236328125
Iteration 10100: Loss = -10813.7158203125
Iteration 10200: Loss = -10813.7099609375
Iteration 10300: Loss = -10813.703125
Iteration 10400: Loss = -10813.697265625
Iteration 10500: Loss = -10813.69140625
Iteration 10600: Loss = -10813.6865234375
Iteration 10700: Loss = -10813.6796875
Iteration 10800: Loss = -10813.6748046875
Iteration 10900: Loss = -10813.669921875
Iteration 11000: Loss = -10813.6630859375
Iteration 11100: Loss = -10813.6591796875
Iteration 11200: Loss = -10813.654296875
Iteration 11300: Loss = -10813.650390625
Iteration 11400: Loss = -10813.6416015625
Iteration 11500: Loss = -10813.634765625
Iteration 11600: Loss = -10813.62890625
Iteration 11700: Loss = -10813.6220703125
Iteration 11800: Loss = -10813.6171875
Iteration 11900: Loss = -10813.61328125
Iteration 12000: Loss = -10813.607421875
Iteration 12100: Loss = -10813.6025390625
Iteration 12200: Loss = -10813.599609375
Iteration 12300: Loss = -10813.5966796875
Iteration 12400: Loss = -10813.5927734375
Iteration 12500: Loss = -10813.5908203125
Iteration 12600: Loss = -10813.5869140625
Iteration 12700: Loss = -10813.5869140625
Iteration 12800: Loss = -10813.5810546875
Iteration 12900: Loss = -10813.578125
Iteration 13000: Loss = -10813.5771484375
Iteration 13100: Loss = -10813.5732421875
Iteration 13200: Loss = -10813.5712890625
Iteration 13300: Loss = -10813.56640625
Iteration 13400: Loss = -10813.5634765625
Iteration 13500: Loss = -10813.5595703125
Iteration 13600: Loss = -10813.556640625
Iteration 13700: Loss = -10813.552734375
Iteration 13800: Loss = -10813.5498046875
Iteration 13900: Loss = -10813.546875
Iteration 14000: Loss = -10813.5419921875
Iteration 14100: Loss = -10813.5390625
Iteration 14200: Loss = -10813.533203125
Iteration 14300: Loss = -10813.5244140625
Iteration 14400: Loss = -10813.51171875
Iteration 14500: Loss = -10813.4873046875
Iteration 14600: Loss = -10813.44921875
Iteration 14700: Loss = -10813.4228515625
Iteration 14800: Loss = -10813.404296875
Iteration 14900: Loss = -10813.3896484375
Iteration 15000: Loss = -10813.375
Iteration 15100: Loss = -10813.361328125
Iteration 15200: Loss = -10813.3525390625
Iteration 15300: Loss = -10813.3447265625
Iteration 15400: Loss = -10813.3359375
Iteration 15500: Loss = -10813.3291015625
Iteration 15600: Loss = -10813.3212890625
Iteration 15700: Loss = -10813.3125
Iteration 15800: Loss = -10813.3046875
Iteration 15900: Loss = -10813.2939453125
Iteration 16000: Loss = -10813.283203125
Iteration 16100: Loss = -10813.271484375
Iteration 16200: Loss = -10813.2587890625
Iteration 16300: Loss = -10813.2470703125
Iteration 16400: Loss = -10813.2333984375
Iteration 16500: Loss = -10813.220703125
Iteration 16600: Loss = -10813.205078125
Iteration 16700: Loss = -10813.19140625
Iteration 16800: Loss = -10812.953125
Iteration 16900: Loss = -10812.927734375
Iteration 17000: Loss = -10812.9169921875
Iteration 17100: Loss = -10812.9091796875
Iteration 17200: Loss = -10812.89453125
Iteration 17300: Loss = -10812.8828125
Iteration 17400: Loss = -10812.875
Iteration 17500: Loss = -10812.8515625
Iteration 17600: Loss = -10812.837890625
Iteration 17700: Loss = -10812.8330078125
Iteration 17800: Loss = -10812.8251953125
Iteration 17900: Loss = -10812.8212890625
Iteration 18000: Loss = -10812.81640625
Iteration 18100: Loss = -10812.80859375
Iteration 18200: Loss = -10812.798828125
Iteration 18300: Loss = -10812.7333984375
Iteration 18400: Loss = -10812.716796875
Iteration 18500: Loss = -10812.517578125
Iteration 18600: Loss = -10812.451171875
Iteration 18700: Loss = -10812.44921875
Iteration 18800: Loss = -10812.443359375
Iteration 18900: Loss = -10812.287109375
Iteration 19000: Loss = -10812.2822265625
Iteration 19100: Loss = -10812.279296875
Iteration 19200: Loss = -10812.2841796875
1
Iteration 19300: Loss = -10812.279296875
Iteration 19400: Loss = -10812.2783203125
Iteration 19500: Loss = -10812.27734375
Iteration 19600: Loss = -10812.27734375
Iteration 19700: Loss = -10812.279296875
1
Iteration 19800: Loss = -10812.2763671875
Iteration 19900: Loss = -10812.275390625
Iteration 20000: Loss = -10812.275390625
Iteration 20100: Loss = -10812.2744140625
Iteration 20200: Loss = -10812.2744140625
Iteration 20300: Loss = -10812.2734375
Iteration 20400: Loss = -10812.25390625
Iteration 20500: Loss = -10812.2529296875
Iteration 20600: Loss = -10812.244140625
Iteration 20700: Loss = -10812.23828125
Iteration 20800: Loss = -10812.23046875
Iteration 20900: Loss = -10812.2314453125
1
Iteration 21000: Loss = -10812.2314453125
2
Iteration 21100: Loss = -10812.2314453125
3
Iteration 21200: Loss = -10812.2275390625
Iteration 21300: Loss = -10812.2275390625
Iteration 21400: Loss = -10812.2275390625
Iteration 21500: Loss = -10812.2255859375
Iteration 21600: Loss = -10812.2265625
1
Iteration 21700: Loss = -10812.2255859375
Iteration 21800: Loss = -10812.2265625
1
Iteration 21900: Loss = -10812.2275390625
2
Iteration 22000: Loss = -10812.2275390625
3
Iteration 22100: Loss = -10812.2265625
4
Iteration 22200: Loss = -10812.2275390625
5
Iteration 22300: Loss = -10812.2275390625
6
Iteration 22400: Loss = -10812.2255859375
Iteration 22500: Loss = -10812.2265625
1
Iteration 22600: Loss = -10812.228515625
2
Iteration 22700: Loss = -10812.228515625
3
Iteration 22800: Loss = -10812.2255859375
Iteration 22900: Loss = -10812.2275390625
1
Iteration 23000: Loss = -10812.2275390625
2
Iteration 23100: Loss = -10812.2275390625
3
Iteration 23200: Loss = -10812.2265625
4
Iteration 23300: Loss = -10812.2265625
5
Iteration 23400: Loss = -10812.2255859375
Iteration 23500: Loss = -10812.2275390625
1
Iteration 23600: Loss = -10812.228515625
2
Iteration 23700: Loss = -10812.2265625
3
Iteration 23800: Loss = -10812.2265625
4
Iteration 23900: Loss = -10812.2265625
5
Iteration 24000: Loss = -10812.2265625
6
Iteration 24100: Loss = -10812.2265625
7
Iteration 24200: Loss = -10812.2265625
8
Iteration 24300: Loss = -10812.2275390625
9
Iteration 24400: Loss = -10812.2275390625
10
Iteration 24500: Loss = -10812.2275390625
11
Iteration 24600: Loss = -10812.2265625
12
Iteration 24700: Loss = -10812.2275390625
13
Iteration 24800: Loss = -10812.2265625
14
Iteration 24900: Loss = -10812.2275390625
15
Stopping early at iteration 24900 due to no improvement.
pi: tensor([[0.9861, 0.0139],
        [0.1455, 0.8545]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 5.3430e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1574, 0.1648],
         [0.3603, 0.2753]],

        [[0.6999, 0.1449],
         [0.9618, 0.2697]],

        [[0.9162, 0.0976],
         [0.9592, 0.0699]],

        [[0.9808, 0.2044],
         [0.9812, 0.4435]],

        [[0.1858, 0.2135],
         [0.7198, 0.9477]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: -0.007201490658206174
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0006662945754822943
Average Adjusted Rand Index: -0.0020889240667239083
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34192.84765625
Iteration 100: Loss = -23528.89453125
Iteration 200: Loss = -14124.666015625
Iteration 300: Loss = -11554.5107421875
Iteration 400: Loss = -11238.3720703125
Iteration 500: Loss = -11116.9619140625
Iteration 600: Loss = -11045.8212890625
Iteration 700: Loss = -10994.154296875
Iteration 800: Loss = -10954.88671875
Iteration 900: Loss = -10930.2744140625
Iteration 1000: Loss = -10914.0302734375
Iteration 1100: Loss = -10903.5078125
Iteration 1200: Loss = -10895.421875
Iteration 1300: Loss = -10889.328125
Iteration 1400: Loss = -10882.7177734375
Iteration 1500: Loss = -10878.2724609375
Iteration 1600: Loss = -10873.7626953125
Iteration 1700: Loss = -10869.9873046875
Iteration 1800: Loss = -10865.8134765625
Iteration 1900: Loss = -10861.4638671875
Iteration 2000: Loss = -10857.98828125
Iteration 2100: Loss = -10854.4951171875
Iteration 2200: Loss = -10851.3388671875
Iteration 2300: Loss = -10848.693359375
Iteration 2400: Loss = -10845.3271484375
Iteration 2500: Loss = -10842.533203125
Iteration 2600: Loss = -10840.3759765625
Iteration 2700: Loss = -10838.4736328125
Iteration 2800: Loss = -10836.595703125
Iteration 2900: Loss = -10834.8642578125
Iteration 3000: Loss = -10833.4052734375
Iteration 3100: Loss = -10832.0556640625
Iteration 3200: Loss = -10830.7568359375
Iteration 3300: Loss = -10829.48046875
Iteration 3400: Loss = -10828.291015625
Iteration 3500: Loss = -10827.23046875
Iteration 3600: Loss = -10826.24609375
Iteration 3700: Loss = -10825.2919921875
Iteration 3800: Loss = -10824.0712890625
Iteration 3900: Loss = -10822.9443359375
Iteration 4000: Loss = -10822.177734375
Iteration 4100: Loss = -10821.490234375
Iteration 4200: Loss = -10820.89453125
Iteration 4300: Loss = -10820.3251953125
Iteration 4400: Loss = -10819.7490234375
Iteration 4500: Loss = -10819.296875
Iteration 4600: Loss = -10818.7626953125
Iteration 4700: Loss = -10818.1279296875
Iteration 4800: Loss = -10817.505859375
Iteration 4900: Loss = -10816.7392578125
Iteration 5000: Loss = -10816.1669921875
Iteration 5100: Loss = -10815.8291015625
Iteration 5200: Loss = -10815.58984375
Iteration 5300: Loss = -10815.4033203125
Iteration 5400: Loss = -10815.2529296875
Iteration 5500: Loss = -10815.125
Iteration 5600: Loss = -10815.0126953125
Iteration 5700: Loss = -10814.9140625
Iteration 5800: Loss = -10814.8251953125
Iteration 5900: Loss = -10814.7470703125
Iteration 6000: Loss = -10814.671875
Iteration 6100: Loss = -10814.6064453125
Iteration 6200: Loss = -10814.5439453125
Iteration 6300: Loss = -10814.4873046875
Iteration 6400: Loss = -10814.4345703125
Iteration 6500: Loss = -10814.384765625
Iteration 6600: Loss = -10814.3388671875
Iteration 6700: Loss = -10814.294921875
Iteration 6800: Loss = -10814.2548828125
Iteration 6900: Loss = -10814.2177734375
Iteration 7000: Loss = -10814.18359375
Iteration 7100: Loss = -10814.1494140625
Iteration 7200: Loss = -10814.1181640625
Iteration 7300: Loss = -10814.0888671875
Iteration 7400: Loss = -10814.0615234375
Iteration 7500: Loss = -10814.0341796875
Iteration 7600: Loss = -10814.009765625
Iteration 7700: Loss = -10813.986328125
Iteration 7800: Loss = -10813.9658203125
Iteration 7900: Loss = -10813.9453125
Iteration 8000: Loss = -10813.92578125
Iteration 8100: Loss = -10813.9072265625
Iteration 8200: Loss = -10813.8896484375
Iteration 8300: Loss = -10813.873046875
Iteration 8400: Loss = -10813.857421875
Iteration 8500: Loss = -10813.8427734375
Iteration 8600: Loss = -10813.8291015625
Iteration 8700: Loss = -10813.8154296875
Iteration 8800: Loss = -10813.8037109375
Iteration 8900: Loss = -10813.7919921875
Iteration 9000: Loss = -10813.7802734375
Iteration 9100: Loss = -10813.7705078125
Iteration 9200: Loss = -10813.7607421875
Iteration 9300: Loss = -10813.7509765625
Iteration 9400: Loss = -10813.7412109375
Iteration 9500: Loss = -10813.734375
Iteration 9600: Loss = -10813.724609375
Iteration 9700: Loss = -10813.71875
Iteration 9800: Loss = -10813.7109375
Iteration 9900: Loss = -10813.7041015625
Iteration 10000: Loss = -10813.6962890625
Iteration 10100: Loss = -10813.689453125
Iteration 10200: Loss = -10813.6845703125
Iteration 10300: Loss = -10813.6796875
Iteration 10400: Loss = -10813.673828125
Iteration 10500: Loss = -10813.6689453125
Iteration 10600: Loss = -10813.666015625
Iteration 10700: Loss = -10813.6591796875
Iteration 10800: Loss = -10813.6552734375
Iteration 10900: Loss = -10813.65234375
Iteration 11000: Loss = -10813.6474609375
Iteration 11100: Loss = -10813.6455078125
Iteration 11200: Loss = -10813.640625
Iteration 11300: Loss = -10813.638671875
Iteration 11400: Loss = -10813.634765625
Iteration 11500: Loss = -10813.6318359375
Iteration 11600: Loss = -10813.6298828125
Iteration 11700: Loss = -10813.626953125
Iteration 11800: Loss = -10813.6240234375
Iteration 11900: Loss = -10813.62109375
Iteration 12000: Loss = -10813.6201171875
Iteration 12100: Loss = -10813.6162109375
Iteration 12200: Loss = -10813.6142578125
Iteration 12300: Loss = -10813.6123046875
Iteration 12400: Loss = -10813.611328125
Iteration 12500: Loss = -10813.6083984375
Iteration 12600: Loss = -10813.6083984375
Iteration 12700: Loss = -10813.60546875
Iteration 12800: Loss = -10813.6044921875
Iteration 12900: Loss = -10813.6015625
Iteration 13000: Loss = -10813.599609375
Iteration 13100: Loss = -10813.599609375
Iteration 13200: Loss = -10813.599609375
Iteration 13300: Loss = -10813.5966796875
Iteration 13400: Loss = -10813.5966796875
Iteration 13500: Loss = -10813.5947265625
Iteration 13600: Loss = -10813.5927734375
Iteration 13700: Loss = -10813.5908203125
Iteration 13800: Loss = -10813.5908203125
Iteration 13900: Loss = -10813.5908203125
Iteration 14000: Loss = -10813.587890625
Iteration 14100: Loss = -10813.5869140625
Iteration 14200: Loss = -10813.5869140625
Iteration 14300: Loss = -10813.5849609375
Iteration 14400: Loss = -10813.5830078125
Iteration 14500: Loss = -10813.58203125
Iteration 14600: Loss = -10813.58203125
Iteration 14700: Loss = -10813.5810546875
Iteration 14800: Loss = -10813.5791015625
Iteration 14900: Loss = -10813.5771484375
Iteration 15000: Loss = -10813.5771484375
Iteration 15100: Loss = -10813.5751953125
Iteration 15200: Loss = -10813.5751953125
Iteration 15300: Loss = -10813.5732421875
Iteration 15400: Loss = -10813.572265625
Iteration 15500: Loss = -10813.5712890625
Iteration 15600: Loss = -10813.5712890625
Iteration 15700: Loss = -10813.5693359375
Iteration 15800: Loss = -10813.5673828125
Iteration 15900: Loss = -10813.5693359375
1
Iteration 16000: Loss = -10813.56640625
Iteration 16100: Loss = -10813.5654296875
Iteration 16200: Loss = -10813.564453125
Iteration 16300: Loss = -10813.564453125
Iteration 16400: Loss = -10813.5625
Iteration 16500: Loss = -10813.5625
Iteration 16600: Loss = -10813.5625
Iteration 16700: Loss = -10813.5625
Iteration 16800: Loss = -10813.5595703125
Iteration 16900: Loss = -10813.5615234375
1
Iteration 17000: Loss = -10813.560546875
2
Iteration 17100: Loss = -10813.5576171875
Iteration 17200: Loss = -10813.5576171875
Iteration 17300: Loss = -10813.5576171875
Iteration 17400: Loss = -10813.55859375
1
Iteration 17500: Loss = -10813.55859375
2
Iteration 17600: Loss = -10813.5576171875
Iteration 17700: Loss = -10813.5576171875
Iteration 17800: Loss = -10813.5576171875
Iteration 17900: Loss = -10813.5556640625
Iteration 18000: Loss = -10813.5576171875
1
Iteration 18100: Loss = -10813.5576171875
2
Iteration 18200: Loss = -10813.5556640625
Iteration 18300: Loss = -10813.5537109375
Iteration 18400: Loss = -10813.5546875
1
Iteration 18500: Loss = -10813.5556640625
2
Iteration 18600: Loss = -10813.556640625
3
Iteration 18700: Loss = -10813.5546875
4
Iteration 18800: Loss = -10813.5546875
5
Iteration 18900: Loss = -10813.5537109375
Iteration 19000: Loss = -10813.5546875
1
Iteration 19100: Loss = -10813.552734375
Iteration 19200: Loss = -10813.5537109375
1
Iteration 19300: Loss = -10813.552734375
Iteration 19400: Loss = -10813.552734375
Iteration 19500: Loss = -10813.5537109375
1
Iteration 19600: Loss = -10813.5537109375
2
Iteration 19700: Loss = -10813.552734375
Iteration 19800: Loss = -10813.552734375
Iteration 19900: Loss = -10813.5537109375
1
Iteration 20000: Loss = -10813.5517578125
Iteration 20100: Loss = -10813.5537109375
1
Iteration 20200: Loss = -10813.5537109375
2
Iteration 20300: Loss = -10813.5517578125
Iteration 20400: Loss = -10813.552734375
1
Iteration 20500: Loss = -10813.5517578125
Iteration 20600: Loss = -10813.5517578125
Iteration 20700: Loss = -10813.5517578125
Iteration 20800: Loss = -10813.5537109375
1
Iteration 20900: Loss = -10813.552734375
2
Iteration 21000: Loss = -10813.5517578125
Iteration 21100: Loss = -10813.5537109375
1
Iteration 21200: Loss = -10813.5537109375
2
Iteration 21300: Loss = -10813.5537109375
3
Iteration 21400: Loss = -10813.5517578125
Iteration 21500: Loss = -10813.5537109375
1
Iteration 21600: Loss = -10813.552734375
2
Iteration 21700: Loss = -10813.552734375
3
Iteration 21800: Loss = -10813.552734375
4
Iteration 21900: Loss = -10813.5517578125
Iteration 22000: Loss = -10813.5537109375
1
Iteration 22100: Loss = -10813.5517578125
Iteration 22200: Loss = -10813.5546875
1
Iteration 22300: Loss = -10813.552734375
2
Iteration 22400: Loss = -10813.5517578125
Iteration 22500: Loss = -10813.5517578125
Iteration 22600: Loss = -10813.5517578125
Iteration 22700: Loss = -10813.552734375
1
Iteration 22800: Loss = -10813.5537109375
2
Iteration 22900: Loss = -10813.552734375
3
Iteration 23000: Loss = -10813.552734375
4
Iteration 23100: Loss = -10813.552734375
5
Iteration 23200: Loss = -10813.5537109375
6
Iteration 23300: Loss = -10813.5537109375
7
Iteration 23400: Loss = -10813.552734375
8
Iteration 23500: Loss = -10813.5546875
9
Iteration 23600: Loss = -10813.552734375
10
Iteration 23700: Loss = -10813.552734375
11
Iteration 23800: Loss = -10813.552734375
12
Iteration 23900: Loss = -10813.552734375
13
Iteration 24000: Loss = -10813.552734375
14
Iteration 24100: Loss = -10813.552734375
15
Stopping early at iteration 24100 due to no improvement.
pi: tensor([[9.9999e-01, 1.1268e-05],
        [1.0000e+00, 4.8608e-08]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0343, 0.9657], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1580, 0.1595],
         [0.9915, 0.1596]],

        [[0.8273, 0.6963],
         [0.0134, 0.9879]],

        [[0.0499, 0.6430],
         [0.0218, 0.0144]],

        [[0.0180, 0.1546],
         [0.0425, 0.0333]],

        [[0.9841, 0.2590],
         [0.9757, 0.8160]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002189086264713694
Average Adjusted Rand Index: 0.0
[-0.0006662945754822943, -0.002189086264713694] [-0.0020889240667239083, 0.0] [10812.2275390625, 10813.552734375]
-------------------------------------
This iteration is 18
True Objective function: Loss = -10966.357112769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41663.609375
Iteration 100: Loss = -24043.03125
Iteration 200: Loss = -13960.3037109375
Iteration 300: Loss = -12079.302734375
Iteration 400: Loss = -11662.57421875
Iteration 500: Loss = -11481.41796875
Iteration 600: Loss = -11354.3271484375
Iteration 700: Loss = -11296.8818359375
Iteration 800: Loss = -11236.373046875
Iteration 900: Loss = -11205.05859375
Iteration 1000: Loss = -11184.6650390625
Iteration 1100: Loss = -11167.0048828125
Iteration 1200: Loss = -11144.791015625
Iteration 1300: Loss = -11136.6611328125
Iteration 1400: Loss = -11130.79296875
Iteration 1500: Loss = -11126.267578125
Iteration 1600: Loss = -11122.5546875
Iteration 1700: Loss = -11119.583984375
Iteration 1800: Loss = -11117.1171875
Iteration 1900: Loss = -11115.02734375
Iteration 2000: Loss = -11113.2373046875
Iteration 2100: Loss = -11111.6875
Iteration 2200: Loss = -11110.3359375
Iteration 2300: Loss = -11109.1494140625
Iteration 2400: Loss = -11108.1044921875
Iteration 2500: Loss = -11107.173828125
Iteration 2600: Loss = -11106.3466796875
Iteration 2700: Loss = -11105.6064453125
Iteration 2800: Loss = -11104.939453125
Iteration 2900: Loss = -11104.337890625
Iteration 3000: Loss = -11103.796875
Iteration 3100: Loss = -11103.3037109375
Iteration 3200: Loss = -11102.8544921875
Iteration 3300: Loss = -11102.44921875
Iteration 3400: Loss = -11102.0771484375
Iteration 3500: Loss = -11101.7373046875
Iteration 3600: Loss = -11101.4248046875
Iteration 3700: Loss = -11101.1396484375
Iteration 3800: Loss = -11100.875
Iteration 3900: Loss = -11100.634765625
Iteration 4000: Loss = -11100.41015625
Iteration 4100: Loss = -11100.2001953125
Iteration 4200: Loss = -11100.0107421875
Iteration 4300: Loss = -11099.8310546875
Iteration 4400: Loss = -11099.6630859375
Iteration 4500: Loss = -11099.509765625
Iteration 4600: Loss = -11099.3642578125
Iteration 4700: Loss = -11099.228515625
Iteration 4800: Loss = -11099.1025390625
Iteration 4900: Loss = -11098.978515625
Iteration 5000: Loss = -11098.86328125
Iteration 5100: Loss = -11098.75390625
Iteration 5200: Loss = -11098.6142578125
Iteration 5300: Loss = -11091.685546875
Iteration 5400: Loss = -11091.05859375
Iteration 5500: Loss = -11090.7548828125
Iteration 5600: Loss = -11090.56640625
Iteration 5700: Loss = -11090.4296875
Iteration 5800: Loss = -11090.3173828125
Iteration 5900: Loss = -11090.216796875
Iteration 6000: Loss = -11090.1220703125
Iteration 6100: Loss = -11090.029296875
Iteration 6200: Loss = -11089.927734375
Iteration 6300: Loss = -11089.8173828125
Iteration 6400: Loss = -11089.7021484375
Iteration 6500: Loss = -11089.6064453125
Iteration 6600: Loss = -11089.5244140625
Iteration 6700: Loss = -11089.4443359375
Iteration 6800: Loss = -11089.357421875
Iteration 6900: Loss = -11089.279296875
Iteration 7000: Loss = -11089.205078125
Iteration 7100: Loss = -11089.1376953125
Iteration 7200: Loss = -11089.0771484375
Iteration 7300: Loss = -11089.0185546875
Iteration 7400: Loss = -11088.9658203125
Iteration 7500: Loss = -11088.919921875
Iteration 7600: Loss = -11088.8759765625
Iteration 7700: Loss = -11088.8359375
Iteration 7800: Loss = -11088.7998046875
Iteration 7900: Loss = -11088.7685546875
Iteration 8000: Loss = -11088.7373046875
Iteration 8100: Loss = -11088.708984375
Iteration 8200: Loss = -11088.6845703125
Iteration 8300: Loss = -11088.66015625
Iteration 8400: Loss = -11088.6376953125
Iteration 8500: Loss = -11088.6162109375
Iteration 8600: Loss = -11088.59765625
Iteration 8700: Loss = -11088.580078125
Iteration 8800: Loss = -11088.5634765625
Iteration 8900: Loss = -11088.541015625
Iteration 9000: Loss = -11085.775390625
Iteration 9100: Loss = -11085.580078125
Iteration 9200: Loss = -11085.5087890625
Iteration 9300: Loss = -11085.462890625
Iteration 9400: Loss = -11085.4306640625
Iteration 9500: Loss = -11085.4072265625
Iteration 9600: Loss = -11085.3876953125
Iteration 9700: Loss = -11085.3720703125
Iteration 9800: Loss = -11085.357421875
Iteration 9900: Loss = -11085.3447265625
Iteration 10000: Loss = -11085.333984375
Iteration 10100: Loss = -11085.3212890625
Iteration 10200: Loss = -11085.3134765625
Iteration 10300: Loss = -11085.306640625
Iteration 10400: Loss = -11085.2978515625
Iteration 10500: Loss = -11085.2890625
Iteration 10600: Loss = -11085.283203125
Iteration 10700: Loss = -11085.2763671875
Iteration 10800: Loss = -11085.271484375
Iteration 10900: Loss = -11085.2646484375
Iteration 11000: Loss = -11085.259765625
Iteration 11100: Loss = -11085.25390625
Iteration 11200: Loss = -11085.248046875
Iteration 11300: Loss = -11085.2451171875
Iteration 11400: Loss = -11085.240234375
Iteration 11500: Loss = -11085.236328125
Iteration 11600: Loss = -11085.2314453125
Iteration 11700: Loss = -11085.2294921875
Iteration 11800: Loss = -11085.2265625
Iteration 11900: Loss = -11085.2216796875
Iteration 12000: Loss = -11085.21875
Iteration 12100: Loss = -11085.2158203125
Iteration 12200: Loss = -11085.2119140625
Iteration 12300: Loss = -11085.2080078125
Iteration 12400: Loss = -11085.2060546875
Iteration 12500: Loss = -11085.203125
Iteration 12600: Loss = -11085.201171875
Iteration 12700: Loss = -11085.197265625
Iteration 12800: Loss = -11085.193359375
Iteration 12900: Loss = -11085.1904296875
Iteration 13000: Loss = -11085.1884765625
Iteration 13100: Loss = -11085.1865234375
Iteration 13200: Loss = -11085.18359375
Iteration 13300: Loss = -11085.1796875
Iteration 13400: Loss = -11085.177734375
Iteration 13500: Loss = -11085.1767578125
Iteration 13600: Loss = -11085.1748046875
Iteration 13700: Loss = -11085.171875
Iteration 13800: Loss = -11085.171875
Iteration 13900: Loss = -11085.169921875
Iteration 14000: Loss = -11085.16796875
Iteration 14100: Loss = -11085.166015625
Iteration 14200: Loss = -11085.1640625
Iteration 14300: Loss = -11085.162109375
Iteration 14400: Loss = -11085.1611328125
Iteration 14500: Loss = -11085.16015625
Iteration 14600: Loss = -11085.16015625
Iteration 14700: Loss = -11085.158203125
Iteration 14800: Loss = -11085.1572265625
Iteration 14900: Loss = -11085.1552734375
Iteration 15000: Loss = -11085.1552734375
Iteration 15100: Loss = -11085.154296875
Iteration 15200: Loss = -11085.15234375
Iteration 15300: Loss = -11085.1513671875
Iteration 15400: Loss = -11085.150390625
Iteration 15500: Loss = -11085.1474609375
Iteration 15600: Loss = -11085.146484375
Iteration 15700: Loss = -11085.14453125
Iteration 15800: Loss = -11085.142578125
Iteration 15900: Loss = -11085.140625
Iteration 16000: Loss = -11085.1376953125
Iteration 16100: Loss = -11085.134765625
Iteration 16200: Loss = -11085.1298828125
Iteration 16300: Loss = -11085.1220703125
Iteration 16400: Loss = -11085.111328125
Iteration 16500: Loss = -11084.2392578125
Iteration 16600: Loss = -11084.01171875
Iteration 16700: Loss = -11083.9150390625
Iteration 16800: Loss = -11083.880859375
Iteration 16900: Loss = -11083.861328125
Iteration 17000: Loss = -11083.8515625
Iteration 17100: Loss = -11083.845703125
Iteration 17200: Loss = -11083.84375
Iteration 17300: Loss = -11083.83984375
Iteration 17400: Loss = -11083.8359375
Iteration 17500: Loss = -11083.8359375
Iteration 17600: Loss = -11083.8349609375
Iteration 17700: Loss = -11083.8349609375
Iteration 17800: Loss = -11083.830078125
Iteration 17900: Loss = -11083.8291015625
Iteration 18000: Loss = -11083.8271484375
Iteration 18100: Loss = -11083.828125
1
Iteration 18200: Loss = -11083.82421875
Iteration 18300: Loss = -11083.826171875
1
Iteration 18400: Loss = -11083.826171875
2
Iteration 18500: Loss = -11083.8251953125
3
Iteration 18600: Loss = -11083.82421875
Iteration 18700: Loss = -11083.82421875
Iteration 18800: Loss = -11083.8251953125
1
Iteration 18900: Loss = -11083.8251953125
2
Iteration 19000: Loss = -11083.8232421875
Iteration 19100: Loss = -11083.82421875
1
Iteration 19200: Loss = -11083.822265625
Iteration 19300: Loss = -11083.822265625
Iteration 19400: Loss = -11083.82421875
1
Iteration 19500: Loss = -11083.8251953125
2
Iteration 19600: Loss = -11083.82421875
3
Iteration 19700: Loss = -11083.8232421875
4
Iteration 19800: Loss = -11083.8232421875
5
Iteration 19900: Loss = -11083.822265625
Iteration 20000: Loss = -11083.822265625
Iteration 20100: Loss = -11083.8212890625
Iteration 20200: Loss = -11083.8212890625
Iteration 20300: Loss = -11083.822265625
1
Iteration 20400: Loss = -11083.8212890625
Iteration 20500: Loss = -11083.8193359375
Iteration 20600: Loss = -11083.095703125
Iteration 20700: Loss = -11083.0712890625
Iteration 20800: Loss = -11082.7763671875
Iteration 20900: Loss = -11082.7763671875
Iteration 21000: Loss = -11082.77734375
1
Iteration 21100: Loss = -11082.775390625
Iteration 21200: Loss = -11082.775390625
Iteration 21300: Loss = -11082.7763671875
1
Iteration 21400: Loss = -11082.7763671875
2
Iteration 21500: Loss = -11082.7763671875
3
Iteration 21600: Loss = -11082.7724609375
Iteration 21700: Loss = -11082.771484375
Iteration 21800: Loss = -11082.7734375
1
Iteration 21900: Loss = -11082.771484375
Iteration 22000: Loss = -11082.7529296875
Iteration 22100: Loss = -11082.7294921875
Iteration 22200: Loss = -11082.7294921875
Iteration 22300: Loss = -11082.7294921875
Iteration 22400: Loss = -11082.728515625
Iteration 22500: Loss = -11082.7275390625
Iteration 22600: Loss = -11082.7275390625
Iteration 22700: Loss = -11082.7236328125
Iteration 22800: Loss = -11082.72265625
Iteration 22900: Loss = -11082.720703125
Iteration 23000: Loss = -11082.6494140625
Iteration 23100: Loss = -11082.63671875
Iteration 23200: Loss = -11082.6259765625
Iteration 23300: Loss = -11082.623046875
Iteration 23400: Loss = -11082.6201171875
Iteration 23500: Loss = -11082.6142578125
Iteration 23600: Loss = -11082.6123046875
Iteration 23700: Loss = -11082.611328125
Iteration 23800: Loss = -11082.6123046875
1
Iteration 23900: Loss = -11082.611328125
Iteration 24000: Loss = -11082.611328125
Iteration 24100: Loss = -11082.6103515625
Iteration 24200: Loss = -11082.6064453125
Iteration 24300: Loss = -11082.60546875
Iteration 24400: Loss = -11082.6044921875
Iteration 24500: Loss = -11082.603515625
Iteration 24600: Loss = -11082.60546875
1
Iteration 24700: Loss = -11082.6064453125
2
Iteration 24800: Loss = -11082.6044921875
3
Iteration 24900: Loss = -11082.60546875
4
Iteration 25000: Loss = -11082.6044921875
5
Iteration 25100: Loss = -11082.60546875
6
Iteration 25200: Loss = -11082.603515625
Iteration 25300: Loss = -11082.6064453125
1
Iteration 25400: Loss = -11082.6044921875
2
Iteration 25500: Loss = -11082.6044921875
3
Iteration 25600: Loss = -11082.603515625
Iteration 25700: Loss = -11082.60546875
1
Iteration 25800: Loss = -11082.603515625
Iteration 25900: Loss = -11082.6044921875
1
Iteration 26000: Loss = -11082.6044921875
2
Iteration 26100: Loss = -11082.603515625
Iteration 26200: Loss = -11082.6044921875
1
Iteration 26300: Loss = -11082.603515625
Iteration 26400: Loss = -11082.6044921875
1
Iteration 26500: Loss = -11082.6025390625
Iteration 26600: Loss = -11082.603515625
1
Iteration 26700: Loss = -11082.60546875
2
Iteration 26800: Loss = -11082.6044921875
3
Iteration 26900: Loss = -11082.603515625
4
Iteration 27000: Loss = -11082.5947265625
Iteration 27100: Loss = -11082.5947265625
Iteration 27200: Loss = -11082.595703125
1
Iteration 27300: Loss = -11082.595703125
2
Iteration 27400: Loss = -11082.595703125
3
Iteration 27500: Loss = -11082.59375
Iteration 27600: Loss = -11082.5947265625
1
Iteration 27700: Loss = -11082.59375
Iteration 27800: Loss = -11082.5927734375
Iteration 27900: Loss = -11082.59375
1
Iteration 28000: Loss = -11082.59375
2
Iteration 28100: Loss = -11082.5966796875
3
Iteration 28200: Loss = -11082.59375
4
Iteration 28300: Loss = -11082.5947265625
5
Iteration 28400: Loss = -11082.5947265625
6
Iteration 28500: Loss = -11082.59375
7
Iteration 28600: Loss = -11082.5927734375
Iteration 28700: Loss = -11082.59375
1
Iteration 28800: Loss = -11082.5947265625
2
Iteration 28900: Loss = -11082.5947265625
3
Iteration 29000: Loss = -11082.595703125
4
Iteration 29100: Loss = -11082.5947265625
5
Iteration 29200: Loss = -11082.5947265625
6
Iteration 29300: Loss = -11082.59375
7
Iteration 29400: Loss = -11082.595703125
8
Iteration 29500: Loss = -11082.59375
9
Iteration 29600: Loss = -11082.5947265625
10
Iteration 29700: Loss = -11082.59375
11
Iteration 29800: Loss = -11082.591796875
Iteration 29900: Loss = -11082.59375
1
pi: tensor([[9.7728e-01, 2.2724e-02],
        [9.9999e-01, 5.8984e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9642, 0.0358], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1610, 0.2190],
         [0.9428, 0.3831]],

        [[0.8772, 0.2630],
         [0.0885, 0.6554]],

        [[0.0278, 0.2606],
         [0.3496, 0.0505]],

        [[0.0375, 0.2380],
         [0.2457, 0.4357]],

        [[0.0096, 0.2260],
         [0.0366, 0.0635]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002101672322569199
Average Adjusted Rand Index: -0.0013534209428170464
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20561.6953125
Iteration 100: Loss = -14956.2861328125
Iteration 200: Loss = -12078.8076171875
Iteration 300: Loss = -11373.208984375
Iteration 400: Loss = -11265.03125
Iteration 500: Loss = -11223.2119140625
Iteration 600: Loss = -11191.0009765625
Iteration 700: Loss = -11165.3935546875
Iteration 800: Loss = -11142.0791015625
Iteration 900: Loss = -11127.1826171875
Iteration 1000: Loss = -11116.9482421875
Iteration 1100: Loss = -11110.5751953125
Iteration 1200: Loss = -11106.8671875
Iteration 1300: Loss = -11104.5029296875
Iteration 1400: Loss = -11102.8603515625
Iteration 1500: Loss = -11101.650390625
Iteration 1600: Loss = -11100.7236328125
Iteration 1700: Loss = -11099.9931640625
Iteration 1800: Loss = -11099.3984375
Iteration 1900: Loss = -11098.9052734375
Iteration 2000: Loss = -11098.4921875
Iteration 2100: Loss = -11098.142578125
Iteration 2200: Loss = -11097.837890625
Iteration 2300: Loss = -11097.5751953125
Iteration 2400: Loss = -11097.341796875
Iteration 2500: Loss = -11097.134765625
Iteration 2600: Loss = -11096.951171875
Iteration 2700: Loss = -11096.78515625
Iteration 2800: Loss = -11096.634765625
Iteration 2900: Loss = -11096.498046875
Iteration 3000: Loss = -11096.373046875
Iteration 3100: Loss = -11096.2587890625
Iteration 3200: Loss = -11096.15234375
Iteration 3300: Loss = -11096.05078125
Iteration 3400: Loss = -11095.94140625
Iteration 3500: Loss = -11095.5859375
Iteration 3600: Loss = -11086.7041015625
Iteration 3700: Loss = -11086.09375
Iteration 3800: Loss = -11085.8291015625
Iteration 3900: Loss = -11085.650390625
Iteration 4000: Loss = -11085.513671875
Iteration 4100: Loss = -11085.3984375
Iteration 4200: Loss = -11085.30078125
Iteration 4300: Loss = -11085.216796875
Iteration 4400: Loss = -11085.140625
Iteration 4500: Loss = -11085.0732421875
Iteration 4600: Loss = -11085.0126953125
Iteration 4700: Loss = -11084.9580078125
Iteration 4800: Loss = -11084.908203125
Iteration 4900: Loss = -11084.86328125
Iteration 5000: Loss = -11084.822265625
Iteration 5100: Loss = -11084.7861328125
Iteration 5200: Loss = -11084.7490234375
Iteration 5300: Loss = -11084.7177734375
Iteration 5400: Loss = -11084.6875
Iteration 5500: Loss = -11084.662109375
Iteration 5600: Loss = -11084.6357421875
Iteration 5700: Loss = -11084.6123046875
Iteration 5800: Loss = -11084.591796875
Iteration 5900: Loss = -11084.5712890625
Iteration 6000: Loss = -11084.552734375
Iteration 6100: Loss = -11084.5341796875
Iteration 6200: Loss = -11084.5185546875
Iteration 6300: Loss = -11084.5029296875
Iteration 6400: Loss = -11084.4892578125
Iteration 6500: Loss = -11084.4765625
Iteration 6600: Loss = -11084.462890625
Iteration 6700: Loss = -11084.451171875
Iteration 6800: Loss = -11084.439453125
Iteration 6900: Loss = -11084.4287109375
Iteration 7000: Loss = -11084.41796875
Iteration 7100: Loss = -11084.4111328125
Iteration 7200: Loss = -11084.40234375
Iteration 7300: Loss = -11084.3935546875
Iteration 7400: Loss = -11084.38671875
Iteration 7500: Loss = -11084.37890625
Iteration 7600: Loss = -11084.373046875
Iteration 7700: Loss = -11084.3662109375
Iteration 7800: Loss = -11084.3603515625
Iteration 7900: Loss = -11084.3544921875
Iteration 8000: Loss = -11084.349609375
Iteration 8100: Loss = -11084.34375
Iteration 8200: Loss = -11084.3388671875
Iteration 8300: Loss = -11084.3349609375
Iteration 8400: Loss = -11084.3310546875
Iteration 8500: Loss = -11084.3271484375
Iteration 8600: Loss = -11084.3232421875
Iteration 8700: Loss = -11084.3193359375
Iteration 8800: Loss = -11084.3154296875
Iteration 8900: Loss = -11084.3134765625
Iteration 9000: Loss = -11084.3095703125
Iteration 9100: Loss = -11084.3076171875
Iteration 9200: Loss = -11084.3046875
Iteration 9300: Loss = -11084.302734375
Iteration 9400: Loss = -11084.2998046875
Iteration 9500: Loss = -11084.296875
Iteration 9600: Loss = -11084.294921875
Iteration 9700: Loss = -11084.2919921875
Iteration 9800: Loss = -11084.291015625
Iteration 9900: Loss = -11084.2890625
Iteration 10000: Loss = -11084.2861328125
Iteration 10100: Loss = -11084.2861328125
Iteration 10200: Loss = -11084.2841796875
Iteration 10300: Loss = -11084.283203125
Iteration 10400: Loss = -11084.28125
Iteration 10500: Loss = -11084.2802734375
Iteration 10600: Loss = -11084.279296875
Iteration 10700: Loss = -11084.2763671875
Iteration 10800: Loss = -11084.27734375
1
Iteration 10900: Loss = -11084.2744140625
Iteration 11000: Loss = -11084.2744140625
Iteration 11100: Loss = -11084.2734375
Iteration 11200: Loss = -11084.2734375
Iteration 11300: Loss = -11084.271484375
Iteration 11400: Loss = -11084.2705078125
Iteration 11500: Loss = -11084.2705078125
Iteration 11600: Loss = -11084.267578125
Iteration 11700: Loss = -11084.2685546875
1
Iteration 11800: Loss = -11084.271484375
2
Iteration 11900: Loss = -11084.2666015625
Iteration 12000: Loss = -11084.265625
Iteration 12100: Loss = -11084.2646484375
Iteration 12200: Loss = -11084.265625
1
Iteration 12300: Loss = -11084.265625
2
Iteration 12400: Loss = -11084.2646484375
Iteration 12500: Loss = -11084.263671875
Iteration 12600: Loss = -11084.26171875
Iteration 12700: Loss = -11084.2626953125
1
Iteration 12800: Loss = -11084.26171875
Iteration 12900: Loss = -11084.26171875
Iteration 13000: Loss = -11084.26171875
Iteration 13100: Loss = -11084.259765625
Iteration 13200: Loss = -11084.259765625
Iteration 13300: Loss = -11084.2607421875
1
Iteration 13400: Loss = -11084.2607421875
2
Iteration 13500: Loss = -11084.2578125
Iteration 13600: Loss = -11084.259765625
1
Iteration 13700: Loss = -11084.2587890625
2
Iteration 13800: Loss = -11084.2568359375
Iteration 13900: Loss = -11084.2548828125
Iteration 14000: Loss = -11083.4619140625
Iteration 14100: Loss = -11083.2978515625
Iteration 14200: Loss = -11083.2421875
Iteration 14300: Loss = -11083.21875
Iteration 14400: Loss = -11083.1923828125
Iteration 14500: Loss = -11083.173828125
Iteration 14600: Loss = -11083.169921875
Iteration 14700: Loss = -11082.7861328125
Iteration 14800: Loss = -11082.7392578125
Iteration 14900: Loss = -11082.7060546875
Iteration 15000: Loss = -11082.7001953125
Iteration 15100: Loss = -11082.6904296875
Iteration 15200: Loss = -11082.6875
Iteration 15300: Loss = -11082.68359375
Iteration 15400: Loss = -11082.6708984375
Iteration 15500: Loss = -11082.66796875
Iteration 15600: Loss = -11082.666015625
Iteration 15700: Loss = -11082.6650390625
Iteration 15800: Loss = -11082.6494140625
Iteration 15900: Loss = -11082.640625
Iteration 16000: Loss = -11082.63671875
Iteration 16100: Loss = -11082.6318359375
Iteration 16200: Loss = -11082.6298828125
Iteration 16300: Loss = -11082.62890625
Iteration 16400: Loss = -11082.6259765625
Iteration 16500: Loss = -11082.6259765625
Iteration 16600: Loss = -11082.6259765625
Iteration 16700: Loss = -11082.625
Iteration 16800: Loss = -11082.62109375
Iteration 16900: Loss = -11082.619140625
Iteration 17000: Loss = -11082.6181640625
Iteration 17100: Loss = -11082.6171875
Iteration 17200: Loss = -11082.6181640625
1
Iteration 17300: Loss = -11082.6181640625
2
Iteration 17400: Loss = -11082.6171875
Iteration 17500: Loss = -11082.6171875
Iteration 17600: Loss = -11082.615234375
Iteration 17700: Loss = -11082.615234375
Iteration 17800: Loss = -11082.6142578125
Iteration 17900: Loss = -11082.615234375
1
Iteration 18000: Loss = -11082.6142578125
Iteration 18100: Loss = -11082.615234375
1
Iteration 18200: Loss = -11082.61328125
Iteration 18300: Loss = -11082.615234375
1
Iteration 18400: Loss = -11082.6142578125
2
Iteration 18500: Loss = -11082.61328125
Iteration 18600: Loss = -11082.61328125
Iteration 18700: Loss = -11082.615234375
1
Iteration 18800: Loss = -11082.6142578125
2
Iteration 18900: Loss = -11082.6142578125
3
Iteration 19000: Loss = -11082.6142578125
4
Iteration 19100: Loss = -11082.615234375
5
Iteration 19200: Loss = -11082.6142578125
6
Iteration 19300: Loss = -11082.6142578125
7
Iteration 19400: Loss = -11082.6142578125
8
Iteration 19500: Loss = -11082.6142578125
9
Iteration 19600: Loss = -11082.6142578125
10
Iteration 19700: Loss = -11082.6142578125
11
Iteration 19800: Loss = -11082.6142578125
12
Iteration 19900: Loss = -11082.6142578125
13
Iteration 20000: Loss = -11082.615234375
14
Iteration 20100: Loss = -11082.6142578125
15
Stopping early at iteration 20100 due to no improvement.
pi: tensor([[3.1400e-06, 1.0000e+00],
        [9.7956e-01, 2.0439e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.1325e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1604, 0.1627],
         [0.1149, 0.1674]],

        [[0.9932, 0.1530],
         [0.3869, 0.1149]],

        [[0.1982, 0.2294],
         [0.9516, 0.0067]],

        [[0.0671, 0.2497],
         [0.1217, 0.1271]],

        [[0.9902, 0.2056],
         [0.9725, 0.4010]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.0009280641332928472
Average Adjusted Rand Index: 0.0009738677314241814
[-0.0002101672322569199, -0.0009280641332928472] [-0.0013534209428170464, 0.0009738677314241814] [11082.595703125, 11082.6142578125]
-------------------------------------
This iteration is 19
True Objective function: Loss = -10951.685217245691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35118.71484375
Iteration 100: Loss = -21857.0390625
Iteration 200: Loss = -13496.251953125
Iteration 300: Loss = -11681.9375
Iteration 400: Loss = -11393.33203125
Iteration 500: Loss = -11273.0322265625
Iteration 600: Loss = -11207.173828125
Iteration 700: Loss = -11157.6904296875
Iteration 800: Loss = -11130.505859375
Iteration 900: Loss = -11101.25
Iteration 1000: Loss = -11087.44140625
Iteration 1100: Loss = -11072.33203125
Iteration 1200: Loss = -11064.611328125
Iteration 1300: Loss = -11058.8671875
Iteration 1400: Loss = -11053.6015625
Iteration 1500: Loss = -11048.6806640625
Iteration 1600: Loss = -11043.521484375
Iteration 1700: Loss = -11036.9794921875
Iteration 1800: Loss = -11031.19140625
Iteration 1900: Loss = -11027.5654296875
Iteration 2000: Loss = -11023.6337890625
Iteration 2100: Loss = -11020.5791015625
Iteration 2200: Loss = -11019.009765625
Iteration 2300: Loss = -11017.76953125
Iteration 2400: Loss = -11016.716796875
Iteration 2500: Loss = -11015.73828125
Iteration 2600: Loss = -11014.3037109375
Iteration 2700: Loss = -11012.2353515625
Iteration 2800: Loss = -11011.19921875
Iteration 2900: Loss = -11010.4619140625
Iteration 3000: Loss = -11009.85546875
Iteration 3100: Loss = -11009.3330078125
Iteration 3200: Loss = -11008.869140625
Iteration 3300: Loss = -11008.455078125
Iteration 3400: Loss = -11008.080078125
Iteration 3500: Loss = -11007.7392578125
Iteration 3600: Loss = -11007.427734375
Iteration 3700: Loss = -11007.1416015625
Iteration 3800: Loss = -11006.8759765625
Iteration 3900: Loss = -11006.630859375
Iteration 4000: Loss = -11006.4033203125
Iteration 4100: Loss = -11006.19140625
Iteration 4200: Loss = -11005.990234375
Iteration 4300: Loss = -11005.8046875
Iteration 4400: Loss = -11005.6259765625
Iteration 4500: Loss = -11005.45703125
Iteration 4600: Loss = -11005.2939453125
Iteration 4700: Loss = -11005.1396484375
Iteration 4800: Loss = -11004.9892578125
Iteration 4900: Loss = -11004.8466796875
Iteration 5000: Loss = -11004.7060546875
Iteration 5100: Loss = -11004.5712890625
Iteration 5200: Loss = -11004.443359375
Iteration 5300: Loss = -11004.31640625
Iteration 5400: Loss = -11004.1884765625
Iteration 5500: Loss = -11004.0615234375
Iteration 5600: Loss = -11003.8505859375
Iteration 5700: Loss = -11003.5595703125
Iteration 5800: Loss = -11003.451171875
Iteration 5900: Loss = -11003.3212890625
Iteration 6000: Loss = -10999.8330078125
Iteration 6100: Loss = -10999.734375
Iteration 6200: Loss = -10999.6611328125
Iteration 6300: Loss = -10999.595703125
Iteration 6400: Loss = -10999.5361328125
Iteration 6500: Loss = -10999.0517578125
Iteration 6600: Loss = -10995.5546875
Iteration 6700: Loss = -10995.4501953125
Iteration 6800: Loss = -10995.365234375
Iteration 6900: Loss = -10995.267578125
Iteration 7000: Loss = -10995.1279296875
Iteration 7100: Loss = -10994.9296875
Iteration 7200: Loss = -10994.70703125
Iteration 7300: Loss = -10994.5380859375
Iteration 7400: Loss = -10994.41796875
Iteration 7500: Loss = -10994.330078125
Iteration 7600: Loss = -10994.263671875
Iteration 7700: Loss = -10994.2080078125
Iteration 7800: Loss = -10994.1640625
Iteration 7900: Loss = -10994.125
Iteration 8000: Loss = -10994.0908203125
Iteration 8100: Loss = -10994.0615234375
Iteration 8200: Loss = -10994.03515625
Iteration 8300: Loss = -10994.009765625
Iteration 8400: Loss = -10993.9873046875
Iteration 8500: Loss = -10993.9677734375
Iteration 8600: Loss = -10993.9482421875
Iteration 8700: Loss = -10993.9296875
Iteration 8800: Loss = -10993.9150390625
Iteration 8900: Loss = -10993.900390625
Iteration 9000: Loss = -10993.8837890625
Iteration 9100: Loss = -10993.8720703125
Iteration 9200: Loss = -10993.857421875
Iteration 9300: Loss = -10993.84765625
Iteration 9400: Loss = -10993.8359375
Iteration 9500: Loss = -10993.8271484375
Iteration 9600: Loss = -10993.81640625
Iteration 9700: Loss = -10993.806640625
Iteration 9800: Loss = -10993.7978515625
Iteration 9900: Loss = -10993.7890625
Iteration 10000: Loss = -10993.783203125
Iteration 10100: Loss = -10993.775390625
Iteration 10200: Loss = -10993.7685546875
Iteration 10300: Loss = -10993.76171875
Iteration 10400: Loss = -10993.7578125
Iteration 10500: Loss = -10993.751953125
Iteration 10600: Loss = -10993.7470703125
Iteration 10700: Loss = -10993.7412109375
Iteration 10800: Loss = -10993.7373046875
Iteration 10900: Loss = -10993.732421875
Iteration 11000: Loss = -10993.7294921875
Iteration 11100: Loss = -10993.7236328125
Iteration 11200: Loss = -10993.720703125
Iteration 11300: Loss = -10993.7177734375
Iteration 11400: Loss = -10993.7158203125
Iteration 11500: Loss = -10993.712890625
Iteration 11600: Loss = -10993.7109375
Iteration 11700: Loss = -10993.70703125
Iteration 11800: Loss = -10993.705078125
Iteration 11900: Loss = -10993.703125
Iteration 12000: Loss = -10993.697265625
Iteration 12100: Loss = -10993.6845703125
Iteration 12200: Loss = -10993.4423828125
Iteration 12300: Loss = -10993.2705078125
Iteration 12400: Loss = -10993.267578125
Iteration 12500: Loss = -10993.267578125
Iteration 12600: Loss = -10993.263671875
Iteration 12700: Loss = -10993.15625
Iteration 12800: Loss = -10992.537109375
Iteration 12900: Loss = -10992.0234375
Iteration 13000: Loss = -10991.94921875
Iteration 13100: Loss = -10991.64453125
Iteration 13200: Loss = -10991.3427734375
Iteration 13300: Loss = -10991.2021484375
Iteration 13400: Loss = -10991.095703125
Iteration 13500: Loss = -10991.0556640625
Iteration 13600: Loss = -10990.896484375
Iteration 13700: Loss = -10990.6123046875
Iteration 13800: Loss = -10990.5888671875
Iteration 13900: Loss = -10990.5263671875
Iteration 14000: Loss = -10990.4853515625
Iteration 14100: Loss = -10990.4423828125
Iteration 14200: Loss = -10990.3408203125
Iteration 14300: Loss = -10990.265625
Iteration 14400: Loss = -10990.056640625
Iteration 14500: Loss = -10989.95703125
Iteration 14600: Loss = -10989.9150390625
Iteration 14700: Loss = -10989.865234375
Iteration 14800: Loss = -10989.8408203125
Iteration 14900: Loss = -10989.7900390625
Iteration 15000: Loss = -10989.75390625
Iteration 15100: Loss = -10989.6796875
Iteration 15200: Loss = -10989.6484375
Iteration 15300: Loss = -10989.5537109375
Iteration 15400: Loss = -10989.50390625
Iteration 15500: Loss = -10989.4755859375
Iteration 15600: Loss = -10989.42578125
Iteration 15700: Loss = -10989.3603515625
Iteration 15800: Loss = -10989.3134765625
Iteration 15900: Loss = -10989.2314453125
Iteration 16000: Loss = -10989.0263671875
Iteration 16100: Loss = -10987.802734375
Iteration 16200: Loss = -10987.50390625
Iteration 16300: Loss = -10987.501953125
Iteration 16400: Loss = -10987.5009765625
Iteration 16500: Loss = -10987.5009765625
Iteration 16600: Loss = -10987.5
Iteration 16700: Loss = -10987.5009765625
1
Iteration 16800: Loss = -10987.5
Iteration 16900: Loss = -10987.4990234375
Iteration 17000: Loss = -10987.5009765625
1
Iteration 17100: Loss = -10987.5
2
Iteration 17200: Loss = -10987.5009765625
3
Iteration 17300: Loss = -10987.5
4
Iteration 17400: Loss = -10987.5
5
Iteration 17500: Loss = -10987.5
6
Iteration 17600: Loss = -10987.4990234375
Iteration 17700: Loss = -10987.4990234375
Iteration 17800: Loss = -10987.4990234375
Iteration 17900: Loss = -10987.498046875
Iteration 18000: Loss = -10987.498046875
Iteration 18100: Loss = -10987.4990234375
1
Iteration 18200: Loss = -10987.498046875
Iteration 18300: Loss = -10987.498046875
Iteration 18400: Loss = -10987.4990234375
1
Iteration 18500: Loss = -10987.4990234375
2
Iteration 18600: Loss = -10987.4990234375
3
Iteration 18700: Loss = -10987.498046875
Iteration 18800: Loss = -10987.498046875
Iteration 18900: Loss = -10987.4970703125
Iteration 19000: Loss = -10987.4990234375
1
Iteration 19100: Loss = -10987.498046875
2
Iteration 19200: Loss = -10987.498046875
3
Iteration 19300: Loss = -10987.4931640625
Iteration 19400: Loss = -10987.48828125
Iteration 19500: Loss = -10987.486328125
Iteration 19600: Loss = -10987.4873046875
1
Iteration 19700: Loss = -10987.4873046875
2
Iteration 19800: Loss = -10987.4892578125
3
Iteration 19900: Loss = -10987.48828125
4
Iteration 20000: Loss = -10987.4892578125
5
Iteration 20100: Loss = -10987.4873046875
6
Iteration 20200: Loss = -10987.4873046875
7
Iteration 20300: Loss = -10987.4873046875
8
Iteration 20400: Loss = -10987.48828125
9
Iteration 20500: Loss = -10987.4873046875
10
Iteration 20600: Loss = -10987.4873046875
11
Iteration 20700: Loss = -10987.486328125
Iteration 20800: Loss = -10987.4873046875
1
Iteration 20900: Loss = -10987.4853515625
Iteration 21000: Loss = -10987.486328125
1
Iteration 21100: Loss = -10987.486328125
2
Iteration 21200: Loss = -10987.486328125
3
Iteration 21300: Loss = -10987.486328125
4
Iteration 21400: Loss = -10987.486328125
5
Iteration 21500: Loss = -10987.486328125
6
Iteration 21600: Loss = -10987.4873046875
7
Iteration 21700: Loss = -10987.4873046875
8
Iteration 21800: Loss = -10987.4873046875
9
Iteration 21900: Loss = -10987.4853515625
Iteration 22000: Loss = -10987.4873046875
1
Iteration 22100: Loss = -10987.4853515625
Iteration 22200: Loss = -10987.4873046875
1
Iteration 22300: Loss = -10987.4873046875
2
Iteration 22400: Loss = -10987.4873046875
3
Iteration 22500: Loss = -10987.4853515625
Iteration 22600: Loss = -10987.486328125
1
Iteration 22700: Loss = -10987.4833984375
Iteration 22800: Loss = -10987.4814453125
Iteration 22900: Loss = -10987.4814453125
Iteration 23000: Loss = -10987.4814453125
Iteration 23100: Loss = -10987.4833984375
1
Iteration 23200: Loss = -10987.4833984375
2
Iteration 23300: Loss = -10987.4814453125
Iteration 23400: Loss = -10987.482421875
1
Iteration 23500: Loss = -10987.482421875
2
Iteration 23600: Loss = -10987.4814453125
Iteration 23700: Loss = -10987.4833984375
1
Iteration 23800: Loss = -10987.4814453125
Iteration 23900: Loss = -10987.4814453125
Iteration 24000: Loss = -10987.48046875
Iteration 24100: Loss = -10987.482421875
1
Iteration 24200: Loss = -10987.4814453125
2
Iteration 24300: Loss = -10987.4814453125
3
Iteration 24400: Loss = -10987.4833984375
4
Iteration 24500: Loss = -10987.482421875
5
Iteration 24600: Loss = -10987.482421875
6
Iteration 24700: Loss = -10987.4814453125
7
Iteration 24800: Loss = -10987.482421875
8
Iteration 24900: Loss = -10987.482421875
9
Iteration 25000: Loss = -10987.482421875
10
Iteration 25100: Loss = -10987.482421875
11
Iteration 25200: Loss = -10987.482421875
12
Iteration 25300: Loss = -10987.482421875
13
Iteration 25400: Loss = -10987.4833984375
14
Iteration 25500: Loss = -10987.4814453125
15
Stopping early at iteration 25500 due to no improvement.
pi: tensor([[4.1029e-06, 1.0000e+00],
        [7.4342e-02, 9.2566e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.4762e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0681, 0.2124],
         [0.1338, 0.1641]],

        [[0.1482, 0.2635],
         [0.8829, 0.4706]],

        [[0.9762, 0.1023],
         [0.9156, 0.5763]],

        [[0.8539, 0.1145],
         [0.0314, 0.0087]],

        [[0.0128, 0.1216],
         [0.0909, 0.0909]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.029454859676146822
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001544024649856474
Average Adjusted Rand Index: -0.005437604326796251
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19670.298828125
Iteration 100: Loss = -14407.8916015625
Iteration 200: Loss = -11542.9873046875
Iteration 300: Loss = -11186.92578125
Iteration 400: Loss = -11120.240234375
Iteration 500: Loss = -11092.509765625
Iteration 600: Loss = -11077.3427734375
Iteration 700: Loss = -11067.7314453125
Iteration 800: Loss = -11060.4033203125
Iteration 900: Loss = -11054.2529296875
Iteration 1000: Loss = -11049.572265625
Iteration 1100: Loss = -11045.283203125
Iteration 1200: Loss = -11040.5966796875
Iteration 1300: Loss = -11035.5380859375
Iteration 1400: Loss = -11031.9453125
Iteration 1500: Loss = -11028.9345703125
Iteration 1600: Loss = -11025.2568359375
Iteration 1700: Loss = -11022.595703125
Iteration 1800: Loss = -11019.642578125
Iteration 1900: Loss = -11017.0107421875
Iteration 2000: Loss = -11014.4375
Iteration 2100: Loss = -11012.69921875
Iteration 2200: Loss = -11010.9990234375
Iteration 2300: Loss = -11009.3603515625
Iteration 2400: Loss = -11007.8125
Iteration 2500: Loss = -11006.328125
Iteration 2600: Loss = -11004.8857421875
Iteration 2700: Loss = -11003.623046875
Iteration 2800: Loss = -11002.59375
Iteration 2900: Loss = -11001.7421875
Iteration 3000: Loss = -11001.041015625
Iteration 3100: Loss = -11000.49609375
Iteration 3200: Loss = -11000.068359375
Iteration 3300: Loss = -10999.7294921875
Iteration 3400: Loss = -10999.44921875
Iteration 3500: Loss = -10999.2109375
Iteration 3600: Loss = -10999.0009765625
Iteration 3700: Loss = -10998.80859375
Iteration 3800: Loss = -10998.6142578125
Iteration 3900: Loss = -10998.384765625
Iteration 4000: Loss = -10997.9853515625
Iteration 4100: Loss = -10997.3935546875
Iteration 4200: Loss = -10997.0595703125
Iteration 4300: Loss = -10996.865234375
Iteration 4400: Loss = -10996.7255859375
Iteration 4500: Loss = -10996.6083984375
Iteration 4600: Loss = -10996.5107421875
Iteration 4700: Loss = -10996.4248046875
Iteration 4800: Loss = -10996.34765625
Iteration 4900: Loss = -10996.279296875
Iteration 5000: Loss = -10996.216796875
Iteration 5100: Loss = -10996.16015625
Iteration 5200: Loss = -10996.1044921875
Iteration 5300: Loss = -10996.05078125
Iteration 5400: Loss = -10995.9951171875
Iteration 5500: Loss = -10995.9287109375
Iteration 5600: Loss = -10995.8408203125
Iteration 5700: Loss = -10995.677734375
Iteration 5800: Loss = -10995.34765625
Iteration 5900: Loss = -10995.029296875
Iteration 6000: Loss = -10994.8798828125
Iteration 6100: Loss = -10994.798828125
Iteration 6200: Loss = -10994.7236328125
Iteration 6300: Loss = -10994.4462890625
Iteration 6400: Loss = -10994.3623046875
Iteration 6500: Loss = -10994.2919921875
Iteration 6600: Loss = -10994.2177734375
Iteration 6700: Loss = -10994.0927734375
Iteration 6800: Loss = -10993.3154296875
Iteration 6900: Loss = -10993.1083984375
Iteration 7000: Loss = -10993.0146484375
Iteration 7100: Loss = -10992.9208984375
Iteration 7200: Loss = -10992.7119140625
Iteration 7300: Loss = -10992.22265625
Iteration 7400: Loss = -10991.9013671875
Iteration 7500: Loss = -10991.6611328125
Iteration 7600: Loss = -10991.48046875
Iteration 7700: Loss = -10991.3447265625
Iteration 7800: Loss = -10991.248046875
Iteration 7900: Loss = -10991.173828125
Iteration 8000: Loss = -10991.0986328125
Iteration 8100: Loss = -10991.0546875
Iteration 8200: Loss = -10991.0244140625
Iteration 8300: Loss = -10991.0009765625
Iteration 8400: Loss = -10990.9814453125
Iteration 8500: Loss = -10990.966796875
Iteration 8600: Loss = -10990.955078125
Iteration 8700: Loss = -10990.9443359375
Iteration 8800: Loss = -10990.935546875
Iteration 8900: Loss = -10990.9267578125
Iteration 9000: Loss = -10990.9208984375
Iteration 9100: Loss = -10990.9150390625
Iteration 9200: Loss = -10990.9091796875
Iteration 9300: Loss = -10990.9033203125
Iteration 9400: Loss = -10990.900390625
Iteration 9500: Loss = -10990.896484375
Iteration 9600: Loss = -10990.8916015625
Iteration 9700: Loss = -10990.8896484375
Iteration 9800: Loss = -10990.884765625
Iteration 9900: Loss = -10990.8818359375
Iteration 10000: Loss = -10990.86328125
Iteration 10100: Loss = -10990.8564453125
Iteration 10200: Loss = -10990.853515625
Iteration 10300: Loss = -10990.8515625
Iteration 10400: Loss = -10990.849609375
Iteration 10500: Loss = -10990.8486328125
Iteration 10600: Loss = -10990.8466796875
Iteration 10700: Loss = -10990.8447265625
Iteration 10800: Loss = -10990.84375
Iteration 10900: Loss = -10990.84375
Iteration 11000: Loss = -10990.841796875
Iteration 11100: Loss = -10990.841796875
Iteration 11200: Loss = -10990.8408203125
Iteration 11300: Loss = -10990.8388671875
Iteration 11400: Loss = -10990.8369140625
Iteration 11500: Loss = -10990.8369140625
Iteration 11600: Loss = -10990.8359375
Iteration 11700: Loss = -10990.833984375
Iteration 11800: Loss = -10990.833984375
Iteration 11900: Loss = -10990.833984375
Iteration 12000: Loss = -10990.8330078125
Iteration 12100: Loss = -10990.83203125
Iteration 12200: Loss = -10990.8310546875
Iteration 12300: Loss = -10990.8310546875
Iteration 12400: Loss = -10990.8310546875
Iteration 12500: Loss = -10990.830078125
Iteration 12600: Loss = -10990.830078125
Iteration 12700: Loss = -10990.8291015625
Iteration 12800: Loss = -10990.8271484375
Iteration 12900: Loss = -10990.828125
1
Iteration 13000: Loss = -10990.828125
2
Iteration 13100: Loss = -10990.8271484375
Iteration 13200: Loss = -10990.8271484375
Iteration 13300: Loss = -10990.828125
1
Iteration 13400: Loss = -10990.826171875
Iteration 13500: Loss = -10990.826171875
Iteration 13600: Loss = -10990.826171875
Iteration 13700: Loss = -10990.8271484375
1
Iteration 13800: Loss = -10990.8251953125
Iteration 13900: Loss = -10990.8251953125
Iteration 14000: Loss = -10990.826171875
1
Iteration 14100: Loss = -10990.82421875
Iteration 14200: Loss = -10990.826171875
1
Iteration 14300: Loss = -10990.82421875
Iteration 14400: Loss = -10990.82421875
Iteration 14500: Loss = -10990.82421875
Iteration 14600: Loss = -10990.82421875
Iteration 14700: Loss = -10990.82421875
Iteration 14800: Loss = -10990.8251953125
1
Iteration 14900: Loss = -10990.8232421875
Iteration 15000: Loss = -10990.822265625
Iteration 15100: Loss = -10990.82421875
1
Iteration 15200: Loss = -10990.82421875
2
Iteration 15300: Loss = -10990.8232421875
3
Iteration 15400: Loss = -10990.8232421875
4
Iteration 15500: Loss = -10990.822265625
Iteration 15600: Loss = -10990.8232421875
1
Iteration 15700: Loss = -10990.8232421875
2
Iteration 15800: Loss = -10990.822265625
Iteration 15900: Loss = -10990.7490234375
Iteration 16000: Loss = -10990.75
1
Iteration 16100: Loss = -10990.7490234375
Iteration 16200: Loss = -10990.7490234375
Iteration 16300: Loss = -10990.748046875
Iteration 16400: Loss = -10990.75
1
Iteration 16500: Loss = -10990.75
2
Iteration 16600: Loss = -10990.7490234375
3
Iteration 16700: Loss = -10990.748046875
Iteration 16800: Loss = -10990.75
1
Iteration 16900: Loss = -10990.7490234375
2
Iteration 17000: Loss = -10990.7490234375
3
Iteration 17100: Loss = -10990.7490234375
4
Iteration 17200: Loss = -10990.748046875
Iteration 17300: Loss = -10990.748046875
Iteration 17400: Loss = -10990.7490234375
1
Iteration 17500: Loss = -10990.7470703125
Iteration 17600: Loss = -10990.7490234375
1
Iteration 17700: Loss = -10990.748046875
2
Iteration 17800: Loss = -10990.75
3
Iteration 17900: Loss = -10990.748046875
4
Iteration 18000: Loss = -10990.7470703125
Iteration 18100: Loss = -10990.748046875
1
Iteration 18200: Loss = -10990.7470703125
Iteration 18300: Loss = -10990.748046875
1
Iteration 18400: Loss = -10990.748046875
2
Iteration 18500: Loss = -10990.7490234375
3
Iteration 18600: Loss = -10990.7490234375
4
Iteration 18700: Loss = -10990.75
5
Iteration 18800: Loss = -10990.75
6
Iteration 18900: Loss = -10990.7470703125
Iteration 19000: Loss = -10990.7470703125
Iteration 19100: Loss = -10990.7470703125
Iteration 19200: Loss = -10990.7470703125
Iteration 19300: Loss = -10990.7470703125
Iteration 19400: Loss = -10990.748046875
1
Iteration 19500: Loss = -10990.748046875
2
Iteration 19600: Loss = -10990.748046875
3
Iteration 19700: Loss = -10990.7490234375
4
Iteration 19800: Loss = -10990.7470703125
Iteration 19900: Loss = -10990.748046875
1
Iteration 20000: Loss = -10990.7470703125
Iteration 20100: Loss = -10990.7490234375
1
Iteration 20200: Loss = -10990.748046875
2
Iteration 20300: Loss = -10990.748046875
3
Iteration 20400: Loss = -10990.7470703125
Iteration 20500: Loss = -10990.7490234375
1
Iteration 20600: Loss = -10990.7490234375
2
Iteration 20700: Loss = -10990.748046875
3
Iteration 20800: Loss = -10990.7470703125
Iteration 20900: Loss = -10990.748046875
1
Iteration 21000: Loss = -10990.7470703125
Iteration 21100: Loss = -10990.7470703125
Iteration 21200: Loss = -10990.7470703125
Iteration 21300: Loss = -10990.748046875
1
Iteration 21400: Loss = -10990.748046875
2
Iteration 21500: Loss = -10990.748046875
3
Iteration 21600: Loss = -10990.7490234375
4
Iteration 21700: Loss = -10990.748046875
5
Iteration 21800: Loss = -10990.7470703125
Iteration 21900: Loss = -10990.748046875
1
Iteration 22000: Loss = -10990.7470703125
Iteration 22100: Loss = -10990.7470703125
Iteration 22200: Loss = -10990.7470703125
Iteration 22300: Loss = -10990.748046875
1
Iteration 22400: Loss = -10990.748046875
2
Iteration 22500: Loss = -10990.748046875
3
Iteration 22600: Loss = -10990.7470703125
Iteration 22700: Loss = -10990.748046875
1
Iteration 22800: Loss = -10990.7490234375
2
Iteration 22900: Loss = -10990.7470703125
Iteration 23000: Loss = -10990.734375
Iteration 23100: Loss = -10990.7353515625
1
Iteration 23200: Loss = -10990.736328125
2
Iteration 23300: Loss = -10990.7353515625
3
Iteration 23400: Loss = -10990.736328125
4
Iteration 23500: Loss = -10990.7353515625
5
Iteration 23600: Loss = -10990.7333984375
Iteration 23700: Loss = -10990.736328125
1
Iteration 23800: Loss = -10990.734375
2
Iteration 23900: Loss = -10990.7314453125
Iteration 24000: Loss = -10990.7314453125
Iteration 24100: Loss = -10990.7314453125
Iteration 24200: Loss = -10990.73046875
Iteration 24300: Loss = -10990.73046875
Iteration 24400: Loss = -10990.73046875
Iteration 24500: Loss = -10990.73046875
Iteration 24600: Loss = -10990.728515625
Iteration 24700: Loss = -10990.728515625
Iteration 24800: Loss = -10990.7275390625
Iteration 24900: Loss = -10990.7294921875
1
Iteration 25000: Loss = -10990.728515625
2
Iteration 25100: Loss = -10990.728515625
3
Iteration 25200: Loss = -10990.728515625
4
Iteration 25300: Loss = -10990.728515625
5
Iteration 25400: Loss = -10990.7294921875
6
Iteration 25500: Loss = -10990.728515625
7
Iteration 25600: Loss = -10990.7275390625
Iteration 25700: Loss = -10990.7294921875
1
Iteration 25800: Loss = -10990.7294921875
2
Iteration 25900: Loss = -10990.728515625
3
Iteration 26000: Loss = -10990.728515625
4
Iteration 26100: Loss = -10990.728515625
5
Iteration 26200: Loss = -10990.728515625
6
Iteration 26300: Loss = -10990.728515625
7
Iteration 26400: Loss = -10990.728515625
8
Iteration 26500: Loss = -10990.728515625
9
Iteration 26600: Loss = -10990.7294921875
10
Iteration 26700: Loss = -10990.728515625
11
Iteration 26800: Loss = -10990.7294921875
12
Iteration 26900: Loss = -10990.728515625
13
Iteration 27000: Loss = -10990.728515625
14
Iteration 27100: Loss = -10990.728515625
15
Stopping early at iteration 27100 due to no improvement.
pi: tensor([[1.7879e-07, 1.0000e+00],
        [7.2844e-02, 9.2716e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9875, 0.0125], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1582, 0.2952],
         [0.6025, 0.1685]],

        [[0.6690, 0.2442],
         [0.9913, 0.8354]],

        [[0.0133, 0.1019],
         [0.2690, 0.9879]],

        [[0.0750, 0.1128],
         [0.7003, 0.3345]],

        [[0.9830, 0.1240],
         [0.7244, 0.9785]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002122848738815338
Average Adjusted Rand Index: 0.0013698505841700963
[-0.001544024649856474, 0.002122848738815338] [-0.005437604326796251, 0.0013698505841700963] [10987.4814453125, 10990.728515625]
-------------------------------------
This iteration is 20
True Objective function: Loss = -10793.048826877
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42997.99609375
Iteration 100: Loss = -24206.373046875
Iteration 200: Loss = -13881.2265625
Iteration 300: Loss = -11777.3984375
Iteration 400: Loss = -11351.9052734375
Iteration 500: Loss = -11160.6953125
Iteration 600: Loss = -11067.0615234375
Iteration 700: Loss = -11004.4833984375
Iteration 800: Loss = -10967.087890625
Iteration 900: Loss = -10938.375
Iteration 1000: Loss = -10916.1064453125
Iteration 1100: Loss = -10903.87890625
Iteration 1200: Loss = -10894.4521484375
Iteration 1300: Loss = -10886.2294921875
Iteration 1400: Loss = -10879.7802734375
Iteration 1500: Loss = -10874.8427734375
Iteration 1600: Loss = -10870.912109375
Iteration 1700: Loss = -10867.689453125
Iteration 1800: Loss = -10864.9951171875
Iteration 1900: Loss = -10862.7119140625
Iteration 2000: Loss = -10860.751953125
Iteration 2100: Loss = -10859.0498046875
Iteration 2200: Loss = -10857.5615234375
Iteration 2300: Loss = -10856.2490234375
Iteration 2400: Loss = -10855.0869140625
Iteration 2500: Loss = -10854.0498046875
Iteration 2600: Loss = -10853.1201171875
Iteration 2700: Loss = -10852.283203125
Iteration 2800: Loss = -10851.52734375
Iteration 2900: Loss = -10850.8369140625
Iteration 3000: Loss = -10850.2060546875
Iteration 3100: Loss = -10849.634765625
Iteration 3200: Loss = -10849.1181640625
Iteration 3300: Loss = -10848.6455078125
Iteration 3400: Loss = -10848.2099609375
Iteration 3500: Loss = -10847.8095703125
Iteration 3600: Loss = -10847.44140625
Iteration 3700: Loss = -10847.099609375
Iteration 3800: Loss = -10846.7841796875
Iteration 3900: Loss = -10846.4912109375
Iteration 4000: Loss = -10846.22265625
Iteration 4100: Loss = -10845.9697265625
Iteration 4200: Loss = -10845.7333984375
Iteration 4300: Loss = -10845.5146484375
Iteration 4400: Loss = -10845.3115234375
Iteration 4500: Loss = -10845.1181640625
Iteration 4600: Loss = -10844.939453125
Iteration 4700: Loss = -10844.7724609375
Iteration 4800: Loss = -10844.61328125
Iteration 4900: Loss = -10844.466796875
Iteration 5000: Loss = -10844.328125
Iteration 5100: Loss = -10844.197265625
Iteration 5200: Loss = -10844.0732421875
Iteration 5300: Loss = -10843.9599609375
Iteration 5400: Loss = -10843.8515625
Iteration 5500: Loss = -10843.7490234375
Iteration 5600: Loss = -10843.6552734375
Iteration 5700: Loss = -10843.5634765625
Iteration 5800: Loss = -10843.4755859375
Iteration 5900: Loss = -10843.396484375
Iteration 6000: Loss = -10843.318359375
Iteration 6100: Loss = -10843.2470703125
Iteration 6200: Loss = -10843.1787109375
Iteration 6300: Loss = -10843.11328125
Iteration 6400: Loss = -10843.0537109375
Iteration 6500: Loss = -10842.99609375
Iteration 6600: Loss = -10842.94140625
Iteration 6700: Loss = -10842.888671875
Iteration 6800: Loss = -10842.8388671875
Iteration 6900: Loss = -10842.794921875
Iteration 7000: Loss = -10842.75
Iteration 7100: Loss = -10842.7080078125
Iteration 7200: Loss = -10842.669921875
Iteration 7300: Loss = -10842.6318359375
Iteration 7400: Loss = -10842.595703125
Iteration 7500: Loss = -10842.5615234375
Iteration 7600: Loss = -10842.53125
Iteration 7700: Loss = -10842.5009765625
Iteration 7800: Loss = -10842.47265625
Iteration 7900: Loss = -10842.4443359375
Iteration 8000: Loss = -10842.419921875
Iteration 8100: Loss = -10842.3955078125
Iteration 8200: Loss = -10842.37109375
Iteration 8300: Loss = -10842.349609375
Iteration 8400: Loss = -10842.3291015625
Iteration 8500: Loss = -10842.3076171875
Iteration 8600: Loss = -10842.2900390625
Iteration 8700: Loss = -10842.271484375
Iteration 8800: Loss = -10842.2548828125
Iteration 8900: Loss = -10842.23828125
Iteration 9000: Loss = -10842.2236328125
Iteration 9100: Loss = -10842.208984375
Iteration 9200: Loss = -10842.1943359375
Iteration 9300: Loss = -10842.1806640625
Iteration 9400: Loss = -10842.1689453125
Iteration 9500: Loss = -10842.158203125
Iteration 9600: Loss = -10842.146484375
Iteration 9700: Loss = -10842.134765625
Iteration 9800: Loss = -10842.125
Iteration 9900: Loss = -10842.1162109375
Iteration 10000: Loss = -10842.1064453125
Iteration 10100: Loss = -10842.09765625
Iteration 10200: Loss = -10842.08984375
Iteration 10300: Loss = -10842.08203125
Iteration 10400: Loss = -10842.0732421875
Iteration 10500: Loss = -10842.06640625
Iteration 10600: Loss = -10842.0615234375
Iteration 10700: Loss = -10842.0537109375
Iteration 10800: Loss = -10842.0478515625
Iteration 10900: Loss = -10842.0400390625
Iteration 11000: Loss = -10842.0341796875
Iteration 11100: Loss = -10842.0283203125
Iteration 11200: Loss = -10842.0234375
Iteration 11300: Loss = -10842.0166015625
Iteration 11400: Loss = -10842.01171875
Iteration 11500: Loss = -10842.005859375
Iteration 11600: Loss = -10842.001953125
Iteration 11700: Loss = -10841.998046875
Iteration 11800: Loss = -10841.9931640625
Iteration 11900: Loss = -10841.9912109375
Iteration 12000: Loss = -10841.986328125
Iteration 12100: Loss = -10841.9833984375
Iteration 12200: Loss = -10841.982421875
Iteration 12300: Loss = -10841.9794921875
Iteration 12400: Loss = -10841.9765625
Iteration 12500: Loss = -10841.9736328125
Iteration 12600: Loss = -10841.9736328125
Iteration 12700: Loss = -10841.970703125
Iteration 12800: Loss = -10841.9716796875
1
Iteration 12900: Loss = -10841.9677734375
Iteration 13000: Loss = -10841.96484375
Iteration 13100: Loss = -10841.962890625
Iteration 13200: Loss = -10841.962890625
Iteration 13300: Loss = -10841.962890625
Iteration 13400: Loss = -10841.9609375
Iteration 13500: Loss = -10841.9599609375
Iteration 13600: Loss = -10841.9580078125
Iteration 13700: Loss = -10841.95703125
Iteration 13800: Loss = -10841.955078125
Iteration 13900: Loss = -10841.9560546875
1
Iteration 14000: Loss = -10841.9541015625
Iteration 14100: Loss = -10841.9541015625
Iteration 14200: Loss = -10841.953125
Iteration 14300: Loss = -10841.953125
Iteration 14400: Loss = -10841.953125
Iteration 14500: Loss = -10841.9521484375
Iteration 14600: Loss = -10841.951171875
Iteration 14700: Loss = -10841.9501953125
Iteration 14800: Loss = -10841.9501953125
Iteration 14900: Loss = -10841.94921875
Iteration 15000: Loss = -10841.94921875
Iteration 15100: Loss = -10841.94921875
Iteration 15200: Loss = -10841.9501953125
1
Iteration 15300: Loss = -10841.947265625
Iteration 15400: Loss = -10841.94921875
1
Iteration 15500: Loss = -10841.94921875
2
Iteration 15600: Loss = -10841.9482421875
3
Iteration 15700: Loss = -10841.9482421875
4
Iteration 15800: Loss = -10841.9482421875
5
Iteration 15900: Loss = -10841.947265625
Iteration 16000: Loss = -10841.947265625
Iteration 16100: Loss = -10841.9501953125
1
Iteration 16200: Loss = -10841.9482421875
2
Iteration 16300: Loss = -10841.9462890625
Iteration 16400: Loss = -10841.9443359375
Iteration 16500: Loss = -10841.9462890625
1
Iteration 16600: Loss = -10841.947265625
2
Iteration 16700: Loss = -10841.9462890625
3
Iteration 16800: Loss = -10841.9462890625
4
Iteration 16900: Loss = -10841.9462890625
5
Iteration 17000: Loss = -10841.9462890625
6
Iteration 17100: Loss = -10841.947265625
7
Iteration 17200: Loss = -10841.9462890625
8
Iteration 17300: Loss = -10841.9453125
9
Iteration 17400: Loss = -10841.9462890625
10
Iteration 17500: Loss = -10841.9453125
11
Iteration 17600: Loss = -10841.9443359375
Iteration 17700: Loss = -10841.9453125
1
Iteration 17800: Loss = -10841.9453125
2
Iteration 17900: Loss = -10841.9462890625
3
Iteration 18000: Loss = -10841.9443359375
Iteration 18100: Loss = -10841.9462890625
1
Iteration 18200: Loss = -10841.9453125
2
Iteration 18300: Loss = -10841.9462890625
3
Iteration 18400: Loss = -10841.9453125
4
Iteration 18500: Loss = -10841.9443359375
Iteration 18600: Loss = -10841.943359375
Iteration 18700: Loss = -10841.9462890625
1
Iteration 18800: Loss = -10841.9453125
2
Iteration 18900: Loss = -10841.9453125
3
Iteration 19000: Loss = -10841.947265625
4
Iteration 19100: Loss = -10841.9443359375
5
Iteration 19200: Loss = -10841.9453125
6
Iteration 19300: Loss = -10841.9453125
7
Iteration 19400: Loss = -10841.9443359375
8
Iteration 19500: Loss = -10841.9453125
9
Iteration 19600: Loss = -10841.9453125
10
Iteration 19700: Loss = -10841.9462890625
11
Iteration 19800: Loss = -10841.9462890625
12
Iteration 19900: Loss = -10841.9443359375
13
Iteration 20000: Loss = -10841.9453125
14
Iteration 20100: Loss = -10841.9443359375
15
Stopping early at iteration 20100 due to no improvement.
pi: tensor([[9.9992e-01, 7.7538e-05],
        [9.6907e-01, 3.0931e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9983e-01, 1.6941e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1590, 0.1507],
         [0.2250, 0.8615]],

        [[0.2813, 0.1738],
         [0.9495, 0.0114]],

        [[0.0199, 0.1609],
         [0.0947, 0.8170]],

        [[0.0086, 0.2288],
         [0.1314, 0.0108]],

        [[0.0097, 0.1671],
         [0.4867, 0.0135]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31861.0546875
Iteration 100: Loss = -22218.083984375
Iteration 200: Loss = -15865.234375
Iteration 300: Loss = -13085.220703125
Iteration 400: Loss = -11853.5322265625
Iteration 500: Loss = -11328.0009765625
Iteration 600: Loss = -11139.2587890625
Iteration 700: Loss = -11054.48828125
Iteration 800: Loss = -11036.056640625
Iteration 900: Loss = -11019.5107421875
Iteration 1000: Loss = -11009.3642578125
Iteration 1100: Loss = -10998.9833984375
Iteration 1200: Loss = -10990.2919921875
Iteration 1300: Loss = -10982.3818359375
Iteration 1400: Loss = -10973.9013671875
Iteration 1500: Loss = -10966.1865234375
Iteration 1600: Loss = -10955.1201171875
Iteration 1700: Loss = -10944.068359375
Iteration 1800: Loss = -10931.2666015625
Iteration 1900: Loss = -10920.1298828125
Iteration 2000: Loss = -10907.970703125
Iteration 2100: Loss = -10901.7568359375
Iteration 2200: Loss = -10897.240234375
Iteration 2300: Loss = -10893.9462890625
Iteration 2400: Loss = -10891.6982421875
Iteration 2500: Loss = -10890.1064453125
Iteration 2600: Loss = -10888.9140625
Iteration 2700: Loss = -10887.9580078125
Iteration 2800: Loss = -10887.0927734375
Iteration 2900: Loss = -10885.986328125
Iteration 3000: Loss = -10882.9169921875
Iteration 3100: Loss = -10880.8076171875
Iteration 3200: Loss = -10880.0576171875
Iteration 3300: Loss = -10879.568359375
Iteration 3400: Loss = -10879.1826171875
Iteration 3500: Loss = -10878.830078125
Iteration 3600: Loss = -10872.19140625
Iteration 3700: Loss = -10871.8017578125
Iteration 3800: Loss = -10871.5302734375
Iteration 3900: Loss = -10871.30078125
Iteration 4000: Loss = -10871.1005859375
Iteration 4100: Loss = -10870.9248046875
Iteration 4200: Loss = -10870.765625
Iteration 4300: Loss = -10870.623046875
Iteration 4400: Loss = -10870.4921875
Iteration 4500: Loss = -10870.3740234375
Iteration 4600: Loss = -10870.265625
Iteration 4700: Loss = -10870.1669921875
Iteration 4800: Loss = -10870.07421875
Iteration 4900: Loss = -10869.990234375
Iteration 5000: Loss = -10869.9111328125
Iteration 5100: Loss = -10869.837890625
Iteration 5200: Loss = -10869.76953125
Iteration 5300: Loss = -10869.703125
Iteration 5400: Loss = -10869.638671875
Iteration 5500: Loss = -10869.5654296875
Iteration 5600: Loss = -10869.361328125
Iteration 5700: Loss = -10862.4208984375
Iteration 5800: Loss = -10862.2138671875
Iteration 5900: Loss = -10862.1357421875
Iteration 6000: Loss = -10862.076171875
Iteration 6100: Loss = -10862.0263671875
Iteration 6200: Loss = -10861.9833984375
Iteration 6300: Loss = -10861.94140625
Iteration 6400: Loss = -10861.90625
Iteration 6500: Loss = -10861.873046875
Iteration 6600: Loss = -10861.841796875
Iteration 6700: Loss = -10861.8134765625
Iteration 6800: Loss = -10861.787109375
Iteration 6900: Loss = -10861.7607421875
Iteration 7000: Loss = -10861.7373046875
Iteration 7100: Loss = -10861.7158203125
Iteration 7200: Loss = -10861.6953125
Iteration 7300: Loss = -10861.677734375
Iteration 7400: Loss = -10861.66015625
Iteration 7500: Loss = -10861.642578125
Iteration 7600: Loss = -10861.626953125
Iteration 7700: Loss = -10861.6123046875
Iteration 7800: Loss = -10861.5966796875
Iteration 7900: Loss = -10861.583984375
Iteration 8000: Loss = -10861.572265625
Iteration 8100: Loss = -10861.5615234375
Iteration 8200: Loss = -10861.5498046875
Iteration 8300: Loss = -10861.5400390625
Iteration 8400: Loss = -10861.5302734375
Iteration 8500: Loss = -10861.51953125
Iteration 8600: Loss = -10861.513671875
Iteration 8700: Loss = -10861.50390625
Iteration 8800: Loss = -10861.4970703125
Iteration 8900: Loss = -10861.48828125
Iteration 9000: Loss = -10861.4833984375
Iteration 9100: Loss = -10861.474609375
Iteration 9200: Loss = -10861.46875
Iteration 9300: Loss = -10861.4638671875
Iteration 9400: Loss = -10861.4599609375
Iteration 9500: Loss = -10861.4521484375
Iteration 9600: Loss = -10861.4482421875
Iteration 9700: Loss = -10861.443359375
Iteration 9800: Loss = -10861.4384765625
Iteration 9900: Loss = -10861.43359375
Iteration 10000: Loss = -10861.4296875
Iteration 10100: Loss = -10861.42578125
Iteration 10200: Loss = -10861.423828125
Iteration 10300: Loss = -10861.4189453125
Iteration 10400: Loss = -10861.416015625
Iteration 10500: Loss = -10861.412109375
Iteration 10600: Loss = -10861.4091796875
Iteration 10700: Loss = -10861.40625
Iteration 10800: Loss = -10861.40234375
Iteration 10900: Loss = -10861.3994140625
Iteration 11000: Loss = -10861.38671875
Iteration 11100: Loss = -10854.357421875
Iteration 11200: Loss = -10852.3701171875
Iteration 11300: Loss = -10852.318359375
Iteration 11400: Loss = -10852.294921875
Iteration 11500: Loss = -10852.28125
Iteration 11600: Loss = -10852.271484375
Iteration 11700: Loss = -10852.2646484375
Iteration 11800: Loss = -10852.2607421875
Iteration 11900: Loss = -10852.2548828125
Iteration 12000: Loss = -10852.25
Iteration 12100: Loss = -10852.2470703125
Iteration 12200: Loss = -10852.2451171875
Iteration 12300: Loss = -10852.2421875
Iteration 12400: Loss = -10852.2412109375
Iteration 12500: Loss = -10852.2373046875
Iteration 12600: Loss = -10852.236328125
Iteration 12700: Loss = -10852.234375
Iteration 12800: Loss = -10852.2333984375
Iteration 12900: Loss = -10852.2314453125
Iteration 13000: Loss = -10852.2314453125
Iteration 13100: Loss = -10852.2294921875
Iteration 13200: Loss = -10852.2265625
Iteration 13300: Loss = -10852.2275390625
1
Iteration 13400: Loss = -10852.2265625
Iteration 13500: Loss = -10852.224609375
Iteration 13600: Loss = -10852.224609375
Iteration 13700: Loss = -10852.224609375
Iteration 13800: Loss = -10852.2216796875
Iteration 13900: Loss = -10852.2216796875
Iteration 14000: Loss = -10852.2197265625
Iteration 14100: Loss = -10852.2197265625
Iteration 14200: Loss = -10852.2197265625
Iteration 14300: Loss = -10852.2197265625
Iteration 14400: Loss = -10852.2177734375
Iteration 14500: Loss = -10852.21875
1
Iteration 14600: Loss = -10852.2177734375
Iteration 14700: Loss = -10852.2177734375
Iteration 14800: Loss = -10852.2177734375
Iteration 14900: Loss = -10852.2177734375
Iteration 15000: Loss = -10852.21484375
Iteration 15100: Loss = -10852.21875
1
Iteration 15200: Loss = -10852.216796875
2
Iteration 15300: Loss = -10852.21484375
Iteration 15400: Loss = -10852.2138671875
Iteration 15500: Loss = -10852.2158203125
1
Iteration 15600: Loss = -10852.2138671875
Iteration 15700: Loss = -10852.2138671875
Iteration 15800: Loss = -10852.2138671875
Iteration 15900: Loss = -10852.2138671875
Iteration 16000: Loss = -10852.2138671875
Iteration 16100: Loss = -10852.2119140625
Iteration 16200: Loss = -10852.2119140625
Iteration 16300: Loss = -10852.2138671875
1
Iteration 16400: Loss = -10852.2119140625
Iteration 16500: Loss = -10852.2119140625
Iteration 16600: Loss = -10852.2119140625
Iteration 16700: Loss = -10852.2119140625
Iteration 16800: Loss = -10852.2119140625
Iteration 16900: Loss = -10852.2119140625
Iteration 17000: Loss = -10852.2119140625
Iteration 17100: Loss = -10852.2119140625
Iteration 17200: Loss = -10852.2119140625
Iteration 17300: Loss = -10852.2119140625
Iteration 17400: Loss = -10852.2119140625
Iteration 17500: Loss = -10852.2119140625
Iteration 17600: Loss = -10852.2099609375
Iteration 17700: Loss = -10852.2119140625
1
Iteration 17800: Loss = -10852.2109375
2
Iteration 17900: Loss = -10852.2119140625
3
Iteration 18000: Loss = -10852.2099609375
Iteration 18100: Loss = -10852.2099609375
Iteration 18200: Loss = -10852.2109375
1
Iteration 18300: Loss = -10852.2099609375
Iteration 18400: Loss = -10852.2109375
1
Iteration 18500: Loss = -10852.2099609375
Iteration 18600: Loss = -10852.2099609375
Iteration 18700: Loss = -10852.2099609375
Iteration 18800: Loss = -10852.212890625
1
Iteration 18900: Loss = -10852.2109375
2
Iteration 19000: Loss = -10852.2109375
3
Iteration 19100: Loss = -10852.2099609375
Iteration 19200: Loss = -10852.2109375
1
Iteration 19300: Loss = -10852.2099609375
Iteration 19400: Loss = -10852.208984375
Iteration 19500: Loss = -10852.2099609375
1
Iteration 19600: Loss = -10852.2099609375
2
Iteration 19700: Loss = -10852.2099609375
3
Iteration 19800: Loss = -10852.2099609375
4
Iteration 19900: Loss = -10852.2099609375
5
Iteration 20000: Loss = -10852.2099609375
6
Iteration 20100: Loss = -10852.2109375
7
Iteration 20200: Loss = -10852.2099609375
8
Iteration 20300: Loss = -10852.2099609375
9
Iteration 20400: Loss = -10852.2080078125
Iteration 20500: Loss = -10852.2099609375
1
Iteration 20600: Loss = -10852.2080078125
Iteration 20700: Loss = -10852.2099609375
1
Iteration 20800: Loss = -10852.208984375
2
Iteration 20900: Loss = -10852.2099609375
3
Iteration 21000: Loss = -10852.2080078125
Iteration 21100: Loss = -10852.208984375
1
Iteration 21200: Loss = -10852.2080078125
Iteration 21300: Loss = -10852.2099609375
1
Iteration 21400: Loss = -10852.208984375
2
Iteration 21500: Loss = -10852.2109375
3
Iteration 21600: Loss = -10852.2099609375
4
Iteration 21700: Loss = -10852.2099609375
5
Iteration 21800: Loss = -10852.2099609375
6
Iteration 21900: Loss = -10852.2099609375
7
Iteration 22000: Loss = -10852.2080078125
Iteration 22100: Loss = -10852.2109375
1
Iteration 22200: Loss = -10852.208984375
2
Iteration 22300: Loss = -10852.2099609375
3
Iteration 22400: Loss = -10852.2099609375
4
Iteration 22500: Loss = -10852.2099609375
5
Iteration 22600: Loss = -10852.2109375
6
Iteration 22700: Loss = -10852.2099609375
7
Iteration 22800: Loss = -10852.2099609375
8
Iteration 22900: Loss = -10852.2099609375
9
Iteration 23000: Loss = -10852.2080078125
Iteration 23100: Loss = -10852.2099609375
1
Iteration 23200: Loss = -10852.2080078125
Iteration 23300: Loss = -10852.2099609375
1
Iteration 23400: Loss = -10852.2109375
2
Iteration 23500: Loss = -10852.2099609375
3
Iteration 23600: Loss = -10852.2099609375
4
Iteration 23700: Loss = -10852.2109375
5
Iteration 23800: Loss = -10852.2099609375
6
Iteration 23900: Loss = -10852.2099609375
7
Iteration 24000: Loss = -10852.208984375
8
Iteration 24100: Loss = -10852.208984375
9
Iteration 24200: Loss = -10852.2099609375
10
Iteration 24300: Loss = -10852.2099609375
11
Iteration 24400: Loss = -10852.2099609375
12
Iteration 24500: Loss = -10852.2099609375
13
Iteration 24600: Loss = -10852.2080078125
Iteration 24700: Loss = -10852.208984375
1
Iteration 24800: Loss = -10852.208984375
2
Iteration 24900: Loss = -10852.2080078125
Iteration 25000: Loss = -10852.2099609375
1
Iteration 25100: Loss = -10852.2080078125
Iteration 25200: Loss = -10852.2080078125
Iteration 25300: Loss = -10852.2080078125
Iteration 25400: Loss = -10852.2080078125
Iteration 25500: Loss = -10852.2080078125
Iteration 25600: Loss = -10852.2080078125
Iteration 25700: Loss = -10852.2080078125
Iteration 25800: Loss = -10852.2080078125
Iteration 25900: Loss = -10852.208984375
1
Iteration 26000: Loss = -10852.2080078125
Iteration 26100: Loss = -10852.2080078125
Iteration 26200: Loss = -10852.208984375
1
Iteration 26300: Loss = -10852.2080078125
Iteration 26400: Loss = -10852.2080078125
Iteration 26500: Loss = -10852.2080078125
Iteration 26600: Loss = -10852.2099609375
1
Iteration 26700: Loss = -10852.208984375
2
Iteration 26800: Loss = -10852.208984375
3
Iteration 26900: Loss = -10852.2080078125
Iteration 27000: Loss = -10852.2080078125
Iteration 27100: Loss = -10852.2099609375
1
Iteration 27200: Loss = -10852.208984375
2
Iteration 27300: Loss = -10852.2099609375
3
Iteration 27400: Loss = -10852.2099609375
4
Iteration 27500: Loss = -10852.208984375
5
Iteration 27600: Loss = -10852.2099609375
6
Iteration 27700: Loss = -10852.2099609375
7
Iteration 27800: Loss = -10852.2099609375
8
Iteration 27900: Loss = -10852.2080078125
Iteration 28000: Loss = -10852.208984375
1
Iteration 28100: Loss = -10852.2080078125
Iteration 28200: Loss = -10852.2080078125
Iteration 28300: Loss = -10852.2080078125
Iteration 28400: Loss = -10852.208984375
1
Iteration 28500: Loss = -10852.208984375
2
Iteration 28600: Loss = -10852.2080078125
Iteration 28700: Loss = -10852.2080078125
Iteration 28800: Loss = -10852.2080078125
Iteration 28900: Loss = -10852.2080078125
Iteration 29000: Loss = -10852.2099609375
1
Iteration 29100: Loss = -10852.2080078125
Iteration 29200: Loss = -10852.2080078125
Iteration 29300: Loss = -10852.2080078125
Iteration 29400: Loss = -10852.2080078125
Iteration 29500: Loss = -10852.2080078125
Iteration 29600: Loss = -10852.208984375
1
Iteration 29700: Loss = -10852.208984375
2
Iteration 29800: Loss = -10852.2080078125
Iteration 29900: Loss = -10852.2080078125
pi: tensor([[9.9916e-03, 9.9001e-01],
        [1.0000e+00, 1.0104e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1591, 0.1112],
         [0.0245, 0.1593]],

        [[0.9813, 0.2190],
         [0.5939, 0.0180]],

        [[0.0477, 0.6514],
         [0.1173, 0.8389]],

        [[0.0118, 0.1550],
         [0.9149, 0.0407]],

        [[0.0107, 0.1603],
         [0.9228, 0.4260]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007469763950242848
Average Adjusted Rand Index: -0.0011027420863486436
[0.0, -0.0007469763950242848] [0.0, -0.0011027420863486436] [10841.9443359375, 10852.2080078125]
-------------------------------------
This iteration is 21
True Objective function: Loss = -10788.500708504685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47370.3515625
Iteration 100: Loss = -27579.048828125
Iteration 200: Loss = -14402.2158203125
Iteration 300: Loss = -12211.251953125
Iteration 400: Loss = -11628.7470703125
Iteration 500: Loss = -11376.4970703125
Iteration 600: Loss = -11230.1142578125
Iteration 700: Loss = -11155.701171875
Iteration 800: Loss = -11104.287109375
Iteration 900: Loss = -11068.30859375
Iteration 1000: Loss = -11043.68359375
Iteration 1100: Loss = -11021.0673828125
Iteration 1200: Loss = -10999.935546875
Iteration 1300: Loss = -10972.255859375
Iteration 1400: Loss = -10958.9697265625
Iteration 1500: Loss = -10948.703125
Iteration 1600: Loss = -10934.4970703125
Iteration 1700: Loss = -10927.181640625
Iteration 1800: Loss = -10921.3388671875
Iteration 1900: Loss = -10916.2509765625
Iteration 2000: Loss = -10912.3515625
Iteration 2100: Loss = -10909.2705078125
Iteration 2200: Loss = -10901.955078125
Iteration 2300: Loss = -10899.94140625
Iteration 2400: Loss = -10897.9326171875
Iteration 2500: Loss = -10892.53125
Iteration 2600: Loss = -10891.015625
Iteration 2700: Loss = -10889.7392578125
Iteration 2800: Loss = -10888.615234375
Iteration 2900: Loss = -10887.62890625
Iteration 3000: Loss = -10886.7490234375
Iteration 3100: Loss = -10885.9443359375
Iteration 3200: Loss = -10885.158203125
Iteration 3300: Loss = -10884.3798828125
Iteration 3400: Loss = -10883.7373046875
Iteration 3500: Loss = -10883.193359375
Iteration 3600: Loss = -10882.6923828125
Iteration 3700: Loss = -10882.2548828125
Iteration 3800: Loss = -10881.86328125
Iteration 3900: Loss = -10881.5029296875
Iteration 4000: Loss = -10881.166015625
Iteration 4100: Loss = -10880.8544921875
Iteration 4200: Loss = -10880.564453125
Iteration 4300: Loss = -10880.2919921875
Iteration 4400: Loss = -10880.0341796875
Iteration 4500: Loss = -10879.7939453125
Iteration 4600: Loss = -10879.568359375
Iteration 4700: Loss = -10879.3505859375
Iteration 4800: Loss = -10879.1416015625
Iteration 4900: Loss = -10878.9365234375
Iteration 5000: Loss = -10878.7255859375
Iteration 5100: Loss = -10878.4970703125
Iteration 5200: Loss = -10878.1904296875
Iteration 5300: Loss = -10877.6806640625
Iteration 5400: Loss = -10877.06640625
Iteration 5500: Loss = -10876.6455078125
Iteration 5600: Loss = -10875.98046875
Iteration 5700: Loss = -10872.921875
Iteration 5800: Loss = -10872.4365234375
Iteration 5900: Loss = -10872.1806640625
Iteration 6000: Loss = -10871.998046875
Iteration 6100: Loss = -10871.8505859375
Iteration 6200: Loss = -10871.7265625
Iteration 6300: Loss = -10871.62109375
Iteration 6400: Loss = -10871.5263671875
Iteration 6500: Loss = -10871.4384765625
Iteration 6600: Loss = -10871.361328125
Iteration 6700: Loss = -10871.287109375
Iteration 6800: Loss = -10871.2216796875
Iteration 6900: Loss = -10871.16015625
Iteration 7000: Loss = -10871.1025390625
Iteration 7100: Loss = -10871.048828125
Iteration 7200: Loss = -10871.0009765625
Iteration 7300: Loss = -10870.9521484375
Iteration 7400: Loss = -10870.9091796875
Iteration 7500: Loss = -10870.869140625
Iteration 7600: Loss = -10870.830078125
Iteration 7700: Loss = -10870.7939453125
Iteration 7800: Loss = -10870.7587890625
Iteration 7900: Loss = -10870.7275390625
Iteration 8000: Loss = -10870.697265625
Iteration 8100: Loss = -10870.6669921875
Iteration 8200: Loss = -10870.6416015625
Iteration 8300: Loss = -10870.6142578125
Iteration 8400: Loss = -10870.58984375
Iteration 8500: Loss = -10870.5673828125
Iteration 8600: Loss = -10870.5458984375
Iteration 8700: Loss = -10870.525390625
Iteration 8800: Loss = -10870.50390625
Iteration 8900: Loss = -10870.48828125
Iteration 9000: Loss = -10870.46875
Iteration 9100: Loss = -10870.4521484375
Iteration 9200: Loss = -10870.4375
Iteration 9300: Loss = -10870.4208984375
Iteration 9400: Loss = -10870.4072265625
Iteration 9500: Loss = -10870.39453125
Iteration 9600: Loss = -10870.3818359375
Iteration 9700: Loss = -10870.3671875
Iteration 9800: Loss = -10870.3564453125
Iteration 9900: Loss = -10870.345703125
Iteration 10000: Loss = -10870.333984375
Iteration 10100: Loss = -10870.3251953125
Iteration 10200: Loss = -10870.3154296875
Iteration 10300: Loss = -10870.3056640625
Iteration 10400: Loss = -10870.296875
Iteration 10500: Loss = -10870.2880859375
Iteration 10600: Loss = -10870.28125
Iteration 10700: Loss = -10870.2724609375
Iteration 10800: Loss = -10870.265625
Iteration 10900: Loss = -10870.2568359375
Iteration 11000: Loss = -10870.2490234375
Iteration 11100: Loss = -10870.2431640625
Iteration 11200: Loss = -10870.23828125
Iteration 11300: Loss = -10870.23046875
Iteration 11400: Loss = -10870.224609375
Iteration 11500: Loss = -10870.21875
Iteration 11600: Loss = -10870.212890625
Iteration 11700: Loss = -10870.2080078125
Iteration 11800: Loss = -10870.2021484375
Iteration 11900: Loss = -10870.197265625
Iteration 12000: Loss = -10870.1923828125
Iteration 12100: Loss = -10870.1845703125
Iteration 12200: Loss = -10870.1826171875
Iteration 12300: Loss = -10870.177734375
Iteration 12400: Loss = -10870.173828125
Iteration 12500: Loss = -10870.1708984375
Iteration 12600: Loss = -10870.166015625
Iteration 12700: Loss = -10870.162109375
Iteration 12800: Loss = -10870.1591796875
Iteration 12900: Loss = -10870.154296875
Iteration 13000: Loss = -10870.1513671875
Iteration 13100: Loss = -10870.1455078125
Iteration 13200: Loss = -10870.1396484375
Iteration 13300: Loss = -10870.12890625
Iteration 13400: Loss = -10870.1083984375
Iteration 13500: Loss = -10870.0966796875
Iteration 13600: Loss = -10870.0888671875
Iteration 13700: Loss = -10870.08203125
Iteration 13800: Loss = -10870.0771484375
Iteration 13900: Loss = -10870.07421875
Iteration 14000: Loss = -10870.0703125
Iteration 14100: Loss = -10870.068359375
Iteration 14200: Loss = -10870.064453125
Iteration 14300: Loss = -10870.0634765625
Iteration 14400: Loss = -10870.060546875
Iteration 14500: Loss = -10870.05859375
Iteration 14600: Loss = -10870.0576171875
Iteration 14700: Loss = -10870.0537109375
Iteration 14800: Loss = -10870.0537109375
Iteration 14900: Loss = -10870.0537109375
Iteration 15000: Loss = -10870.0498046875
Iteration 15100: Loss = -10870.05078125
1
Iteration 15200: Loss = -10870.048828125
Iteration 15300: Loss = -10870.0478515625
Iteration 15400: Loss = -10870.0458984375
Iteration 15500: Loss = -10870.05078125
1
Iteration 15600: Loss = -10870.0439453125
Iteration 15700: Loss = -10870.0458984375
1
Iteration 15800: Loss = -10870.0439453125
Iteration 15900: Loss = -10870.0439453125
Iteration 16000: Loss = -10870.041015625
Iteration 16100: Loss = -10870.0419921875
1
Iteration 16200: Loss = -10870.0400390625
Iteration 16300: Loss = -10870.0400390625
Iteration 16400: Loss = -10870.0390625
Iteration 16500: Loss = -10870.037109375
Iteration 16600: Loss = -10870.0361328125
Iteration 16700: Loss = -10870.033203125
Iteration 16800: Loss = -10870.03125
Iteration 16900: Loss = -10870.0302734375
Iteration 17000: Loss = -10870.02734375
Iteration 17100: Loss = -10870.025390625
Iteration 17200: Loss = -10870.021484375
Iteration 17300: Loss = -10870.0126953125
Iteration 17400: Loss = -10870.0048828125
Iteration 17500: Loss = -10869.9931640625
Iteration 17600: Loss = -10869.9775390625
Iteration 17700: Loss = -10869.95703125
Iteration 17800: Loss = -10869.9267578125
Iteration 17900: Loss = -10869.8955078125
Iteration 18000: Loss = -10869.8857421875
Iteration 18100: Loss = -10869.8759765625
Iteration 18200: Loss = -10869.873046875
Iteration 18300: Loss = -10869.869140625
Iteration 18400: Loss = -10869.8642578125
Iteration 18500: Loss = -10869.85546875
Iteration 18600: Loss = -10869.8486328125
Iteration 18700: Loss = -10869.8427734375
Iteration 18800: Loss = -10869.837890625
Iteration 18900: Loss = -10869.826171875
Iteration 19000: Loss = -10869.8203125
Iteration 19100: Loss = -10869.8154296875
Iteration 19200: Loss = -10869.8134765625
Iteration 19300: Loss = -10869.814453125
1
Iteration 19400: Loss = -10869.8154296875
2
Iteration 19500: Loss = -10869.8154296875
3
Iteration 19600: Loss = -10869.8154296875
4
Iteration 19700: Loss = -10869.814453125
5
Iteration 19800: Loss = -10869.8134765625
Iteration 19900: Loss = -10869.8134765625
Iteration 20000: Loss = -10869.8125
Iteration 20100: Loss = -10869.8125
Iteration 20200: Loss = -10869.8134765625
1
Iteration 20300: Loss = -10869.8125
Iteration 20400: Loss = -10869.8125
Iteration 20500: Loss = -10869.8134765625
1
Iteration 20600: Loss = -10869.8125
Iteration 20700: Loss = -10869.8134765625
1
Iteration 20800: Loss = -10869.8125
Iteration 20900: Loss = -10869.8134765625
1
Iteration 21000: Loss = -10869.8115234375
Iteration 21100: Loss = -10869.8134765625
1
Iteration 21200: Loss = -10869.8134765625
2
Iteration 21300: Loss = -10869.8134765625
3
Iteration 21400: Loss = -10869.8125
4
Iteration 21500: Loss = -10869.8125
5
Iteration 21600: Loss = -10869.8125
6
Iteration 21700: Loss = -10869.8115234375
Iteration 21800: Loss = -10869.8115234375
Iteration 21900: Loss = -10869.8125
1
Iteration 22000: Loss = -10869.8115234375
Iteration 22100: Loss = -10869.8125
1
Iteration 22200: Loss = -10869.8115234375
Iteration 22300: Loss = -10869.8125
1
Iteration 22400: Loss = -10869.8115234375
Iteration 22500: Loss = -10869.8115234375
Iteration 22600: Loss = -10869.8115234375
Iteration 22700: Loss = -10869.8115234375
Iteration 22800: Loss = -10869.810546875
Iteration 22900: Loss = -10869.73046875
Iteration 23000: Loss = -10869.646484375
Iteration 23100: Loss = -10869.6005859375
Iteration 23200: Loss = -10869.189453125
Iteration 23300: Loss = -10869.1259765625
Iteration 23400: Loss = -10869.111328125
Iteration 23500: Loss = -10869.1025390625
Iteration 23600: Loss = -10869.099609375
Iteration 23700: Loss = -10869.095703125
Iteration 23800: Loss = -10869.05859375
Iteration 23900: Loss = -10869.056640625
Iteration 24000: Loss = -10868.9892578125
Iteration 24100: Loss = -10868.9892578125
Iteration 24200: Loss = -10868.9853515625
Iteration 24300: Loss = -10868.9443359375
Iteration 24400: Loss = -10868.916015625
Iteration 24500: Loss = -10868.91015625
Iteration 24600: Loss = -10868.8916015625
Iteration 24700: Loss = -10868.890625
Iteration 24800: Loss = -10868.890625
Iteration 24900: Loss = -10868.8896484375
Iteration 25000: Loss = -10868.8896484375
Iteration 25100: Loss = -10868.888671875
Iteration 25200: Loss = -10868.8857421875
Iteration 25300: Loss = -10868.884765625
Iteration 25400: Loss = -10868.8828125
Iteration 25500: Loss = -10868.87890625
Iteration 25600: Loss = -10868.8798828125
1
Iteration 25700: Loss = -10868.87890625
Iteration 25800: Loss = -10868.87890625
Iteration 25900: Loss = -10868.8779296875
Iteration 26000: Loss = -10868.8759765625
Iteration 26100: Loss = -10868.876953125
1
Iteration 26200: Loss = -10868.8779296875
2
Iteration 26300: Loss = -10868.876953125
3
Iteration 26400: Loss = -10868.876953125
4
Iteration 26500: Loss = -10868.8779296875
5
Iteration 26600: Loss = -10868.87890625
6
Iteration 26700: Loss = -10868.876953125
7
Iteration 26800: Loss = -10868.876953125
8
Iteration 26900: Loss = -10868.8759765625
Iteration 27000: Loss = -10868.876953125
1
Iteration 27100: Loss = -10868.876953125
2
Iteration 27200: Loss = -10868.8759765625
Iteration 27300: Loss = -10868.8759765625
Iteration 27400: Loss = -10868.8779296875
1
Iteration 27500: Loss = -10868.876953125
2
Iteration 27600: Loss = -10868.8779296875
3
Iteration 27700: Loss = -10868.876953125
4
Iteration 27800: Loss = -10868.876953125
5
Iteration 27900: Loss = -10868.876953125
6
Iteration 28000: Loss = -10868.876953125
7
Iteration 28100: Loss = -10868.8779296875
8
Iteration 28200: Loss = -10868.876953125
9
Iteration 28300: Loss = -10868.876953125
10
Iteration 28400: Loss = -10868.8779296875
11
Iteration 28500: Loss = -10868.876953125
12
Iteration 28600: Loss = -10868.8759765625
Iteration 28700: Loss = -10868.8701171875
Iteration 28800: Loss = -10868.8701171875
Iteration 28900: Loss = -10868.8701171875
Iteration 29000: Loss = -10868.869140625
Iteration 29100: Loss = -10868.8701171875
1
Iteration 29200: Loss = -10868.8681640625
Iteration 29300: Loss = -10868.869140625
1
Iteration 29400: Loss = -10868.869140625
2
Iteration 29500: Loss = -10868.8701171875
3
Iteration 29600: Loss = -10868.87109375
4
Iteration 29700: Loss = -10868.869140625
5
Iteration 29800: Loss = -10868.8671875
Iteration 29900: Loss = -10868.8681640625
1
pi: tensor([[9.8691e-01, 1.3089e-02],
        [1.0000e+00, 6.5622e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0064, 0.9936], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1590, 0.0829],
         [0.0694, 0.1569]],

        [[0.0642, 0.2521],
         [0.9585, 0.9704]],

        [[0.3396, 0.2513],
         [0.9363, 0.9660]],

        [[0.9151, 0.2130],
         [0.0085, 0.9897]],

        [[0.0317, 0.2636],
         [0.0830, 0.8649]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.001837287128747465
Average Adjusted Rand Index: 2.5649657737379564e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36915.0859375
Iteration 100: Loss = -25266.5546875
Iteration 200: Loss = -14701.9150390625
Iteration 300: Loss = -12212.5361328125
Iteration 400: Loss = -11580.220703125
Iteration 500: Loss = -11328.873046875
Iteration 600: Loss = -11199.6533203125
Iteration 700: Loss = -11127.97265625
Iteration 800: Loss = -11098.5966796875
Iteration 900: Loss = -11081.205078125
Iteration 1000: Loss = -11068.638671875
Iteration 1100: Loss = -11058.318359375
Iteration 1200: Loss = -11047.7861328125
Iteration 1300: Loss = -11040.37890625
Iteration 1400: Loss = -11034.9501953125
Iteration 1500: Loss = -11030.384765625
Iteration 1600: Loss = -11025.3984375
Iteration 1700: Loss = -11020.3359375
Iteration 1800: Loss = -11016.2958984375
Iteration 1900: Loss = -11012.314453125
Iteration 2000: Loss = -11008.5478515625
Iteration 2100: Loss = -11004.3671875
Iteration 2200: Loss = -11000.7529296875
Iteration 2300: Loss = -10997.755859375
Iteration 2400: Loss = -10995.0888671875
Iteration 2500: Loss = -10992.5546875
Iteration 2600: Loss = -10990.18359375
Iteration 2700: Loss = -10987.7509765625
Iteration 2800: Loss = -10984.998046875
Iteration 2900: Loss = -10983.322265625
Iteration 3000: Loss = -10981.990234375
Iteration 3100: Loss = -10980.4072265625
Iteration 3200: Loss = -10979.2822265625
Iteration 3300: Loss = -10978.572265625
Iteration 3400: Loss = -10977.9677734375
Iteration 3500: Loss = -10977.443359375
Iteration 3600: Loss = -10976.9873046875
Iteration 3700: Loss = -10975.822265625
Iteration 3800: Loss = -10975.4501953125
Iteration 3900: Loss = -10975.119140625
Iteration 4000: Loss = -10974.7822265625
Iteration 4100: Loss = -10974.1298828125
Iteration 4200: Loss = -10972.787109375
Iteration 4300: Loss = -10972.4501953125
Iteration 4400: Loss = -10972.1083984375
Iteration 4500: Loss = -10971.8583984375
Iteration 4600: Loss = -10971.2255859375
Iteration 4700: Loss = -10971.0546875
Iteration 4800: Loss = -10970.90625
Iteration 4900: Loss = -10970.767578125
Iteration 5000: Loss = -10970.6416015625
Iteration 5100: Loss = -10970.5234375
Iteration 5200: Loss = -10970.4130859375
Iteration 5300: Loss = -10970.3056640625
Iteration 5400: Loss = -10970.19140625
Iteration 5500: Loss = -10969.8671875
Iteration 5600: Loss = -10969.390625
Iteration 5700: Loss = -10969.2197265625
Iteration 5800: Loss = -10969.11328125
Iteration 5900: Loss = -10969.0283203125
Iteration 6000: Loss = -10968.953125
Iteration 6100: Loss = -10968.8837890625
Iteration 6200: Loss = -10968.822265625
Iteration 6300: Loss = -10968.763671875
Iteration 6400: Loss = -10968.7109375
Iteration 6500: Loss = -10968.6591796875
Iteration 6600: Loss = -10968.6123046875
Iteration 6700: Loss = -10968.568359375
Iteration 6800: Loss = -10968.5244140625
Iteration 6900: Loss = -10968.486328125
Iteration 7000: Loss = -10968.447265625
Iteration 7100: Loss = -10968.4130859375
Iteration 7200: Loss = -10968.37890625
Iteration 7300: Loss = -10968.3486328125
Iteration 7400: Loss = -10968.3173828125
Iteration 7500: Loss = -10968.2900390625
Iteration 7600: Loss = -10968.263671875
Iteration 7700: Loss = -10968.2373046875
Iteration 7800: Loss = -10968.212890625
Iteration 7900: Loss = -10968.1923828125
Iteration 8000: Loss = -10968.16796875
Iteration 8100: Loss = -10968.1484375
Iteration 8200: Loss = -10968.12890625
Iteration 8300: Loss = -10968.1103515625
Iteration 8400: Loss = -10968.09375
Iteration 8500: Loss = -10968.076171875
Iteration 8600: Loss = -10968.060546875
Iteration 8700: Loss = -10968.0458984375
Iteration 8800: Loss = -10968.03125
Iteration 8900: Loss = -10968.017578125
Iteration 9000: Loss = -10967.5791015625
Iteration 9100: Loss = -10967.5478515625
Iteration 9200: Loss = -10967.53515625
Iteration 9300: Loss = -10967.5234375
Iteration 9400: Loss = -10967.51171875
Iteration 9500: Loss = -10967.501953125
Iteration 9600: Loss = -10967.4921875
Iteration 9700: Loss = -10967.482421875
Iteration 9800: Loss = -10967.47265625
Iteration 9900: Loss = -10967.4658203125
Iteration 10000: Loss = -10967.458984375
Iteration 10100: Loss = -10967.4501953125
Iteration 10200: Loss = -10967.443359375
Iteration 10300: Loss = -10967.4375
Iteration 10400: Loss = -10967.4306640625
Iteration 10500: Loss = -10967.423828125
Iteration 10600: Loss = -10967.419921875
Iteration 10700: Loss = -10967.4130859375
Iteration 10800: Loss = -10967.408203125
Iteration 10900: Loss = -10967.4033203125
Iteration 11000: Loss = -10967.3984375
Iteration 11100: Loss = -10967.39453125
Iteration 11200: Loss = -10967.390625
Iteration 11300: Loss = -10967.3857421875
Iteration 11400: Loss = -10967.380859375
Iteration 11500: Loss = -10967.37890625
Iteration 11600: Loss = -10967.3759765625
Iteration 11700: Loss = -10967.373046875
Iteration 11800: Loss = -10967.3681640625
Iteration 11900: Loss = -10967.3662109375
Iteration 12000: Loss = -10967.36328125
Iteration 12100: Loss = -10966.3798828125
Iteration 12200: Loss = -10966.375
Iteration 12300: Loss = -10966.3720703125
Iteration 12400: Loss = -10966.3701171875
Iteration 12500: Loss = -10966.3681640625
Iteration 12600: Loss = -10966.365234375
Iteration 12700: Loss = -10966.3642578125
Iteration 12800: Loss = -10966.361328125
Iteration 12900: Loss = -10966.3603515625
Iteration 13000: Loss = -10966.3583984375
Iteration 13100: Loss = -10966.357421875
Iteration 13200: Loss = -10966.35546875
Iteration 13300: Loss = -10966.3544921875
Iteration 13400: Loss = -10966.3505859375
Iteration 13500: Loss = -10966.3505859375
Iteration 13600: Loss = -10966.349609375
Iteration 13700: Loss = -10966.3486328125
Iteration 13800: Loss = -10966.3466796875
Iteration 13900: Loss = -10966.345703125
Iteration 14000: Loss = -10966.345703125
Iteration 14100: Loss = -10966.3447265625
Iteration 14200: Loss = -10966.3427734375
Iteration 14300: Loss = -10966.3408203125
Iteration 14400: Loss = -10966.3408203125
Iteration 14500: Loss = -10966.3388671875
Iteration 14600: Loss = -10966.3388671875
Iteration 14700: Loss = -10966.337890625
Iteration 14800: Loss = -10966.337890625
Iteration 14900: Loss = -10966.3349609375
Iteration 15000: Loss = -10966.337890625
1
Iteration 15100: Loss = -10966.3349609375
Iteration 15200: Loss = -10966.3349609375
Iteration 15300: Loss = -10966.33203125
Iteration 15400: Loss = -10966.33203125
Iteration 15500: Loss = -10966.3291015625
Iteration 15600: Loss = -10966.3271484375
Iteration 15700: Loss = -10966.3232421875
Iteration 15800: Loss = -10966.3232421875
Iteration 15900: Loss = -10966.3212890625
Iteration 16000: Loss = -10966.3212890625
Iteration 16100: Loss = -10966.318359375
Iteration 16200: Loss = -10966.3173828125
Iteration 16300: Loss = -10966.3173828125
Iteration 16400: Loss = -10966.3173828125
Iteration 16500: Loss = -10966.31640625
Iteration 16600: Loss = -10966.3154296875
Iteration 16700: Loss = -10966.3154296875
Iteration 16800: Loss = -10966.3154296875
Iteration 16900: Loss = -10966.3154296875
Iteration 17000: Loss = -10966.314453125
Iteration 17100: Loss = -10966.314453125
Iteration 17200: Loss = -10966.3134765625
Iteration 17300: Loss = -10966.314453125
1
Iteration 17400: Loss = -10966.3134765625
Iteration 17500: Loss = -10966.3134765625
Iteration 17600: Loss = -10966.3125
Iteration 17700: Loss = -10966.3134765625
1
Iteration 17800: Loss = -10966.310546875
Iteration 17900: Loss = -10966.3125
1
Iteration 18000: Loss = -10966.3115234375
2
Iteration 18100: Loss = -10966.3134765625
3
Iteration 18200: Loss = -10966.3125
4
Iteration 18300: Loss = -10966.310546875
Iteration 18400: Loss = -10966.3115234375
1
Iteration 18500: Loss = -10966.310546875
Iteration 18600: Loss = -10966.3115234375
1
Iteration 18700: Loss = -10966.3115234375
2
Iteration 18800: Loss = -10966.310546875
Iteration 18900: Loss = -10966.310546875
Iteration 19000: Loss = -10966.310546875
Iteration 19100: Loss = -10966.310546875
Iteration 19200: Loss = -10966.310546875
Iteration 19300: Loss = -10966.3095703125
Iteration 19400: Loss = -10966.310546875
1
Iteration 19500: Loss = -10966.3134765625
2
Iteration 19600: Loss = -10965.3095703125
Iteration 19700: Loss = -10965.2978515625
Iteration 19800: Loss = -10965.30078125
1
Iteration 19900: Loss = -10965.2978515625
Iteration 20000: Loss = -10965.296875
Iteration 20100: Loss = -10965.298828125
1
Iteration 20200: Loss = -10965.298828125
2
Iteration 20300: Loss = -10965.2978515625
3
Iteration 20400: Loss = -10965.296875
Iteration 20500: Loss = -10965.2978515625
1
Iteration 20600: Loss = -10965.296875
Iteration 20700: Loss = -10965.2978515625
1
Iteration 20800: Loss = -10965.298828125
2
Iteration 20900: Loss = -10965.2978515625
3
Iteration 21000: Loss = -10964.9150390625
Iteration 21100: Loss = -10962.46484375
Iteration 21200: Loss = -10959.2705078125
Iteration 21300: Loss = -10954.3310546875
Iteration 21400: Loss = -10952.34375
Iteration 21500: Loss = -10949.705078125
Iteration 21600: Loss = -10949.650390625
Iteration 21700: Loss = -10949.4482421875
Iteration 21800: Loss = -10948.7578125
Iteration 21900: Loss = -10948.259765625
Iteration 22000: Loss = -10945.3056640625
Iteration 22100: Loss = -10944.1240234375
Iteration 22200: Loss = -10941.6708984375
Iteration 22300: Loss = -10938.8896484375
Iteration 22400: Loss = -10935.1435546875
Iteration 22500: Loss = -10932.53125
Iteration 22600: Loss = -10927.1943359375
Iteration 22700: Loss = -10922.35546875
Iteration 22800: Loss = -10918.4970703125
Iteration 22900: Loss = -10913.27734375
Iteration 23000: Loss = -10910.0234375
Iteration 23100: Loss = -10905.623046875
Iteration 23200: Loss = -10901.287109375
Iteration 23300: Loss = -10891.3271484375
Iteration 23400: Loss = -10869.763671875
Iteration 23500: Loss = -10867.7880859375
Iteration 23600: Loss = -10867.544921875
Iteration 23700: Loss = -10862.5712890625
Iteration 23800: Loss = -10862.416015625
Iteration 23900: Loss = -10862.3759765625
Iteration 24000: Loss = -10862.359375
Iteration 24100: Loss = -10862.27734375
Iteration 24200: Loss = -10862.2294921875
Iteration 24300: Loss = -10861.2490234375
Iteration 24400: Loss = -10857.9521484375
Iteration 24500: Loss = -10857.935546875
Iteration 24600: Loss = -10857.9267578125
Iteration 24700: Loss = -10857.9228515625
Iteration 24800: Loss = -10857.9189453125
Iteration 24900: Loss = -10857.916015625
Iteration 25000: Loss = -10857.9130859375
Iteration 25100: Loss = -10857.8935546875
Iteration 25200: Loss = -10857.888671875
Iteration 25300: Loss = -10857.8876953125
Iteration 25400: Loss = -10857.8876953125
Iteration 25500: Loss = -10857.8876953125
Iteration 25600: Loss = -10857.8818359375
Iteration 25700: Loss = -10857.802734375
Iteration 25800: Loss = -10857.8037109375
1
Iteration 25900: Loss = -10857.80078125
Iteration 26000: Loss = -10857.55859375
Iteration 26100: Loss = -10856.353515625
Iteration 26200: Loss = -10855.2724609375
Iteration 26300: Loss = -10855.203125
Iteration 26400: Loss = -10855.2001953125
Iteration 26500: Loss = -10855.193359375
Iteration 26600: Loss = -10855.1923828125
Iteration 26700: Loss = -10855.1884765625
Iteration 26800: Loss = -10855.1884765625
Iteration 26900: Loss = -10855.173828125
Iteration 27000: Loss = -10855.1484375
Iteration 27100: Loss = -10855.1455078125
Iteration 27200: Loss = -10855.1455078125
Iteration 27300: Loss = -10855.1455078125
Iteration 27400: Loss = -10855.1259765625
Iteration 27500: Loss = -10855.0791015625
Iteration 27600: Loss = -10855.078125
Iteration 27700: Loss = -10855.048828125
Iteration 27800: Loss = -10855.0380859375
Iteration 27900: Loss = -10855.005859375
Iteration 28000: Loss = -10854.986328125
Iteration 28100: Loss = -10854.984375
Iteration 28200: Loss = -10854.9853515625
1
Iteration 28300: Loss = -10854.984375
Iteration 28400: Loss = -10854.9853515625
1
Iteration 28500: Loss = -10854.4072265625
Iteration 28600: Loss = -10854.392578125
Iteration 28700: Loss = -10854.3935546875
1
Iteration 28800: Loss = -10854.392578125
Iteration 28900: Loss = -10854.384765625
Iteration 29000: Loss = -10854.3818359375
Iteration 29100: Loss = -10854.37890625
Iteration 29200: Loss = -10854.3798828125
1
Iteration 29300: Loss = -10854.37890625
Iteration 29400: Loss = -10854.3798828125
1
Iteration 29500: Loss = -10854.3779296875
Iteration 29600: Loss = -10854.37109375
Iteration 29700: Loss = -10854.3671875
Iteration 29800: Loss = -10854.3369140625
Iteration 29900: Loss = -10854.3359375
pi: tensor([[0.8563, 0.1437],
        [0.0356, 0.9644]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 4.0768e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1633, 0.1548],
         [0.7180, 0.2656]],

        [[0.9879, 0.1791],
         [0.2966, 0.2049]],

        [[0.9898, 0.1499],
         [0.0944, 0.0496]],

        [[0.9843, 0.1205],
         [0.2929, 0.0918]],

        [[0.1753, 0.0956],
         [0.9339, 0.9933]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.1537171964665678
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.45710609345530817
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.17814992400290205
Average Adjusted Rand Index: 0.2908256016228841
[-0.001837287128747465, 0.17814992400290205] [2.5649657737379564e-05, 0.2908256016228841] [10868.869140625, 10854.3369140625]
-------------------------------------
This iteration is 22
True Objective function: Loss = -10904.761265977699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52556.6875
Iteration 100: Loss = -32678.560546875
Iteration 200: Loss = -18280.013671875
Iteration 300: Loss = -13024.4619140625
Iteration 400: Loss = -11751.2578125
Iteration 500: Loss = -11348.365234375
Iteration 600: Loss = -11209.888671875
Iteration 700: Loss = -11153.5087890625
Iteration 800: Loss = -11122.6162109375
Iteration 900: Loss = -11095.990234375
Iteration 1000: Loss = -11080.072265625
Iteration 1100: Loss = -11066.7333984375
Iteration 1200: Loss = -11060.78515625
Iteration 1300: Loss = -11056.1630859375
Iteration 1400: Loss = -11046.9306640625
Iteration 1500: Loss = -11043.9052734375
Iteration 1600: Loss = -11041.5078125
Iteration 1700: Loss = -11039.498046875
Iteration 1800: Loss = -11037.7841796875
Iteration 1900: Loss = -11036.2880859375
Iteration 2000: Loss = -11035.0126953125
Iteration 2100: Loss = -11033.8994140625
Iteration 2200: Loss = -11032.91796875
Iteration 2300: Loss = -11032.052734375
Iteration 2400: Loss = -11031.2822265625
Iteration 2500: Loss = -11030.59375
Iteration 2600: Loss = -11029.9736328125
Iteration 2700: Loss = -11029.4150390625
Iteration 2800: Loss = -11028.9052734375
Iteration 2900: Loss = -11023.4267578125
Iteration 3000: Loss = -11022.6494140625
Iteration 3100: Loss = -11022.1787109375
Iteration 3200: Loss = -11021.775390625
Iteration 3300: Loss = -11021.4111328125
Iteration 3400: Loss = -11021.091796875
Iteration 3500: Loss = -11020.80078125
Iteration 3600: Loss = -11020.5361328125
Iteration 3700: Loss = -11020.29296875
Iteration 3800: Loss = -11020.06640625
Iteration 3900: Loss = -11019.8583984375
Iteration 4000: Loss = -11019.6669921875
Iteration 4100: Loss = -11019.48828125
Iteration 4200: Loss = -11019.3232421875
Iteration 4300: Loss = -11019.1689453125
Iteration 4400: Loss = -11019.0263671875
Iteration 4500: Loss = -11018.8935546875
Iteration 4600: Loss = -11018.7685546875
Iteration 4700: Loss = -11018.6513671875
Iteration 4800: Loss = -11018.541015625
Iteration 4900: Loss = -11018.4384765625
Iteration 5000: Loss = -11018.3427734375
Iteration 5100: Loss = -11018.25
Iteration 5200: Loss = -11018.1650390625
Iteration 5300: Loss = -11018.0849609375
Iteration 5400: Loss = -11018.0078125
Iteration 5500: Loss = -11017.9345703125
Iteration 5600: Loss = -11017.8642578125
Iteration 5700: Loss = -11017.7958984375
Iteration 5800: Loss = -11017.728515625
Iteration 5900: Loss = -11017.662109375
Iteration 6000: Loss = -11017.59375
Iteration 6100: Loss = -11017.51953125
Iteration 6200: Loss = -11017.4384765625
Iteration 6300: Loss = -11017.34375
Iteration 6400: Loss = -11017.224609375
Iteration 6500: Loss = -11017.0576171875
Iteration 6600: Loss = -11016.8388671875
Iteration 6700: Loss = -11016.6826171875
Iteration 6800: Loss = -11016.6328125
Iteration 6900: Loss = -11016.607421875
Iteration 7000: Loss = -11016.5859375
Iteration 7100: Loss = -11016.5634765625
Iteration 7200: Loss = -11016.5439453125
Iteration 7300: Loss = -11016.525390625
Iteration 7400: Loss = -11016.5078125
Iteration 7500: Loss = -11016.4873046875
Iteration 7600: Loss = -11016.470703125
Iteration 7700: Loss = -11016.4541015625
Iteration 7800: Loss = -11016.4365234375
Iteration 7900: Loss = -11016.4189453125
Iteration 8000: Loss = -11016.396484375
Iteration 8100: Loss = -11016.37109375
Iteration 8200: Loss = -11016.3447265625
Iteration 8300: Loss = -11016.326171875
Iteration 8400: Loss = -11016.30859375
Iteration 8500: Loss = -11016.28515625
Iteration 8600: Loss = -11016.2607421875
Iteration 8700: Loss = -11016.236328125
Iteration 8800: Loss = -11016.216796875
Iteration 8900: Loss = -11016.2001953125
Iteration 9000: Loss = -11016.181640625
Iteration 9100: Loss = -11016.166015625
Iteration 9200: Loss = -11016.150390625
Iteration 9300: Loss = -11016.1318359375
Iteration 9400: Loss = -11016.10546875
Iteration 9500: Loss = -11016.060546875
Iteration 9600: Loss = -11015.9296875
Iteration 9700: Loss = -11015.5732421875
Iteration 9800: Loss = -11015.32421875
Iteration 9900: Loss = -11015.2578125
Iteration 10000: Loss = -11014.994140625
Iteration 10100: Loss = -11014.060546875
Iteration 10200: Loss = -11012.5078125
Iteration 10300: Loss = -11012.388671875
Iteration 10400: Loss = -11012.3134765625
Iteration 10500: Loss = -11012.208984375
Iteration 10600: Loss = -11012.1328125
Iteration 10700: Loss = -11012.1005859375
Iteration 10800: Loss = -11012.0654296875
Iteration 10900: Loss = -11012.0244140625
Iteration 11000: Loss = -11012.0048828125
Iteration 11100: Loss = -11011.98828125
Iteration 11200: Loss = -11011.9755859375
Iteration 11300: Loss = -11011.962890625
Iteration 11400: Loss = -11011.9482421875
Iteration 11500: Loss = -11011.935546875
Iteration 11600: Loss = -11011.9150390625
Iteration 11700: Loss = -11011.892578125
Iteration 11800: Loss = -11011.8837890625
Iteration 11900: Loss = -11011.8759765625
Iteration 12000: Loss = -11011.87109375
Iteration 12100: Loss = -11011.8662109375
Iteration 12200: Loss = -11011.861328125
Iteration 12300: Loss = -11011.8583984375
Iteration 12400: Loss = -11011.85546875
Iteration 12500: Loss = -11011.853515625
Iteration 12600: Loss = -11011.8505859375
Iteration 12700: Loss = -11011.84765625
Iteration 12800: Loss = -11011.84375
Iteration 12900: Loss = -11011.841796875
Iteration 13000: Loss = -11011.8408203125
Iteration 13100: Loss = -11011.837890625
Iteration 13200: Loss = -11011.8359375
Iteration 13300: Loss = -11011.8349609375
Iteration 13400: Loss = -11011.8330078125
Iteration 13500: Loss = -11011.83203125
Iteration 13600: Loss = -11011.830078125
Iteration 13700: Loss = -11011.8291015625
Iteration 13800: Loss = -11011.8271484375
Iteration 13900: Loss = -11011.826171875
Iteration 14000: Loss = -11011.8251953125
Iteration 14100: Loss = -11011.82421875
Iteration 14200: Loss = -11011.822265625
Iteration 14300: Loss = -11011.822265625
Iteration 14400: Loss = -11011.8203125
Iteration 14500: Loss = -11011.8203125
Iteration 14600: Loss = -11011.8203125
Iteration 14700: Loss = -11011.8193359375
Iteration 14800: Loss = -11011.8173828125
Iteration 14900: Loss = -11011.8173828125
Iteration 15000: Loss = -11011.8173828125
Iteration 15100: Loss = -11011.81640625
Iteration 15200: Loss = -11011.81640625
Iteration 15300: Loss = -11011.8154296875
Iteration 15400: Loss = -11011.814453125
Iteration 15500: Loss = -11011.814453125
Iteration 15600: Loss = -11011.8134765625
Iteration 15700: Loss = -11011.8134765625
Iteration 15800: Loss = -11011.814453125
1
Iteration 15900: Loss = -11011.8134765625
Iteration 16000: Loss = -11011.8134765625
Iteration 16100: Loss = -11011.8125
Iteration 16200: Loss = -11011.8134765625
1
Iteration 16300: Loss = -11011.810546875
Iteration 16400: Loss = -11011.810546875
Iteration 16500: Loss = -11011.8125
1
Iteration 16600: Loss = -11011.810546875
Iteration 16700: Loss = -11011.80859375
Iteration 16800: Loss = -11011.810546875
1
Iteration 16900: Loss = -11011.8095703125
2
Iteration 17000: Loss = -11011.810546875
3
Iteration 17100: Loss = -11011.810546875
4
Iteration 17200: Loss = -11011.8095703125
5
Iteration 17300: Loss = -11011.810546875
6
Iteration 17400: Loss = -11011.8076171875
Iteration 17500: Loss = -11011.80859375
1
Iteration 17600: Loss = -11011.8095703125
2
Iteration 17700: Loss = -11011.810546875
3
Iteration 17800: Loss = -11011.8076171875
Iteration 17900: Loss = -11011.8056640625
Iteration 18000: Loss = -11011.8095703125
1
Iteration 18100: Loss = -11011.8076171875
2
Iteration 18200: Loss = -11011.806640625
3
Iteration 18300: Loss = -11011.8056640625
Iteration 18400: Loss = -11011.802734375
Iteration 18500: Loss = -11011.0361328125
Iteration 18600: Loss = -11011.03515625
Iteration 18700: Loss = -11011.0341796875
Iteration 18800: Loss = -11011.033203125
Iteration 18900: Loss = -11011.03515625
1
Iteration 19000: Loss = -11011.03515625
2
Iteration 19100: Loss = -11011.0341796875
3
Iteration 19200: Loss = -11011.03515625
4
Iteration 19300: Loss = -11011.033203125
Iteration 19400: Loss = -11011.033203125
Iteration 19500: Loss = -11011.033203125
Iteration 19600: Loss = -11011.0341796875
1
Iteration 19700: Loss = -11011.0341796875
2
Iteration 19800: Loss = -11011.0341796875
3
Iteration 19900: Loss = -11011.03515625
4
Iteration 20000: Loss = -11011.0341796875
5
Iteration 20100: Loss = -11011.033203125
Iteration 20200: Loss = -11011.033203125
Iteration 20300: Loss = -11011.033203125
Iteration 20400: Loss = -11011.033203125
Iteration 20500: Loss = -11011.0341796875
1
Iteration 20600: Loss = -11011.033203125
Iteration 20700: Loss = -11011.033203125
Iteration 20800: Loss = -11011.03515625
1
Iteration 20900: Loss = -11011.0341796875
2
Iteration 21000: Loss = -11011.0341796875
3
Iteration 21100: Loss = -11011.0322265625
Iteration 21200: Loss = -11011.0322265625
Iteration 21300: Loss = -11011.03515625
1
Iteration 21400: Loss = -11011.0341796875
2
Iteration 21500: Loss = -11011.0322265625
Iteration 21600: Loss = -11011.03515625
1
Iteration 21700: Loss = -11011.03125
Iteration 21800: Loss = -11011.0341796875
1
Iteration 21900: Loss = -11011.033203125
2
Iteration 22000: Loss = -11011.033203125
3
Iteration 22100: Loss = -11011.0341796875
4
Iteration 22200: Loss = -11011.0322265625
5
Iteration 22300: Loss = -11011.0341796875
6
Iteration 22400: Loss = -11011.0341796875
7
Iteration 22500: Loss = -11011.0341796875
8
Iteration 22600: Loss = -11011.0341796875
9
Iteration 22700: Loss = -11011.033203125
10
Iteration 22800: Loss = -11011.0341796875
11
Iteration 22900: Loss = -11011.033203125
12
Iteration 23000: Loss = -11011.033203125
13
Iteration 23100: Loss = -11011.0322265625
14
Iteration 23200: Loss = -11011.033203125
15
Stopping early at iteration 23200 due to no improvement.
pi: tensor([[5.1408e-01, 4.8592e-01],
        [2.1253e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0600, 0.9400], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.8901, 0.1920],
         [0.0434, 0.1617]],

        [[0.9837, 0.2154],
         [0.9849, 0.9687]],

        [[0.8541, 0.1832],
         [0.9831, 0.0400]],

        [[0.0079, 0.0708],
         [0.9823, 0.9048]],

        [[0.9607, 0.1919],
         [0.5683, 0.9746]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.0013479301801878508
Average Adjusted Rand Index: -0.000513468013109402
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45736.69921875
Iteration 100: Loss = -26217.3046875
Iteration 200: Loss = -14620.7900390625
Iteration 300: Loss = -12152.1259765625
Iteration 400: Loss = -11637.1787109375
Iteration 500: Loss = -11457.04296875
Iteration 600: Loss = -11364.3291015625
Iteration 700: Loss = -11275.0126953125
Iteration 800: Loss = -11222.251953125
Iteration 900: Loss = -11166.76171875
Iteration 1000: Loss = -11133.8115234375
Iteration 1100: Loss = -11117.3359375
Iteration 1200: Loss = -11103.19921875
Iteration 1300: Loss = -11095.0615234375
Iteration 1400: Loss = -11089.197265625
Iteration 1500: Loss = -11084.0732421875
Iteration 1600: Loss = -11079.73828125
Iteration 1700: Loss = -11075.92578125
Iteration 1800: Loss = -11070.3203125
Iteration 1900: Loss = -11060.083984375
Iteration 2000: Loss = -11052.69140625
Iteration 2100: Loss = -11049.490234375
Iteration 2200: Loss = -11046.6796875
Iteration 2300: Loss = -11044.7236328125
Iteration 2400: Loss = -11043.2470703125
Iteration 2500: Loss = -11041.9072265625
Iteration 2600: Loss = -11038.763671875
Iteration 2700: Loss = -11033.9794921875
Iteration 2800: Loss = -11032.7314453125
Iteration 2900: Loss = -11031.8505859375
Iteration 3000: Loss = -11031.123046875
Iteration 3100: Loss = -11030.486328125
Iteration 3200: Loss = -11029.923828125
Iteration 3300: Loss = -11029.423828125
Iteration 3400: Loss = -11028.9677734375
Iteration 3500: Loss = -11028.5556640625
Iteration 3600: Loss = -11028.1767578125
Iteration 3700: Loss = -11027.8291015625
Iteration 3800: Loss = -11027.505859375
Iteration 3900: Loss = -11027.2099609375
Iteration 4000: Loss = -11026.935546875
Iteration 4100: Loss = -11026.681640625
Iteration 4200: Loss = -11026.447265625
Iteration 4300: Loss = -11026.2265625
Iteration 4400: Loss = -11026.0224609375
Iteration 4500: Loss = -11025.828125
Iteration 4600: Loss = -11025.6494140625
Iteration 4700: Loss = -11025.482421875
Iteration 4800: Loss = -11025.328125
Iteration 4900: Loss = -11025.181640625
Iteration 5000: Loss = -11025.046875
Iteration 5100: Loss = -11024.91796875
Iteration 5200: Loss = -11024.796875
Iteration 5300: Loss = -11024.6845703125
Iteration 5400: Loss = -11024.5791015625
Iteration 5500: Loss = -11024.4794921875
Iteration 5600: Loss = -11024.38671875
Iteration 5700: Loss = -11024.2978515625
Iteration 5800: Loss = -11024.2158203125
Iteration 5900: Loss = -11024.1376953125
Iteration 6000: Loss = -11024.064453125
Iteration 6100: Loss = -11023.99609375
Iteration 6200: Loss = -11023.9306640625
Iteration 6300: Loss = -11023.8681640625
Iteration 6400: Loss = -11023.8095703125
Iteration 6500: Loss = -11023.75390625
Iteration 6600: Loss = -11023.7001953125
Iteration 6700: Loss = -11023.65234375
Iteration 6800: Loss = -11023.6044921875
Iteration 6900: Loss = -11023.5595703125
Iteration 7000: Loss = -11023.5185546875
Iteration 7100: Loss = -11023.4794921875
Iteration 7200: Loss = -11023.439453125
Iteration 7300: Loss = -11023.404296875
Iteration 7400: Loss = -11023.3701171875
Iteration 7500: Loss = -11023.3359375
Iteration 7600: Loss = -11023.3056640625
Iteration 7700: Loss = -11023.2763671875
Iteration 7800: Loss = -11023.2470703125
Iteration 7900: Loss = -11023.2197265625
Iteration 8000: Loss = -11023.1943359375
Iteration 8100: Loss = -11023.16796875
Iteration 8200: Loss = -11022.99609375
Iteration 8300: Loss = -11017.8076171875
Iteration 8400: Loss = -11017.55859375
Iteration 8500: Loss = -11017.451171875
Iteration 8600: Loss = -11017.376953125
Iteration 8700: Loss = -11017.32421875
Iteration 8800: Loss = -11017.2822265625
Iteration 8900: Loss = -11017.24609375
Iteration 9000: Loss = -11017.2119140625
Iteration 9100: Loss = -11017.185546875
Iteration 9200: Loss = -11017.16015625
Iteration 9300: Loss = -11017.1376953125
Iteration 9400: Loss = -11017.1171875
Iteration 9500: Loss = -11017.095703125
Iteration 9600: Loss = -11017.0791015625
Iteration 9700: Loss = -11017.0625
Iteration 9800: Loss = -11017.0478515625
Iteration 9900: Loss = -11017.0322265625
Iteration 10000: Loss = -11017.0185546875
Iteration 10100: Loss = -11017.005859375
Iteration 10200: Loss = -11016.9931640625
Iteration 10300: Loss = -11016.984375
Iteration 10400: Loss = -11016.9736328125
Iteration 10500: Loss = -11016.9638671875
Iteration 10600: Loss = -11016.9560546875
Iteration 10700: Loss = -11016.9453125
Iteration 10800: Loss = -11016.9384765625
Iteration 10900: Loss = -11016.9306640625
Iteration 11000: Loss = -11016.9228515625
Iteration 11100: Loss = -11016.916015625
Iteration 11200: Loss = -11016.9091796875
Iteration 11300: Loss = -11016.9033203125
Iteration 11400: Loss = -11016.8984375
Iteration 11500: Loss = -11016.892578125
Iteration 11600: Loss = -11016.8857421875
Iteration 11700: Loss = -11016.8818359375
Iteration 11800: Loss = -11016.876953125
Iteration 11900: Loss = -11016.873046875
Iteration 12000: Loss = -11016.8681640625
Iteration 12100: Loss = -11016.865234375
Iteration 12200: Loss = -11016.8603515625
Iteration 12300: Loss = -11016.8564453125
Iteration 12400: Loss = -11016.8525390625
Iteration 12500: Loss = -11016.849609375
Iteration 12600: Loss = -11016.8466796875
Iteration 12700: Loss = -11016.84375
Iteration 12800: Loss = -11016.8408203125
Iteration 12900: Loss = -11016.8369140625
Iteration 13000: Loss = -11016.833984375
Iteration 13100: Loss = -11016.833984375
Iteration 13200: Loss = -11016.8310546875
Iteration 13300: Loss = -11016.828125
Iteration 13400: Loss = -11016.826171875
Iteration 13500: Loss = -11016.8232421875
Iteration 13600: Loss = -11016.822265625
Iteration 13700: Loss = -11016.8193359375
Iteration 13800: Loss = -11016.8154296875
Iteration 13900: Loss = -11016.8173828125
1
Iteration 14000: Loss = -11016.8134765625
Iteration 14100: Loss = -11016.8125
Iteration 14200: Loss = -11016.8115234375
Iteration 14300: Loss = -11016.810546875
Iteration 14400: Loss = -11016.8076171875
Iteration 14500: Loss = -11016.80859375
1
Iteration 14600: Loss = -11016.806640625
Iteration 14700: Loss = -11016.802734375
Iteration 14800: Loss = -11016.8037109375
1
Iteration 14900: Loss = -11016.8037109375
2
Iteration 15000: Loss = -11016.8017578125
Iteration 15100: Loss = -11016.7998046875
Iteration 15200: Loss = -11016.7998046875
Iteration 15300: Loss = -11016.7998046875
Iteration 15400: Loss = -11016.798828125
Iteration 15500: Loss = -11016.7978515625
Iteration 15600: Loss = -11016.7958984375
Iteration 15700: Loss = -11016.7958984375
Iteration 15800: Loss = -11016.7958984375
Iteration 15900: Loss = -11016.7939453125
Iteration 16000: Loss = -11016.79296875
Iteration 16100: Loss = -11016.79296875
Iteration 16200: Loss = -11016.79296875
Iteration 16300: Loss = -11016.791015625
Iteration 16400: Loss = -11016.7919921875
1
Iteration 16500: Loss = -11016.7890625
Iteration 16600: Loss = -11016.7890625
Iteration 16700: Loss = -11016.7880859375
Iteration 16800: Loss = -11016.7880859375
Iteration 16900: Loss = -11016.7880859375
Iteration 17000: Loss = -11016.7880859375
Iteration 17100: Loss = -11016.7861328125
Iteration 17200: Loss = -11016.78515625
Iteration 17300: Loss = -11016.783203125
Iteration 17400: Loss = -11016.783203125
Iteration 17500: Loss = -11016.7822265625
Iteration 17600: Loss = -11016.779296875
Iteration 17700: Loss = -11016.7783203125
Iteration 17800: Loss = -11016.775390625
Iteration 17900: Loss = -11016.7734375
Iteration 18000: Loss = -11016.767578125
Iteration 18100: Loss = -11016.7587890625
Iteration 18200: Loss = -11016.736328125
Iteration 18300: Loss = -11016.65234375
Iteration 18400: Loss = -11016.29296875
Iteration 18500: Loss = -11015.630859375
Iteration 18600: Loss = -11015.2763671875
Iteration 18700: Loss = -11014.0830078125
Iteration 18800: Loss = -11013.6728515625
Iteration 18900: Loss = -11013.638671875
Iteration 19000: Loss = -11013.6396484375
1
Iteration 19100: Loss = -11013.6328125
Iteration 19200: Loss = -11013.625
Iteration 19300: Loss = -11013.6220703125
Iteration 19400: Loss = -11013.6171875
Iteration 19500: Loss = -11013.615234375
Iteration 19600: Loss = -11013.6123046875
Iteration 19700: Loss = -11013.609375
Iteration 19800: Loss = -11013.607421875
Iteration 19900: Loss = -11013.607421875
Iteration 20000: Loss = -11013.6064453125
Iteration 20100: Loss = -11013.6064453125
Iteration 20200: Loss = -11013.599609375
Iteration 20300: Loss = -11013.59765625
Iteration 20400: Loss = -11013.5966796875
Iteration 20500: Loss = -11013.5966796875
Iteration 20600: Loss = -11013.595703125
Iteration 20700: Loss = -11013.595703125
Iteration 20800: Loss = -11013.5947265625
Iteration 20900: Loss = -11013.595703125
1
Iteration 21000: Loss = -11013.5927734375
Iteration 21100: Loss = -11013.59375
1
Iteration 21200: Loss = -11013.5927734375
Iteration 21300: Loss = -11013.59375
1
Iteration 21400: Loss = -11013.591796875
Iteration 21500: Loss = -11013.591796875
Iteration 21600: Loss = -11013.591796875
Iteration 21700: Loss = -11013.591796875
Iteration 21800: Loss = -11013.5908203125
Iteration 21900: Loss = -11013.591796875
1
Iteration 22000: Loss = -11013.5908203125
Iteration 22100: Loss = -11013.5908203125
Iteration 22200: Loss = -11013.591796875
1
Iteration 22300: Loss = -11013.5908203125
Iteration 22400: Loss = -11013.5908203125
Iteration 22500: Loss = -11013.5908203125
Iteration 22600: Loss = -11013.5908203125
Iteration 22700: Loss = -11013.58984375
Iteration 22800: Loss = -11013.58984375
Iteration 22900: Loss = -11013.591796875
1
Iteration 23000: Loss = -11013.58984375
Iteration 23100: Loss = -11013.58984375
Iteration 23200: Loss = -11013.5908203125
1
Iteration 23300: Loss = -11013.58984375
Iteration 23400: Loss = -11013.58984375
Iteration 23500: Loss = -11013.58984375
Iteration 23600: Loss = -11013.5888671875
Iteration 23700: Loss = -11013.5888671875
Iteration 23800: Loss = -11013.5888671875
Iteration 23900: Loss = -11013.5888671875
Iteration 24000: Loss = -11013.587890625
Iteration 24100: Loss = -11013.5888671875
1
Iteration 24200: Loss = -11013.58984375
2
Iteration 24300: Loss = -11013.587890625
Iteration 24400: Loss = -11013.5888671875
1
Iteration 24500: Loss = -11013.58984375
2
Iteration 24600: Loss = -11013.5888671875
3
Iteration 24700: Loss = -11013.58984375
4
Iteration 24800: Loss = -11013.5888671875
5
Iteration 24900: Loss = -11013.58984375
6
Iteration 25000: Loss = -11013.5888671875
7
Iteration 25100: Loss = -11013.58984375
8
Iteration 25200: Loss = -11013.587890625
Iteration 25300: Loss = -11013.58984375
1
Iteration 25400: Loss = -11013.58984375
2
Iteration 25500: Loss = -11013.58984375
3
Iteration 25600: Loss = -11013.587890625
Iteration 25700: Loss = -11013.58984375
1
Iteration 25800: Loss = -11013.587890625
Iteration 25900: Loss = -11013.587890625
Iteration 26000: Loss = -11013.587890625
Iteration 26100: Loss = -11013.5888671875
1
Iteration 26200: Loss = -11013.58984375
2
Iteration 26300: Loss = -11013.5888671875
3
Iteration 26400: Loss = -11013.58984375
4
Iteration 26500: Loss = -11013.5888671875
5
Iteration 26600: Loss = -11013.587890625
Iteration 26700: Loss = -11013.58984375
1
Iteration 26800: Loss = -11013.5888671875
2
Iteration 26900: Loss = -11013.5888671875
3
Iteration 27000: Loss = -11013.5888671875
4
Iteration 27100: Loss = -11013.5888671875
5
Iteration 27200: Loss = -11013.58984375
6
Iteration 27300: Loss = -11013.5888671875
7
Iteration 27400: Loss = -11013.5888671875
8
Iteration 27500: Loss = -11013.58984375
9
Iteration 27600: Loss = -11013.5888671875
10
Iteration 27700: Loss = -11013.5888671875
11
Iteration 27800: Loss = -11013.58984375
12
Iteration 27900: Loss = -11013.587890625
Iteration 28000: Loss = -11013.5888671875
1
Iteration 28100: Loss = -11013.587890625
Iteration 28200: Loss = -11013.5888671875
1
Iteration 28300: Loss = -11013.587890625
Iteration 28400: Loss = -11013.5888671875
1
Iteration 28500: Loss = -11013.5888671875
2
Iteration 28600: Loss = -11013.5888671875
3
Iteration 28700: Loss = -11013.587890625
Iteration 28800: Loss = -11013.58984375
1
Iteration 28900: Loss = -11013.58984375
2
Iteration 29000: Loss = -11013.587890625
Iteration 29100: Loss = -11013.58984375
1
Iteration 29200: Loss = -11013.5888671875
2
Iteration 29300: Loss = -11013.5888671875
3
Iteration 29400: Loss = -11013.587890625
Iteration 29500: Loss = -11013.5888671875
1
Iteration 29600: Loss = -11013.5888671875
2
Iteration 29700: Loss = -11013.587890625
Iteration 29800: Loss = -11013.58984375
1
Iteration 29900: Loss = -11013.587890625
pi: tensor([[7.2032e-01, 2.7968e-01],
        [1.2534e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0456, 0.9544], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2476, 0.0971],
         [0.0336, 0.1637]],

        [[0.0723, 0.2248],
         [0.2862, 0.1268]],

        [[0.0468, 0.1099],
         [0.0362, 0.9830]],

        [[0.9023, 0.2291],
         [0.0506, 0.0473]],

        [[0.0266, 0.1279],
         [0.6254, 0.0166]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.0017817067833058632
Average Adjusted Rand Index: -0.00026980122784698733
[-0.0013479301801878508, 0.0017817067833058632] [-0.000513468013109402, -0.00026980122784698733] [11011.033203125, 11013.58984375]
-------------------------------------
This iteration is 23
True Objective function: Loss = -10797.586830719172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32543.677734375
Iteration 100: Loss = -18153.30859375
Iteration 200: Loss = -12348.923828125
Iteration 300: Loss = -11352.0810546875
Iteration 400: Loss = -11176.7890625
Iteration 500: Loss = -11083.91796875
Iteration 600: Loss = -11030.990234375
Iteration 700: Loss = -11001.2939453125
Iteration 800: Loss = -10983.7529296875
Iteration 900: Loss = -10967.9716796875
Iteration 1000: Loss = -10961.71484375
Iteration 1100: Loss = -10957.3427734375
Iteration 1200: Loss = -10953.8564453125
Iteration 1300: Loss = -10950.8125
Iteration 1400: Loss = -10947.74609375
Iteration 1500: Loss = -10944.55078125
Iteration 1600: Loss = -10942.1796875
Iteration 1700: Loss = -10940.6162109375
Iteration 1800: Loss = -10939.4453125
Iteration 1900: Loss = -10938.494140625
Iteration 2000: Loss = -10937.6943359375
Iteration 2100: Loss = -10937.005859375
Iteration 2200: Loss = -10936.40625
Iteration 2300: Loss = -10935.87890625
Iteration 2400: Loss = -10935.4091796875
Iteration 2500: Loss = -10934.9892578125
Iteration 2600: Loss = -10934.609375
Iteration 2700: Loss = -10934.2685546875
Iteration 2800: Loss = -10933.9501953125
Iteration 2900: Loss = -10933.646484375
Iteration 3000: Loss = -10933.390625
Iteration 3100: Loss = -10933.1572265625
Iteration 3200: Loss = -10932.9404296875
Iteration 3300: Loss = -10932.7431640625
Iteration 3400: Loss = -10932.55859375
Iteration 3500: Loss = -10932.3857421875
Iteration 3600: Loss = -10932.2275390625
Iteration 3700: Loss = -10932.080078125
Iteration 3800: Loss = -10931.9423828125
Iteration 3900: Loss = -10931.814453125
Iteration 4000: Loss = -10931.697265625
Iteration 4100: Loss = -10931.587890625
Iteration 4200: Loss = -10931.4814453125
Iteration 4300: Loss = -10931.3828125
Iteration 4400: Loss = -10931.29296875
Iteration 4500: Loss = -10931.20703125
Iteration 4600: Loss = -10931.1259765625
Iteration 4700: Loss = -10931.0498046875
Iteration 4800: Loss = -10930.978515625
Iteration 4900: Loss = -10930.91015625
Iteration 5000: Loss = -10930.84765625
Iteration 5100: Loss = -10930.78515625
Iteration 5200: Loss = -10930.728515625
Iteration 5300: Loss = -10930.6728515625
Iteration 5400: Loss = -10930.619140625
Iteration 5500: Loss = -10930.56640625
Iteration 5600: Loss = -10930.5146484375
Iteration 5700: Loss = -10930.4619140625
Iteration 5800: Loss = -10930.404296875
Iteration 5900: Loss = -10930.330078125
Iteration 6000: Loss = -10930.1875
Iteration 6100: Loss = -10929.77734375
Iteration 6200: Loss = -10929.1708984375
Iteration 6300: Loss = -10928.958984375
Iteration 6400: Loss = -10928.8330078125
Iteration 6500: Loss = -10928.728515625
Iteration 6600: Loss = -10928.626953125
Iteration 6700: Loss = -10928.5400390625
Iteration 6800: Loss = -10928.4697265625
Iteration 6900: Loss = -10928.4052734375
Iteration 7000: Loss = -10928.3388671875
Iteration 7100: Loss = -10928.263671875
Iteration 7200: Loss = -10928.16796875
Iteration 7300: Loss = -10928.0595703125
Iteration 7400: Loss = -10927.96875
Iteration 7500: Loss = -10927.8974609375
Iteration 7600: Loss = -10927.8408203125
Iteration 7700: Loss = -10927.783203125
Iteration 7800: Loss = -10927.732421875
Iteration 7900: Loss = -10927.6826171875
Iteration 8000: Loss = -10927.63671875
Iteration 8100: Loss = -10927.5966796875
Iteration 8200: Loss = -10927.5556640625
Iteration 8300: Loss = -10927.521484375
Iteration 8400: Loss = -10927.482421875
Iteration 8500: Loss = -10927.44921875
Iteration 8600: Loss = -10927.4140625
Iteration 8700: Loss = -10927.3798828125
Iteration 8800: Loss = -10927.34765625
Iteration 8900: Loss = -10927.3125
Iteration 9000: Loss = -10927.27734375
Iteration 9100: Loss = -10927.2451171875
Iteration 9200: Loss = -10927.212890625
Iteration 9300: Loss = -10927.1787109375
Iteration 9400: Loss = -10927.1455078125
Iteration 9500: Loss = -10927.109375
Iteration 9600: Loss = -10927.0732421875
Iteration 9700: Loss = -10927.0341796875
Iteration 9800: Loss = -10926.9931640625
Iteration 9900: Loss = -10926.9501953125
Iteration 10000: Loss = -10926.90234375
Iteration 10100: Loss = -10926.8427734375
Iteration 10200: Loss = -10926.640625
Iteration 10300: Loss = -10925.3955078125
Iteration 10400: Loss = -10925.2216796875
Iteration 10500: Loss = -10925.111328125
Iteration 10600: Loss = -10925.0078125
Iteration 10700: Loss = -10924.87890625
Iteration 10800: Loss = -10923.8037109375
Iteration 10900: Loss = -10923.44140625
Iteration 11000: Loss = -10921.5126953125
Iteration 11100: Loss = -10920.740234375
Iteration 11200: Loss = -10918.8623046875
Iteration 11300: Loss = -10912.4443359375
Iteration 11400: Loss = -10887.85546875
Iteration 11500: Loss = -10858.8662109375
Iteration 11600: Loss = -10838.951171875
Iteration 11700: Loss = -10830.1181640625
Iteration 11800: Loss = -10820.4853515625
Iteration 11900: Loss = -10819.4345703125
Iteration 12000: Loss = -10818.5439453125
Iteration 12100: Loss = -10814.3330078125
Iteration 12200: Loss = -10804.6826171875
Iteration 12300: Loss = -10799.248046875
Iteration 12400: Loss = -10785.9345703125
Iteration 12500: Loss = -10777.6904296875
Iteration 12600: Loss = -10770.1064453125
Iteration 12700: Loss = -10769.83203125
Iteration 12800: Loss = -10769.4697265625
Iteration 12900: Loss = -10763.3642578125
Iteration 13000: Loss = -10763.283203125
Iteration 13100: Loss = -10763.248046875
Iteration 13200: Loss = -10763.228515625
Iteration 13300: Loss = -10763.2138671875
Iteration 13400: Loss = -10756.26953125
Iteration 13500: Loss = -10754.82421875
Iteration 13600: Loss = -10754.7783203125
Iteration 13700: Loss = -10754.7568359375
Iteration 13800: Loss = -10754.744140625
Iteration 13900: Loss = -10754.7353515625
Iteration 14000: Loss = -10754.728515625
Iteration 14100: Loss = -10754.72265625
Iteration 14200: Loss = -10754.7177734375
Iteration 14300: Loss = -10754.71484375
Iteration 14400: Loss = -10754.7099609375
Iteration 14500: Loss = -10754.70703125
Iteration 14600: Loss = -10754.560546875
Iteration 14700: Loss = -10754.548828125
Iteration 14800: Loss = -10754.544921875
Iteration 14900: Loss = -10754.5419921875
Iteration 15000: Loss = -10754.541015625
Iteration 15100: Loss = -10754.5390625
Iteration 15200: Loss = -10754.5380859375
Iteration 15300: Loss = -10754.537109375
Iteration 15400: Loss = -10754.537109375
Iteration 15500: Loss = -10754.5361328125
Iteration 15600: Loss = -10754.53515625
Iteration 15700: Loss = -10754.5341796875
Iteration 15800: Loss = -10754.533203125
Iteration 15900: Loss = -10754.533203125
Iteration 16000: Loss = -10754.53125
Iteration 16100: Loss = -10754.53125
Iteration 16200: Loss = -10754.53125
Iteration 16300: Loss = -10754.529296875
Iteration 16400: Loss = -10754.529296875
Iteration 16500: Loss = -10754.529296875
Iteration 16600: Loss = -10754.5302734375
1
Iteration 16700: Loss = -10754.5283203125
Iteration 16800: Loss = -10754.5283203125
Iteration 16900: Loss = -10754.52734375
Iteration 17000: Loss = -10754.5283203125
1
Iteration 17100: Loss = -10754.5126953125
Iteration 17200: Loss = -10753.763671875
Iteration 17300: Loss = -10753.759765625
Iteration 17400: Loss = -10753.7568359375
Iteration 17500: Loss = -10753.7587890625
1
Iteration 17600: Loss = -10753.7578125
2
Iteration 17700: Loss = -10753.7578125
3
Iteration 17800: Loss = -10753.755859375
Iteration 17900: Loss = -10753.755859375
Iteration 18000: Loss = -10753.6865234375
Iteration 18100: Loss = -10753.685546875
Iteration 18200: Loss = -10753.685546875
Iteration 18300: Loss = -10753.6845703125
Iteration 18400: Loss = -10753.6845703125
Iteration 18500: Loss = -10753.6845703125
Iteration 18600: Loss = -10753.685546875
1
Iteration 18700: Loss = -10753.6845703125
Iteration 18800: Loss = -10753.6845703125
Iteration 18900: Loss = -10753.6845703125
Iteration 19000: Loss = -10753.685546875
1
Iteration 19100: Loss = -10753.68359375
Iteration 19200: Loss = -10753.685546875
1
Iteration 19300: Loss = -10753.6845703125
2
Iteration 19400: Loss = -10753.6845703125
3
Iteration 19500: Loss = -10753.6845703125
4
Iteration 19600: Loss = -10753.6845703125
5
Iteration 19700: Loss = -10753.6845703125
6
Iteration 19800: Loss = -10753.68359375
Iteration 19900: Loss = -10753.6845703125
1
Iteration 20000: Loss = -10753.68359375
Iteration 20100: Loss = -10753.6845703125
1
Iteration 20200: Loss = -10753.6845703125
2
Iteration 20300: Loss = -10753.6826171875
Iteration 20400: Loss = -10753.68359375
1
Iteration 20500: Loss = -10753.6826171875
Iteration 20600: Loss = -10753.6845703125
1
Iteration 20700: Loss = -10753.6826171875
Iteration 20800: Loss = -10753.6826171875
Iteration 20900: Loss = -10753.6826171875
Iteration 21000: Loss = -10753.6826171875
Iteration 21100: Loss = -10753.681640625
Iteration 21200: Loss = -10753.6806640625
Iteration 21300: Loss = -10753.67578125
Iteration 21400: Loss = -10753.67578125
Iteration 21500: Loss = -10753.67578125
Iteration 21600: Loss = -10753.67578125
Iteration 21700: Loss = -10753.67578125
Iteration 21800: Loss = -10753.67578125
Iteration 21900: Loss = -10753.67578125
Iteration 22000: Loss = -10753.6767578125
1
Iteration 22100: Loss = -10753.6767578125
2
Iteration 22200: Loss = -10753.6767578125
3
Iteration 22300: Loss = -10753.673828125
Iteration 22400: Loss = -10753.6669921875
Iteration 22500: Loss = -10753.666015625
Iteration 22600: Loss = -10753.6669921875
1
Iteration 22700: Loss = -10753.666015625
Iteration 22800: Loss = -10753.666015625
Iteration 22900: Loss = -10753.666015625
Iteration 23000: Loss = -10753.6650390625
Iteration 23100: Loss = -10753.666015625
1
Iteration 23200: Loss = -10753.666015625
2
Iteration 23300: Loss = -10753.6572265625
Iteration 23400: Loss = -10753.654296875
Iteration 23500: Loss = -10753.6533203125
Iteration 23600: Loss = -10753.6533203125
Iteration 23700: Loss = -10753.654296875
1
Iteration 23800: Loss = -10753.654296875
2
Iteration 23900: Loss = -10753.654296875
3
Iteration 24000: Loss = -10753.654296875
4
Iteration 24100: Loss = -10753.654296875
5
Iteration 24200: Loss = -10753.6533203125
Iteration 24300: Loss = -10753.6533203125
Iteration 24400: Loss = -10753.6533203125
Iteration 24500: Loss = -10753.65234375
Iteration 24600: Loss = -10753.65234375
Iteration 24700: Loss = -10753.6533203125
1
Iteration 24800: Loss = -10753.6533203125
2
Iteration 24900: Loss = -10753.6494140625
Iteration 25000: Loss = -10753.6484375
Iteration 25100: Loss = -10753.6494140625
1
Iteration 25200: Loss = -10753.650390625
2
Iteration 25300: Loss = -10753.6484375
Iteration 25400: Loss = -10753.6494140625
1
Iteration 25500: Loss = -10753.6484375
Iteration 25600: Loss = -10753.6494140625
1
Iteration 25700: Loss = -10753.6494140625
2
Iteration 25800: Loss = -10753.6494140625
3
Iteration 25900: Loss = -10753.6494140625
4
Iteration 26000: Loss = -10753.6494140625
5
Iteration 26100: Loss = -10753.6494140625
6
Iteration 26200: Loss = -10753.6494140625
7
Iteration 26300: Loss = -10753.6484375
Iteration 26400: Loss = -10753.6484375
Iteration 26500: Loss = -10753.6494140625
1
Iteration 26600: Loss = -10753.6494140625
2
Iteration 26700: Loss = -10753.6474609375
Iteration 26800: Loss = -10753.6435546875
Iteration 26900: Loss = -10753.6435546875
Iteration 27000: Loss = -10753.64453125
1
Iteration 27100: Loss = -10753.6435546875
Iteration 27200: Loss = -10753.64453125
1
Iteration 27300: Loss = -10753.6416015625
Iteration 27400: Loss = -10753.63671875
Iteration 27500: Loss = -10753.63671875
Iteration 27600: Loss = -10753.6376953125
1
Iteration 27700: Loss = -10753.6376953125
2
Iteration 27800: Loss = -10753.63671875
Iteration 27900: Loss = -10753.6376953125
1
Iteration 28000: Loss = -10753.63671875
Iteration 28100: Loss = -10753.63671875
Iteration 28200: Loss = -10753.63671875
Iteration 28300: Loss = -10753.63671875
Iteration 28400: Loss = -10753.6376953125
1
Iteration 28500: Loss = -10753.6376953125
2
Iteration 28600: Loss = -10753.63671875
Iteration 28700: Loss = -10753.63671875
Iteration 28800: Loss = -10753.6376953125
1
Iteration 28900: Loss = -10753.6357421875
Iteration 29000: Loss = -10753.6376953125
1
Iteration 29100: Loss = -10753.6357421875
Iteration 29200: Loss = -10753.63671875
1
Iteration 29300: Loss = -10753.63671875
2
Iteration 29400: Loss = -10753.63671875
3
Iteration 29500: Loss = -10753.63671875
4
Iteration 29600: Loss = -10753.63671875
5
Iteration 29700: Loss = -10753.638671875
6
Iteration 29800: Loss = -10753.6357421875
Iteration 29900: Loss = -10753.6357421875
pi: tensor([[0.8220, 0.1780],
        [0.2265, 0.7735]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4924, 0.5076], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2447, 0.0939],
         [0.1367, 0.2039]],

        [[0.5270, 0.1065],
         [0.9597, 0.2619]],

        [[0.2755, 0.0966],
         [0.1756, 0.1902]],

        [[0.3859, 0.0920],
         [0.7473, 0.9812]],

        [[0.0220, 0.0925],
         [0.0340, 0.6848]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 10
Adjusted Rand Index: 0.6363588278853274
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080875752406894
Global Adjusted Rand Index: 0.8314068905433201
Average Adjusted Rand Index: 0.8343388628672684
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47773.125
Iteration 100: Loss = -26406.587890625
Iteration 200: Loss = -14128.4267578125
Iteration 300: Loss = -11818.32421875
Iteration 400: Loss = -11411.76171875
Iteration 500: Loss = -11246.5888671875
Iteration 600: Loss = -11164.5869140625
Iteration 700: Loss = -11118.8330078125
Iteration 800: Loss = -11076.63671875
Iteration 900: Loss = -11043.0400390625
Iteration 1000: Loss = -11027.46484375
Iteration 1100: Loss = -11013.5068359375
Iteration 1200: Loss = -11000.1689453125
Iteration 1300: Loss = -10992.693359375
Iteration 1400: Loss = -10987.142578125
Iteration 1500: Loss = -10982.70703125
Iteration 1600: Loss = -10979.0458984375
Iteration 1700: Loss = -10975.9619140625
Iteration 1800: Loss = -10973.3310546875
Iteration 1900: Loss = -10971.0634765625
Iteration 2000: Loss = -10969.0908203125
Iteration 2100: Loss = -10967.36328125
Iteration 2200: Loss = -10965.837890625
Iteration 2300: Loss = -10964.4814453125
Iteration 2400: Loss = -10963.2734375
Iteration 2500: Loss = -10962.1884765625
Iteration 2600: Loss = -10961.1796875
Iteration 2700: Loss = -10950.1103515625
Iteration 2800: Loss = -10949.0849609375
Iteration 2900: Loss = -10948.3447265625
Iteration 3000: Loss = -10947.69921875
Iteration 3100: Loss = -10947.1201171875
Iteration 3200: Loss = -10946.5732421875
Iteration 3300: Loss = -10942.5537109375
Iteration 3400: Loss = -10942.107421875
Iteration 3500: Loss = -10941.708984375
Iteration 3600: Loss = -10941.3369140625
Iteration 3700: Loss = -10941.0029296875
Iteration 3800: Loss = -10940.6904296875
Iteration 3900: Loss = -10940.404296875
Iteration 4000: Loss = -10940.13671875
Iteration 4100: Loss = -10938.0478515625
Iteration 4200: Loss = -10933.673828125
Iteration 4300: Loss = -10933.3935546875
Iteration 4400: Loss = -10933.1591796875
Iteration 4500: Loss = -10932.9453125
Iteration 4600: Loss = -10932.751953125
Iteration 4700: Loss = -10932.5712890625
Iteration 4800: Loss = -10932.4052734375
Iteration 4900: Loss = -10932.251953125
Iteration 5000: Loss = -10932.107421875
Iteration 5100: Loss = -10931.9716796875
Iteration 5200: Loss = -10931.84765625
Iteration 5300: Loss = -10931.7294921875
Iteration 5400: Loss = -10931.6171875
Iteration 5500: Loss = -10931.513671875
Iteration 5600: Loss = -10931.4150390625
Iteration 5700: Loss = -10931.3193359375
Iteration 5800: Loss = -10931.2138671875
Iteration 5900: Loss = -10931.1201171875
Iteration 6000: Loss = -10931.0263671875
Iteration 6100: Loss = -10930.9306640625
Iteration 6200: Loss = -10930.826171875
Iteration 6300: Loss = -10930.7060546875
Iteration 6400: Loss = -10930.5498046875
Iteration 6500: Loss = -10930.095703125
Iteration 6600: Loss = -10929.505859375
Iteration 6700: Loss = -10929.31640625
Iteration 6800: Loss = -10929.2685546875
Iteration 6900: Loss = -10929.234375
Iteration 7000: Loss = -10929.1982421875
Iteration 7100: Loss = -10929.1318359375
Iteration 7200: Loss = -10929.025390625
Iteration 7300: Loss = -10928.9541015625
Iteration 7400: Loss = -10928.890625
Iteration 7500: Loss = -10928.8359375
Iteration 7600: Loss = -10928.7939453125
Iteration 7700: Loss = -10928.7197265625
Iteration 7800: Loss = -10928.6630859375
Iteration 7900: Loss = -10928.615234375
Iteration 8000: Loss = -10928.5869140625
Iteration 8100: Loss = -10928.5224609375
Iteration 8200: Loss = -10928.4306640625
Iteration 8300: Loss = -10928.3935546875
Iteration 8400: Loss = -10928.36328125
Iteration 8500: Loss = -10928.3408203125
Iteration 8600: Loss = -10928.3154296875
Iteration 8700: Loss = -10928.2880859375
Iteration 8800: Loss = -10928.263671875
Iteration 8900: Loss = -10928.23046875
Iteration 9000: Loss = -10928.1982421875
Iteration 9100: Loss = -10928.1650390625
Iteration 9200: Loss = -10928.1298828125
Iteration 9300: Loss = -10928.0927734375
Iteration 9400: Loss = -10928.0654296875
Iteration 9500: Loss = -10928.0439453125
Iteration 9600: Loss = -10928.017578125
Iteration 9700: Loss = -10927.994140625
Iteration 9800: Loss = -10927.96875
Iteration 9900: Loss = -10927.94140625
Iteration 10000: Loss = -10927.9150390625
Iteration 10100: Loss = -10927.8857421875
Iteration 10200: Loss = -10927.8525390625
Iteration 10300: Loss = -10927.818359375
Iteration 10400: Loss = -10927.78515625
Iteration 10500: Loss = -10927.755859375
Iteration 10600: Loss = -10927.724609375
Iteration 10700: Loss = -10927.6962890625
Iteration 10800: Loss = -10927.662109375
Iteration 10900: Loss = -10927.626953125
Iteration 11000: Loss = -10927.58984375
Iteration 11100: Loss = -10927.5478515625
Iteration 11200: Loss = -10927.486328125
Iteration 11300: Loss = -10927.2958984375
Iteration 11400: Loss = -10927.1396484375
Iteration 11500: Loss = -10927.0322265625
Iteration 11600: Loss = -10926.912109375
Iteration 11700: Loss = -10926.7734375
Iteration 11800: Loss = -10926.6103515625
Iteration 11900: Loss = -10926.4267578125
Iteration 12000: Loss = -10926.22265625
Iteration 12100: Loss = -10926.0126953125
Iteration 12200: Loss = -10925.822265625
Iteration 12300: Loss = -10925.6708984375
Iteration 12400: Loss = -10925.560546875
Iteration 12500: Loss = -10925.486328125
Iteration 12600: Loss = -10925.4326171875
Iteration 12700: Loss = -10925.3828125
Iteration 12800: Loss = -10925.33984375
Iteration 12900: Loss = -10925.306640625
Iteration 13000: Loss = -10925.2626953125
Iteration 13100: Loss = -10925.2294921875
Iteration 13200: Loss = -10925.1982421875
Iteration 13300: Loss = -10925.1748046875
Iteration 13400: Loss = -10925.1494140625
Iteration 13500: Loss = -10925.1240234375
Iteration 13600: Loss = -10925.0947265625
Iteration 13700: Loss = -10925.05078125
Iteration 13800: Loss = -10925.037109375
Iteration 13900: Loss = -10925.0224609375
Iteration 14000: Loss = -10925.009765625
Iteration 14100: Loss = -10925.001953125
Iteration 14200: Loss = -10924.994140625
Iteration 14300: Loss = -10924.9873046875
Iteration 14400: Loss = -10924.98046875
Iteration 14500: Loss = -10924.9765625
Iteration 14600: Loss = -10924.974609375
Iteration 14700: Loss = -10924.970703125
Iteration 14800: Loss = -10924.96875
Iteration 14900: Loss = -10924.966796875
Iteration 15000: Loss = -10924.9658203125
Iteration 15100: Loss = -10924.96484375
Iteration 15200: Loss = -10924.9638671875
Iteration 15300: Loss = -10924.9638671875
Iteration 15400: Loss = -10924.9638671875
Iteration 15500: Loss = -10924.962890625
Iteration 15600: Loss = -10924.9619140625
Iteration 15700: Loss = -10924.9619140625
Iteration 15800: Loss = -10924.9619140625
Iteration 15900: Loss = -10924.9609375
Iteration 16000: Loss = -10924.9609375
Iteration 16100: Loss = -10924.958984375
Iteration 16200: Loss = -10924.9599609375
1
Iteration 16300: Loss = -10924.9599609375
2
Iteration 16400: Loss = -10924.9580078125
Iteration 16500: Loss = -10924.9580078125
Iteration 16600: Loss = -10924.9580078125
Iteration 16700: Loss = -10924.958984375
1
Iteration 16800: Loss = -10924.9580078125
Iteration 16900: Loss = -10924.958984375
1
Iteration 17000: Loss = -10924.95703125
Iteration 17100: Loss = -10924.958984375
1
Iteration 17200: Loss = -10924.9423828125
Iteration 17300: Loss = -10924.9375
Iteration 17400: Loss = -10924.935546875
Iteration 17500: Loss = -10924.9375
1
Iteration 17600: Loss = -10924.9365234375
2
Iteration 17700: Loss = -10924.9375
3
Iteration 17800: Loss = -10924.935546875
Iteration 17900: Loss = -10924.9365234375
1
Iteration 18000: Loss = -10924.9345703125
Iteration 18100: Loss = -10924.9365234375
1
Iteration 18200: Loss = -10924.9365234375
2
Iteration 18300: Loss = -10924.9365234375
3
Iteration 18400: Loss = -10924.935546875
4
Iteration 18500: Loss = -10924.93359375
Iteration 18600: Loss = -10924.935546875
1
Iteration 18700: Loss = -10924.9345703125
2
Iteration 18800: Loss = -10924.935546875
3
Iteration 18900: Loss = -10924.9345703125
4
Iteration 19000: Loss = -10924.935546875
5
Iteration 19100: Loss = -10924.935546875
6
Iteration 19200: Loss = -10924.935546875
7
Iteration 19300: Loss = -10924.935546875
8
Iteration 19400: Loss = -10924.9345703125
9
Iteration 19500: Loss = -10924.9345703125
10
Iteration 19600: Loss = -10924.935546875
11
Iteration 19700: Loss = -10924.935546875
12
Iteration 19800: Loss = -10924.93359375
Iteration 19900: Loss = -10924.935546875
1
Iteration 20000: Loss = -10924.93359375
Iteration 20100: Loss = -10924.9345703125
1
Iteration 20200: Loss = -10924.93359375
Iteration 20300: Loss = -10924.93359375
Iteration 20400: Loss = -10924.9345703125
1
Iteration 20500: Loss = -10924.9345703125
2
Iteration 20600: Loss = -10924.93359375
Iteration 20700: Loss = -10924.93359375
Iteration 20800: Loss = -10924.93359375
Iteration 20900: Loss = -10924.9345703125
1
Iteration 21000: Loss = -10924.93359375
Iteration 21100: Loss = -10924.93359375
Iteration 21200: Loss = -10924.9345703125
1
Iteration 21300: Loss = -10924.93359375
Iteration 21400: Loss = -10924.9345703125
1
Iteration 21500: Loss = -10924.93359375
Iteration 21600: Loss = -10924.93359375
Iteration 21700: Loss = -10924.9326171875
Iteration 21800: Loss = -10924.93359375
1
Iteration 21900: Loss = -10924.93359375
2
Iteration 22000: Loss = -10924.93359375
3
Iteration 22100: Loss = -10924.9326171875
Iteration 22200: Loss = -10924.93359375
1
Iteration 22300: Loss = -10924.9345703125
2
Iteration 22400: Loss = -10924.93359375
3
Iteration 22500: Loss = -10924.93359375
4
Iteration 22600: Loss = -10924.9326171875
Iteration 22700: Loss = -10924.9345703125
1
Iteration 22800: Loss = -10924.9345703125
2
Iteration 22900: Loss = -10924.9345703125
3
Iteration 23000: Loss = -10924.9326171875
Iteration 23100: Loss = -10924.9345703125
1
Iteration 23200: Loss = -10924.935546875
2
Iteration 23300: Loss = -10924.93359375
3
Iteration 23400: Loss = -10924.93359375
4
Iteration 23500: Loss = -10924.9345703125
5
Iteration 23600: Loss = -10924.93359375
6
Iteration 23700: Loss = -10924.931640625
Iteration 23800: Loss = -10924.9326171875
1
Iteration 23900: Loss = -10924.9345703125
2
Iteration 24000: Loss = -10924.9326171875
3
Iteration 24100: Loss = -10924.9345703125
4
Iteration 24200: Loss = -10924.9326171875
5
Iteration 24300: Loss = -10924.9345703125
6
Iteration 24400: Loss = -10924.931640625
Iteration 24500: Loss = -10924.93359375
1
Iteration 24600: Loss = -10924.93359375
2
Iteration 24700: Loss = -10924.93359375
3
Iteration 24800: Loss = -10924.93359375
4
Iteration 24900: Loss = -10924.93359375
5
Iteration 25000: Loss = -10924.93359375
6
Iteration 25100: Loss = -10924.93359375
7
Iteration 25200: Loss = -10924.93359375
8
Iteration 25300: Loss = -10924.9345703125
9
Iteration 25400: Loss = -10924.9326171875
10
Iteration 25500: Loss = -10924.9345703125
11
Iteration 25600: Loss = -10924.9345703125
12
Iteration 25700: Loss = -10924.9345703125
13
Iteration 25800: Loss = -10924.9345703125
14
Iteration 25900: Loss = -10924.931640625
Iteration 26000: Loss = -10924.93359375
1
Iteration 26100: Loss = -10924.93359375
2
Iteration 26200: Loss = -10924.93359375
3
Iteration 26300: Loss = -10924.93359375
4
Iteration 26400: Loss = -10924.9345703125
5
Iteration 26500: Loss = -10924.9326171875
6
Iteration 26600: Loss = -10924.935546875
7
Iteration 26700: Loss = -10924.9326171875
8
Iteration 26800: Loss = -10924.93359375
9
Iteration 26900: Loss = -10924.9345703125
10
Iteration 27000: Loss = -10924.9326171875
11
Iteration 27100: Loss = -10924.9326171875
12
Iteration 27200: Loss = -10924.93359375
13
Iteration 27300: Loss = -10924.9326171875
14
Iteration 27400: Loss = -10924.93359375
15
Stopping early at iteration 27400 due to no improvement.
pi: tensor([[9.5429e-01, 4.5705e-02],
        [9.9993e-01, 6.6208e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9486, 0.0514], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1556, 0.2158],
         [0.0178, 0.3499]],

        [[0.0966, 0.1668],
         [0.0457, 0.9153]],

        [[0.0188, 0.2102],
         [0.0120, 0.0902]],

        [[0.8844, 0.2299],
         [0.0309, 0.0448]],

        [[0.7274, 0.2399],
         [0.0889, 0.0378]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: -0.0013714698506328818
Average Adjusted Rand Index: -0.0027454699938062816
[0.8314068905433201, -0.0013714698506328818] [0.8343388628672684, -0.0027454699938062816] [10753.6357421875, 10924.93359375]
-------------------------------------
This iteration is 24
True Objective function: Loss = -10905.253699763682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35414.98828125
Iteration 100: Loss = -21117.462890625
Iteration 200: Loss = -13172.80859375
Iteration 300: Loss = -11853.1552734375
Iteration 400: Loss = -11537.3828125
Iteration 500: Loss = -11406.6787109375
Iteration 600: Loss = -11316.912109375
Iteration 700: Loss = -11264.6796875
Iteration 800: Loss = -11222.8017578125
Iteration 900: Loss = -11190.8408203125
Iteration 1000: Loss = -11168.0224609375
Iteration 1100: Loss = -11152.3076171875
Iteration 1200: Loss = -11139.546875
Iteration 1300: Loss = -11125.47265625
Iteration 1400: Loss = -11112.9931640625
Iteration 1500: Loss = -11105.1875
Iteration 1600: Loss = -11099.23828125
Iteration 1700: Loss = -11094.8642578125
Iteration 1800: Loss = -11090.4189453125
Iteration 1900: Loss = -11085.9541015625
Iteration 2000: Loss = -11081.81640625
Iteration 2100: Loss = -11077.8759765625
Iteration 2200: Loss = -11074.029296875
Iteration 2300: Loss = -11069.4677734375
Iteration 2400: Loss = -11064.669921875
Iteration 2500: Loss = -11061.166015625
Iteration 2600: Loss = -11058.7958984375
Iteration 2700: Loss = -11056.7216796875
Iteration 2800: Loss = -11054.5048828125
Iteration 2900: Loss = -11052.6318359375
Iteration 3000: Loss = -11051.30078125
Iteration 3100: Loss = -11050.3203125
Iteration 3200: Loss = -11049.5556640625
Iteration 3300: Loss = -11048.9345703125
Iteration 3400: Loss = -11048.408203125
Iteration 3500: Loss = -11047.9453125
Iteration 3600: Loss = -11047.5263671875
Iteration 3700: Loss = -11047.1298828125
Iteration 3800: Loss = -11046.69921875
Iteration 3900: Loss = -11045.84375
Iteration 4000: Loss = -11044.09765625
Iteration 4100: Loss = -11043.2890625
Iteration 4200: Loss = -11042.826171875
Iteration 4300: Loss = -11042.462890625
Iteration 4400: Loss = -11038.8046875
Iteration 4500: Loss = -11037.3564453125
Iteration 4600: Loss = -11037.0908203125
Iteration 4700: Loss = -11036.869140625
Iteration 4800: Loss = -11036.67578125
Iteration 4900: Loss = -11036.4951171875
Iteration 5000: Loss = -11031.369140625
Iteration 5100: Loss = -11030.982421875
Iteration 5200: Loss = -11030.78125
Iteration 5300: Loss = -11030.6171875
Iteration 5400: Loss = -11030.4716796875
Iteration 5500: Loss = -11030.3408203125
Iteration 5600: Loss = -11030.22265625
Iteration 5700: Loss = -11030.1142578125
Iteration 5800: Loss = -11030.013671875
Iteration 5900: Loss = -11029.91796875
Iteration 6000: Loss = -11029.828125
Iteration 6100: Loss = -11029.7412109375
Iteration 6200: Loss = -11028.5869140625
Iteration 6300: Loss = -11024.7099609375
Iteration 6400: Loss = -11024.5771484375
Iteration 6500: Loss = -11024.486328125
Iteration 6600: Loss = -11024.4140625
Iteration 6700: Loss = -11024.3515625
Iteration 6800: Loss = -11024.2958984375
Iteration 6900: Loss = -11024.2451171875
Iteration 7000: Loss = -11024.201171875
Iteration 7100: Loss = -11024.158203125
Iteration 7200: Loss = -11024.1201171875
Iteration 7300: Loss = -11024.08203125
Iteration 7400: Loss = -11024.05078125
Iteration 7500: Loss = -11024.0185546875
Iteration 7600: Loss = -11023.9892578125
Iteration 7700: Loss = -11023.9609375
Iteration 7800: Loss = -11023.93359375
Iteration 7900: Loss = -11023.91015625
Iteration 8000: Loss = -11023.8857421875
Iteration 8100: Loss = -11023.8642578125
Iteration 8200: Loss = -11023.84375
Iteration 8300: Loss = -11023.82421875
Iteration 8400: Loss = -11023.8056640625
Iteration 8500: Loss = -11023.7861328125
Iteration 8600: Loss = -11023.771484375
Iteration 8700: Loss = -11023.75390625
Iteration 8800: Loss = -11023.734375
Iteration 8900: Loss = -11023.720703125
Iteration 9000: Loss = -11023.7041015625
Iteration 9100: Loss = -11023.689453125
Iteration 9200: Loss = -11023.6796875
Iteration 9300: Loss = -11023.666015625
Iteration 9400: Loss = -11023.6572265625
Iteration 9500: Loss = -11023.64453125
Iteration 9600: Loss = -11023.6376953125
Iteration 9700: Loss = -11023.6279296875
Iteration 9800: Loss = -11023.6201171875
Iteration 9900: Loss = -11023.609375
Iteration 10000: Loss = -11023.6005859375
Iteration 10100: Loss = -11023.5869140625
Iteration 10200: Loss = -11023.5693359375
Iteration 10300: Loss = -11021.4111328125
Iteration 10400: Loss = -11021.0625
Iteration 10500: Loss = -11021.0390625
Iteration 10600: Loss = -11021.029296875
Iteration 10700: Loss = -11021.0224609375
Iteration 10800: Loss = -11021.015625
Iteration 10900: Loss = -11021.005859375
Iteration 11000: Loss = -11021.0
Iteration 11100: Loss = -11017.87109375
Iteration 11200: Loss = -11015.7138671875
Iteration 11300: Loss = -11015.5927734375
Iteration 11400: Loss = -11015.5361328125
Iteration 11500: Loss = -11015.501953125
Iteration 11600: Loss = -11015.478515625
Iteration 11700: Loss = -11015.4619140625
Iteration 11800: Loss = -11015.4462890625
Iteration 11900: Loss = -11015.4365234375
Iteration 12000: Loss = -11015.4267578125
Iteration 12100: Loss = -11015.4208984375
Iteration 12200: Loss = -11015.4130859375
Iteration 12300: Loss = -11015.408203125
Iteration 12400: Loss = -11015.4033203125
Iteration 12500: Loss = -11015.400390625
Iteration 12600: Loss = -11015.3955078125
Iteration 12700: Loss = -11015.3935546875
Iteration 12800: Loss = -11015.3896484375
Iteration 12900: Loss = -11015.3876953125
Iteration 13000: Loss = -11015.3857421875
Iteration 13100: Loss = -11015.3837890625
Iteration 13200: Loss = -11015.3798828125
Iteration 13300: Loss = -11015.3779296875
Iteration 13400: Loss = -11015.376953125
Iteration 13500: Loss = -11015.375
Iteration 13600: Loss = -11015.3720703125
Iteration 13700: Loss = -11015.373046875
1
Iteration 13800: Loss = -11015.3720703125
Iteration 13900: Loss = -11015.369140625
Iteration 14000: Loss = -11015.369140625
Iteration 14100: Loss = -11015.365234375
Iteration 14200: Loss = -11015.365234375
Iteration 14300: Loss = -11015.3642578125
Iteration 14400: Loss = -11015.3642578125
Iteration 14500: Loss = -11015.36328125
Iteration 14600: Loss = -11015.36328125
Iteration 14700: Loss = -11015.3623046875
Iteration 14800: Loss = -11015.359375
Iteration 14900: Loss = -11015.3603515625
1
Iteration 15000: Loss = -11015.359375
Iteration 15100: Loss = -11015.3603515625
1
Iteration 15200: Loss = -11015.359375
Iteration 15300: Loss = -11015.3583984375
Iteration 15400: Loss = -11015.357421875
Iteration 15500: Loss = -11015.357421875
Iteration 15600: Loss = -11015.3564453125
Iteration 15700: Loss = -11015.3564453125
Iteration 15800: Loss = -11015.3564453125
Iteration 15900: Loss = -11015.3544921875
Iteration 16000: Loss = -11015.353515625
Iteration 16100: Loss = -11015.3544921875
1
Iteration 16200: Loss = -11015.3544921875
2
Iteration 16300: Loss = -11015.3544921875
3
Iteration 16400: Loss = -11015.353515625
Iteration 16500: Loss = -11015.353515625
Iteration 16600: Loss = -11015.3525390625
Iteration 16700: Loss = -11015.3525390625
Iteration 16800: Loss = -11015.3525390625
Iteration 16900: Loss = -11015.3525390625
Iteration 17000: Loss = -11015.3515625
Iteration 17100: Loss = -11015.3505859375
Iteration 17200: Loss = -11015.3515625
1
Iteration 17300: Loss = -11015.349609375
Iteration 17400: Loss = -11015.3515625
1
Iteration 17500: Loss = -11015.3505859375
2
Iteration 17600: Loss = -11015.3505859375
3
Iteration 17700: Loss = -11015.349609375
Iteration 17800: Loss = -11015.349609375
Iteration 17900: Loss = -11015.349609375
Iteration 18000: Loss = -11015.34765625
Iteration 18100: Loss = -11015.341796875
Iteration 18200: Loss = -11015.3115234375
Iteration 18300: Loss = -11015.1787109375
Iteration 18400: Loss = -11015.09765625
Iteration 18500: Loss = -11015.060546875
Iteration 18600: Loss = -11015.04296875
Iteration 18700: Loss = -11015.0361328125
Iteration 18800: Loss = -11015.033203125
Iteration 18900: Loss = -11015.03125
Iteration 19000: Loss = -11015.0224609375
Iteration 19100: Loss = -11015.0107421875
Iteration 19200: Loss = -11015.0087890625
Iteration 19300: Loss = -11015.0009765625
Iteration 19400: Loss = -11015.0009765625
Iteration 19500: Loss = -11015.0009765625
Iteration 19600: Loss = -11015.0
Iteration 19700: Loss = -11015.0
Iteration 19800: Loss = -11014.998046875
Iteration 19900: Loss = -11014.9990234375
1
Iteration 20000: Loss = -11014.998046875
Iteration 20100: Loss = -11014.9970703125
Iteration 20200: Loss = -11014.998046875
1
Iteration 20300: Loss = -11014.9970703125
Iteration 20400: Loss = -11014.99609375
Iteration 20500: Loss = -11014.9912109375
Iteration 20600: Loss = -11014.9892578125
Iteration 20700: Loss = -11014.9892578125
Iteration 20800: Loss = -11014.9892578125
Iteration 20900: Loss = -11014.9892578125
Iteration 21000: Loss = -11014.9892578125
Iteration 21100: Loss = -11014.9892578125
Iteration 21200: Loss = -11014.9892578125
Iteration 21300: Loss = -11014.9892578125
Iteration 21400: Loss = -11014.98828125
Iteration 21500: Loss = -11014.98828125
Iteration 21600: Loss = -11014.9892578125
1
Iteration 21700: Loss = -11014.98828125
Iteration 21800: Loss = -11014.98828125
Iteration 21900: Loss = -11014.98828125
Iteration 22000: Loss = -11014.98828125
Iteration 22100: Loss = -11014.9892578125
1
Iteration 22200: Loss = -11014.98828125
Iteration 22300: Loss = -11014.98828125
Iteration 22400: Loss = -11014.9892578125
1
Iteration 22500: Loss = -11014.9892578125
2
Iteration 22600: Loss = -11014.9892578125
3
Iteration 22700: Loss = -11014.98828125
Iteration 22800: Loss = -11014.9873046875
Iteration 22900: Loss = -11014.9873046875
Iteration 23000: Loss = -11014.98828125
1
Iteration 23100: Loss = -11014.9892578125
2
Iteration 23200: Loss = -11014.98828125
3
Iteration 23300: Loss = -11014.98828125
4
Iteration 23400: Loss = -11014.98828125
5
Iteration 23500: Loss = -11014.9892578125
6
Iteration 23600: Loss = -11014.9892578125
7
Iteration 23700: Loss = -11014.9892578125
8
Iteration 23800: Loss = -11014.990234375
9
Iteration 23900: Loss = -11014.98828125
10
Iteration 24000: Loss = -11014.98828125
11
Iteration 24100: Loss = -11014.98828125
12
Iteration 24200: Loss = -11014.990234375
13
Iteration 24300: Loss = -11014.9892578125
14
Iteration 24400: Loss = -11014.98828125
15
Stopping early at iteration 24400 due to no improvement.
pi: tensor([[1.0000e+00, 3.4144e-06],
        [8.2864e-01, 1.7136e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8542, 0.1458], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1600, 0.2042],
         [0.9089, 0.3110]],

        [[0.3301, 0.2069],
         [0.5697, 0.0153]],

        [[0.8704, 0.1832],
         [0.4568, 0.8084]],

        [[0.8527, 0.5123],
         [0.9702, 0.9917]],

        [[0.9050, 0.1629],
         [0.0899, 0.0563]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.038727527943711815
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013262016243261081
Average Adjusted Rand Index: -0.007745505588742363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47298.31640625
Iteration 100: Loss = -27394.873046875
Iteration 200: Loss = -14960.478515625
Iteration 300: Loss = -12161.8408203125
Iteration 400: Loss = -11585.65234375
Iteration 500: Loss = -11348.2021484375
Iteration 600: Loss = -11239.361328125
Iteration 700: Loss = -11177.0673828125
Iteration 800: Loss = -11141.3525390625
Iteration 900: Loss = -11117.6103515625
Iteration 1000: Loss = -11094.8828125
Iteration 1100: Loss = -11081.9755859375
Iteration 1200: Loss = -11073.0771484375
Iteration 1300: Loss = -11066.0400390625
Iteration 1400: Loss = -11060.29296875
Iteration 1500: Loss = -11055.4697265625
Iteration 1600: Loss = -11051.27734375
Iteration 1700: Loss = -11047.4873046875
Iteration 1800: Loss = -11044.0380859375
Iteration 1900: Loss = -11041.115234375
Iteration 2000: Loss = -11038.7490234375
Iteration 2100: Loss = -11036.76953125
Iteration 2200: Loss = -11035.099609375
Iteration 2300: Loss = -11033.6640625
Iteration 2400: Loss = -11032.400390625
Iteration 2500: Loss = -11031.27734375
Iteration 2600: Loss = -11030.265625
Iteration 2700: Loss = -11029.361328125
Iteration 2800: Loss = -11028.5556640625
Iteration 2900: Loss = -11027.8271484375
Iteration 3000: Loss = -11027.1630859375
Iteration 3100: Loss = -11026.5576171875
Iteration 3200: Loss = -11026.005859375
Iteration 3300: Loss = -11025.501953125
Iteration 3400: Loss = -11025.037109375
Iteration 3500: Loss = -11024.609375
Iteration 3600: Loss = -11024.216796875
Iteration 3700: Loss = -11023.8515625
Iteration 3800: Loss = -11023.5146484375
Iteration 3900: Loss = -11023.2041015625
Iteration 4000: Loss = -11022.9130859375
Iteration 4100: Loss = -11022.6435546875
Iteration 4200: Loss = -11022.3916015625
Iteration 4300: Loss = -11022.15625
Iteration 4400: Loss = -11021.9384765625
Iteration 4500: Loss = -11021.7353515625
Iteration 4600: Loss = -11021.5439453125
Iteration 4700: Loss = -11021.365234375
Iteration 4800: Loss = -11021.1982421875
Iteration 4900: Loss = -11021.0419921875
Iteration 5000: Loss = -11020.8935546875
Iteration 5100: Loss = -11020.7548828125
Iteration 5200: Loss = -11020.626953125
Iteration 5300: Loss = -11020.501953125
Iteration 5400: Loss = -11020.38671875
Iteration 5500: Loss = -11020.27734375
Iteration 5600: Loss = -11020.1767578125
Iteration 5700: Loss = -11020.0791015625
Iteration 5800: Loss = -11019.986328125
Iteration 5900: Loss = -11019.9013671875
Iteration 6000: Loss = -11019.8203125
Iteration 6100: Loss = -11019.7431640625
Iteration 6200: Loss = -11019.671875
Iteration 6300: Loss = -11019.6015625
Iteration 6400: Loss = -11019.537109375
Iteration 6500: Loss = -11019.4736328125
Iteration 6600: Loss = -11019.4130859375
Iteration 6700: Loss = -11019.3583984375
Iteration 6800: Loss = -11019.306640625
Iteration 6900: Loss = -11019.255859375
Iteration 7000: Loss = -11019.208984375
Iteration 7100: Loss = -11019.1630859375
Iteration 7200: Loss = -11019.1220703125
Iteration 7300: Loss = -11019.080078125
Iteration 7400: Loss = -11019.041015625
Iteration 7500: Loss = -11019.00390625
Iteration 7600: Loss = -11018.9677734375
Iteration 7700: Loss = -11018.9345703125
Iteration 7800: Loss = -11018.900390625
Iteration 7900: Loss = -11018.8662109375
Iteration 8000: Loss = -11018.8359375
Iteration 8100: Loss = -11018.8046875
Iteration 8200: Loss = -11018.7744140625
Iteration 8300: Loss = -11018.7421875
Iteration 8400: Loss = -11018.7109375
Iteration 8500: Loss = -11018.6806640625
Iteration 8600: Loss = -11018.6494140625
Iteration 8700: Loss = -11018.619140625
Iteration 8800: Loss = -11018.587890625
Iteration 8900: Loss = -11018.560546875
Iteration 9000: Loss = -11018.53515625
Iteration 9100: Loss = -11018.509765625
Iteration 9200: Loss = -11018.486328125
Iteration 9300: Loss = -11018.46484375
Iteration 9400: Loss = -11018.4453125
Iteration 9500: Loss = -11018.4267578125
Iteration 9600: Loss = -11018.4072265625
Iteration 9700: Loss = -11018.392578125
Iteration 9800: Loss = -11018.3759765625
Iteration 9900: Loss = -11018.3583984375
Iteration 10000: Loss = -11018.3447265625
Iteration 10100: Loss = -11018.3291015625
Iteration 10200: Loss = -11018.31640625
Iteration 10300: Loss = -11018.302734375
Iteration 10400: Loss = -11018.2880859375
Iteration 10500: Loss = -11018.27734375
Iteration 10600: Loss = -11018.2626953125
Iteration 10700: Loss = -11018.251953125
Iteration 10800: Loss = -11018.236328125
Iteration 10900: Loss = -11018.2236328125
Iteration 11000: Loss = -11018.2099609375
Iteration 11100: Loss = -11018.197265625
Iteration 11200: Loss = -11018.18359375
Iteration 11300: Loss = -11018.1689453125
Iteration 11400: Loss = -11018.15234375
Iteration 11500: Loss = -11018.1318359375
Iteration 11600: Loss = -11018.111328125
Iteration 11700: Loss = -11018.0869140625
Iteration 11800: Loss = -11018.056640625
Iteration 11900: Loss = -11018.0205078125
Iteration 12000: Loss = -11017.9716796875
Iteration 12100: Loss = -11017.9052734375
Iteration 12200: Loss = -11017.8095703125
Iteration 12300: Loss = -11017.654296875
Iteration 12400: Loss = -11017.4033203125
Iteration 12500: Loss = -11017.0458984375
Iteration 12600: Loss = -11016.59375
Iteration 12700: Loss = -11016.17578125
Iteration 12800: Loss = -11015.927734375
Iteration 12900: Loss = -11015.771484375
Iteration 13000: Loss = -11015.6923828125
Iteration 13100: Loss = -11015.6083984375
Iteration 13200: Loss = -11015.51953125
Iteration 13300: Loss = -11015.451171875
Iteration 13400: Loss = -11015.388671875
Iteration 13500: Loss = -11015.3271484375
Iteration 13600: Loss = -11015.271484375
Iteration 13700: Loss = -11015.2109375
Iteration 13800: Loss = -11013.90234375
Iteration 13900: Loss = -11013.572265625
Iteration 14000: Loss = -11013.484375
Iteration 14100: Loss = -11013.37890625
Iteration 14200: Loss = -11013.3466796875
Iteration 14300: Loss = -11013.33203125
Iteration 14400: Loss = -11013.3203125
Iteration 14500: Loss = -11013.3115234375
Iteration 14600: Loss = -11013.3037109375
Iteration 14700: Loss = -11013.2939453125
Iteration 14800: Loss = -11013.2890625
Iteration 14900: Loss = -11013.283203125
Iteration 15000: Loss = -11013.2802734375
Iteration 15100: Loss = -11013.2763671875
Iteration 15200: Loss = -11013.2734375
Iteration 15300: Loss = -11013.271484375
Iteration 15400: Loss = -11013.26953125
Iteration 15500: Loss = -11013.26171875
Iteration 15600: Loss = -11013.244140625
Iteration 15700: Loss = -11013.189453125
Iteration 15800: Loss = -11013.1865234375
Iteration 15900: Loss = -11013.185546875
Iteration 16000: Loss = -11013.18359375
Iteration 16100: Loss = -11013.1826171875
Iteration 16200: Loss = -11013.1826171875
Iteration 16300: Loss = -11013.181640625
Iteration 16400: Loss = -11013.1806640625
Iteration 16500: Loss = -11013.1806640625
Iteration 16600: Loss = -11013.1806640625
Iteration 16700: Loss = -11013.1806640625
Iteration 16800: Loss = -11013.1787109375
Iteration 16900: Loss = -11013.1787109375
Iteration 17000: Loss = -11013.1796875
1
Iteration 17100: Loss = -11013.1787109375
Iteration 17200: Loss = -11013.177734375
Iteration 17300: Loss = -11013.1767578125
Iteration 17400: Loss = -11013.177734375
1
Iteration 17500: Loss = -11013.1767578125
Iteration 17600: Loss = -11013.1767578125
Iteration 17700: Loss = -11013.1767578125
Iteration 17800: Loss = -11013.17578125
Iteration 17900: Loss = -11013.1767578125
1
Iteration 18000: Loss = -11013.17578125
Iteration 18100: Loss = -11013.1748046875
Iteration 18200: Loss = -11013.17578125
1
Iteration 18300: Loss = -11013.1748046875
Iteration 18400: Loss = -11013.1767578125
1
Iteration 18500: Loss = -11013.17578125
2
Iteration 18600: Loss = -11013.17578125
3
Iteration 18700: Loss = -11013.1767578125
4
Iteration 18800: Loss = -11013.1767578125
5
Iteration 18900: Loss = -11013.17578125
6
Iteration 19000: Loss = -11013.17578125
7
Iteration 19100: Loss = -11013.17578125
8
Iteration 19200: Loss = -11013.17578125
9
Iteration 19300: Loss = -11013.17578125
10
Iteration 19400: Loss = -11013.17578125
11
Iteration 19500: Loss = -11013.1767578125
12
Iteration 19600: Loss = -11013.173828125
Iteration 19700: Loss = -11013.177734375
1
Iteration 19800: Loss = -11013.1748046875
2
Iteration 19900: Loss = -11013.1748046875
3
Iteration 20000: Loss = -11013.173828125
Iteration 20100: Loss = -11013.1748046875
1
Iteration 20200: Loss = -11013.1748046875
2
Iteration 20300: Loss = -11013.1748046875
3
Iteration 20400: Loss = -11013.173828125
Iteration 20500: Loss = -11013.1748046875
1
Iteration 20600: Loss = -11013.1748046875
2
Iteration 20700: Loss = -11013.173828125
Iteration 20800: Loss = -11013.17578125
1
Iteration 20900: Loss = -11013.1748046875
2
Iteration 21000: Loss = -11013.17578125
3
Iteration 21100: Loss = -11013.1748046875
4
Iteration 21200: Loss = -11013.173828125
Iteration 21300: Loss = -11013.17578125
1
Iteration 21400: Loss = -11013.17578125
2
Iteration 21500: Loss = -11013.17578125
3
Iteration 21600: Loss = -11013.1748046875
4
Iteration 21700: Loss = -11013.17578125
5
Iteration 21800: Loss = -11013.173828125
Iteration 21900: Loss = -11013.1748046875
1
Iteration 22000: Loss = -11013.17578125
2
Iteration 22100: Loss = -11013.173828125
Iteration 22200: Loss = -11013.17578125
1
Iteration 22300: Loss = -11013.1748046875
2
Iteration 22400: Loss = -11013.1748046875
3
Iteration 22500: Loss = -11013.1748046875
4
Iteration 22600: Loss = -11013.173828125
Iteration 22700: Loss = -11013.1748046875
1
Iteration 22800: Loss = -11013.1748046875
2
Iteration 22900: Loss = -11013.173828125
Iteration 23000: Loss = -11013.1748046875
1
Iteration 23100: Loss = -11013.173828125
Iteration 23200: Loss = -11013.173828125
Iteration 23300: Loss = -11013.1748046875
1
Iteration 23400: Loss = -11013.17578125
2
Iteration 23500: Loss = -11013.17578125
3
Iteration 23600: Loss = -11013.17578125
4
Iteration 23700: Loss = -11013.1728515625
Iteration 23800: Loss = -11013.1748046875
1
Iteration 23900: Loss = -11013.1748046875
2
Iteration 24000: Loss = -11013.1748046875
3
Iteration 24100: Loss = -11013.1748046875
4
Iteration 24200: Loss = -11013.173828125
5
Iteration 24300: Loss = -11013.173828125
6
Iteration 24400: Loss = -11013.1748046875
7
Iteration 24500: Loss = -11013.173828125
8
Iteration 24600: Loss = -11013.1748046875
9
Iteration 24700: Loss = -11013.1748046875
10
Iteration 24800: Loss = -11013.1748046875
11
Iteration 24900: Loss = -11013.1748046875
12
Iteration 25000: Loss = -11013.1748046875
13
Iteration 25100: Loss = -11013.1748046875
14
Iteration 25200: Loss = -11013.1748046875
15
Stopping early at iteration 25200 due to no improvement.
pi: tensor([[4.4062e-01, 5.5938e-01],
        [1.8555e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1185, 0.8815], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3201, 0.2103],
         [0.9764, 0.1602]],

        [[0.1307, 0.2029],
         [0.7809, 0.0173]],

        [[0.1861, 0.1864],
         [0.8079, 0.0266]],

        [[0.0141, 0.0739],
         [0.0119, 0.0846]],

        [[0.2807, 0.1246],
         [0.4566, 0.0156]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.036735798464907495
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.0011022058511799635
Average Adjusted Rand Index: -0.008723493933470014
[-0.0013262016243261081, -0.0011022058511799635] [-0.007745505588742363, -0.008723493933470014] [11014.98828125, 11013.1748046875]
-------------------------------------
This iteration is 25
True Objective function: Loss = -10882.900996510527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37400.5
Iteration 100: Loss = -22306.3984375
Iteration 200: Loss = -13820.1513671875
Iteration 300: Loss = -11539.126953125
Iteration 400: Loss = -11179.0234375
Iteration 500: Loss = -11088.599609375
Iteration 600: Loss = -11050.37109375
Iteration 700: Loss = -11024.96484375
Iteration 800: Loss = -11012.7724609375
Iteration 900: Loss = -11004.654296875
Iteration 1000: Loss = -10998.8271484375
Iteration 1100: Loss = -10994.4765625
Iteration 1200: Loss = -10991.125
Iteration 1300: Loss = -10988.48046875
Iteration 1400: Loss = -10986.3515625
Iteration 1500: Loss = -10984.607421875
Iteration 1600: Loss = -10983.16015625
Iteration 1700: Loss = -10981.9462890625
Iteration 1800: Loss = -10980.91796875
Iteration 1900: Loss = -10980.037109375
Iteration 2000: Loss = -10979.279296875
Iteration 2100: Loss = -10978.6181640625
Iteration 2200: Loss = -10978.0390625
Iteration 2300: Loss = -10977.5283203125
Iteration 2400: Loss = -10977.078125
Iteration 2500: Loss = -10976.6748046875
Iteration 2600: Loss = -10976.3134765625
Iteration 2700: Loss = -10975.9921875
Iteration 2800: Loss = -10975.69921875
Iteration 2900: Loss = -10975.4296875
Iteration 3000: Loss = -10975.1904296875
Iteration 3100: Loss = -10974.97265625
Iteration 3200: Loss = -10974.7734375
Iteration 3300: Loss = -10974.5908203125
Iteration 3400: Loss = -10974.4248046875
Iteration 3500: Loss = -10974.2685546875
Iteration 3600: Loss = -10974.126953125
Iteration 3700: Loss = -10973.99609375
Iteration 3800: Loss = -10973.87890625
Iteration 3900: Loss = -10973.7705078125
Iteration 4000: Loss = -10973.671875
Iteration 4100: Loss = -10973.5830078125
Iteration 4200: Loss = -10973.5009765625
Iteration 4300: Loss = -10973.4267578125
Iteration 4400: Loss = -10973.3583984375
Iteration 4500: Loss = -10973.2958984375
Iteration 4600: Loss = -10973.2392578125
Iteration 4700: Loss = -10973.1884765625
Iteration 4800: Loss = -10973.140625
Iteration 4900: Loss = -10973.0986328125
Iteration 5000: Loss = -10973.060546875
Iteration 5100: Loss = -10973.0263671875
Iteration 5200: Loss = -10972.9951171875
Iteration 5300: Loss = -10972.96484375
Iteration 5400: Loss = -10972.9375
Iteration 5500: Loss = -10972.9140625
Iteration 5600: Loss = -10972.890625
Iteration 5700: Loss = -10972.8701171875
Iteration 5800: Loss = -10972.8515625
Iteration 5900: Loss = -10972.8330078125
Iteration 6000: Loss = -10972.8173828125
Iteration 6100: Loss = -10972.80078125
Iteration 6200: Loss = -10972.7841796875
Iteration 6300: Loss = -10972.7705078125
Iteration 6400: Loss = -10972.7578125
Iteration 6500: Loss = -10972.7470703125
Iteration 6600: Loss = -10972.7353515625
Iteration 6700: Loss = -10972.7265625
Iteration 6800: Loss = -10972.716796875
Iteration 6900: Loss = -10972.70703125
Iteration 7000: Loss = -10972.697265625
Iteration 7100: Loss = -10972.689453125
Iteration 7200: Loss = -10972.6806640625
Iteration 7300: Loss = -10972.673828125
Iteration 7400: Loss = -10972.666015625
Iteration 7500: Loss = -10972.658203125
Iteration 7600: Loss = -10972.6513671875
Iteration 7700: Loss = -10972.646484375
Iteration 7800: Loss = -10972.638671875
Iteration 7900: Loss = -10972.634765625
Iteration 8000: Loss = -10972.6279296875
Iteration 8100: Loss = -10972.625
Iteration 8200: Loss = -10972.6181640625
Iteration 8300: Loss = -10972.6142578125
Iteration 8400: Loss = -10972.6083984375
Iteration 8500: Loss = -10972.60546875
Iteration 8600: Loss = -10972.6015625
Iteration 8700: Loss = -10972.59765625
Iteration 8800: Loss = -10972.5927734375
Iteration 8900: Loss = -10972.58984375
Iteration 9000: Loss = -10972.5859375
Iteration 9100: Loss = -10972.5849609375
Iteration 9200: Loss = -10972.5810546875
Iteration 9300: Loss = -10972.580078125
Iteration 9400: Loss = -10972.576171875
Iteration 9500: Loss = -10972.57421875
Iteration 9600: Loss = -10972.572265625
Iteration 9700: Loss = -10972.5693359375
Iteration 9800: Loss = -10972.5673828125
Iteration 9900: Loss = -10972.5654296875
Iteration 10000: Loss = -10972.564453125
Iteration 10100: Loss = -10972.5615234375
Iteration 10200: Loss = -10972.5615234375
Iteration 10300: Loss = -10972.55859375
Iteration 10400: Loss = -10972.5556640625
Iteration 10500: Loss = -10972.556640625
1
Iteration 10600: Loss = -10972.5546875
Iteration 10700: Loss = -10972.552734375
Iteration 10800: Loss = -10972.5517578125
Iteration 10900: Loss = -10972.55078125
Iteration 11000: Loss = -10972.5478515625
Iteration 11100: Loss = -10972.5478515625
Iteration 11200: Loss = -10972.546875
Iteration 11300: Loss = -10972.544921875
Iteration 11400: Loss = -10972.544921875
Iteration 11500: Loss = -10972.5419921875
Iteration 11600: Loss = -10972.5400390625
Iteration 11700: Loss = -10972.5390625
Iteration 11800: Loss = -10972.537109375
Iteration 11900: Loss = -10972.5341796875
Iteration 12000: Loss = -10972.529296875
Iteration 12100: Loss = -10972.5166015625
Iteration 12200: Loss = -10972.48046875
Iteration 12300: Loss = -10972.4052734375
Iteration 12400: Loss = -10967.0908203125
Iteration 12500: Loss = -10966.48828125
Iteration 12600: Loss = -10965.447265625
Iteration 12700: Loss = -10965.0615234375
Iteration 12800: Loss = -10964.7744140625
Iteration 12900: Loss = -10963.8251953125
Iteration 13000: Loss = -10963.7626953125
Iteration 13100: Loss = -10963.732421875
Iteration 13200: Loss = -10963.697265625
Iteration 13300: Loss = -10963.6826171875
Iteration 13400: Loss = -10963.671875
Iteration 13500: Loss = -10963.6552734375
Iteration 13600: Loss = -10963.63671875
Iteration 13700: Loss = -10963.6298828125
Iteration 13800: Loss = -10963.626953125
Iteration 13900: Loss = -10963.62109375
Iteration 14000: Loss = -10963.615234375
Iteration 14100: Loss = -10963.6083984375
Iteration 14200: Loss = -10963.607421875
Iteration 14300: Loss = -10963.6044921875
Iteration 14400: Loss = -10963.6015625
Iteration 14500: Loss = -10963.6005859375
Iteration 14600: Loss = -10963.5986328125
Iteration 14700: Loss = -10963.5966796875
Iteration 14800: Loss = -10963.59375
Iteration 14900: Loss = -10963.5927734375
Iteration 15000: Loss = -10963.591796875
Iteration 15100: Loss = -10963.58984375
Iteration 15200: Loss = -10963.5908203125
1
Iteration 15300: Loss = -10963.5908203125
2
Iteration 15400: Loss = -10963.5888671875
Iteration 15500: Loss = -10963.5830078125
Iteration 15600: Loss = -10963.5791015625
Iteration 15700: Loss = -10963.578125
Iteration 15800: Loss = -10963.5771484375
Iteration 15900: Loss = -10963.576171875
Iteration 16000: Loss = -10963.5771484375
1
Iteration 16100: Loss = -10963.57421875
Iteration 16200: Loss = -10963.5751953125
1
Iteration 16300: Loss = -10963.57421875
Iteration 16400: Loss = -10963.57421875
Iteration 16500: Loss = -10963.5732421875
Iteration 16600: Loss = -10963.5751953125
1
Iteration 16700: Loss = -10963.572265625
Iteration 16800: Loss = -10963.5712890625
Iteration 16900: Loss = -10963.5712890625
Iteration 17000: Loss = -10963.5732421875
1
Iteration 17100: Loss = -10963.5712890625
Iteration 17200: Loss = -10963.572265625
1
Iteration 17300: Loss = -10963.5703125
Iteration 17400: Loss = -10963.5712890625
1
Iteration 17500: Loss = -10963.5712890625
2
Iteration 17600: Loss = -10963.5703125
Iteration 17700: Loss = -10963.5703125
Iteration 17800: Loss = -10963.5703125
Iteration 17900: Loss = -10963.5693359375
Iteration 18000: Loss = -10963.568359375
Iteration 18100: Loss = -10963.568359375
Iteration 18200: Loss = -10963.568359375
Iteration 18300: Loss = -10963.568359375
Iteration 18400: Loss = -10963.5673828125
Iteration 18500: Loss = -10963.5673828125
Iteration 18600: Loss = -10963.5673828125
Iteration 18700: Loss = -10963.568359375
1
Iteration 18800: Loss = -10963.5654296875
Iteration 18900: Loss = -10963.56640625
1
Iteration 19000: Loss = -10963.5654296875
Iteration 19100: Loss = -10963.5654296875
Iteration 19200: Loss = -10963.5712890625
1
Iteration 19300: Loss = -10963.564453125
Iteration 19400: Loss = -10963.5654296875
1
Iteration 19500: Loss = -10963.564453125
Iteration 19600: Loss = -10963.564453125
Iteration 19700: Loss = -10963.5654296875
1
Iteration 19800: Loss = -10963.564453125
Iteration 19900: Loss = -10963.5634765625
Iteration 20000: Loss = -10963.564453125
1
Iteration 20100: Loss = -10963.5693359375
2
Iteration 20200: Loss = -10963.564453125
3
Iteration 20300: Loss = -10963.564453125
4
Iteration 20400: Loss = -10963.564453125
5
Iteration 20500: Loss = -10963.5634765625
Iteration 20600: Loss = -10963.564453125
1
Iteration 20700: Loss = -10963.5634765625
Iteration 20800: Loss = -10963.5634765625
Iteration 20900: Loss = -10963.5625
Iteration 21000: Loss = -10963.5634765625
1
Iteration 21100: Loss = -10963.564453125
2
Iteration 21200: Loss = -10963.5625
Iteration 21300: Loss = -10963.5654296875
1
Iteration 21400: Loss = -10963.5634765625
2
Iteration 21500: Loss = -10963.5634765625
3
Iteration 21600: Loss = -10963.5634765625
4
Iteration 21700: Loss = -10963.5634765625
5
Iteration 21800: Loss = -10963.5634765625
6
Iteration 21900: Loss = -10963.5634765625
7
Iteration 22000: Loss = -10963.5634765625
8
Iteration 22100: Loss = -10963.5634765625
9
Iteration 22200: Loss = -10963.5625
Iteration 22300: Loss = -10963.564453125
1
Iteration 22400: Loss = -10963.5634765625
2
Iteration 22500: Loss = -10963.5625
Iteration 22600: Loss = -10963.5625
Iteration 22700: Loss = -10963.5625
Iteration 22800: Loss = -10963.5634765625
1
Iteration 22900: Loss = -10963.5634765625
2
Iteration 23000: Loss = -10963.5615234375
Iteration 23100: Loss = -10963.5615234375
Iteration 23200: Loss = -10963.5615234375
Iteration 23300: Loss = -10963.5625
1
Iteration 23400: Loss = -10963.560546875
Iteration 23500: Loss = -10963.5595703125
Iteration 23600: Loss = -10963.5595703125
Iteration 23700: Loss = -10963.5615234375
1
Iteration 23800: Loss = -10963.560546875
2
Iteration 23900: Loss = -10963.560546875
3
Iteration 24000: Loss = -10963.5595703125
Iteration 24100: Loss = -10963.560546875
1
Iteration 24200: Loss = -10963.5615234375
2
Iteration 24300: Loss = -10963.560546875
3
Iteration 24400: Loss = -10963.5615234375
4
Iteration 24500: Loss = -10963.560546875
5
Iteration 24600: Loss = -10963.560546875
6
Iteration 24700: Loss = -10963.560546875
7
Iteration 24800: Loss = -10963.560546875
8
Iteration 24900: Loss = -10963.560546875
9
Iteration 25000: Loss = -10963.5615234375
10
Iteration 25100: Loss = -10963.560546875
11
Iteration 25200: Loss = -10963.560546875
12
Iteration 25300: Loss = -10963.5634765625
13
Iteration 25400: Loss = -10963.560546875
14
Iteration 25500: Loss = -10963.560546875
15
Stopping early at iteration 25500 due to no improvement.
pi: tensor([[9.9996e-01, 3.9223e-05],
        [1.3698e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0412, 0.9588], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4777, 0.1468],
         [0.7967, 0.1650]],

        [[0.1662, 0.1051],
         [0.5616, 0.5948]],

        [[0.0701, 0.0892],
         [0.4463, 0.1310]],

        [[0.0096, 0.1415],
         [0.8948, 0.0079]],

        [[0.0323, 0.1436],
         [0.9864, 0.1642]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.002911726262720568
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.00485601903559462
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.011530202595462608
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
Global Adjusted Rand Index: 0.000426286523559823
Average Adjusted Rand Index: -0.00010802337139034952
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21433.701171875
Iteration 100: Loss = -14883.2392578125
Iteration 200: Loss = -11923.9365234375
Iteration 300: Loss = -11141.4912109375
Iteration 400: Loss = -11049.27734375
Iteration 500: Loss = -11016.337890625
Iteration 600: Loss = -10999.1103515625
Iteration 700: Loss = -10989.876953125
Iteration 800: Loss = -10984.88671875
Iteration 900: Loss = -10982.12890625
Iteration 1000: Loss = -10980.1884765625
Iteration 1100: Loss = -10978.6630859375
Iteration 1200: Loss = -10977.4873046875
Iteration 1300: Loss = -10976.67578125
Iteration 1400: Loss = -10976.0703125
Iteration 1500: Loss = -10975.5927734375
Iteration 1600: Loss = -10975.205078125
Iteration 1700: Loss = -10974.8837890625
Iteration 1800: Loss = -10974.615234375
Iteration 1900: Loss = -10974.384765625
Iteration 2000: Loss = -10974.1875
Iteration 2100: Loss = -10974.015625
Iteration 2200: Loss = -10973.865234375
Iteration 2300: Loss = -10973.734375
Iteration 2400: Loss = -10973.6171875
Iteration 2500: Loss = -10973.51171875
Iteration 2600: Loss = -10973.4189453125
Iteration 2700: Loss = -10973.3349609375
Iteration 2800: Loss = -10973.259765625
Iteration 2900: Loss = -10973.189453125
Iteration 3000: Loss = -10973.1279296875
Iteration 3100: Loss = -10973.0712890625
Iteration 3200: Loss = -10973.0185546875
Iteration 3300: Loss = -10972.9716796875
Iteration 3400: Loss = -10972.9267578125
Iteration 3500: Loss = -10972.8876953125
Iteration 3600: Loss = -10972.8486328125
Iteration 3700: Loss = -10972.8154296875
Iteration 3800: Loss = -10972.7841796875
Iteration 3900: Loss = -10972.751953125
Iteration 4000: Loss = -10972.7255859375
Iteration 4100: Loss = -10972.701171875
Iteration 4200: Loss = -10972.6767578125
Iteration 4300: Loss = -10972.654296875
Iteration 4400: Loss = -10972.6318359375
Iteration 4500: Loss = -10972.61328125
Iteration 4600: Loss = -10972.595703125
Iteration 4700: Loss = -10972.5771484375
Iteration 4800: Loss = -10972.55859375
Iteration 4900: Loss = -10972.5439453125
Iteration 5000: Loss = -10972.52734375
Iteration 5100: Loss = -10972.5126953125
Iteration 5200: Loss = -10972.49609375
Iteration 5300: Loss = -10972.4814453125
Iteration 5400: Loss = -10972.46875
Iteration 5500: Loss = -10972.4541015625
Iteration 5600: Loss = -10972.4384765625
Iteration 5700: Loss = -10972.423828125
Iteration 5800: Loss = -10972.41015625
Iteration 5900: Loss = -10972.392578125
Iteration 6000: Loss = -10972.375
Iteration 6100: Loss = -10972.3583984375
Iteration 6200: Loss = -10972.3359375
Iteration 6300: Loss = -10972.314453125
Iteration 6400: Loss = -10972.291015625
Iteration 6500: Loss = -10972.265625
Iteration 6600: Loss = -10972.240234375
Iteration 6700: Loss = -10972.2119140625
Iteration 6800: Loss = -10972.1787109375
Iteration 6900: Loss = -10972.140625
Iteration 7000: Loss = -10972.0986328125
Iteration 7100: Loss = -10972.048828125
Iteration 7200: Loss = -10971.9921875
Iteration 7300: Loss = -10971.9248046875
Iteration 7400: Loss = -10971.8505859375
Iteration 7500: Loss = -10971.7685546875
Iteration 7600: Loss = -10971.6787109375
Iteration 7700: Loss = -10971.5859375
Iteration 7800: Loss = -10971.4931640625
Iteration 7900: Loss = -10971.3955078125
Iteration 8000: Loss = -10971.3037109375
Iteration 8100: Loss = -10971.21484375
Iteration 8200: Loss = -10971.1396484375
Iteration 8300: Loss = -10971.083984375
Iteration 8400: Loss = -10971.04296875
Iteration 8500: Loss = -10971.0126953125
Iteration 8600: Loss = -10970.990234375
Iteration 8700: Loss = -10970.9716796875
Iteration 8800: Loss = -10970.9580078125
Iteration 8900: Loss = -10970.947265625
Iteration 9000: Loss = -10970.939453125
Iteration 9100: Loss = -10970.9326171875
Iteration 9200: Loss = -10970.92578125
Iteration 9300: Loss = -10970.919921875
Iteration 9400: Loss = -10970.9140625
Iteration 9500: Loss = -10970.91015625
Iteration 9600: Loss = -10970.908203125
Iteration 9700: Loss = -10970.9052734375
Iteration 9800: Loss = -10970.90234375
Iteration 9900: Loss = -10970.8994140625
Iteration 10000: Loss = -10970.8994140625
Iteration 10100: Loss = -10970.896484375
Iteration 10200: Loss = -10970.89453125
Iteration 10300: Loss = -10970.892578125
Iteration 10400: Loss = -10970.8935546875
1
Iteration 10500: Loss = -10970.890625
Iteration 10600: Loss = -10970.8896484375
Iteration 10700: Loss = -10970.888671875
Iteration 10800: Loss = -10970.888671875
Iteration 10900: Loss = -10970.8857421875
Iteration 11000: Loss = -10970.8857421875
Iteration 11100: Loss = -10970.8828125
Iteration 11200: Loss = -10970.8828125
Iteration 11300: Loss = -10970.8798828125
Iteration 11400: Loss = -10970.8779296875
Iteration 11500: Loss = -10970.875
Iteration 11600: Loss = -10970.87109375
Iteration 11700: Loss = -10970.8681640625
Iteration 11800: Loss = -10970.8583984375
Iteration 11900: Loss = -10970.84765625
Iteration 12000: Loss = -10970.83203125
Iteration 12100: Loss = -10970.802734375
Iteration 12200: Loss = -10970.7529296875
Iteration 12300: Loss = -10970.65234375
Iteration 12400: Loss = -10970.224609375
Iteration 12500: Loss = -10840.7265625
Iteration 12600: Loss = -10838.6181640625
Iteration 12700: Loss = -10838.3564453125
Iteration 12800: Loss = -10838.244140625
Iteration 12900: Loss = -10838.1826171875
Iteration 13000: Loss = -10838.1455078125
Iteration 13100: Loss = -10838.119140625
Iteration 13200: Loss = -10838.1005859375
Iteration 13300: Loss = -10838.0869140625
Iteration 13400: Loss = -10838.0771484375
Iteration 13500: Loss = -10838.068359375
Iteration 13600: Loss = -10838.0615234375
Iteration 13700: Loss = -10838.0556640625
Iteration 13800: Loss = -10838.0498046875
Iteration 13900: Loss = -10838.046875
Iteration 14000: Loss = -10838.0419921875
Iteration 14100: Loss = -10838.041015625
Iteration 14200: Loss = -10838.0380859375
Iteration 14300: Loss = -10838.0361328125
Iteration 14400: Loss = -10838.0341796875
Iteration 14500: Loss = -10838.0322265625
Iteration 14600: Loss = -10838.0302734375
Iteration 14700: Loss = -10838.029296875
Iteration 14800: Loss = -10838.0283203125
Iteration 14900: Loss = -10838.02734375
Iteration 15000: Loss = -10838.0263671875
Iteration 15100: Loss = -10838.025390625
Iteration 15200: Loss = -10838.025390625
Iteration 15300: Loss = -10838.0244140625
Iteration 15400: Loss = -10838.0234375
Iteration 15500: Loss = -10838.0224609375
Iteration 15600: Loss = -10838.021484375
Iteration 15700: Loss = -10838.0234375
1
Iteration 15800: Loss = -10838.021484375
Iteration 15900: Loss = -10838.021484375
Iteration 16000: Loss = -10838.01953125
Iteration 16100: Loss = -10838.01953125
Iteration 16200: Loss = -10838.0205078125
1
Iteration 16300: Loss = -10838.01953125
Iteration 16400: Loss = -10838.01953125
Iteration 16500: Loss = -10838.01953125
Iteration 16600: Loss = -10838.01953125
Iteration 16700: Loss = -10838.0185546875
Iteration 16800: Loss = -10838.01953125
1
Iteration 16900: Loss = -10838.0185546875
Iteration 17000: Loss = -10838.0185546875
Iteration 17100: Loss = -10838.017578125
Iteration 17200: Loss = -10838.017578125
Iteration 17300: Loss = -10838.017578125
Iteration 17400: Loss = -10838.0185546875
1
Iteration 17500: Loss = -10838.017578125
Iteration 17600: Loss = -10838.017578125
Iteration 17700: Loss = -10838.017578125
Iteration 17800: Loss = -10838.017578125
Iteration 17900: Loss = -10838.017578125
Iteration 18000: Loss = -10838.017578125
Iteration 18100: Loss = -10838.0185546875
1
Iteration 18200: Loss = -10838.0166015625
Iteration 18300: Loss = -10838.017578125
1
Iteration 18400: Loss = -10838.017578125
2
Iteration 18500: Loss = -10838.0166015625
Iteration 18600: Loss = -10838.0166015625
Iteration 18700: Loss = -10838.0166015625
Iteration 18800: Loss = -10838.0166015625
Iteration 18900: Loss = -10838.0166015625
Iteration 19000: Loss = -10838.0166015625
Iteration 19100: Loss = -10838.0166015625
Iteration 19200: Loss = -10838.015625
Iteration 19300: Loss = -10838.017578125
1
Iteration 19400: Loss = -10838.0166015625
2
Iteration 19500: Loss = -10838.0166015625
3
Iteration 19600: Loss = -10838.017578125
4
Iteration 19700: Loss = -10838.015625
Iteration 19800: Loss = -10838.0166015625
1
Iteration 19900: Loss = -10838.017578125
2
Iteration 20000: Loss = -10838.0166015625
3
Iteration 20100: Loss = -10838.015625
Iteration 20200: Loss = -10838.0166015625
1
Iteration 20300: Loss = -10838.015625
Iteration 20400: Loss = -10838.0166015625
1
Iteration 20500: Loss = -10838.017578125
2
Iteration 20600: Loss = -10838.0185546875
3
Iteration 20700: Loss = -10838.017578125
4
Iteration 20800: Loss = -10838.0166015625
5
Iteration 20900: Loss = -10838.015625
Iteration 21000: Loss = -10838.0166015625
1
Iteration 21100: Loss = -10838.015625
Iteration 21200: Loss = -10838.015625
Iteration 21300: Loss = -10838.0166015625
1
Iteration 21400: Loss = -10838.0166015625
2
Iteration 21500: Loss = -10838.0166015625
3
Iteration 21600: Loss = -10838.015625
Iteration 21700: Loss = -10838.017578125
1
Iteration 21800: Loss = -10838.0166015625
2
Iteration 21900: Loss = -10838.017578125
3
Iteration 22000: Loss = -10838.015625
Iteration 22100: Loss = -10838.015625
Iteration 22200: Loss = -10838.017578125
1
Iteration 22300: Loss = -10838.0166015625
2
Iteration 22400: Loss = -10838.0166015625
3
Iteration 22500: Loss = -10838.0166015625
4
Iteration 22600: Loss = -10838.017578125
5
Iteration 22700: Loss = -10838.017578125
6
Iteration 22800: Loss = -10838.0166015625
7
Iteration 22900: Loss = -10838.0166015625
8
Iteration 23000: Loss = -10838.0166015625
9
Iteration 23100: Loss = -10838.017578125
10
Iteration 23200: Loss = -10838.0166015625
11
Iteration 23300: Loss = -10838.0166015625
12
Iteration 23400: Loss = -10838.0166015625
13
Iteration 23500: Loss = -10838.0166015625
14
Iteration 23600: Loss = -10838.0166015625
15
Stopping early at iteration 23600 due to no improvement.
pi: tensor([[0.7041, 0.2959],
        [0.2616, 0.7384]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4869, 0.5131], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2637, 0.1058],
         [0.0620, 0.1918]],

        [[0.3256, 0.0971],
         [0.0789, 0.0093]],

        [[0.9728, 0.1035],
         [0.4450, 0.9298]],

        [[0.6384, 0.0917],
         [0.9724, 0.7375]],

        [[0.0098, 0.1045],
         [0.3544, 0.9150]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 11
Adjusted Rand Index: 0.6044118965160035
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
Global Adjusted Rand Index: 0.8683608122555858
Average Adjusted Rand Index: 0.8732038517698728
[0.000426286523559823, 0.8683608122555858] [-0.00010802337139034952, 0.8732038517698728] [10963.560546875, 10838.0166015625]
-------------------------------------
This iteration is 26
True Objective function: Loss = -10890.95908574616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -53140.86328125
Iteration 100: Loss = -33739.265625
Iteration 200: Loss = -18527.3125
Iteration 300: Loss = -13319.77734375
Iteration 400: Loss = -11987.1748046875
Iteration 500: Loss = -11556.7099609375
Iteration 600: Loss = -11367.3076171875
Iteration 700: Loss = -11269.17578125
Iteration 800: Loss = -11223.0517578125
Iteration 900: Loss = -11186.939453125
Iteration 1000: Loss = -11157.2861328125
Iteration 1100: Loss = -11136.23828125
Iteration 1200: Loss = -11125.1318359375
Iteration 1300: Loss = -11117.080078125
Iteration 1400: Loss = -11105.47265625
Iteration 1500: Loss = -11096.7060546875
Iteration 1600: Loss = -11090.328125
Iteration 1700: Loss = -11086.2802734375
Iteration 1800: Loss = -11073.2099609375
Iteration 1900: Loss = -11070.59765625
Iteration 2000: Loss = -11068.6279296875
Iteration 2100: Loss = -11066.904296875
Iteration 2200: Loss = -11065.3095703125
Iteration 2300: Loss = -11063.8251953125
Iteration 2400: Loss = -11062.4873046875
Iteration 2500: Loss = -11060.275390625
Iteration 2600: Loss = -11057.98046875
Iteration 2700: Loss = -11056.5224609375
Iteration 2800: Loss = -11055.5263671875
Iteration 2900: Loss = -11052.9501953125
Iteration 3000: Loss = -11049.4521484375
Iteration 3100: Loss = -11048.818359375
Iteration 3200: Loss = -11048.3203125
Iteration 3300: Loss = -11047.8828125
Iteration 3400: Loss = -11047.4853515625
Iteration 3500: Loss = -11047.1279296875
Iteration 3600: Loss = -11046.8017578125
Iteration 3700: Loss = -11046.501953125
Iteration 3800: Loss = -11046.2255859375
Iteration 3900: Loss = -11045.9638671875
Iteration 4000: Loss = -11045.716796875
Iteration 4100: Loss = -11045.4853515625
Iteration 4200: Loss = -11045.2744140625
Iteration 4300: Loss = -11045.0830078125
Iteration 4400: Loss = -11044.9111328125
Iteration 4500: Loss = -11044.7490234375
Iteration 4600: Loss = -11044.5986328125
Iteration 4700: Loss = -11044.45703125
Iteration 4800: Loss = -11044.32421875
Iteration 4900: Loss = -11044.201171875
Iteration 5000: Loss = -11044.0849609375
Iteration 5100: Loss = -11043.9775390625
Iteration 5200: Loss = -11043.8740234375
Iteration 5300: Loss = -11043.7763671875
Iteration 5400: Loss = -11043.6865234375
Iteration 5500: Loss = -11043.6005859375
Iteration 5600: Loss = -11043.5185546875
Iteration 5700: Loss = -11043.4404296875
Iteration 5800: Loss = -11043.3671875
Iteration 5900: Loss = -11043.296875
Iteration 6000: Loss = -11043.23046875
Iteration 6100: Loss = -11043.16796875
Iteration 6200: Loss = -11043.1064453125
Iteration 6300: Loss = -11043.0322265625
Iteration 6400: Loss = -11038.3486328125
Iteration 6500: Loss = -11038.1572265625
Iteration 6600: Loss = -11038.0732421875
Iteration 6700: Loss = -11038.009765625
Iteration 6800: Loss = -11037.951171875
Iteration 6900: Loss = -11037.8798828125
Iteration 7000: Loss = -11037.58984375
Iteration 7100: Loss = -11037.455078125
Iteration 7200: Loss = -11037.40625
Iteration 7300: Loss = -11037.365234375
Iteration 7400: Loss = -11037.33203125
Iteration 7500: Loss = -11037.3017578125
Iteration 7600: Loss = -11037.1328125
Iteration 7700: Loss = -11032.341796875
Iteration 7800: Loss = -11032.1943359375
Iteration 7900: Loss = -11032.1171875
Iteration 8000: Loss = -11032.0615234375
Iteration 8100: Loss = -11032.0146484375
Iteration 8200: Loss = -11031.9775390625
Iteration 8300: Loss = -11031.9453125
Iteration 8400: Loss = -11031.9140625
Iteration 8500: Loss = -11031.8896484375
Iteration 8600: Loss = -11031.861328125
Iteration 8700: Loss = -11031.8408203125
Iteration 8800: Loss = -11031.8203125
Iteration 8900: Loss = -11031.802734375
Iteration 9000: Loss = -11031.7841796875
Iteration 9100: Loss = -11031.76953125
Iteration 9200: Loss = -11031.751953125
Iteration 9300: Loss = -11031.73828125
Iteration 9400: Loss = -11031.724609375
Iteration 9500: Loss = -11031.712890625
Iteration 9600: Loss = -11031.7021484375
Iteration 9700: Loss = -11031.689453125
Iteration 9800: Loss = -11031.6806640625
Iteration 9900: Loss = -11031.669921875
Iteration 10000: Loss = -11031.6591796875
Iteration 10100: Loss = -11031.650390625
Iteration 10200: Loss = -11031.642578125
Iteration 10300: Loss = -11031.634765625
Iteration 10400: Loss = -11031.6279296875
Iteration 10500: Loss = -11031.6201171875
Iteration 10600: Loss = -11031.6142578125
Iteration 10700: Loss = -11031.6064453125
Iteration 10800: Loss = -11031.6025390625
Iteration 10900: Loss = -11031.595703125
Iteration 11000: Loss = -11031.58984375
Iteration 11100: Loss = -11031.5859375
Iteration 11200: Loss = -11031.5810546875
Iteration 11300: Loss = -11031.5751953125
Iteration 11400: Loss = -11031.5712890625
Iteration 11500: Loss = -11031.5673828125
Iteration 11600: Loss = -11031.5634765625
Iteration 11700: Loss = -11031.560546875
Iteration 11800: Loss = -11031.5556640625
Iteration 11900: Loss = -11031.5517578125
Iteration 12000: Loss = -11031.548828125
Iteration 12100: Loss = -11031.546875
Iteration 12200: Loss = -11031.54296875
Iteration 12300: Loss = -11031.5400390625
Iteration 12400: Loss = -11031.5390625
Iteration 12500: Loss = -11031.537109375
Iteration 12600: Loss = -11031.533203125
Iteration 12700: Loss = -11031.53125
Iteration 12800: Loss = -11031.5302734375
Iteration 12900: Loss = -11031.5283203125
Iteration 13000: Loss = -11031.525390625
Iteration 13100: Loss = -11031.5234375
Iteration 13200: Loss = -11031.5244140625
1
Iteration 13300: Loss = -11031.521484375
Iteration 13400: Loss = -11031.51953125
Iteration 13500: Loss = -11031.5185546875
Iteration 13600: Loss = -11031.515625
Iteration 13700: Loss = -11031.515625
Iteration 13800: Loss = -11031.515625
Iteration 13900: Loss = -11031.5126953125
Iteration 14000: Loss = -11031.5107421875
Iteration 14100: Loss = -11031.509765625
Iteration 14200: Loss = -11031.5107421875
1
Iteration 14300: Loss = -11031.5078125
Iteration 14400: Loss = -11031.5078125
Iteration 14500: Loss = -11031.5068359375
Iteration 14600: Loss = -11031.5068359375
Iteration 14700: Loss = -11031.505859375
Iteration 14800: Loss = -11031.505859375
Iteration 14900: Loss = -11031.50390625
Iteration 15000: Loss = -11031.5029296875
Iteration 15100: Loss = -11031.5029296875
Iteration 15200: Loss = -11031.5029296875
Iteration 15300: Loss = -11031.501953125
Iteration 15400: Loss = -11031.5009765625
Iteration 15500: Loss = -11031.5
Iteration 15600: Loss = -11031.5
Iteration 15700: Loss = -11031.5
Iteration 15800: Loss = -11031.498046875
Iteration 15900: Loss = -11031.5
1
Iteration 16000: Loss = -11031.498046875
Iteration 16100: Loss = -11031.498046875
Iteration 16200: Loss = -11031.498046875
Iteration 16300: Loss = -11031.4970703125
Iteration 16400: Loss = -11031.4970703125
Iteration 16500: Loss = -11031.4970703125
Iteration 16600: Loss = -11031.4951171875
Iteration 16700: Loss = -11031.4951171875
Iteration 16800: Loss = -11031.49609375
1
Iteration 16900: Loss = -11031.49609375
2
Iteration 17000: Loss = -11031.4951171875
Iteration 17100: Loss = -11031.49609375
1
Iteration 17200: Loss = -11031.494140625
Iteration 17300: Loss = -11031.494140625
Iteration 17400: Loss = -11031.4931640625
Iteration 17500: Loss = -11031.494140625
1
Iteration 17600: Loss = -11031.4931640625
Iteration 17700: Loss = -11031.4931640625
Iteration 17800: Loss = -11031.494140625
1
Iteration 17900: Loss = -11031.494140625
2
Iteration 18000: Loss = -11031.494140625
3
Iteration 18100: Loss = -11031.494140625
4
Iteration 18200: Loss = -11031.494140625
5
Iteration 18300: Loss = -11031.4931640625
Iteration 18400: Loss = -11031.494140625
1
Iteration 18500: Loss = -11031.4921875
Iteration 18600: Loss = -11031.5048828125
1
Iteration 18700: Loss = -11031.4912109375
Iteration 18800: Loss = -11031.4912109375
Iteration 18900: Loss = -11031.490234375
Iteration 19000: Loss = -11031.4912109375
1
Iteration 19100: Loss = -11031.4912109375
2
Iteration 19200: Loss = -11031.4912109375
3
Iteration 19300: Loss = -11031.490234375
Iteration 19400: Loss = -11031.4912109375
1
Iteration 19500: Loss = -11031.4912109375
2
Iteration 19600: Loss = -11031.4912109375
3
Iteration 19700: Loss = -11031.4921875
4
Iteration 19800: Loss = -11031.4912109375
5
Iteration 19900: Loss = -11031.4921875
6
Iteration 20000: Loss = -11031.4912109375
7
Iteration 20100: Loss = -11031.4912109375
8
Iteration 20200: Loss = -11031.4912109375
9
Iteration 20300: Loss = -11031.4912109375
10
Iteration 20400: Loss = -11031.4912109375
11
Iteration 20500: Loss = -11031.490234375
Iteration 20600: Loss = -11031.4912109375
1
Iteration 20700: Loss = -11031.490234375
Iteration 20800: Loss = -11031.4912109375
1
Iteration 20900: Loss = -11031.4912109375
2
Iteration 21000: Loss = -11031.490234375
Iteration 21100: Loss = -11031.4921875
1
Iteration 21200: Loss = -11031.4912109375
2
Iteration 21300: Loss = -11031.4921875
3
Iteration 21400: Loss = -11031.4931640625
4
Iteration 21500: Loss = -11031.4912109375
5
Iteration 21600: Loss = -11031.4921875
6
Iteration 21700: Loss = -11031.4912109375
7
Iteration 21800: Loss = -11031.4912109375
8
Iteration 21900: Loss = -11031.4931640625
9
Iteration 22000: Loss = -11031.4912109375
10
Iteration 22100: Loss = -11031.4912109375
11
Iteration 22200: Loss = -11031.4931640625
12
Iteration 22300: Loss = -11031.4921875
13
Iteration 22400: Loss = -11031.4921875
14
Iteration 22500: Loss = -11031.4921875
15
Stopping early at iteration 22500 due to no improvement.
pi: tensor([[1.6409e-02, 9.8359e-01],
        [5.7589e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7345e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.7242, 0.1781],
         [0.0191, 0.1637]],

        [[0.0118, 0.1461],
         [0.6290, 0.7583]],

        [[0.2337, 0.1738],
         [0.2657, 0.6925]],

        [[0.9904, 0.1631],
         [0.9724, 0.1834]],

        [[0.6931, 0.7769],
         [0.9190, 0.8889]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32620.23046875
Iteration 100: Loss = -19278.990234375
Iteration 200: Loss = -13064.4658203125
Iteration 300: Loss = -11949.3740234375
Iteration 400: Loss = -11574.17578125
Iteration 500: Loss = -11406.755859375
Iteration 600: Loss = -11303.5
Iteration 700: Loss = -11249.443359375
Iteration 800: Loss = -11217.376953125
Iteration 900: Loss = -11195.0244140625
Iteration 1000: Loss = -11180.5498046875
Iteration 1100: Loss = -11169.0693359375
Iteration 1200: Loss = -11160.671875
Iteration 1300: Loss = -11152.5615234375
Iteration 1400: Loss = -11146.0498046875
Iteration 1500: Loss = -11138.1103515625
Iteration 1600: Loss = -11133.2109375
Iteration 1700: Loss = -11128.759765625
Iteration 1800: Loss = -11124.498046875
Iteration 1900: Loss = -11121.0478515625
Iteration 2000: Loss = -11117.0390625
Iteration 2100: Loss = -11113.3515625
Iteration 2200: Loss = -11110.1572265625
Iteration 2300: Loss = -11106.9130859375
Iteration 2400: Loss = -11103.3525390625
Iteration 2500: Loss = -11099.12890625
Iteration 2600: Loss = -11091.3935546875
Iteration 2700: Loss = -11086.7646484375
Iteration 2800: Loss = -11081.798828125
Iteration 2900: Loss = -11079.771484375
Iteration 3000: Loss = -11077.9912109375
Iteration 3100: Loss = -11072.6220703125
Iteration 3200: Loss = -11071.61328125
Iteration 3300: Loss = -11070.8515625
Iteration 3400: Loss = -11070.2607421875
Iteration 3500: Loss = -11068.1552734375
Iteration 3600: Loss = -11067.294921875
Iteration 3700: Loss = -11066.7607421875
Iteration 3800: Loss = -11064.7158203125
Iteration 3900: Loss = -11060.361328125
Iteration 4000: Loss = -11059.2578125
Iteration 4100: Loss = -11057.9560546875
Iteration 4200: Loss = -11057.19921875
Iteration 4300: Loss = -11056.765625
Iteration 4400: Loss = -11056.4462890625
Iteration 4500: Loss = -11056.1142578125
Iteration 4600: Loss = -11055.4599609375
Iteration 4700: Loss = -11052.1962890625
Iteration 4800: Loss = -11050.93359375
Iteration 4900: Loss = -11050.5986328125
Iteration 5000: Loss = -11050.37109375
Iteration 5100: Loss = -11050.173828125
Iteration 5200: Loss = -11050.0146484375
Iteration 5300: Loss = -11049.7958984375
Iteration 5400: Loss = -11046.5263671875
Iteration 5500: Loss = -11046.3857421875
Iteration 5600: Loss = -11046.2822265625
Iteration 5700: Loss = -11046.1953125
Iteration 5800: Loss = -11042.7705078125
Iteration 5900: Loss = -11042.2900390625
Iteration 6000: Loss = -11042.134765625
Iteration 6100: Loss = -11042.0439453125
Iteration 6200: Loss = -11041.974609375
Iteration 6300: Loss = -11041.916015625
Iteration 6400: Loss = -11041.8701171875
Iteration 6500: Loss = -11041.8291015625
Iteration 6600: Loss = -11041.7919921875
Iteration 6700: Loss = -11041.7578125
Iteration 6800: Loss = -11038.712890625
Iteration 6900: Loss = -11037.1767578125
Iteration 7000: Loss = -11036.9892578125
Iteration 7100: Loss = -11036.8740234375
Iteration 7200: Loss = -11036.80078125
Iteration 7300: Loss = -11036.74609375
Iteration 7400: Loss = -11036.705078125
Iteration 7500: Loss = -11036.6728515625
Iteration 7600: Loss = -11036.64453125
Iteration 7700: Loss = -11036.6201171875
Iteration 7800: Loss = -11036.5986328125
Iteration 7900: Loss = -11036.5810546875
Iteration 8000: Loss = -11036.5634765625
Iteration 8100: Loss = -11036.546875
Iteration 8200: Loss = -11036.533203125
Iteration 8300: Loss = -11036.51953125
Iteration 8400: Loss = -11036.5078125
Iteration 8500: Loss = -11036.4951171875
Iteration 8600: Loss = -11036.4833984375
Iteration 8700: Loss = -11036.4716796875
Iteration 8800: Loss = -11036.4609375
Iteration 8900: Loss = -11036.4482421875
Iteration 9000: Loss = -11036.4375
Iteration 9100: Loss = -11036.4140625
Iteration 9200: Loss = -11035.9853515625
Iteration 9300: Loss = -11032.09765625
Iteration 9400: Loss = -11031.9326171875
Iteration 9500: Loss = -11031.849609375
Iteration 9600: Loss = -11031.7978515625
Iteration 9700: Loss = -11031.7607421875
Iteration 9800: Loss = -11031.73046875
Iteration 9900: Loss = -11031.70703125
Iteration 10000: Loss = -11031.6865234375
Iteration 10100: Loss = -11031.6708984375
Iteration 10200: Loss = -11031.65625
Iteration 10300: Loss = -11031.6435546875
Iteration 10400: Loss = -11031.6328125
Iteration 10500: Loss = -11031.623046875
Iteration 10600: Loss = -11031.61328125
Iteration 10700: Loss = -11031.6044921875
Iteration 10800: Loss = -11031.5986328125
Iteration 10900: Loss = -11031.58984375
Iteration 11000: Loss = -11031.5859375
Iteration 11100: Loss = -11031.5771484375
Iteration 11200: Loss = -11031.5751953125
Iteration 11300: Loss = -11031.5703125
Iteration 11400: Loss = -11031.5654296875
Iteration 11500: Loss = -11031.560546875
Iteration 11600: Loss = -11031.556640625
Iteration 11700: Loss = -11031.5537109375
Iteration 11800: Loss = -11031.548828125
Iteration 11900: Loss = -11031.5458984375
Iteration 12000: Loss = -11031.54296875
Iteration 12100: Loss = -11031.541015625
Iteration 12200: Loss = -11031.537109375
Iteration 12300: Loss = -11031.53515625
Iteration 12400: Loss = -11031.5322265625
Iteration 12500: Loss = -11031.53125
Iteration 12600: Loss = -11031.5283203125
Iteration 12700: Loss = -11031.52734375
Iteration 12800: Loss = -11031.5263671875
Iteration 12900: Loss = -11031.5234375
Iteration 13000: Loss = -11031.521484375
Iteration 13100: Loss = -11031.51953125
Iteration 13200: Loss = -11031.5185546875
Iteration 13300: Loss = -11031.5166015625
Iteration 13400: Loss = -11031.5146484375
Iteration 13500: Loss = -11031.5146484375
Iteration 13600: Loss = -11031.5126953125
Iteration 13700: Loss = -11031.5126953125
Iteration 13800: Loss = -11031.51171875
Iteration 13900: Loss = -11031.509765625
Iteration 14000: Loss = -11031.509765625
Iteration 14100: Loss = -11031.5107421875
1
Iteration 14200: Loss = -11031.5078125
Iteration 14300: Loss = -11031.505859375
Iteration 14400: Loss = -11031.5048828125
Iteration 14500: Loss = -11031.50390625
Iteration 14600: Loss = -11031.5048828125
1
Iteration 14700: Loss = -11031.5048828125
2
Iteration 14800: Loss = -11031.501953125
Iteration 14900: Loss = -11031.5009765625
Iteration 15000: Loss = -11031.5009765625
Iteration 15100: Loss = -11031.5
Iteration 15200: Loss = -11031.5009765625
1
Iteration 15300: Loss = -11031.498046875
Iteration 15400: Loss = -11031.498046875
Iteration 15500: Loss = -11031.4970703125
Iteration 15600: Loss = -11031.49609375
Iteration 15700: Loss = -11031.49609375
Iteration 15800: Loss = -11031.49609375
Iteration 15900: Loss = -11031.49609375
Iteration 16000: Loss = -11031.4951171875
Iteration 16100: Loss = -11031.4951171875
Iteration 16200: Loss = -11031.49609375
1
Iteration 16300: Loss = -11031.4931640625
Iteration 16400: Loss = -11031.494140625
1
Iteration 16500: Loss = -11031.4912109375
Iteration 16600: Loss = -11031.4794921875
Iteration 16700: Loss = -11030.912109375
Iteration 16800: Loss = -11030.822265625
Iteration 16900: Loss = -11030.798828125
Iteration 17000: Loss = -11030.7939453125
Iteration 17100: Loss = -11030.7900390625
Iteration 17200: Loss = -11030.7890625
Iteration 17300: Loss = -11030.7890625
Iteration 17400: Loss = -11030.7890625
Iteration 17500: Loss = -11030.787109375
Iteration 17600: Loss = -11030.7861328125
Iteration 17700: Loss = -11030.7841796875
Iteration 17800: Loss = -11030.78515625
1
Iteration 17900: Loss = -11030.7841796875
Iteration 18000: Loss = -11030.783203125
Iteration 18100: Loss = -11030.78125
Iteration 18200: Loss = -11030.7822265625
1
Iteration 18300: Loss = -11030.7802734375
Iteration 18400: Loss = -11030.7802734375
Iteration 18500: Loss = -11030.7822265625
1
Iteration 18600: Loss = -11030.7802734375
Iteration 18700: Loss = -11030.7802734375
Iteration 18800: Loss = -11030.7802734375
Iteration 18900: Loss = -11030.779296875
Iteration 19000: Loss = -11030.7744140625
Iteration 19100: Loss = -11030.7744140625
Iteration 19200: Loss = -11030.7734375
Iteration 19300: Loss = -11030.7705078125
Iteration 19400: Loss = -11030.771484375
1
Iteration 19500: Loss = -11030.771484375
2
Iteration 19600: Loss = -11030.771484375
3
Iteration 19700: Loss = -11030.7724609375
4
Iteration 19800: Loss = -11030.771484375
5
Iteration 19900: Loss = -11030.76953125
Iteration 20000: Loss = -11030.7705078125
1
Iteration 20100: Loss = -11030.76953125
Iteration 20200: Loss = -11030.7685546875
Iteration 20300: Loss = -11030.7705078125
1
Iteration 20400: Loss = -11030.7685546875
Iteration 20500: Loss = -11030.7705078125
1
Iteration 20600: Loss = -11030.7705078125
2
Iteration 20700: Loss = -11030.7705078125
3
Iteration 20800: Loss = -11030.76953125
4
Iteration 20900: Loss = -11030.771484375
5
Iteration 21000: Loss = -11030.7705078125
6
Iteration 21100: Loss = -11030.76953125
7
Iteration 21200: Loss = -11030.7705078125
8
Iteration 21300: Loss = -11030.7685546875
Iteration 21400: Loss = -11030.76953125
1
Iteration 21500: Loss = -11030.7685546875
Iteration 21600: Loss = -11030.7705078125
1
Iteration 21700: Loss = -11030.7685546875
Iteration 21800: Loss = -11030.7705078125
1
Iteration 21900: Loss = -11030.76953125
2
Iteration 22000: Loss = -11030.76953125
3
Iteration 22100: Loss = -11030.7705078125
4
Iteration 22200: Loss = -11030.7685546875
Iteration 22300: Loss = -11030.7705078125
1
Iteration 22400: Loss = -11030.7705078125
2
Iteration 22500: Loss = -11030.76953125
3
Iteration 22600: Loss = -11030.7685546875
Iteration 22700: Loss = -11030.76953125
1
Iteration 22800: Loss = -11030.76953125
2
Iteration 22900: Loss = -11030.7705078125
3
Iteration 23000: Loss = -11030.7705078125
4
Iteration 23100: Loss = -11030.771484375
5
Iteration 23200: Loss = -11030.7705078125
6
Iteration 23300: Loss = -11030.76953125
7
Iteration 23400: Loss = -11030.76953125
8
Iteration 23500: Loss = -11030.7685546875
Iteration 23600: Loss = -11030.771484375
1
Iteration 23700: Loss = -11030.76953125
2
Iteration 23800: Loss = -11030.771484375
3
Iteration 23900: Loss = -11030.7685546875
Iteration 24000: Loss = -11030.7685546875
Iteration 24100: Loss = -11030.76953125
1
Iteration 24200: Loss = -11030.7685546875
Iteration 24300: Loss = -11030.76953125
1
Iteration 24400: Loss = -11030.7685546875
Iteration 24500: Loss = -11030.76953125
1
Iteration 24600: Loss = -11030.7705078125
2
Iteration 24700: Loss = -11030.76953125
3
Iteration 24800: Loss = -11030.7705078125
4
Iteration 24900: Loss = -11030.7724609375
5
Iteration 25000: Loss = -11030.76953125
6
Iteration 25100: Loss = -11030.767578125
Iteration 25200: Loss = -11030.76953125
1
Iteration 25300: Loss = -11030.76953125
2
Iteration 25400: Loss = -11030.7685546875
3
Iteration 25500: Loss = -11030.7705078125
4
Iteration 25600: Loss = -11030.7685546875
5
Iteration 25700: Loss = -11030.7705078125
6
Iteration 25800: Loss = -11030.7705078125
7
Iteration 25900: Loss = -11030.76953125
8
Iteration 26000: Loss = -11030.7685546875
9
Iteration 26100: Loss = -11030.76953125
10
Iteration 26200: Loss = -11030.76953125
11
Iteration 26300: Loss = -11030.7705078125
12
Iteration 26400: Loss = -11030.7705078125
13
Iteration 26500: Loss = -11030.7685546875
14
Iteration 26600: Loss = -11030.7705078125
15
Stopping early at iteration 26600 due to no improvement.
pi: tensor([[3.7752e-01, 6.2248e-01],
        [2.2062e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1003, 0.8997], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1403, 0.1516],
         [0.1621, 0.1648]],

        [[0.9526, 0.1135],
         [0.9250, 0.1035]],

        [[0.0536, 0.2229],
         [0.6171, 0.6533]],

        [[0.9713, 0.1077],
         [0.6173, 0.0974]],

        [[0.7149, 0.6011],
         [0.9847, 0.9477]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0008896015903045093
Average Adjusted Rand Index: 0.0005099680735181882
[0.0, 0.0008896015903045093] [0.0, 0.0005099680735181882] [11031.4921875, 11030.7705078125]
-------------------------------------
This iteration is 27
True Objective function: Loss = -10812.433284644521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -56471.48046875
Iteration 100: Loss = -40642.09375
Iteration 200: Loss = -27724.869140625
Iteration 300: Loss = -18223.546875
Iteration 400: Loss = -13533.1708984375
Iteration 500: Loss = -11791.6552734375
Iteration 600: Loss = -11221.11328125
Iteration 700: Loss = -11055.76953125
Iteration 800: Loss = -11005.2373046875
Iteration 900: Loss = -10975.890625
Iteration 1000: Loss = -10968.228515625
Iteration 1100: Loss = -10962.6875
Iteration 1200: Loss = -10953.361328125
Iteration 1300: Loss = -10950.4296875
Iteration 1400: Loss = -10947.41015625
Iteration 1500: Loss = -10944.3740234375
Iteration 1600: Loss = -10938.390625
Iteration 1700: Loss = -10931.255859375
Iteration 1800: Loss = -10929.55078125
Iteration 1900: Loss = -10928.841796875
Iteration 2000: Loss = -10928.3642578125
Iteration 2100: Loss = -10927.9990234375
Iteration 2200: Loss = -10927.6962890625
Iteration 2300: Loss = -10927.44140625
Iteration 2400: Loss = -10927.23046875
Iteration 2500: Loss = -10927.0498046875
Iteration 2600: Loss = -10926.8876953125
Iteration 2700: Loss = -10926.7373046875
Iteration 2800: Loss = -10926.60546875
Iteration 2900: Loss = -10926.4892578125
Iteration 3000: Loss = -10926.3798828125
Iteration 3100: Loss = -10926.2763671875
Iteration 3200: Loss = -10926.169921875
Iteration 3300: Loss = -10926.0419921875
Iteration 3400: Loss = -10925.8046875
Iteration 3500: Loss = -10924.3828125
Iteration 3600: Loss = -10921.98828125
Iteration 3700: Loss = -10921.7041015625
Iteration 3800: Loss = -10921.5810546875
Iteration 3900: Loss = -10921.494140625
Iteration 4000: Loss = -10921.4287109375
Iteration 4100: Loss = -10921.37109375
Iteration 4200: Loss = -10921.3212890625
Iteration 4300: Loss = -10921.275390625
Iteration 4400: Loss = -10921.2333984375
Iteration 4500: Loss = -10921.1953125
Iteration 4600: Loss = -10921.16015625
Iteration 4700: Loss = -10921.12890625
Iteration 4800: Loss = -10921.099609375
Iteration 4900: Loss = -10921.072265625
Iteration 5000: Loss = -10921.0458984375
Iteration 5100: Loss = -10921.0224609375
Iteration 5200: Loss = -10921.0009765625
Iteration 5300: Loss = -10920.9794921875
Iteration 5400: Loss = -10920.9599609375
Iteration 5500: Loss = -10920.94140625
Iteration 5600: Loss = -10920.927734375
Iteration 5700: Loss = -10920.9111328125
Iteration 5800: Loss = -10920.8974609375
Iteration 5900: Loss = -10920.8828125
Iteration 6000: Loss = -10920.8701171875
Iteration 6100: Loss = -10920.8603515625
Iteration 6200: Loss = -10920.84765625
Iteration 6300: Loss = -10920.837890625
Iteration 6400: Loss = -10920.826171875
Iteration 6500: Loss = -10920.81640625
Iteration 6600: Loss = -10920.8056640625
Iteration 6700: Loss = -10920.7978515625
Iteration 6800: Loss = -10920.7880859375
Iteration 6900: Loss = -10920.779296875
Iteration 7000: Loss = -10920.771484375
Iteration 7100: Loss = -10920.76171875
Iteration 7200: Loss = -10920.75390625
Iteration 7300: Loss = -10920.7451171875
Iteration 7400: Loss = -10920.7353515625
Iteration 7500: Loss = -10920.7265625
Iteration 7600: Loss = -10920.7177734375
Iteration 7700: Loss = -10920.7099609375
Iteration 7800: Loss = -10920.7041015625
Iteration 7900: Loss = -10920.6962890625
Iteration 8000: Loss = -10920.6904296875
Iteration 8100: Loss = -10920.6845703125
Iteration 8200: Loss = -10920.677734375
Iteration 8300: Loss = -10920.671875
Iteration 8400: Loss = -10920.66796875
Iteration 8500: Loss = -10920.662109375
Iteration 8600: Loss = -10920.658203125
Iteration 8700: Loss = -10920.654296875
Iteration 8800: Loss = -10920.6494140625
Iteration 8900: Loss = -10920.64453125
Iteration 9000: Loss = -10920.6396484375
Iteration 9100: Loss = -10920.6357421875
Iteration 9200: Loss = -10920.630859375
Iteration 9300: Loss = -10920.625
Iteration 9400: Loss = -10920.62109375
Iteration 9500: Loss = -10920.6171875
Iteration 9600: Loss = -10920.611328125
Iteration 9700: Loss = -10920.6064453125
Iteration 9800: Loss = -10920.6005859375
Iteration 9900: Loss = -10920.595703125
Iteration 10000: Loss = -10920.58984375
Iteration 10100: Loss = -10920.5849609375
Iteration 10200: Loss = -10920.580078125
Iteration 10300: Loss = -10920.5771484375
Iteration 10400: Loss = -10920.57421875
Iteration 10500: Loss = -10920.5703125
Iteration 10600: Loss = -10920.5703125
Iteration 10700: Loss = -10920.5654296875
Iteration 10800: Loss = -10920.5634765625
Iteration 10900: Loss = -10920.5595703125
Iteration 11000: Loss = -10920.55859375
Iteration 11100: Loss = -10920.5556640625
Iteration 11200: Loss = -10920.552734375
Iteration 11300: Loss = -10920.55078125
Iteration 11400: Loss = -10920.546875
Iteration 11500: Loss = -10920.54296875
Iteration 11600: Loss = -10920.5419921875
Iteration 11700: Loss = -10920.537109375
Iteration 11800: Loss = -10920.5341796875
Iteration 11900: Loss = -10920.53125
Iteration 12000: Loss = -10920.529296875
Iteration 12100: Loss = -10920.5234375
Iteration 12200: Loss = -10920.5205078125
Iteration 12300: Loss = -10920.513671875
Iteration 12400: Loss = -10920.5087890625
Iteration 12500: Loss = -10920.505859375
Iteration 12600: Loss = -10920.498046875
Iteration 12700: Loss = -10920.490234375
Iteration 12800: Loss = -10920.4853515625
Iteration 12900: Loss = -10920.4755859375
Iteration 13000: Loss = -10920.4677734375
Iteration 13100: Loss = -10920.4560546875
Iteration 13200: Loss = -10920.4423828125
Iteration 13300: Loss = -10920.4267578125
Iteration 13400: Loss = -10920.4091796875
Iteration 13500: Loss = -10920.384765625
Iteration 13600: Loss = -10920.35546875
Iteration 13700: Loss = -10920.3173828125
Iteration 13800: Loss = -10920.2685546875
Iteration 13900: Loss = -10920.2158203125
Iteration 14000: Loss = -10920.1669921875
Iteration 14100: Loss = -10920.12890625
Iteration 14200: Loss = -10920.1044921875
Iteration 14300: Loss = -10920.08984375
Iteration 14400: Loss = -10920.0810546875
Iteration 14500: Loss = -10920.078125
Iteration 14600: Loss = -10920.07421875
Iteration 14700: Loss = -10920.07421875
Iteration 14800: Loss = -10920.07421875
Iteration 14900: Loss = -10920.07421875
Iteration 15000: Loss = -10920.0732421875
Iteration 15100: Loss = -10920.072265625
Iteration 15200: Loss = -10920.0712890625
Iteration 15300: Loss = -10920.072265625
1
Iteration 15400: Loss = -10920.072265625
2
Iteration 15500: Loss = -10920.0712890625
Iteration 15600: Loss = -10920.0703125
Iteration 15700: Loss = -10920.0732421875
1
Iteration 15800: Loss = -10920.0712890625
2
Iteration 15900: Loss = -10920.072265625
3
Iteration 16000: Loss = -10920.0712890625
4
Iteration 16100: Loss = -10920.0712890625
5
Iteration 16200: Loss = -10920.072265625
6
Iteration 16300: Loss = -10920.0703125
Iteration 16400: Loss = -10920.0654296875
Iteration 16500: Loss = -10920.06640625
1
Iteration 16600: Loss = -10920.0654296875
Iteration 16700: Loss = -10920.06640625
1
Iteration 16800: Loss = -10920.06640625
2
Iteration 16900: Loss = -10920.0458984375
Iteration 17000: Loss = -10919.9990234375
Iteration 17100: Loss = -10920.0009765625
1
Iteration 17200: Loss = -10920.0
2
Iteration 17300: Loss = -10920.0
3
Iteration 17400: Loss = -10920.0
4
Iteration 17500: Loss = -10919.9609375
Iteration 17600: Loss = -10919.8349609375
Iteration 17700: Loss = -10919.8330078125
Iteration 17800: Loss = -10919.833984375
1
Iteration 17900: Loss = -10919.833984375
2
Iteration 18000: Loss = -10919.8330078125
Iteration 18100: Loss = -10919.833984375
1
Iteration 18200: Loss = -10919.83203125
Iteration 18300: Loss = -10919.8349609375
1
Iteration 18400: Loss = -10919.83203125
Iteration 18500: Loss = -10919.8330078125
1
Iteration 18600: Loss = -10919.833984375
2
Iteration 18700: Loss = -10919.83203125
Iteration 18800: Loss = -10919.83203125
Iteration 18900: Loss = -10919.8310546875
Iteration 19000: Loss = -10919.83203125
1
Iteration 19100: Loss = -10919.8310546875
Iteration 19200: Loss = -10919.0693359375
Iteration 19300: Loss = -10918.994140625
Iteration 19400: Loss = -10918.9833984375
Iteration 19500: Loss = -10918.9794921875
Iteration 19600: Loss = -10918.9765625
Iteration 19700: Loss = -10918.974609375
Iteration 19800: Loss = -10918.9736328125
Iteration 19900: Loss = -10918.97265625
Iteration 20000: Loss = -10918.97265625
Iteration 20100: Loss = -10918.9736328125
1
Iteration 20200: Loss = -10918.970703125
Iteration 20300: Loss = -10918.97265625
1
Iteration 20400: Loss = -10918.970703125
Iteration 20500: Loss = -10918.9716796875
1
Iteration 20600: Loss = -10918.9736328125
2
Iteration 20700: Loss = -10918.9716796875
3
Iteration 20800: Loss = -10918.970703125
Iteration 20900: Loss = -10918.970703125
Iteration 21000: Loss = -10918.9716796875
1
Iteration 21100: Loss = -10918.970703125
Iteration 21200: Loss = -10918.970703125
Iteration 21300: Loss = -10918.9697265625
Iteration 21400: Loss = -10918.9716796875
1
Iteration 21500: Loss = -10918.9716796875
2
Iteration 21600: Loss = -10918.9716796875
3
Iteration 21700: Loss = -10918.9716796875
4
Iteration 21800: Loss = -10918.9716796875
5
Iteration 21900: Loss = -10918.970703125
6
Iteration 22000: Loss = -10918.970703125
7
Iteration 22100: Loss = -10918.970703125
8
Iteration 22200: Loss = -10918.9716796875
9
Iteration 22300: Loss = -10918.9716796875
10
Iteration 22400: Loss = -10918.9716796875
11
Iteration 22500: Loss = -10918.9716796875
12
Iteration 22600: Loss = -10918.9697265625
Iteration 22700: Loss = -10918.9716796875
1
Iteration 22800: Loss = -10918.97265625
2
Iteration 22900: Loss = -10918.97265625
3
Iteration 23000: Loss = -10918.9716796875
4
Iteration 23100: Loss = -10918.97265625
5
Iteration 23200: Loss = -10918.970703125
6
Iteration 23300: Loss = -10918.9716796875
7
Iteration 23400: Loss = -10918.9716796875
8
Iteration 23500: Loss = -10918.970703125
9
Iteration 23600: Loss = -10918.970703125
10
Iteration 23700: Loss = -10918.9716796875
11
Iteration 23800: Loss = -10918.9716796875
12
Iteration 23900: Loss = -10918.9716796875
13
Iteration 24000: Loss = -10918.970703125
14
Iteration 24100: Loss = -10918.9716796875
15
Stopping early at iteration 24100 due to no improvement.
pi: tensor([[1.0000e+00, 3.6733e-06],
        [9.8142e-01, 1.8583e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0116, 0.9884], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1588, 0.1729],
         [0.1961, 0.1655]],

        [[0.9808, 0.2764],
         [0.0106, 0.0631]],

        [[0.0444, 0.1647],
         [0.0556, 0.8231]],

        [[0.9871, 0.2083],
         [0.0086, 0.0292]],

        [[0.9569, 0.6770],
         [0.2904, 0.0071]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001507995594217576
Average Adjusted Rand Index: -0.0007272727272727272
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31182.234375
Iteration 100: Loss = -20224.103515625
Iteration 200: Loss = -13434.76953125
Iteration 300: Loss = -11653.9921875
Iteration 400: Loss = -11359.423828125
Iteration 500: Loss = -11244.2919921875
Iteration 600: Loss = -11180.412109375
Iteration 700: Loss = -11137.9384765625
Iteration 800: Loss = -11109.6923828125
Iteration 900: Loss = -11090.4482421875
Iteration 1000: Loss = -11074.7705078125
Iteration 1100: Loss = -11059.9833984375
Iteration 1200: Loss = -11049.380859375
Iteration 1300: Loss = -11041.291015625
Iteration 1400: Loss = -11033.7421875
Iteration 1500: Loss = -11023.85546875
Iteration 1600: Loss = -11018.3056640625
Iteration 1700: Loss = -11012.55078125
Iteration 1800: Loss = -11006.810546875
Iteration 1900: Loss = -11002.2001953125
Iteration 2000: Loss = -10998.05859375
Iteration 2100: Loss = -10991.2724609375
Iteration 2200: Loss = -10987.869140625
Iteration 2300: Loss = -10984.623046875
Iteration 2400: Loss = -10980.1943359375
Iteration 2500: Loss = -10976.6181640625
Iteration 2600: Loss = -10973.3310546875
Iteration 2700: Loss = -10969.478515625
Iteration 2800: Loss = -10967.34765625
Iteration 2900: Loss = -10965.5244140625
Iteration 3000: Loss = -10964.3798828125
Iteration 3100: Loss = -10963.46875
Iteration 3200: Loss = -10962.6865234375
Iteration 3300: Loss = -10961.93359375
Iteration 3400: Loss = -10961.0947265625
Iteration 3500: Loss = -10959.9033203125
Iteration 3600: Loss = -10958.44921875
Iteration 3700: Loss = -10955.666015625
Iteration 3800: Loss = -10954.8623046875
Iteration 3900: Loss = -10954.10546875
Iteration 4000: Loss = -10952.828125
Iteration 4100: Loss = -10951.517578125
Iteration 4200: Loss = -10947.87890625
Iteration 4300: Loss = -10946.359375
Iteration 4400: Loss = -10940.7529296875
Iteration 4500: Loss = -10940.255859375
Iteration 4600: Loss = -10939.931640625
Iteration 4700: Loss = -10939.6748046875
Iteration 4800: Loss = -10939.4580078125
Iteration 4900: Loss = -10939.265625
Iteration 5000: Loss = -10939.09375
Iteration 5100: Loss = -10938.931640625
Iteration 5200: Loss = -10938.6484375
Iteration 5300: Loss = -10931.888671875
Iteration 5400: Loss = -10930.7978515625
Iteration 5500: Loss = -10928.62109375
Iteration 5600: Loss = -10928.412109375
Iteration 5700: Loss = -10928.2529296875
Iteration 5800: Loss = -10928.1201171875
Iteration 5900: Loss = -10928.0078125
Iteration 6000: Loss = -10927.90625
Iteration 6100: Loss = -10927.8134765625
Iteration 6200: Loss = -10927.7333984375
Iteration 6300: Loss = -10927.6572265625
Iteration 6400: Loss = -10927.5888671875
Iteration 6500: Loss = -10927.525390625
Iteration 6600: Loss = -10927.46484375
Iteration 6700: Loss = -10927.408203125
Iteration 6800: Loss = -10927.3544921875
Iteration 6900: Loss = -10927.3017578125
Iteration 7000: Loss = -10927.1376953125
Iteration 7100: Loss = -10926.5537109375
Iteration 7200: Loss = -10926.501953125
Iteration 7300: Loss = -10926.4609375
Iteration 7400: Loss = -10926.4267578125
Iteration 7500: Loss = -10926.3955078125
Iteration 7600: Loss = -10926.36328125
Iteration 7700: Loss = -10926.3349609375
Iteration 7800: Loss = -10926.30859375
Iteration 7900: Loss = -10926.2841796875
Iteration 8000: Loss = -10926.2607421875
Iteration 8100: Loss = -10926.2373046875
Iteration 8200: Loss = -10926.21484375
Iteration 8300: Loss = -10926.1953125
Iteration 8400: Loss = -10926.1748046875
Iteration 8500: Loss = -10926.1552734375
Iteration 8600: Loss = -10926.1337890625
Iteration 8700: Loss = -10926.1142578125
Iteration 8800: Loss = -10926.0966796875
Iteration 8900: Loss = -10925.720703125
Iteration 9000: Loss = -10922.0400390625
Iteration 9100: Loss = -10921.9345703125
Iteration 9200: Loss = -10921.880859375
Iteration 9300: Loss = -10921.8447265625
Iteration 9400: Loss = -10921.8173828125
Iteration 9500: Loss = -10921.7958984375
Iteration 9600: Loss = -10921.775390625
Iteration 9700: Loss = -10921.76171875
Iteration 9800: Loss = -10921.7470703125
Iteration 9900: Loss = -10921.7333984375
Iteration 10000: Loss = -10921.7216796875
Iteration 10100: Loss = -10921.7099609375
Iteration 10200: Loss = -10921.7001953125
Iteration 10300: Loss = -10921.69140625
Iteration 10400: Loss = -10921.68359375
Iteration 10500: Loss = -10921.6748046875
Iteration 10600: Loss = -10921.666015625
Iteration 10700: Loss = -10921.66015625
Iteration 10800: Loss = -10921.65234375
Iteration 10900: Loss = -10921.6474609375
Iteration 11000: Loss = -10921.6416015625
Iteration 11100: Loss = -10921.634765625
Iteration 11200: Loss = -10921.6318359375
Iteration 11300: Loss = -10921.6259765625
Iteration 11400: Loss = -10921.62109375
Iteration 11500: Loss = -10921.6181640625
Iteration 11600: Loss = -10921.61328125
Iteration 11700: Loss = -10921.611328125
Iteration 11800: Loss = -10921.60546875
Iteration 11900: Loss = -10921.6015625
Iteration 12000: Loss = -10921.599609375
Iteration 12100: Loss = -10921.59765625
Iteration 12200: Loss = -10921.5927734375
Iteration 12300: Loss = -10921.591796875
Iteration 12400: Loss = -10921.5888671875
Iteration 12500: Loss = -10921.5849609375
Iteration 12600: Loss = -10921.583984375
Iteration 12700: Loss = -10921.58203125
Iteration 12800: Loss = -10921.580078125
Iteration 12900: Loss = -10921.5771484375
Iteration 13000: Loss = -10921.5751953125
Iteration 13100: Loss = -10921.5732421875
Iteration 13200: Loss = -10921.572265625
Iteration 13300: Loss = -10921.5693359375
Iteration 13400: Loss = -10921.568359375
Iteration 13500: Loss = -10921.5673828125
Iteration 13600: Loss = -10921.56640625
Iteration 13700: Loss = -10921.5634765625
Iteration 13800: Loss = -10921.5634765625
Iteration 13900: Loss = -10921.5625
Iteration 14000: Loss = -10921.5615234375
Iteration 14100: Loss = -10921.55859375
Iteration 14200: Loss = -10921.5576171875
Iteration 14300: Loss = -10921.5576171875
Iteration 14400: Loss = -10921.556640625
Iteration 14500: Loss = -10921.5556640625
Iteration 14600: Loss = -10921.5537109375
Iteration 14700: Loss = -10921.5537109375
Iteration 14800: Loss = -10921.5537109375
Iteration 14900: Loss = -10921.5546875
1
Iteration 15000: Loss = -10921.552734375
Iteration 15100: Loss = -10921.552734375
Iteration 15200: Loss = -10921.55078125
Iteration 15300: Loss = -10921.5498046875
Iteration 15400: Loss = -10921.548828125
Iteration 15500: Loss = -10921.55078125
1
Iteration 15600: Loss = -10921.5498046875
2
Iteration 15700: Loss = -10921.5478515625
Iteration 15800: Loss = -10921.5498046875
1
Iteration 15900: Loss = -10921.546875
Iteration 16000: Loss = -10921.5478515625
1
Iteration 16100: Loss = -10921.546875
Iteration 16200: Loss = -10921.546875
Iteration 16300: Loss = -10921.5478515625
1
Iteration 16400: Loss = -10921.5458984375
Iteration 16500: Loss = -10921.5458984375
Iteration 16600: Loss = -10921.544921875
Iteration 16700: Loss = -10921.544921875
Iteration 16800: Loss = -10921.54296875
Iteration 16900: Loss = -10921.544921875
1
Iteration 17000: Loss = -10921.544921875
2
Iteration 17100: Loss = -10921.5439453125
3
Iteration 17200: Loss = -10921.54296875
Iteration 17300: Loss = -10921.544921875
1
Iteration 17400: Loss = -10921.5439453125
2
Iteration 17500: Loss = -10921.54296875
Iteration 17600: Loss = -10921.54296875
Iteration 17700: Loss = -10921.5419921875
Iteration 17800: Loss = -10921.54296875
1
Iteration 17900: Loss = -10921.5439453125
2
Iteration 18000: Loss = -10921.54296875
3
Iteration 18100: Loss = -10921.5419921875
Iteration 18200: Loss = -10921.5419921875
Iteration 18300: Loss = -10921.5419921875
Iteration 18400: Loss = -10921.5419921875
Iteration 18500: Loss = -10921.5419921875
Iteration 18600: Loss = -10921.54296875
1
Iteration 18700: Loss = -10921.541015625
Iteration 18800: Loss = -10921.5400390625
Iteration 18900: Loss = -10921.5400390625
Iteration 19000: Loss = -10921.5419921875
1
Iteration 19100: Loss = -10921.5419921875
2
Iteration 19200: Loss = -10921.541015625
3
Iteration 19300: Loss = -10921.541015625
4
Iteration 19400: Loss = -10921.54296875
5
Iteration 19500: Loss = -10921.5400390625
Iteration 19600: Loss = -10921.5400390625
Iteration 19700: Loss = -10921.541015625
1
Iteration 19800: Loss = -10921.5400390625
Iteration 19900: Loss = -10921.5400390625
Iteration 20000: Loss = -10921.5390625
Iteration 20100: Loss = -10921.5390625
Iteration 20200: Loss = -10921.5390625
Iteration 20300: Loss = -10921.541015625
1
Iteration 20400: Loss = -10921.541015625
2
Iteration 20500: Loss = -10921.541015625
3
Iteration 20600: Loss = -10921.5400390625
4
Iteration 20700: Loss = -10921.5400390625
5
Iteration 20800: Loss = -10921.5390625
Iteration 20900: Loss = -10921.5390625
Iteration 21000: Loss = -10921.5400390625
1
Iteration 21100: Loss = -10921.5400390625
2
Iteration 21200: Loss = -10921.5400390625
3
Iteration 21300: Loss = -10921.5400390625
4
Iteration 21400: Loss = -10921.541015625
5
Iteration 21500: Loss = -10921.5400390625
6
Iteration 21600: Loss = -10921.5390625
Iteration 21700: Loss = -10921.541015625
1
Iteration 21800: Loss = -10921.5390625
Iteration 21900: Loss = -10921.541015625
1
Iteration 22000: Loss = -10921.5390625
Iteration 22100: Loss = -10921.541015625
1
Iteration 22200: Loss = -10921.541015625
2
Iteration 22300: Loss = -10921.5400390625
3
Iteration 22400: Loss = -10921.541015625
4
Iteration 22500: Loss = -10921.541015625
5
Iteration 22600: Loss = -10921.5400390625
6
Iteration 22700: Loss = -10921.5400390625
7
Iteration 22800: Loss = -10921.5400390625
8
Iteration 22900: Loss = -10921.5419921875
9
Iteration 23000: Loss = -10921.5400390625
10
Iteration 23100: Loss = -10921.5380859375
Iteration 23200: Loss = -10921.5400390625
1
Iteration 23300: Loss = -10921.541015625
2
Iteration 23400: Loss = -10921.5400390625
3
Iteration 23500: Loss = -10921.541015625
4
Iteration 23600: Loss = -10921.5400390625
5
Iteration 23700: Loss = -10921.5390625
6
Iteration 23800: Loss = -10921.5400390625
7
Iteration 23900: Loss = -10921.54296875
8
Iteration 24000: Loss = -10921.5400390625
9
Iteration 24100: Loss = -10921.5400390625
10
Iteration 24200: Loss = -10921.5390625
11
Iteration 24300: Loss = -10921.5390625
12
Iteration 24400: Loss = -10921.5419921875
13
Iteration 24500: Loss = -10921.5390625
14
Iteration 24600: Loss = -10921.541015625
15
Stopping early at iteration 24600 due to no improvement.
pi: tensor([[9.9998e-01, 1.5385e-05],
        [9.9264e-01, 7.3586e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.7130e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1610, 0.2295],
         [0.9457, 0.1053]],

        [[0.9333, 0.0908],
         [0.9930, 0.0092]],

        [[0.9841, 0.1322],
         [0.0387, 0.2514]],

        [[0.9872, 0.7756],
         [0.8491, 0.3468]],

        [[0.0505, 0.0893],
         [0.7289, 0.9725]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.001507995594217576, 0.0] [-0.0007272727272727272, 0.0] [10918.9716796875, 10921.541015625]
-------------------------------------
This iteration is 28
True Objective function: Loss = -10879.456886335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -53287.921875
Iteration 100: Loss = -29950.58984375
Iteration 200: Loss = -15454.9892578125
Iteration 300: Loss = -12240.2236328125
Iteration 400: Loss = -11671.6884765625
Iteration 500: Loss = -11408.576171875
Iteration 600: Loss = -11269.4423828125
Iteration 700: Loss = -11204.6298828125
Iteration 800: Loss = -11160.2998046875
Iteration 900: Loss = -11131.8076171875
Iteration 1000: Loss = -11113.41015625
Iteration 1100: Loss = -11099.287109375
Iteration 1200: Loss = -11072.9384765625
Iteration 1300: Loss = -11062.3212890625
Iteration 1400: Loss = -11055.421875
Iteration 1500: Loss = -11049.8515625
Iteration 1600: Loss = -11045.1953125
Iteration 1700: Loss = -11041.2470703125
Iteration 1800: Loss = -11037.8603515625
Iteration 1900: Loss = -11034.9326171875
Iteration 2000: Loss = -11032.3798828125
Iteration 2100: Loss = -11030.134765625
Iteration 2200: Loss = -11028.1455078125
Iteration 2300: Loss = -11022.984375
Iteration 2400: Loss = -11015.2353515625
Iteration 2500: Loss = -11013.6689453125
Iteration 2600: Loss = -11012.4296875
Iteration 2700: Loss = -11011.33984375
Iteration 2800: Loss = -11010.361328125
Iteration 2900: Loss = -11009.4755859375
Iteration 3000: Loss = -11008.669921875
Iteration 3100: Loss = -11007.9345703125
Iteration 3200: Loss = -11007.2607421875
Iteration 3300: Loss = -11006.6416015625
Iteration 3400: Loss = -11006.0732421875
Iteration 3500: Loss = -11005.5498046875
Iteration 3600: Loss = -11005.0673828125
Iteration 3700: Loss = -11004.6201171875
Iteration 3800: Loss = -10999.70703125
Iteration 3900: Loss = -10998.48828125
Iteration 4000: Loss = -10998.046875
Iteration 4100: Loss = -10997.6767578125
Iteration 4200: Loss = -10997.34375
Iteration 4300: Loss = -10997.0400390625
Iteration 4400: Loss = -10996.7568359375
Iteration 4500: Loss = -10996.498046875
Iteration 4600: Loss = -10996.2548828125
Iteration 4700: Loss = -10996.02734375
Iteration 4800: Loss = -10995.81640625
Iteration 4900: Loss = -10995.619140625
Iteration 5000: Loss = -10995.4345703125
Iteration 5100: Loss = -10995.2607421875
Iteration 5200: Loss = -10995.09765625
Iteration 5300: Loss = -10994.9453125
Iteration 5400: Loss = -10994.80078125
Iteration 5500: Loss = -10994.6650390625
Iteration 5600: Loss = -10994.5380859375
Iteration 5700: Loss = -10994.41796875
Iteration 5800: Loss = -10994.3037109375
Iteration 5900: Loss = -10994.197265625
Iteration 6000: Loss = -10994.095703125
Iteration 6100: Loss = -10994.0
Iteration 6200: Loss = -10993.91015625
Iteration 6300: Loss = -10993.82421875
Iteration 6400: Loss = -10993.7431640625
Iteration 6500: Loss = -10993.6669921875
Iteration 6600: Loss = -10993.595703125
Iteration 6700: Loss = -10993.5263671875
Iteration 6800: Loss = -10993.4609375
Iteration 6900: Loss = -10993.3984375
Iteration 7000: Loss = -10993.3408203125
Iteration 7100: Loss = -10993.2861328125
Iteration 7200: Loss = -10993.232421875
Iteration 7300: Loss = -10993.1845703125
Iteration 7400: Loss = -10993.13671875
Iteration 7500: Loss = -10993.0927734375
Iteration 7600: Loss = -10993.046875
Iteration 7700: Loss = -10993.0087890625
Iteration 7800: Loss = -10992.970703125
Iteration 7900: Loss = -10992.93359375
Iteration 8000: Loss = -10992.8994140625
Iteration 8100: Loss = -10992.8662109375
Iteration 8200: Loss = -10992.8349609375
Iteration 8300: Loss = -10992.806640625
Iteration 8400: Loss = -10992.77734375
Iteration 8500: Loss = -10992.7509765625
Iteration 8600: Loss = -10992.7275390625
Iteration 8700: Loss = -10992.7001953125
Iteration 8800: Loss = -10992.6796875
Iteration 8900: Loss = -10992.6572265625
Iteration 9000: Loss = -10992.634765625
Iteration 9100: Loss = -10992.6162109375
Iteration 9200: Loss = -10992.59765625
Iteration 9300: Loss = -10992.580078125
Iteration 9400: Loss = -10992.5625
Iteration 9500: Loss = -10992.546875
Iteration 9600: Loss = -10992.5322265625
Iteration 9700: Loss = -10992.517578125
Iteration 9800: Loss = -10992.50390625
Iteration 9900: Loss = -10992.4912109375
Iteration 10000: Loss = -10992.478515625
Iteration 10100: Loss = -10992.466796875
Iteration 10200: Loss = -10992.4541015625
Iteration 10300: Loss = -10992.4453125
Iteration 10400: Loss = -10992.4345703125
Iteration 10500: Loss = -10992.4248046875
Iteration 10600: Loss = -10992.416015625
Iteration 10700: Loss = -10992.408203125
Iteration 10800: Loss = -10992.400390625
Iteration 10900: Loss = -10992.3916015625
Iteration 11000: Loss = -10992.3837890625
Iteration 11100: Loss = -10992.376953125
Iteration 11200: Loss = -10992.3701171875
Iteration 11300: Loss = -10992.3623046875
Iteration 11400: Loss = -10992.357421875
Iteration 11500: Loss = -10992.3515625
Iteration 11600: Loss = -10992.345703125
Iteration 11700: Loss = -10992.33984375
Iteration 11800: Loss = -10992.3359375
Iteration 11900: Loss = -10992.330078125
Iteration 12000: Loss = -10992.3271484375
Iteration 12100: Loss = -10992.3212890625
Iteration 12200: Loss = -10992.318359375
Iteration 12300: Loss = -10992.3154296875
Iteration 12400: Loss = -10992.310546875
Iteration 12500: Loss = -10992.306640625
Iteration 12600: Loss = -10992.3037109375
Iteration 12700: Loss = -10992.2998046875
Iteration 12800: Loss = -10992.2958984375
Iteration 12900: Loss = -10992.2939453125
Iteration 13000: Loss = -10992.2900390625
Iteration 13100: Loss = -10992.2880859375
Iteration 13200: Loss = -10992.2861328125
Iteration 13300: Loss = -10992.2841796875
Iteration 13400: Loss = -10992.28125
Iteration 13500: Loss = -10992.2783203125
Iteration 13600: Loss = -10992.27734375
Iteration 13700: Loss = -10992.2763671875
Iteration 13800: Loss = -10992.2744140625
Iteration 13900: Loss = -10992.271484375
Iteration 14000: Loss = -10992.2705078125
Iteration 14100: Loss = -10992.2685546875
Iteration 14200: Loss = -10992.267578125
Iteration 14300: Loss = -10992.265625
Iteration 14400: Loss = -10992.265625
Iteration 14500: Loss = -10992.2646484375
Iteration 14600: Loss = -10992.2626953125
Iteration 14700: Loss = -10992.2626953125
Iteration 14800: Loss = -10992.2607421875
Iteration 14900: Loss = -10992.259765625
Iteration 15000: Loss = -10992.2587890625
Iteration 15100: Loss = -10992.2587890625
Iteration 15200: Loss = -10992.2578125
Iteration 15300: Loss = -10992.2568359375
Iteration 15400: Loss = -10992.2568359375
Iteration 15500: Loss = -10992.2568359375
Iteration 15600: Loss = -10992.2548828125
Iteration 15700: Loss = -10992.25390625
Iteration 15800: Loss = -10992.255859375
1
Iteration 15900: Loss = -10992.2548828125
2
Iteration 16000: Loss = -10992.255859375
3
Iteration 16100: Loss = -10992.251953125
Iteration 16200: Loss = -10992.25390625
1
Iteration 16300: Loss = -10992.251953125
Iteration 16400: Loss = -10992.25390625
1
Iteration 16500: Loss = -10992.2529296875
2
Iteration 16600: Loss = -10992.2529296875
3
Iteration 16700: Loss = -10992.25390625
4
Iteration 16800: Loss = -10992.251953125
Iteration 16900: Loss = -10992.2509765625
Iteration 17000: Loss = -10992.2509765625
Iteration 17100: Loss = -10992.251953125
1
Iteration 17200: Loss = -10992.25
Iteration 17300: Loss = -10992.25
Iteration 17400: Loss = -10992.25
Iteration 17500: Loss = -10992.2490234375
Iteration 17600: Loss = -10992.2490234375
Iteration 17700: Loss = -10992.25
1
Iteration 17800: Loss = -10992.248046875
Iteration 17900: Loss = -10992.248046875
Iteration 18000: Loss = -10992.248046875
Iteration 18100: Loss = -10992.2490234375
1
Iteration 18200: Loss = -10992.248046875
Iteration 18300: Loss = -10992.248046875
Iteration 18400: Loss = -10992.2470703125
Iteration 18500: Loss = -10992.2470703125
Iteration 18600: Loss = -10992.248046875
1
Iteration 18700: Loss = -10992.2451171875
Iteration 18800: Loss = -10992.2451171875
Iteration 18900: Loss = -10992.244140625
Iteration 19000: Loss = -10992.244140625
Iteration 19100: Loss = -10992.244140625
Iteration 19200: Loss = -10992.244140625
Iteration 19300: Loss = -10992.2412109375
Iteration 19400: Loss = -10992.23828125
Iteration 19500: Loss = -10992.2333984375
Iteration 19600: Loss = -10992.2294921875
Iteration 19700: Loss = -10992.212890625
Iteration 19800: Loss = -10992.087890625
Iteration 19900: Loss = -10991.8359375
Iteration 20000: Loss = -10991.8095703125
Iteration 20100: Loss = -10991.767578125
Iteration 20200: Loss = -10991.767578125
Iteration 20300: Loss = -10991.765625
Iteration 20400: Loss = -10991.7666015625
1
Iteration 20500: Loss = -10991.7666015625
2
Iteration 20600: Loss = -10991.765625
Iteration 20700: Loss = -10991.7666015625
1
Iteration 20800: Loss = -10991.765625
Iteration 20900: Loss = -10991.7666015625
1
Iteration 21000: Loss = -10991.7666015625
2
Iteration 21100: Loss = -10991.765625
Iteration 21200: Loss = -10991.7666015625
1
Iteration 21300: Loss = -10991.765625
Iteration 21400: Loss = -10991.7646484375
Iteration 21500: Loss = -10991.7666015625
1
Iteration 21600: Loss = -10991.767578125
2
Iteration 21700: Loss = -10991.767578125
3
Iteration 21800: Loss = -10991.7666015625
4
Iteration 21900: Loss = -10991.765625
5
Iteration 22000: Loss = -10991.7666015625
6
Iteration 22100: Loss = -10991.7666015625
7
Iteration 22200: Loss = -10991.765625
8
Iteration 22300: Loss = -10991.7666015625
9
Iteration 22400: Loss = -10991.765625
10
Iteration 22500: Loss = -10991.765625
11
Iteration 22600: Loss = -10991.76953125
12
Iteration 22700: Loss = -10991.765625
13
Iteration 22800: Loss = -10991.7666015625
14
Iteration 22900: Loss = -10991.7666015625
15
Stopping early at iteration 22900 due to no improvement.
pi: tensor([[9.9997e-01, 3.2710e-05],
        [9.9795e-01, 2.0544e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9202, 0.0798], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1616, 0.1955],
         [0.9269, 0.2508]],

        [[0.0123, 0.1257],
         [0.0099, 0.9587]],

        [[0.0859, 0.1860],
         [0.1174, 0.5338]],

        [[0.6830, 0.1387],
         [0.1090, 0.0105]],

        [[0.7705, 0.1856],
         [0.0727, 0.0099]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23335.3984375
Iteration 100: Loss = -16540.98046875
Iteration 200: Loss = -12689.9150390625
Iteration 300: Loss = -11597.7275390625
Iteration 400: Loss = -11316.4443359375
Iteration 500: Loss = -11217.041015625
Iteration 600: Loss = -11171.1357421875
Iteration 700: Loss = -11140.8740234375
Iteration 800: Loss = -11121.6904296875
Iteration 900: Loss = -11110.4208984375
Iteration 1000: Loss = -11100.5400390625
Iteration 1100: Loss = -11090.3359375
Iteration 1200: Loss = -11081.1064453125
Iteration 1300: Loss = -11071.3857421875
Iteration 1400: Loss = -11062.2802734375
Iteration 1500: Loss = -11051.501953125
Iteration 1600: Loss = -11042.1826171875
Iteration 1700: Loss = -11036.6318359375
Iteration 1800: Loss = -11030.7001953125
Iteration 1900: Loss = -11023.2373046875
Iteration 2000: Loss = -11018.4423828125
Iteration 2100: Loss = -11015.3251953125
Iteration 2200: Loss = -11012.232421875
Iteration 2300: Loss = -11010.470703125
Iteration 2400: Loss = -11008.6591796875
Iteration 2500: Loss = -11006.7353515625
Iteration 2600: Loss = -11004.822265625
Iteration 2700: Loss = -11002.9189453125
Iteration 2800: Loss = -11000.9365234375
Iteration 2900: Loss = -11000.037109375
Iteration 3000: Loss = -10999.4599609375
Iteration 3100: Loss = -10999.033203125
Iteration 3200: Loss = -10998.6953125
Iteration 3300: Loss = -10998.4189453125
Iteration 3400: Loss = -10998.1884765625
Iteration 3500: Loss = -10997.9921875
Iteration 3600: Loss = -10997.8232421875
Iteration 3700: Loss = -10997.6767578125
Iteration 3800: Loss = -10997.5478515625
Iteration 3900: Loss = -10997.43359375
Iteration 4000: Loss = -10997.330078125
Iteration 4100: Loss = -10997.2373046875
Iteration 4200: Loss = -10997.154296875
Iteration 4300: Loss = -10997.0810546875
Iteration 4400: Loss = -10997.0107421875
Iteration 4500: Loss = -10996.94921875
Iteration 4600: Loss = -10996.8916015625
Iteration 4700: Loss = -10996.8369140625
Iteration 4800: Loss = -10996.7880859375
Iteration 4900: Loss = -10996.744140625
Iteration 5000: Loss = -10996.701171875
Iteration 5100: Loss = -10996.662109375
Iteration 5200: Loss = -10996.6259765625
Iteration 5300: Loss = -10996.5927734375
Iteration 5400: Loss = -10996.5615234375
Iteration 5500: Loss = -10996.53125
Iteration 5600: Loss = -10996.5029296875
Iteration 5700: Loss = -10996.4775390625
Iteration 5800: Loss = -10996.4541015625
Iteration 5900: Loss = -10996.4296875
Iteration 6000: Loss = -10996.4091796875
Iteration 6100: Loss = -10996.388671875
Iteration 6200: Loss = -10996.369140625
Iteration 6300: Loss = -10996.353515625
Iteration 6400: Loss = -10996.3359375
Iteration 6500: Loss = -10996.3212890625
Iteration 6600: Loss = -10996.306640625
Iteration 6700: Loss = -10996.2919921875
Iteration 6800: Loss = -10996.275390625
Iteration 6900: Loss = -10996.25
Iteration 7000: Loss = -10995.8447265625
Iteration 7100: Loss = -10995.5966796875
Iteration 7200: Loss = -10995.564453125
Iteration 7300: Loss = -10995.548828125
Iteration 7400: Loss = -10995.53515625
Iteration 7500: Loss = -10995.5244140625
Iteration 7600: Loss = -10995.515625
Iteration 7700: Loss = -10995.505859375
Iteration 7800: Loss = -10995.4970703125
Iteration 7900: Loss = -10995.4873046875
Iteration 8000: Loss = -10995.4169921875
Iteration 8100: Loss = -10993.919921875
Iteration 8200: Loss = -10993.591796875
Iteration 8300: Loss = -10993.4453125
Iteration 8400: Loss = -10993.38671875
Iteration 8500: Loss = -10993.359375
Iteration 8600: Loss = -10993.34375
Iteration 8700: Loss = -10993.3369140625
Iteration 8800: Loss = -10993.3310546875
Iteration 8900: Loss = -10993.3271484375
Iteration 9000: Loss = -10993.3232421875
Iteration 9100: Loss = -10993.3203125
Iteration 9200: Loss = -10993.3173828125
Iteration 9300: Loss = -10993.314453125
Iteration 9400: Loss = -10993.3125
Iteration 9500: Loss = -10993.3095703125
Iteration 9600: Loss = -10993.3076171875
Iteration 9700: Loss = -10993.3046875
Iteration 9800: Loss = -10993.302734375
Iteration 9900: Loss = -10993.30078125
Iteration 10000: Loss = -10993.296875
Iteration 10100: Loss = -10993.296875
Iteration 10200: Loss = -10993.2958984375
Iteration 10300: Loss = -10993.2939453125
Iteration 10400: Loss = -10993.29296875
Iteration 10500: Loss = -10993.291015625
Iteration 10600: Loss = -10993.2880859375
Iteration 10700: Loss = -10993.2880859375
Iteration 10800: Loss = -10993.287109375
Iteration 10900: Loss = -10993.283203125
Iteration 11000: Loss = -10993.2841796875
1
Iteration 11100: Loss = -10993.2822265625
Iteration 11200: Loss = -10993.28125
Iteration 11300: Loss = -10993.28125
Iteration 11400: Loss = -10993.2802734375
Iteration 11500: Loss = -10993.2783203125
Iteration 11600: Loss = -10993.27734375
Iteration 11700: Loss = -10993.2783203125
1
Iteration 11800: Loss = -10993.2763671875
Iteration 11900: Loss = -10993.2763671875
Iteration 12000: Loss = -10993.275390625
Iteration 12100: Loss = -10993.2734375
Iteration 12200: Loss = -10993.2734375
Iteration 12300: Loss = -10993.2724609375
Iteration 12400: Loss = -10993.2724609375
Iteration 12500: Loss = -10993.271484375
Iteration 12600: Loss = -10993.2705078125
Iteration 12700: Loss = -10993.2705078125
Iteration 12800: Loss = -10993.26953125
Iteration 12900: Loss = -10993.26953125
Iteration 13000: Loss = -10993.2685546875
Iteration 13100: Loss = -10993.2685546875
Iteration 13200: Loss = -10993.2685546875
Iteration 13300: Loss = -10993.2685546875
Iteration 13400: Loss = -10993.2666015625
Iteration 13500: Loss = -10993.2666015625
Iteration 13600: Loss = -10993.265625
Iteration 13700: Loss = -10993.267578125
1
Iteration 13800: Loss = -10993.265625
Iteration 13900: Loss = -10993.2646484375
Iteration 14000: Loss = -10993.265625
1
Iteration 14100: Loss = -10993.263671875
Iteration 14200: Loss = -10993.265625
1
Iteration 14300: Loss = -10993.2646484375
2
Iteration 14400: Loss = -10993.263671875
Iteration 14500: Loss = -10993.2646484375
1
Iteration 14600: Loss = -10993.2646484375
2
Iteration 14700: Loss = -10993.2646484375
3
Iteration 14800: Loss = -10993.263671875
Iteration 14900: Loss = -10993.2626953125
Iteration 15000: Loss = -10993.263671875
1
Iteration 15100: Loss = -10993.263671875
2
Iteration 15200: Loss = -10993.2626953125
Iteration 15300: Loss = -10993.263671875
1
Iteration 15400: Loss = -10993.2607421875
Iteration 15500: Loss = -10993.2646484375
1
Iteration 15600: Loss = -10993.26171875
2
Iteration 15700: Loss = -10993.263671875
3
Iteration 15800: Loss = -10993.26171875
4
Iteration 15900: Loss = -10993.2607421875
Iteration 16000: Loss = -10993.26171875
1
Iteration 16100: Loss = -10993.26171875
2
Iteration 16200: Loss = -10993.26171875
3
Iteration 16300: Loss = -10993.2607421875
Iteration 16400: Loss = -10993.26171875
1
Iteration 16500: Loss = -10993.2607421875
Iteration 16600: Loss = -10993.26171875
1
Iteration 16700: Loss = -10993.259765625
Iteration 16800: Loss = -10993.2607421875
1
Iteration 16900: Loss = -10993.2607421875
2
Iteration 17000: Loss = -10993.259765625
Iteration 17100: Loss = -10993.26171875
1
Iteration 17200: Loss = -10993.259765625
Iteration 17300: Loss = -10993.2607421875
1
Iteration 17400: Loss = -10993.259765625
Iteration 17500: Loss = -10993.2607421875
1
Iteration 17600: Loss = -10993.259765625
Iteration 17700: Loss = -10993.2607421875
1
Iteration 17800: Loss = -10993.259765625
Iteration 17900: Loss = -10993.259765625
Iteration 18000: Loss = -10993.2607421875
1
Iteration 18100: Loss = -10993.26171875
2
Iteration 18200: Loss = -10993.259765625
Iteration 18300: Loss = -10993.2607421875
1
Iteration 18400: Loss = -10993.2587890625
Iteration 18500: Loss = -10993.2607421875
1
Iteration 18600: Loss = -10993.259765625
2
Iteration 18700: Loss = -10993.2607421875
3
Iteration 18800: Loss = -10993.2607421875
4
Iteration 18900: Loss = -10993.259765625
5
Iteration 19000: Loss = -10993.2587890625
Iteration 19100: Loss = -10993.259765625
1
Iteration 19200: Loss = -10993.2587890625
Iteration 19300: Loss = -10993.259765625
1
Iteration 19400: Loss = -10993.2587890625
Iteration 19500: Loss = -10993.2607421875
1
Iteration 19600: Loss = -10993.259765625
2
Iteration 19700: Loss = -10993.2607421875
3
Iteration 19800: Loss = -10993.2607421875
4
Iteration 19900: Loss = -10993.259765625
5
Iteration 20000: Loss = -10993.2607421875
6
Iteration 20100: Loss = -10993.2333984375
Iteration 20200: Loss = -10993.208984375
Iteration 20300: Loss = -10993.208984375
Iteration 20400: Loss = -10993.2099609375
1
Iteration 20500: Loss = -10993.2080078125
Iteration 20600: Loss = -10993.2099609375
1
Iteration 20700: Loss = -10993.208984375
2
Iteration 20800: Loss = -10993.2099609375
3
Iteration 20900: Loss = -10993.2080078125
Iteration 21000: Loss = -10993.20703125
Iteration 21100: Loss = -10993.2080078125
1
Iteration 21200: Loss = -10992.09375
Iteration 21300: Loss = -10992.091796875
Iteration 21400: Loss = -10992.0908203125
Iteration 21500: Loss = -10992.0927734375
1
Iteration 21600: Loss = -10992.0908203125
Iteration 21700: Loss = -10992.091796875
1
Iteration 21800: Loss = -10992.091796875
2
Iteration 21900: Loss = -10992.0908203125
Iteration 22000: Loss = -10992.091796875
1
Iteration 22100: Loss = -10992.0927734375
2
Iteration 22200: Loss = -10992.091796875
3
Iteration 22300: Loss = -10992.091796875
4
Iteration 22400: Loss = -10992.091796875
5
Iteration 22500: Loss = -10992.091796875
6
Iteration 22600: Loss = -10992.0927734375
7
Iteration 22700: Loss = -10992.091796875
8
Iteration 22800: Loss = -10992.0927734375
9
Iteration 22900: Loss = -10992.091796875
10
Iteration 23000: Loss = -10992.091796875
11
Iteration 23100: Loss = -10992.091796875
12
Iteration 23200: Loss = -10992.091796875
13
Iteration 23300: Loss = -10992.0908203125
Iteration 23400: Loss = -10992.0927734375
1
Iteration 23500: Loss = -10992.0908203125
Iteration 23600: Loss = -10992.091796875
1
Iteration 23700: Loss = -10992.091796875
2
Iteration 23800: Loss = -10992.091796875
3
Iteration 23900: Loss = -10992.0908203125
Iteration 24000: Loss = -10992.091796875
1
Iteration 24100: Loss = -10992.0927734375
2
Iteration 24200: Loss = -10992.091796875
3
Iteration 24300: Loss = -10992.091796875
4
Iteration 24400: Loss = -10992.091796875
5
Iteration 24500: Loss = -10992.091796875
6
Iteration 24600: Loss = -10992.0927734375
7
Iteration 24700: Loss = -10992.001953125
Iteration 24800: Loss = -10991.783203125
Iteration 24900: Loss = -10991.765625
Iteration 25000: Loss = -10991.7666015625
1
Iteration 25100: Loss = -10991.765625
Iteration 25200: Loss = -10991.7666015625
1
Iteration 25300: Loss = -10991.765625
Iteration 25400: Loss = -10991.765625
Iteration 25500: Loss = -10991.7646484375
Iteration 25600: Loss = -10991.7646484375
Iteration 25700: Loss = -10991.765625
1
Iteration 25800: Loss = -10991.7666015625
2
Iteration 25900: Loss = -10991.7646484375
Iteration 26000: Loss = -10991.765625
1
Iteration 26100: Loss = -10991.7666015625
2
Iteration 26200: Loss = -10991.765625
3
Iteration 26300: Loss = -10991.765625
4
Iteration 26400: Loss = -10991.7646484375
Iteration 26500: Loss = -10991.7646484375
Iteration 26600: Loss = -10991.7646484375
Iteration 26700: Loss = -10991.765625
1
Iteration 26800: Loss = -10991.763671875
Iteration 26900: Loss = -10991.763671875
Iteration 27000: Loss = -10991.7646484375
1
Iteration 27100: Loss = -10991.765625
2
Iteration 27200: Loss = -10991.7646484375
3
Iteration 27300: Loss = -10991.763671875
Iteration 27400: Loss = -10991.7646484375
1
Iteration 27500: Loss = -10991.765625
2
Iteration 27600: Loss = -10991.765625
3
Iteration 27700: Loss = -10991.7646484375
4
Iteration 27800: Loss = -10991.765625
5
Iteration 27900: Loss = -10991.765625
6
Iteration 28000: Loss = -10991.7666015625
7
Iteration 28100: Loss = -10991.765625
8
Iteration 28200: Loss = -10991.765625
9
Iteration 28300: Loss = -10991.7666015625
10
Iteration 28400: Loss = -10991.7666015625
11
Iteration 28500: Loss = -10991.765625
12
Iteration 28600: Loss = -10991.765625
13
Iteration 28700: Loss = -10991.767578125
14
Iteration 28800: Loss = -10991.765625
15
Stopping early at iteration 28800 due to no improvement.
pi: tensor([[4.5897e-06, 1.0000e+00],
        [3.0357e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0798, 0.9202], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2508, 0.1955],
         [0.9915, 0.1616]],

        [[0.4490, 0.1473],
         [0.9912, 0.3144]],

        [[0.9918, 0.1616],
         [0.9817, 0.9042]],

        [[0.0109, 0.2784],
         [0.9607, 0.6227]],

        [[0.0321, 0.1788],
         [0.6484, 0.6384]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [10991.7666015625, 10991.765625]
-------------------------------------
This iteration is 29
True Objective function: Loss = -10949.586584433191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37901.15625
Iteration 100: Loss = -23609.5859375
Iteration 200: Loss = -14021.9619140625
Iteration 300: Loss = -11905.369140625
Iteration 400: Loss = -11548.267578125
Iteration 500: Loss = -11387.1796875
Iteration 600: Loss = -11297.3232421875
Iteration 700: Loss = -11238.8837890625
Iteration 800: Loss = -11193.30078125
Iteration 900: Loss = -11160.7900390625
Iteration 1000: Loss = -11134.583984375
Iteration 1100: Loss = -11115.4560546875
Iteration 1200: Loss = -11100.646484375
Iteration 1300: Loss = -11087.7978515625
Iteration 1400: Loss = -11078.892578125
Iteration 1500: Loss = -11072.9619140625
Iteration 1600: Loss = -11068.380859375
Iteration 1700: Loss = -11064.619140625
Iteration 1800: Loss = -11057.8369140625
Iteration 1900: Loss = -11054.6142578125
Iteration 2000: Loss = -11052.1640625
Iteration 2100: Loss = -11050.078125
Iteration 2200: Loss = -11048.271484375
Iteration 2300: Loss = -11046.6904296875
Iteration 2400: Loss = -11045.2919921875
Iteration 2500: Loss = -11044.044921875
Iteration 2600: Loss = -11042.92578125
Iteration 2700: Loss = -11041.9296875
Iteration 2800: Loss = -11041.0341796875
Iteration 2900: Loss = -11040.2255859375
Iteration 3000: Loss = -11039.490234375
Iteration 3100: Loss = -11038.822265625
Iteration 3200: Loss = -11038.2109375
Iteration 3300: Loss = -11037.6533203125
Iteration 3400: Loss = -11037.138671875
Iteration 3500: Loss = -11036.6669921875
Iteration 3600: Loss = -11036.232421875
Iteration 3700: Loss = -11035.8310546875
Iteration 3800: Loss = -11035.4599609375
Iteration 3900: Loss = -11035.115234375
Iteration 4000: Loss = -11034.7958984375
Iteration 4100: Loss = -11034.4990234375
Iteration 4200: Loss = -11034.224609375
Iteration 4300: Loss = -11033.966796875
Iteration 4400: Loss = -11033.728515625
Iteration 4500: Loss = -11033.5029296875
Iteration 4600: Loss = -11033.294921875
Iteration 4700: Loss = -11033.0986328125
Iteration 4800: Loss = -11032.916015625
Iteration 4900: Loss = -11032.7431640625
Iteration 5000: Loss = -11032.58203125
Iteration 5100: Loss = -11032.4296875
Iteration 5200: Loss = -11032.2900390625
Iteration 5300: Loss = -11032.1552734375
Iteration 5400: Loss = -11032.0302734375
Iteration 5500: Loss = -11031.9111328125
Iteration 5600: Loss = -11031.80078125
Iteration 5700: Loss = -11031.697265625
Iteration 5800: Loss = -11031.5986328125
Iteration 5900: Loss = -11031.50390625
Iteration 6000: Loss = -11031.416015625
Iteration 6100: Loss = -11031.3330078125
Iteration 6200: Loss = -11031.255859375
Iteration 6300: Loss = -11031.1826171875
Iteration 6400: Loss = -11031.11328125
Iteration 6500: Loss = -11031.0458984375
Iteration 6600: Loss = -11030.982421875
Iteration 6700: Loss = -11030.9248046875
Iteration 6800: Loss = -11030.87109375
Iteration 6900: Loss = -11030.81640625
Iteration 7000: Loss = -11030.7666015625
Iteration 7100: Loss = -11030.7197265625
Iteration 7200: Loss = -11030.67578125
Iteration 7300: Loss = -11030.6318359375
Iteration 7400: Loss = -11030.5927734375
Iteration 7500: Loss = -11030.5537109375
Iteration 7600: Loss = -11030.5185546875
Iteration 7700: Loss = -11030.4853515625
Iteration 7800: Loss = -11030.451171875
Iteration 7900: Loss = -11030.421875
Iteration 8000: Loss = -11030.392578125
Iteration 8100: Loss = -11030.3642578125
Iteration 8200: Loss = -11030.337890625
Iteration 8300: Loss = -11030.3134765625
Iteration 8400: Loss = -11030.2890625
Iteration 8500: Loss = -11030.267578125
Iteration 8600: Loss = -11030.2470703125
Iteration 8700: Loss = -11030.2275390625
Iteration 8800: Loss = -11030.20703125
Iteration 8900: Loss = -11030.189453125
Iteration 9000: Loss = -11030.1728515625
Iteration 9100: Loss = -11030.1572265625
Iteration 9200: Loss = -11030.146484375
Iteration 9300: Loss = -11030.1279296875
Iteration 9400: Loss = -11030.11328125
Iteration 9500: Loss = -11030.1005859375
Iteration 9600: Loss = -11030.087890625
Iteration 9700: Loss = -11030.076171875
Iteration 9800: Loss = -11030.0654296875
Iteration 9900: Loss = -11030.0546875
Iteration 10000: Loss = -11030.0439453125
Iteration 10100: Loss = -11030.033203125
Iteration 10200: Loss = -11030.0244140625
Iteration 10300: Loss = -11030.015625
Iteration 10400: Loss = -11030.009765625
Iteration 10500: Loss = -11030.0009765625
Iteration 10600: Loss = -11029.9931640625
Iteration 10700: Loss = -11029.986328125
Iteration 10800: Loss = -11029.9794921875
Iteration 10900: Loss = -11029.97265625
Iteration 11000: Loss = -11029.9677734375
Iteration 11100: Loss = -11029.9638671875
Iteration 11200: Loss = -11029.9560546875
Iteration 11300: Loss = -11029.951171875
Iteration 11400: Loss = -11029.9453125
Iteration 11500: Loss = -11029.94140625
Iteration 11600: Loss = -11029.9384765625
Iteration 11700: Loss = -11029.9345703125
Iteration 11800: Loss = -11029.9306640625
Iteration 11900: Loss = -11029.9248046875
Iteration 12000: Loss = -11029.9228515625
Iteration 12100: Loss = -11029.91796875
Iteration 12200: Loss = -11029.9150390625
Iteration 12300: Loss = -11029.912109375
Iteration 12400: Loss = -11029.908203125
Iteration 12500: Loss = -11029.9072265625
Iteration 12600: Loss = -11029.9052734375
Iteration 12700: Loss = -11029.9013671875
Iteration 12800: Loss = -11029.8974609375
Iteration 12900: Loss = -11029.8974609375
Iteration 13000: Loss = -11029.8955078125
Iteration 13100: Loss = -11029.890625
Iteration 13200: Loss = -11029.8896484375
Iteration 13300: Loss = -11029.8876953125
Iteration 13400: Loss = -11029.8876953125
Iteration 13500: Loss = -11029.88671875
Iteration 13600: Loss = -11029.8837890625
Iteration 13700: Loss = -11029.8818359375
Iteration 13800: Loss = -11029.8818359375
Iteration 13900: Loss = -11029.880859375
Iteration 14000: Loss = -11029.87890625
Iteration 14100: Loss = -11029.876953125
Iteration 14200: Loss = -11029.8759765625
Iteration 14300: Loss = -11029.875
Iteration 14400: Loss = -11029.875
Iteration 14500: Loss = -11029.873046875
Iteration 14600: Loss = -11029.8720703125
Iteration 14700: Loss = -11029.8720703125
Iteration 14800: Loss = -11029.87109375
Iteration 14900: Loss = -11029.87109375
Iteration 15000: Loss = -11029.869140625
Iteration 15100: Loss = -11029.8701171875
1
Iteration 15200: Loss = -11029.869140625
Iteration 15300: Loss = -11029.8681640625
Iteration 15400: Loss = -11029.8671875
Iteration 15500: Loss = -11029.8681640625
1
Iteration 15600: Loss = -11029.8662109375
Iteration 15700: Loss = -11029.859375
Iteration 15800: Loss = -11029.8427734375
Iteration 15900: Loss = -11029.71484375
Iteration 16000: Loss = -11029.251953125
Iteration 16100: Loss = -11029.205078125
Iteration 16200: Loss = -11029.134765625
Iteration 16300: Loss = -11029.072265625
Iteration 16400: Loss = -11029.0185546875
Iteration 16500: Loss = -11028.9921875
Iteration 16600: Loss = -11028.955078125
Iteration 16700: Loss = -11028.8388671875
Iteration 16800: Loss = -11028.7841796875
Iteration 16900: Loss = -11028.5927734375
Iteration 17000: Loss = -11028.544921875
Iteration 17100: Loss = -11028.322265625
Iteration 17200: Loss = -11027.8876953125
Iteration 17300: Loss = -11027.8125
Iteration 17400: Loss = -11027.7861328125
Iteration 17500: Loss = -11027.7646484375
Iteration 17600: Loss = -11027.7451171875
Iteration 17700: Loss = -11027.7265625
Iteration 17800: Loss = -11027.720703125
Iteration 17900: Loss = -11027.70703125
Iteration 18000: Loss = -11027.6787109375
Iteration 18100: Loss = -11027.6650390625
Iteration 18200: Loss = -11027.650390625
Iteration 18300: Loss = -11027.6416015625
Iteration 18400: Loss = -11027.62109375
Iteration 18500: Loss = -11027.5830078125
Iteration 18600: Loss = -11027.55078125
Iteration 18700: Loss = -11027.5244140625
Iteration 18800: Loss = -11027.5087890625
Iteration 18900: Loss = -11027.4833984375
Iteration 19000: Loss = -11027.45703125
Iteration 19100: Loss = -11027.39453125
Iteration 19200: Loss = -11027.3798828125
Iteration 19300: Loss = -11027.326171875
Iteration 19400: Loss = -11027.3017578125
Iteration 19500: Loss = -11027.2783203125
Iteration 19600: Loss = -11027.2529296875
Iteration 19700: Loss = -11027.236328125
Iteration 19800: Loss = -11027.236328125
Iteration 19900: Loss = -11027.2373046875
1
Iteration 20000: Loss = -11027.234375
Iteration 20100: Loss = -11027.2333984375
Iteration 20200: Loss = -11027.2333984375
Iteration 20300: Loss = -11027.234375
1
Iteration 20400: Loss = -11027.2197265625
Iteration 20500: Loss = -11027.2197265625
Iteration 20600: Loss = -11027.216796875
Iteration 20700: Loss = -11027.2099609375
Iteration 20800: Loss = -11027.2099609375
Iteration 20900: Loss = -11027.2080078125
Iteration 21000: Loss = -11027.1884765625
Iteration 21100: Loss = -11027.1806640625
Iteration 21200: Loss = -11027.1748046875
Iteration 21300: Loss = -11027.1650390625
Iteration 21400: Loss = -11027.1650390625
Iteration 21500: Loss = -11027.166015625
1
Iteration 21600: Loss = -11027.162109375
Iteration 21700: Loss = -11027.16015625
Iteration 21800: Loss = -11027.16015625
Iteration 21900: Loss = -11027.1572265625
Iteration 22000: Loss = -11027.1572265625
Iteration 22100: Loss = -11027.1552734375
Iteration 22200: Loss = -11027.15625
1
Iteration 22300: Loss = -11027.1396484375
Iteration 22400: Loss = -11027.134765625
Iteration 22500: Loss = -11027.1298828125
Iteration 22600: Loss = -11027.1318359375
1
Iteration 22700: Loss = -11027.123046875
Iteration 22800: Loss = -11027.123046875
Iteration 22900: Loss = -11027.1220703125
Iteration 23000: Loss = -11027.1201171875
Iteration 23100: Loss = -11027.1064453125
Iteration 23200: Loss = -11027.10546875
Iteration 23300: Loss = -11027.099609375
Iteration 23400: Loss = -11027.09765625
Iteration 23500: Loss = -11027.095703125
Iteration 23600: Loss = -11027.095703125
Iteration 23700: Loss = -11027.0947265625
Iteration 23800: Loss = -11027.0771484375
Iteration 23900: Loss = -11027.078125
1
Iteration 24000: Loss = -11027.07421875
Iteration 24100: Loss = -11027.0634765625
Iteration 24200: Loss = -11027.033203125
Iteration 24300: Loss = -11027.0234375
Iteration 24400: Loss = -11027.0126953125
Iteration 24500: Loss = -11027.01171875
Iteration 24600: Loss = -11027.0107421875
Iteration 24700: Loss = -11027.0078125
Iteration 24800: Loss = -11026.98828125
Iteration 24900: Loss = -11026.947265625
Iteration 25000: Loss = -11026.939453125
Iteration 25100: Loss = -11026.9208984375
Iteration 25200: Loss = -11026.916015625
Iteration 25300: Loss = -11026.9033203125
Iteration 25400: Loss = -11026.896484375
Iteration 25500: Loss = -11026.8916015625
Iteration 25600: Loss = -11026.892578125
1
Iteration 25700: Loss = -11026.8876953125
Iteration 25800: Loss = -11026.888671875
1
Iteration 25900: Loss = -11026.888671875
2
Iteration 26000: Loss = -11026.873046875
Iteration 26100: Loss = -11026.8642578125
Iteration 26200: Loss = -11026.86328125
Iteration 26300: Loss = -11026.861328125
Iteration 26400: Loss = -11026.861328125
Iteration 26500: Loss = -11026.8623046875
1
Iteration 26600: Loss = -11026.8623046875
2
Iteration 26700: Loss = -11026.861328125
Iteration 26800: Loss = -11026.8603515625
Iteration 26900: Loss = -11026.8603515625
Iteration 27000: Loss = -11026.8603515625
Iteration 27100: Loss = -11026.8427734375
Iteration 27200: Loss = -11026.8408203125
Iteration 27300: Loss = -11026.8408203125
Iteration 27400: Loss = -11026.83984375
Iteration 27500: Loss = -11026.8408203125
1
Iteration 27600: Loss = -11026.83984375
Iteration 27700: Loss = -11026.828125
Iteration 27800: Loss = -11026.8232421875
Iteration 27900: Loss = -11026.8232421875
Iteration 28000: Loss = -11026.8232421875
Iteration 28100: Loss = -11026.82421875
1
Iteration 28200: Loss = -11026.8232421875
Iteration 28300: Loss = -11026.822265625
Iteration 28400: Loss = -11026.8212890625
Iteration 28500: Loss = -11026.8212890625
Iteration 28600: Loss = -11026.8232421875
1
Iteration 28700: Loss = -11026.822265625
2
Iteration 28800: Loss = -11026.822265625
3
Iteration 28900: Loss = -11026.822265625
4
Iteration 29000: Loss = -11026.822265625
5
Iteration 29100: Loss = -11026.822265625
6
Iteration 29200: Loss = -11026.8232421875
7
Iteration 29300: Loss = -11026.8212890625
Iteration 29400: Loss = -11026.822265625
1
Iteration 29500: Loss = -11026.8232421875
2
Iteration 29600: Loss = -11026.8212890625
Iteration 29700: Loss = -11026.822265625
1
Iteration 29800: Loss = -11026.822265625
2
Iteration 29900: Loss = -11026.8212890625
pi: tensor([[9.8993e-01, 1.0072e-02],
        [3.9402e-05, 9.9996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 7.7651e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1622, 0.1638],
         [0.2402, 0.1098]],

        [[0.9740, 0.2175],
         [0.2223, 0.9827]],

        [[0.5959, 0.2884],
         [0.0357, 0.0400]],

        [[0.8207, 0.1777],
         [0.4926, 0.9231]],

        [[0.6248, 0.1592],
         [0.9917, 0.9623]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
Global Adjusted Rand Index: -0.0019182131166299715
Average Adjusted Rand Index: -0.0022786341519830692
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37371.20703125
Iteration 100: Loss = -21803.341796875
Iteration 200: Loss = -13383.2958984375
Iteration 300: Loss = -11828.1435546875
Iteration 400: Loss = -11432.015625
Iteration 500: Loss = -11281.0078125
Iteration 600: Loss = -11188.251953125
Iteration 700: Loss = -11142.9130859375
Iteration 800: Loss = -11113.619140625
Iteration 900: Loss = -11088.8330078125
Iteration 1000: Loss = -11078.8232421875
Iteration 1100: Loss = -11071.09765625
Iteration 1200: Loss = -11064.4755859375
Iteration 1300: Loss = -11058.6005859375
Iteration 1400: Loss = -11054.1279296875
Iteration 1500: Loss = -11050.7734375
Iteration 1600: Loss = -11048.1318359375
Iteration 1700: Loss = -11045.9814453125
Iteration 1800: Loss = -11044.1923828125
Iteration 1900: Loss = -11042.6865234375
Iteration 2000: Loss = -11041.40234375
Iteration 2100: Loss = -11040.29296875
Iteration 2200: Loss = -11039.3232421875
Iteration 2300: Loss = -11038.4765625
Iteration 2400: Loss = -11037.73046875
Iteration 2500: Loss = -11037.068359375
Iteration 2600: Loss = -11036.4794921875
Iteration 2700: Loss = -11035.9541015625
Iteration 2800: Loss = -11035.4814453125
Iteration 2900: Loss = -11035.0576171875
Iteration 3000: Loss = -11034.6728515625
Iteration 3100: Loss = -11034.3232421875
Iteration 3200: Loss = -11034.0068359375
Iteration 3300: Loss = -11033.7197265625
Iteration 3400: Loss = -11033.455078125
Iteration 3500: Loss = -11033.2158203125
Iteration 3600: Loss = -11032.994140625
Iteration 3700: Loss = -11032.7880859375
Iteration 3800: Loss = -11032.6044921875
Iteration 3900: Loss = -11032.4287109375
Iteration 4000: Loss = -11032.2685546875
Iteration 4100: Loss = -11032.1201171875
Iteration 4200: Loss = -11031.9814453125
Iteration 4300: Loss = -11031.8525390625
Iteration 4400: Loss = -11031.732421875
Iteration 4500: Loss = -11031.62109375
Iteration 4600: Loss = -11031.5185546875
Iteration 4700: Loss = -11031.419921875
Iteration 4800: Loss = -11031.3291015625
Iteration 4900: Loss = -11031.2431640625
Iteration 5000: Loss = -11031.1630859375
Iteration 5100: Loss = -11031.087890625
Iteration 5200: Loss = -11031.0166015625
Iteration 5300: Loss = -11030.9501953125
Iteration 5400: Loss = -11030.890625
Iteration 5500: Loss = -11030.830078125
Iteration 5600: Loss = -11030.77734375
Iteration 5700: Loss = -11030.724609375
Iteration 5800: Loss = -11030.67578125
Iteration 5900: Loss = -11030.6298828125
Iteration 6000: Loss = -11030.587890625
Iteration 6100: Loss = -11030.5478515625
Iteration 6200: Loss = -11030.5107421875
Iteration 6300: Loss = -11030.4736328125
Iteration 6400: Loss = -11030.439453125
Iteration 6500: Loss = -11030.4091796875
Iteration 6600: Loss = -11030.3779296875
Iteration 6700: Loss = -11030.3505859375
Iteration 6800: Loss = -11030.3232421875
Iteration 6900: Loss = -11030.296875
Iteration 7000: Loss = -11030.2734375
Iteration 7100: Loss = -11030.2509765625
Iteration 7200: Loss = -11030.2314453125
Iteration 7300: Loss = -11030.2119140625
Iteration 7400: Loss = -11030.19140625
Iteration 7500: Loss = -11030.17578125
Iteration 7600: Loss = -11030.1572265625
Iteration 7700: Loss = -11030.1416015625
Iteration 7800: Loss = -11030.1259765625
Iteration 7900: Loss = -11030.111328125
Iteration 8000: Loss = -11030.09765625
Iteration 8100: Loss = -11030.0830078125
Iteration 8200: Loss = -11030.0712890625
Iteration 8300: Loss = -11030.0576171875
Iteration 8400: Loss = -11030.044921875
Iteration 8500: Loss = -11030.0341796875
Iteration 8600: Loss = -11030.0205078125
Iteration 8700: Loss = -11030.009765625
Iteration 8800: Loss = -11029.99609375
Iteration 8900: Loss = -11029.9775390625
Iteration 9000: Loss = -11029.958984375
Iteration 9100: Loss = -11029.923828125
Iteration 9200: Loss = -11029.8505859375
Iteration 9300: Loss = -11029.7431640625
Iteration 9400: Loss = -11029.701171875
Iteration 9500: Loss = -11029.6611328125
Iteration 9600: Loss = -11029.5732421875
Iteration 9700: Loss = -11029.5009765625
Iteration 9800: Loss = -11029.3798828125
Iteration 9900: Loss = -11029.197265625
Iteration 10000: Loss = -11028.8759765625
Iteration 10100: Loss = -11027.3828125
Iteration 10200: Loss = -11027.2119140625
Iteration 10300: Loss = -11027.123046875
Iteration 10400: Loss = -11026.9814453125
Iteration 10500: Loss = -11026.8662109375
Iteration 10600: Loss = -11026.8125
Iteration 10700: Loss = -11026.76171875
Iteration 10800: Loss = -11026.7265625
Iteration 10900: Loss = -11026.697265625
Iteration 11000: Loss = -11026.671875
Iteration 11100: Loss = -11026.6494140625
Iteration 11200: Loss = -11026.6298828125
Iteration 11300: Loss = -11026.61328125
Iteration 11400: Loss = -11026.6005859375
Iteration 11500: Loss = -11026.5859375
Iteration 11600: Loss = -11026.5751953125
Iteration 11700: Loss = -11026.5634765625
Iteration 11800: Loss = -11026.5546875
Iteration 11900: Loss = -11026.5458984375
Iteration 12000: Loss = -11026.5390625
Iteration 12100: Loss = -11026.5322265625
Iteration 12200: Loss = -11026.525390625
Iteration 12300: Loss = -11026.5205078125
Iteration 12400: Loss = -11026.515625
Iteration 12500: Loss = -11026.513671875
Iteration 12600: Loss = -11026.5078125
Iteration 12700: Loss = -11026.5029296875
Iteration 12800: Loss = -11026.5
Iteration 12900: Loss = -11026.498046875
Iteration 13000: Loss = -11026.4951171875
Iteration 13100: Loss = -11026.4931640625
Iteration 13200: Loss = -11026.4921875
Iteration 13300: Loss = -11026.48828125
Iteration 13400: Loss = -11026.486328125
Iteration 13500: Loss = -11026.4853515625
Iteration 13600: Loss = -11026.484375
Iteration 13700: Loss = -11026.4833984375
Iteration 13800: Loss = -11026.482421875
Iteration 13900: Loss = -11026.48046875
Iteration 14000: Loss = -11026.478515625
Iteration 14100: Loss = -11026.4794921875
1
Iteration 14200: Loss = -11026.4765625
Iteration 14300: Loss = -11026.4755859375
Iteration 14400: Loss = -11026.4755859375
Iteration 14500: Loss = -11026.474609375
Iteration 14600: Loss = -11026.47265625
Iteration 14700: Loss = -11026.4716796875
Iteration 14800: Loss = -11026.4716796875
Iteration 14900: Loss = -11026.4716796875
Iteration 15000: Loss = -11026.470703125
Iteration 15100: Loss = -11026.4697265625
Iteration 15200: Loss = -11026.46875
Iteration 15300: Loss = -11026.4697265625
1
Iteration 15400: Loss = -11026.46875
Iteration 15500: Loss = -11026.46875
Iteration 15600: Loss = -11026.4677734375
Iteration 15700: Loss = -11026.466796875
Iteration 15800: Loss = -11026.4658203125
Iteration 15900: Loss = -11026.4658203125
Iteration 16000: Loss = -11026.4677734375
1
Iteration 16100: Loss = -11026.4658203125
Iteration 16200: Loss = -11026.46484375
Iteration 16300: Loss = -11026.46484375
Iteration 16400: Loss = -11026.4638671875
Iteration 16500: Loss = -11026.4638671875
Iteration 16600: Loss = -11026.46484375
1
Iteration 16700: Loss = -11026.46484375
2
Iteration 16800: Loss = -11026.4638671875
Iteration 16900: Loss = -11026.46484375
1
Iteration 17000: Loss = -11026.462890625
Iteration 17100: Loss = -11026.462890625
Iteration 17200: Loss = -11026.462890625
Iteration 17300: Loss = -11026.4638671875
1
Iteration 17400: Loss = -11026.4619140625
Iteration 17500: Loss = -11026.4619140625
Iteration 17600: Loss = -11026.4619140625
Iteration 17700: Loss = -11026.4619140625
Iteration 17800: Loss = -11026.4619140625
Iteration 17900: Loss = -11026.462890625
1
Iteration 18000: Loss = -11026.4619140625
Iteration 18100: Loss = -11026.4619140625
Iteration 18200: Loss = -11026.4619140625
Iteration 18300: Loss = -11026.4609375
Iteration 18400: Loss = -11026.4619140625
1
Iteration 18500: Loss = -11026.458984375
Iteration 18600: Loss = -11026.4609375
1
Iteration 18700: Loss = -11026.4599609375
2
Iteration 18800: Loss = -11026.4609375
3
Iteration 18900: Loss = -11026.462890625
4
Iteration 19000: Loss = -11026.4609375
5
Iteration 19100: Loss = -11026.4599609375
6
Iteration 19200: Loss = -11026.4501953125
Iteration 19300: Loss = -11026.4453125
Iteration 19400: Loss = -11026.271484375
Iteration 19500: Loss = -11025.84765625
Iteration 19600: Loss = -11025.8466796875
Iteration 19700: Loss = -11025.8447265625
Iteration 19800: Loss = -11025.845703125
1
Iteration 19900: Loss = -11025.8427734375
Iteration 20000: Loss = -11025.841796875
Iteration 20100: Loss = -11025.8427734375
1
Iteration 20200: Loss = -11025.83203125
Iteration 20300: Loss = -11025.8291015625
Iteration 20400: Loss = -11025.8291015625
Iteration 20500: Loss = -11025.8291015625
Iteration 20600: Loss = -11025.830078125
1
Iteration 20700: Loss = -11025.8291015625
Iteration 20800: Loss = -11025.828125
Iteration 20900: Loss = -11025.830078125
1
Iteration 21000: Loss = -11025.8291015625
2
Iteration 21100: Loss = -11025.8271484375
Iteration 21200: Loss = -11025.828125
1
Iteration 21300: Loss = -11025.828125
2
Iteration 21400: Loss = -11025.8291015625
3
Iteration 21500: Loss = -11025.8271484375
Iteration 21600: Loss = -11025.8271484375
Iteration 21700: Loss = -11025.8291015625
1
Iteration 21800: Loss = -11025.828125
2
Iteration 21900: Loss = -11025.8291015625
3
Iteration 22000: Loss = -11025.828125
4
Iteration 22100: Loss = -11025.8271484375
Iteration 22200: Loss = -11025.8291015625
1
Iteration 22300: Loss = -11025.8271484375
Iteration 22400: Loss = -11025.828125
1
Iteration 22500: Loss = -11025.828125
2
Iteration 22600: Loss = -11025.828125
3
Iteration 22700: Loss = -11025.828125
4
Iteration 22800: Loss = -11025.826171875
Iteration 22900: Loss = -11025.8291015625
1
Iteration 23000: Loss = -11025.8291015625
2
Iteration 23100: Loss = -11025.8271484375
3
Iteration 23200: Loss = -11025.828125
4
Iteration 23300: Loss = -11025.828125
5
Iteration 23400: Loss = -11025.828125
6
Iteration 23500: Loss = -11025.828125
7
Iteration 23600: Loss = -11025.828125
8
Iteration 23700: Loss = -11025.828125
9
Iteration 23800: Loss = -11025.828125
10
Iteration 23900: Loss = -11025.828125
11
Iteration 24000: Loss = -11025.828125
12
Iteration 24100: Loss = -11025.828125
13
Iteration 24200: Loss = -11025.828125
14
Iteration 24300: Loss = -11025.828125
15
Stopping early at iteration 24300 due to no improvement.
pi: tensor([[7.0054e-01, 2.9946e-01],
        [1.3038e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0268, 0.9732], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.8011, 0.1212],
         [0.3824, 0.1627]],

        [[0.1580, 0.2358],
         [0.7242, 0.0294]],

        [[0.0615, 0.2901],
         [0.0968, 0.4983]],

        [[0.6514, 0.1732],
         [0.0951, 0.7048]],

        [[0.8425, 0.1428],
         [0.3448, 0.2441]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0019182131166299715
Average Adjusted Rand Index: -0.002534951514341837
[-0.0019182131166299715, -0.0019182131166299715] [-0.0022786341519830692, -0.002534951514341837] [11026.8232421875, 11025.828125]
-------------------------------------
This iteration is 30
True Objective function: Loss = -10757.533140305504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32660.015625
Iteration 100: Loss = -20469.630859375
Iteration 200: Loss = -12925.916015625
Iteration 300: Loss = -11554.6708984375
Iteration 400: Loss = -11258.7216796875
Iteration 500: Loss = -11129.2734375
Iteration 600: Loss = -11064.271484375
Iteration 700: Loss = -11028.8447265625
Iteration 800: Loss = -11006.3955078125
Iteration 900: Loss = -10988.2939453125
Iteration 1000: Loss = -10975.9609375
Iteration 1100: Loss = -10965.1455078125
Iteration 1200: Loss = -10954.037109375
Iteration 1300: Loss = -10946.666015625
Iteration 1400: Loss = -10938.0830078125
Iteration 1500: Loss = -10933.1513671875
Iteration 1600: Loss = -10928.173828125
Iteration 1700: Loss = -10923.0078125
Iteration 1800: Loss = -10918.8916015625
Iteration 1900: Loss = -10914.98828125
Iteration 2000: Loss = -10911.927734375
Iteration 2100: Loss = -10907.5810546875
Iteration 2200: Loss = -10902.8798828125
Iteration 2300: Loss = -10901.18359375
Iteration 2400: Loss = -10900.15625
Iteration 2500: Loss = -10899.392578125
Iteration 2600: Loss = -10898.779296875
Iteration 2700: Loss = -10898.1318359375
Iteration 2800: Loss = -10893.7783203125
Iteration 2900: Loss = -10893.0244140625
Iteration 3000: Loss = -10891.2607421875
Iteration 3100: Loss = -10890.0830078125
Iteration 3200: Loss = -10889.5791015625
Iteration 3300: Loss = -10889.236328125
Iteration 3400: Loss = -10888.966796875
Iteration 3500: Loss = -10888.7421875
Iteration 3600: Loss = -10888.5498046875
Iteration 3700: Loss = -10888.3779296875
Iteration 3800: Loss = -10888.2216796875
Iteration 3900: Loss = -10887.8701171875
Iteration 4000: Loss = -10883.8896484375
Iteration 4100: Loss = -10883.5419921875
Iteration 4200: Loss = -10883.330078125
Iteration 4300: Loss = -10882.8583984375
Iteration 4400: Loss = -10878.658203125
Iteration 4500: Loss = -10878.373046875
Iteration 4600: Loss = -10878.16796875
Iteration 4700: Loss = -10877.9990234375
Iteration 4800: Loss = -10877.8525390625
Iteration 4900: Loss = -10877.7216796875
Iteration 5000: Loss = -10877.6044921875
Iteration 5100: Loss = -10877.4990234375
Iteration 5200: Loss = -10877.400390625
Iteration 5300: Loss = -10877.310546875
Iteration 5400: Loss = -10877.2275390625
Iteration 5500: Loss = -10877.1484375
Iteration 5600: Loss = -10877.076171875
Iteration 5700: Loss = -10877.0048828125
Iteration 5800: Loss = -10876.939453125
Iteration 5900: Loss = -10876.875
Iteration 6000: Loss = -10876.814453125
Iteration 6100: Loss = -10876.75390625
Iteration 6200: Loss = -10876.6962890625
Iteration 6300: Loss = -10876.6376953125
Iteration 6400: Loss = -10876.5791015625
Iteration 6500: Loss = -10876.517578125
Iteration 6600: Loss = -10876.4580078125
Iteration 6700: Loss = -10876.3974609375
Iteration 6800: Loss = -10876.3369140625
Iteration 6900: Loss = -10876.2744140625
Iteration 7000: Loss = -10876.2109375
Iteration 7100: Loss = -10876.134765625
Iteration 7200: Loss = -10876.01953125
Iteration 7300: Loss = -10875.7373046875
Iteration 7400: Loss = -10875.205078125
Iteration 7500: Loss = -10874.91015625
Iteration 7600: Loss = -10874.7783203125
Iteration 7700: Loss = -10874.6826171875
Iteration 7800: Loss = -10874.6025390625
Iteration 7900: Loss = -10874.537109375
Iteration 8000: Loss = -10874.48046875
Iteration 8100: Loss = -10874.4326171875
Iteration 8200: Loss = -10874.384765625
Iteration 8300: Loss = -10874.34375
Iteration 8400: Loss = -10874.306640625
Iteration 8500: Loss = -10874.2685546875
Iteration 8600: Loss = -10874.234375
Iteration 8700: Loss = -10874.2001953125
Iteration 8800: Loss = -10874.1708984375
Iteration 8900: Loss = -10874.138671875
Iteration 9000: Loss = -10874.1123046875
Iteration 9100: Loss = -10874.0810546875
Iteration 9200: Loss = -10874.052734375
Iteration 9300: Loss = -10874.025390625
Iteration 9400: Loss = -10874.0009765625
Iteration 9500: Loss = -10873.974609375
Iteration 9600: Loss = -10873.9462890625
Iteration 9700: Loss = -10873.9228515625
Iteration 9800: Loss = -10873.8974609375
Iteration 9900: Loss = -10873.875
Iteration 10000: Loss = -10873.849609375
Iteration 10100: Loss = -10873.826171875
Iteration 10200: Loss = -10873.8037109375
Iteration 10300: Loss = -10873.78125
Iteration 10400: Loss = -10873.759765625
Iteration 10500: Loss = -10873.7373046875
Iteration 10600: Loss = -10873.7177734375
Iteration 10700: Loss = -10873.6982421875
Iteration 10800: Loss = -10873.6787109375
Iteration 10900: Loss = -10873.662109375
Iteration 11000: Loss = -10873.6455078125
Iteration 11100: Loss = -10873.6279296875
Iteration 11200: Loss = -10873.6123046875
Iteration 11300: Loss = -10873.5947265625
Iteration 11400: Loss = -10873.5810546875
Iteration 11500: Loss = -10873.5673828125
Iteration 11600: Loss = -10873.5537109375
Iteration 11700: Loss = -10873.5419921875
Iteration 11800: Loss = -10873.5283203125
Iteration 11900: Loss = -10873.517578125
Iteration 12000: Loss = -10873.50390625
Iteration 12100: Loss = -10873.4951171875
Iteration 12200: Loss = -10873.484375
Iteration 12300: Loss = -10873.4765625
Iteration 12400: Loss = -10873.46875
Iteration 12500: Loss = -10873.4619140625
Iteration 12600: Loss = -10873.4541015625
Iteration 12700: Loss = -10873.4501953125
Iteration 12800: Loss = -10873.4443359375
Iteration 12900: Loss = -10873.4404296875
Iteration 13000: Loss = -10873.4345703125
Iteration 13100: Loss = -10873.4306640625
Iteration 13200: Loss = -10873.42578125
Iteration 13300: Loss = -10873.421875
Iteration 13400: Loss = -10873.4189453125
Iteration 13500: Loss = -10873.4150390625
Iteration 13600: Loss = -10873.4130859375
Iteration 13700: Loss = -10873.41015625
Iteration 13800: Loss = -10873.4052734375
Iteration 13900: Loss = -10873.4033203125
Iteration 14000: Loss = -10873.40234375
Iteration 14100: Loss = -10873.3994140625
Iteration 14200: Loss = -10873.3955078125
Iteration 14300: Loss = -10873.39453125
Iteration 14400: Loss = -10873.392578125
Iteration 14500: Loss = -10873.392578125
Iteration 14600: Loss = -10873.3896484375
Iteration 14700: Loss = -10873.3876953125
Iteration 14800: Loss = -10873.3857421875
Iteration 14900: Loss = -10873.3857421875
Iteration 15000: Loss = -10873.384765625
Iteration 15100: Loss = -10873.3828125
Iteration 15200: Loss = -10873.3818359375
Iteration 15300: Loss = -10873.380859375
Iteration 15400: Loss = -10873.3798828125
Iteration 15500: Loss = -10873.37890625
Iteration 15600: Loss = -10873.37890625
Iteration 15700: Loss = -10873.376953125
Iteration 15800: Loss = -10873.3779296875
1
Iteration 15900: Loss = -10873.375
Iteration 16000: Loss = -10873.3740234375
Iteration 16100: Loss = -10873.375
1
Iteration 16200: Loss = -10873.3740234375
Iteration 16300: Loss = -10873.373046875
Iteration 16400: Loss = -10873.3740234375
1
Iteration 16500: Loss = -10873.373046875
Iteration 16600: Loss = -10873.373046875
Iteration 16700: Loss = -10873.3720703125
Iteration 16800: Loss = -10873.3720703125
Iteration 16900: Loss = -10873.37109375
Iteration 17000: Loss = -10873.3701171875
Iteration 17100: Loss = -10873.37109375
1
Iteration 17200: Loss = -10873.3701171875
Iteration 17300: Loss = -10873.369140625
Iteration 17400: Loss = -10873.3701171875
1
Iteration 17500: Loss = -10873.3701171875
2
Iteration 17600: Loss = -10873.3681640625
Iteration 17700: Loss = -10873.369140625
1
Iteration 17800: Loss = -10873.37109375
2
Iteration 17900: Loss = -10873.369140625
3
Iteration 18000: Loss = -10873.369140625
4
Iteration 18100: Loss = -10873.3681640625
Iteration 18200: Loss = -10873.3681640625
Iteration 18300: Loss = -10873.3681640625
Iteration 18400: Loss = -10873.3681640625
Iteration 18500: Loss = -10873.3671875
Iteration 18600: Loss = -10873.3701171875
1
Iteration 18700: Loss = -10873.3671875
Iteration 18800: Loss = -10873.369140625
1
Iteration 18900: Loss = -10873.3681640625
2
Iteration 19000: Loss = -10873.3681640625
3
Iteration 19100: Loss = -10873.3671875
Iteration 19200: Loss = -10873.3681640625
1
Iteration 19300: Loss = -10873.3671875
Iteration 19400: Loss = -10873.3671875
Iteration 19500: Loss = -10873.3681640625
1
Iteration 19600: Loss = -10873.3671875
Iteration 19700: Loss = -10873.3681640625
1
Iteration 19800: Loss = -10873.369140625
2
Iteration 19900: Loss = -10873.3681640625
3
Iteration 20000: Loss = -10873.3671875
Iteration 20100: Loss = -10873.3671875
Iteration 20200: Loss = -10873.3681640625
1
Iteration 20300: Loss = -10873.3681640625
2
Iteration 20400: Loss = -10873.3681640625
3
Iteration 20500: Loss = -10873.3681640625
4
Iteration 20600: Loss = -10873.3681640625
5
Iteration 20700: Loss = -10873.3681640625
6
Iteration 20800: Loss = -10873.3681640625
7
Iteration 20900: Loss = -10873.3671875
Iteration 21000: Loss = -10873.3671875
Iteration 21100: Loss = -10873.3671875
Iteration 21200: Loss = -10873.3681640625
1
Iteration 21300: Loss = -10873.3662109375
Iteration 21400: Loss = -10873.369140625
1
Iteration 21500: Loss = -10873.3671875
2
Iteration 21600: Loss = -10873.3681640625
3
Iteration 21700: Loss = -10873.3681640625
4
Iteration 21800: Loss = -10873.365234375
Iteration 21900: Loss = -10873.3681640625
1
Iteration 22000: Loss = -10873.3671875
2
Iteration 22100: Loss = -10873.3671875
3
Iteration 22200: Loss = -10873.3671875
4
Iteration 22300: Loss = -10873.3671875
5
Iteration 22400: Loss = -10873.3671875
6
Iteration 22500: Loss = -10873.3671875
7
Iteration 22600: Loss = -10873.3681640625
8
Iteration 22700: Loss = -10873.3671875
9
Iteration 22800: Loss = -10873.3671875
10
Iteration 22900: Loss = -10873.3681640625
11
Iteration 23000: Loss = -10873.3671875
12
Iteration 23100: Loss = -10873.3671875
13
Iteration 23200: Loss = -10873.3671875
14
Iteration 23300: Loss = -10873.3681640625
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[9.9988e-01, 1.2356e-04],
        [5.2208e-01, 4.7792e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9766, 0.0234], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1582, 0.2553],
         [0.9890, 0.9777]],

        [[0.0256, 0.2364],
         [0.2890, 0.7277]],

        [[0.9927, 0.2524],
         [0.8167, 0.3428]],

        [[0.2241, 0.6905],
         [0.9911, 0.0924]],

        [[0.1554, 0.0565],
         [0.2426, 0.5006]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: -0.013574768645372375
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00033952462225520056
Average Adjusted Rand Index: -0.001130311832503313
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36485.6328125
Iteration 100: Loss = -22439.470703125
Iteration 200: Loss = -13436.708984375
Iteration 300: Loss = -11504.7041015625
Iteration 400: Loss = -11188.7607421875
Iteration 500: Loss = -11078.720703125
Iteration 600: Loss = -11021.1875
Iteration 700: Loss = -10986.359375
Iteration 800: Loss = -10963.3076171875
Iteration 900: Loss = -10947.091796875
Iteration 1000: Loss = -10935.1669921875
Iteration 1100: Loss = -10926.09765625
Iteration 1200: Loss = -10919.015625
Iteration 1300: Loss = -10913.361328125
Iteration 1400: Loss = -10908.765625
Iteration 1500: Loss = -10904.9736328125
Iteration 1600: Loss = -10901.8037109375
Iteration 1700: Loss = -10899.1240234375
Iteration 1800: Loss = -10896.8388671875
Iteration 1900: Loss = -10894.87109375
Iteration 2000: Loss = -10893.1640625
Iteration 2100: Loss = -10891.6728515625
Iteration 2200: Loss = -10890.3623046875
Iteration 2300: Loss = -10889.203125
Iteration 2400: Loss = -10888.173828125
Iteration 2500: Loss = -10887.2568359375
Iteration 2600: Loss = -10886.4345703125
Iteration 2700: Loss = -10885.693359375
Iteration 2800: Loss = -10885.025390625
Iteration 2900: Loss = -10884.4228515625
Iteration 3000: Loss = -10883.8740234375
Iteration 3100: Loss = -10883.375
Iteration 3200: Loss = -10882.919921875
Iteration 3300: Loss = -10882.501953125
Iteration 3400: Loss = -10882.1201171875
Iteration 3500: Loss = -10881.767578125
Iteration 3600: Loss = -10881.4453125
Iteration 3700: Loss = -10881.1474609375
Iteration 3800: Loss = -10880.8720703125
Iteration 3900: Loss = -10880.6162109375
Iteration 4000: Loss = -10880.3798828125
Iteration 4100: Loss = -10880.1611328125
Iteration 4200: Loss = -10879.9580078125
Iteration 4300: Loss = -10879.76953125
Iteration 4400: Loss = -10879.5927734375
Iteration 4500: Loss = -10879.4287109375
Iteration 4600: Loss = -10879.2744140625
Iteration 4700: Loss = -10879.1328125
Iteration 4800: Loss = -10878.998046875
Iteration 4900: Loss = -10878.8740234375
Iteration 5000: Loss = -10878.7568359375
Iteration 5100: Loss = -10878.6474609375
Iteration 5200: Loss = -10878.54296875
Iteration 5300: Loss = -10878.447265625
Iteration 5400: Loss = -10878.3564453125
Iteration 5500: Loss = -10878.271484375
Iteration 5600: Loss = -10878.19140625
Iteration 5700: Loss = -10878.1171875
Iteration 5800: Loss = -10878.044921875
Iteration 5900: Loss = -10877.98046875
Iteration 6000: Loss = -10877.9169921875
Iteration 6100: Loss = -10877.8564453125
Iteration 6200: Loss = -10877.80078125
Iteration 6300: Loss = -10877.75
Iteration 6400: Loss = -10877.7001953125
Iteration 6500: Loss = -10877.65234375
Iteration 6600: Loss = -10877.6083984375
Iteration 6700: Loss = -10877.5693359375
Iteration 6800: Loss = -10877.5283203125
Iteration 6900: Loss = -10877.4912109375
Iteration 7000: Loss = -10877.4560546875
Iteration 7100: Loss = -10877.423828125
Iteration 7200: Loss = -10877.3916015625
Iteration 7300: Loss = -10877.36328125
Iteration 7400: Loss = -10877.333984375
Iteration 7500: Loss = -10877.30859375
Iteration 7600: Loss = -10877.283203125
Iteration 7700: Loss = -10877.2607421875
Iteration 7800: Loss = -10877.23828125
Iteration 7900: Loss = -10877.2158203125
Iteration 8000: Loss = -10877.1953125
Iteration 8100: Loss = -10877.177734375
Iteration 8200: Loss = -10877.16015625
Iteration 8300: Loss = -10877.142578125
Iteration 8400: Loss = -10877.126953125
Iteration 8500: Loss = -10877.111328125
Iteration 8600: Loss = -10877.09765625
Iteration 8700: Loss = -10877.0849609375
Iteration 8800: Loss = -10877.0712890625
Iteration 8900: Loss = -10877.05859375
Iteration 9000: Loss = -10877.0478515625
Iteration 9100: Loss = -10877.037109375
Iteration 9200: Loss = -10877.02734375
Iteration 9300: Loss = -10877.017578125
Iteration 9400: Loss = -10877.0078125
Iteration 9500: Loss = -10877.0
Iteration 9600: Loss = -10876.9912109375
Iteration 9700: Loss = -10876.982421875
Iteration 9800: Loss = -10876.9755859375
Iteration 9900: Loss = -10876.96875
Iteration 10000: Loss = -10876.9619140625
Iteration 10100: Loss = -10876.955078125
Iteration 10200: Loss = -10876.9482421875
Iteration 10300: Loss = -10876.9443359375
Iteration 10400: Loss = -10876.9375
Iteration 10500: Loss = -10876.9326171875
Iteration 10600: Loss = -10876.927734375
Iteration 10700: Loss = -10876.921875
Iteration 10800: Loss = -10876.916015625
Iteration 10900: Loss = -10876.9091796875
Iteration 11000: Loss = -10876.9013671875
Iteration 11100: Loss = -10876.888671875
Iteration 11200: Loss = -10876.875
Iteration 11300: Loss = -10876.857421875
Iteration 11400: Loss = -10876.8466796875
Iteration 11500: Loss = -10876.8349609375
Iteration 11600: Loss = -10876.8251953125
Iteration 11700: Loss = -10876.8115234375
Iteration 11800: Loss = -10876.79296875
Iteration 11900: Loss = -10876.763671875
Iteration 12000: Loss = -10876.720703125
Iteration 12100: Loss = -10876.693359375
Iteration 12200: Loss = -10876.6572265625
Iteration 12300: Loss = -10876.638671875
Iteration 12400: Loss = -10876.619140625
Iteration 12500: Loss = -10876.591796875
Iteration 12600: Loss = -10876.3505859375
Iteration 12700: Loss = -10876.2900390625
Iteration 12800: Loss = -10876.2490234375
Iteration 12900: Loss = -10876.2041015625
Iteration 13000: Loss = -10876.1474609375
Iteration 13100: Loss = -10876.0595703125
Iteration 13200: Loss = -10873.705078125
Iteration 13300: Loss = -10873.53515625
Iteration 13400: Loss = -10873.5048828125
Iteration 13500: Loss = -10873.439453125
Iteration 13600: Loss = -10873.390625
Iteration 13700: Loss = -10873.3466796875
Iteration 13800: Loss = -10873.3076171875
Iteration 13900: Loss = -10873.271484375
Iteration 14000: Loss = -10873.2412109375
Iteration 14100: Loss = -10873.21484375
Iteration 14200: Loss = -10873.1865234375
Iteration 14300: Loss = -10873.1689453125
Iteration 14400: Loss = -10873.15625
Iteration 14500: Loss = -10873.1435546875
Iteration 14600: Loss = -10873.1318359375
Iteration 14700: Loss = -10873.1181640625
Iteration 14800: Loss = -10873.0986328125
Iteration 14900: Loss = -10873.0927734375
Iteration 15000: Loss = -10873.0869140625
Iteration 15100: Loss = -10873.0517578125
Iteration 15200: Loss = -10872.92578125
Iteration 15300: Loss = -10872.9130859375
Iteration 15400: Loss = -10872.9052734375
Iteration 15500: Loss = -10872.8984375
Iteration 15600: Loss = -10872.890625
Iteration 15700: Loss = -10872.8876953125
Iteration 15800: Loss = -10872.8837890625
Iteration 15900: Loss = -10872.8828125
Iteration 16000: Loss = -10872.8798828125
Iteration 16100: Loss = -10872.8779296875
Iteration 16200: Loss = -10872.876953125
Iteration 16300: Loss = -10872.8759765625
Iteration 16400: Loss = -10872.875
Iteration 16500: Loss = -10872.873046875
Iteration 16600: Loss = -10872.875
1
Iteration 16700: Loss = -10872.873046875
Iteration 16800: Loss = -10872.8740234375
1
Iteration 16900: Loss = -10872.8720703125
Iteration 17000: Loss = -10872.8720703125
Iteration 17100: Loss = -10872.87109375
Iteration 17200: Loss = -10872.8701171875
Iteration 17300: Loss = -10872.87109375
1
Iteration 17400: Loss = -10872.869140625
Iteration 17500: Loss = -10872.8701171875
1
Iteration 17600: Loss = -10872.8681640625
Iteration 17700: Loss = -10872.8681640625
Iteration 17800: Loss = -10872.8681640625
Iteration 17900: Loss = -10872.8671875
Iteration 18000: Loss = -10872.869140625
1
Iteration 18100: Loss = -10872.869140625
2
Iteration 18200: Loss = -10872.8681640625
3
Iteration 18300: Loss = -10872.869140625
4
Iteration 18400: Loss = -10872.8662109375
Iteration 18500: Loss = -10872.8662109375
Iteration 18600: Loss = -10872.8671875
1
Iteration 18700: Loss = -10872.8671875
2
Iteration 18800: Loss = -10872.8671875
3
Iteration 18900: Loss = -10872.8671875
4
Iteration 19000: Loss = -10872.8662109375
Iteration 19100: Loss = -10872.8671875
1
Iteration 19200: Loss = -10872.8671875
2
Iteration 19300: Loss = -10872.8671875
3
Iteration 19400: Loss = -10872.8671875
4
Iteration 19500: Loss = -10872.8681640625
5
Iteration 19600: Loss = -10872.8662109375
Iteration 19700: Loss = -10872.8662109375
Iteration 19800: Loss = -10872.8662109375
Iteration 19900: Loss = -10872.8671875
1
Iteration 20000: Loss = -10872.8681640625
2
Iteration 20100: Loss = -10872.8662109375
Iteration 20200: Loss = -10872.8662109375
Iteration 20300: Loss = -10872.8662109375
Iteration 20400: Loss = -10872.8662109375
Iteration 20500: Loss = -10872.8671875
1
Iteration 20600: Loss = -10872.8662109375
Iteration 20700: Loss = -10872.8662109375
Iteration 20800: Loss = -10872.8662109375
Iteration 20900: Loss = -10872.8662109375
Iteration 21000: Loss = -10872.8671875
1
Iteration 21100: Loss = -10872.8662109375
Iteration 21200: Loss = -10872.8671875
1
Iteration 21300: Loss = -10872.8662109375
Iteration 21400: Loss = -10872.8662109375
Iteration 21500: Loss = -10872.8662109375
Iteration 21600: Loss = -10872.8671875
1
Iteration 21700: Loss = -10872.8662109375
Iteration 21800: Loss = -10872.865234375
Iteration 21900: Loss = -10872.865234375
Iteration 22000: Loss = -10872.8671875
1
Iteration 22100: Loss = -10872.865234375
Iteration 22200: Loss = -10872.865234375
Iteration 22300: Loss = -10872.8671875
1
Iteration 22400: Loss = -10872.865234375
Iteration 22500: Loss = -10872.8662109375
1
Iteration 22600: Loss = -10872.8662109375
2
Iteration 22700: Loss = -10872.8662109375
3
Iteration 22800: Loss = -10872.8662109375
4
Iteration 22900: Loss = -10872.8662109375
5
Iteration 23000: Loss = -10872.8671875
6
Iteration 23100: Loss = -10872.8662109375
7
Iteration 23200: Loss = -10872.8662109375
8
Iteration 23300: Loss = -10872.8671875
9
Iteration 23400: Loss = -10872.8671875
10
Iteration 23500: Loss = -10872.8662109375
11
Iteration 23600: Loss = -10872.865234375
Iteration 23700: Loss = -10872.8662109375
1
Iteration 23800: Loss = -10872.8662109375
2
Iteration 23900: Loss = -10872.8662109375
3
Iteration 24000: Loss = -10872.865234375
Iteration 24100: Loss = -10872.8671875
1
Iteration 24200: Loss = -10872.8662109375
2
Iteration 24300: Loss = -10872.8662109375
3
Iteration 24400: Loss = -10872.8681640625
4
Iteration 24500: Loss = -10872.8662109375
5
Iteration 24600: Loss = -10872.8671875
6
Iteration 24700: Loss = -10872.8662109375
7
Iteration 24800: Loss = -10872.865234375
Iteration 24900: Loss = -10872.865234375
Iteration 25000: Loss = -10872.8662109375
1
Iteration 25100: Loss = -10872.8681640625
2
Iteration 25200: Loss = -10872.8662109375
3
Iteration 25300: Loss = -10872.8662109375
4
Iteration 25400: Loss = -10872.8671875
5
Iteration 25500: Loss = -10872.8671875
6
Iteration 25600: Loss = -10872.8662109375
7
Iteration 25700: Loss = -10872.8671875
8
Iteration 25800: Loss = -10872.8681640625
9
Iteration 25900: Loss = -10872.8642578125
Iteration 26000: Loss = -10872.8662109375
1
Iteration 26100: Loss = -10872.8662109375
2
Iteration 26200: Loss = -10872.8681640625
3
Iteration 26300: Loss = -10872.8671875
4
Iteration 26400: Loss = -10872.8671875
5
Iteration 26500: Loss = -10872.8681640625
6
Iteration 26600: Loss = -10872.8671875
7
Iteration 26700: Loss = -10872.8662109375
8
Iteration 26800: Loss = -10872.8662109375
9
Iteration 26900: Loss = -10872.865234375
10
Iteration 27000: Loss = -10872.8671875
11
Iteration 27100: Loss = -10872.8681640625
12
Iteration 27200: Loss = -10872.8671875
13
Iteration 27300: Loss = -10872.8671875
14
Iteration 27400: Loss = -10872.8671875
15
Stopping early at iteration 27400 due to no improvement.
pi: tensor([[9.9958e-01, 4.2194e-04],
        [4.6615e-01, 5.3385e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9776, 0.0224], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1582, 0.2583],
         [0.0306, 0.9709]],

        [[0.8911, 0.2368],
         [0.7489, 0.0226]],

        [[0.8875, 0.2521],
         [0.9925, 0.0303]],

        [[0.9693, 0.1914],
         [0.9831, 0.0438]],

        [[0.8544, 0.0555],
         [0.8516, 0.9929]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: -0.013574768645372375
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00033952462225520056
Average Adjusted Rand Index: -0.001130311832503313
[0.00033952462225520056, 0.00033952462225520056] [-0.001130311832503313, -0.001130311832503313] [10873.3681640625, 10872.8671875]
-------------------------------------
This iteration is 31
True Objective function: Loss = -10837.72936779651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44549.50390625
Iteration 100: Loss = -25306.697265625
Iteration 200: Loss = -14110.1005859375
Iteration 300: Loss = -11893.82421875
Iteration 400: Loss = -11503.1640625
Iteration 500: Loss = -11343.6494140625
Iteration 600: Loss = -11251.8525390625
Iteration 700: Loss = -11170.505859375
Iteration 800: Loss = -11098.220703125
Iteration 900: Loss = -11063.8564453125
Iteration 1000: Loss = -11038.8017578125
Iteration 1100: Loss = -11008.9306640625
Iteration 1200: Loss = -10997.0498046875
Iteration 1300: Loss = -10989.759765625
Iteration 1400: Loss = -10983.2763671875
Iteration 1500: Loss = -10975.3251953125
Iteration 1600: Loss = -10969.73046875
Iteration 1700: Loss = -10965.90234375
Iteration 1800: Loss = -10962.52734375
Iteration 1900: Loss = -10959.615234375
Iteration 2000: Loss = -10957.1611328125
Iteration 2100: Loss = -10955.037109375
Iteration 2200: Loss = -10953.173828125
Iteration 2300: Loss = -10951.546875
Iteration 2400: Loss = -10950.1396484375
Iteration 2500: Loss = -10948.92578125
Iteration 2600: Loss = -10947.869140625
Iteration 2700: Loss = -10946.9462890625
Iteration 2800: Loss = -10946.1298828125
Iteration 2900: Loss = -10945.404296875
Iteration 3000: Loss = -10944.7529296875
Iteration 3100: Loss = -10944.16796875
Iteration 3200: Loss = -10943.63671875
Iteration 3300: Loss = -10943.1533203125
Iteration 3400: Loss = -10942.7099609375
Iteration 3500: Loss = -10942.3046875
Iteration 3600: Loss = -10941.9306640625
Iteration 3700: Loss = -10941.5859375
Iteration 3800: Loss = -10941.265625
Iteration 3900: Loss = -10940.96875
Iteration 4000: Loss = -10940.693359375
Iteration 4100: Loss = -10940.439453125
Iteration 4200: Loss = -10940.203125
Iteration 4300: Loss = -10939.98046875
Iteration 4400: Loss = -10939.775390625
Iteration 4500: Loss = -10939.5830078125
Iteration 4600: Loss = -10939.3994140625
Iteration 4700: Loss = -10939.23046875
Iteration 4800: Loss = -10939.0703125
Iteration 4900: Loss = -10938.9228515625
Iteration 5000: Loss = -10938.7822265625
Iteration 5100: Loss = -10938.6484375
Iteration 5200: Loss = -10938.525390625
Iteration 5300: Loss = -10938.4091796875
Iteration 5400: Loss = -10938.30078125
Iteration 5500: Loss = -10938.1953125
Iteration 5600: Loss = -10938.09765625
Iteration 5700: Loss = -10938.0078125
Iteration 5800: Loss = -10937.91796875
Iteration 5900: Loss = -10937.8349609375
Iteration 6000: Loss = -10937.7578125
Iteration 6100: Loss = -10937.6826171875
Iteration 6200: Loss = -10937.6142578125
Iteration 6300: Loss = -10937.5478515625
Iteration 6400: Loss = -10937.484375
Iteration 6500: Loss = -10937.423828125
Iteration 6600: Loss = -10937.3662109375
Iteration 6700: Loss = -10937.310546875
Iteration 6800: Loss = -10937.2607421875
Iteration 6900: Loss = -10937.2109375
Iteration 7000: Loss = -10937.1650390625
Iteration 7100: Loss = -10937.1220703125
Iteration 7200: Loss = -10937.080078125
Iteration 7300: Loss = -10937.04296875
Iteration 7400: Loss = -10937.00390625
Iteration 7500: Loss = -10936.96875
Iteration 7600: Loss = -10936.9365234375
Iteration 7700: Loss = -10936.904296875
Iteration 7800: Loss = -10936.8759765625
Iteration 7900: Loss = -10936.8466796875
Iteration 8000: Loss = -10936.8193359375
Iteration 8100: Loss = -10936.796875
Iteration 8200: Loss = -10936.7734375
Iteration 8300: Loss = -10936.7509765625
Iteration 8400: Loss = -10936.73046875
Iteration 8500: Loss = -10936.712890625
Iteration 8600: Loss = -10936.6953125
Iteration 8700: Loss = -10936.6806640625
Iteration 8800: Loss = -10936.662109375
Iteration 8900: Loss = -10936.6494140625
Iteration 9000: Loss = -10936.63671875
Iteration 9100: Loss = -10936.6240234375
Iteration 9200: Loss = -10936.6123046875
Iteration 9300: Loss = -10936.6005859375
Iteration 9400: Loss = -10936.591796875
Iteration 9500: Loss = -10936.5810546875
Iteration 9600: Loss = -10936.5712890625
Iteration 9700: Loss = -10936.5634765625
Iteration 9800: Loss = -10936.5537109375
Iteration 9900: Loss = -10936.5458984375
Iteration 10000: Loss = -10936.5390625
Iteration 10100: Loss = -10936.53125
Iteration 10200: Loss = -10936.5234375
Iteration 10300: Loss = -10936.5185546875
Iteration 10400: Loss = -10936.51171875
Iteration 10500: Loss = -10936.505859375
Iteration 10600: Loss = -10936.5
Iteration 10700: Loss = -10936.4951171875
Iteration 10800: Loss = -10936.4912109375
Iteration 10900: Loss = -10936.486328125
Iteration 11000: Loss = -10936.482421875
Iteration 11100: Loss = -10936.4765625
Iteration 11200: Loss = -10936.4736328125
Iteration 11300: Loss = -10936.4677734375
Iteration 11400: Loss = -10936.4638671875
Iteration 11500: Loss = -10936.4599609375
Iteration 11600: Loss = -10936.45703125
Iteration 11700: Loss = -10936.4541015625
Iteration 11800: Loss = -10936.451171875
Iteration 11900: Loss = -10936.4462890625
Iteration 12000: Loss = -10936.4453125
Iteration 12100: Loss = -10936.44140625
Iteration 12200: Loss = -10936.439453125
Iteration 12300: Loss = -10936.435546875
Iteration 12400: Loss = -10936.4345703125
Iteration 12500: Loss = -10936.431640625
Iteration 12600: Loss = -10936.4306640625
Iteration 12700: Loss = -10936.4267578125
Iteration 12800: Loss = -10936.42578125
Iteration 12900: Loss = -10936.4228515625
Iteration 13000: Loss = -10936.4208984375
Iteration 13100: Loss = -10936.419921875
Iteration 13200: Loss = -10936.4189453125
Iteration 13300: Loss = -10936.4169921875
Iteration 13400: Loss = -10936.4150390625
Iteration 13500: Loss = -10936.4140625
Iteration 13600: Loss = -10936.412109375
Iteration 13700: Loss = -10936.4111328125
Iteration 13800: Loss = -10936.41015625
Iteration 13900: Loss = -10936.41015625
Iteration 14000: Loss = -10936.4072265625
Iteration 14100: Loss = -10936.4052734375
Iteration 14200: Loss = -10936.404296875
Iteration 14300: Loss = -10936.4033203125
Iteration 14400: Loss = -10936.4013671875
Iteration 14500: Loss = -10936.4033203125
1
Iteration 14600: Loss = -10936.4013671875
Iteration 14700: Loss = -10936.3984375
Iteration 14800: Loss = -10936.3984375
Iteration 14900: Loss = -10936.3984375
Iteration 15000: Loss = -10936.396484375
Iteration 15100: Loss = -10936.39453125
Iteration 15200: Loss = -10936.3955078125
1
Iteration 15300: Loss = -10936.3955078125
2
Iteration 15400: Loss = -10936.3974609375
3
Iteration 15500: Loss = -10936.3935546875
Iteration 15600: Loss = -10936.3935546875
Iteration 15700: Loss = -10936.3935546875
Iteration 15800: Loss = -10936.3916015625
Iteration 15900: Loss = -10936.3916015625
Iteration 16000: Loss = -10936.390625
Iteration 16100: Loss = -10936.3916015625
1
Iteration 16200: Loss = -10936.3896484375
Iteration 16300: Loss = -10936.3896484375
Iteration 16400: Loss = -10936.3896484375
Iteration 16500: Loss = -10936.3876953125
Iteration 16600: Loss = -10936.3876953125
Iteration 16700: Loss = -10936.3876953125
Iteration 16800: Loss = -10936.3876953125
Iteration 16900: Loss = -10936.388671875
1
Iteration 17000: Loss = -10936.388671875
2
Iteration 17100: Loss = -10936.38671875
Iteration 17200: Loss = -10936.3857421875
Iteration 17300: Loss = -10936.3876953125
1
Iteration 17400: Loss = -10936.3857421875
Iteration 17500: Loss = -10936.3837890625
Iteration 17600: Loss = -10936.38671875
1
Iteration 17700: Loss = -10936.384765625
2
Iteration 17800: Loss = -10936.384765625
3
Iteration 17900: Loss = -10936.3876953125
4
Iteration 18000: Loss = -10936.384765625
5
Iteration 18100: Loss = -10936.3837890625
Iteration 18200: Loss = -10936.3837890625
Iteration 18300: Loss = -10936.3828125
Iteration 18400: Loss = -10936.3818359375
Iteration 18500: Loss = -10936.3818359375
Iteration 18600: Loss = -10936.3828125
1
Iteration 18700: Loss = -10936.380859375
Iteration 18800: Loss = -10936.380859375
Iteration 18900: Loss = -10936.380859375
Iteration 19000: Loss = -10936.3798828125
Iteration 19100: Loss = -10936.37890625
Iteration 19200: Loss = -10936.3779296875
Iteration 19300: Loss = -10936.37890625
1
Iteration 19400: Loss = -10936.3759765625
Iteration 19500: Loss = -10936.375
Iteration 19600: Loss = -10936.3720703125
Iteration 19700: Loss = -10936.3701171875
Iteration 19800: Loss = -10936.3642578125
Iteration 19900: Loss = -10936.345703125
Iteration 20000: Loss = -10935.650390625
Iteration 20100: Loss = -10935.041015625
Iteration 20200: Loss = -10934.4892578125
Iteration 20300: Loss = -10934.4248046875
Iteration 20400: Loss = -10934.4091796875
Iteration 20500: Loss = -10934.40625
Iteration 20600: Loss = -10934.40625
Iteration 20700: Loss = -10934.404296875
Iteration 20800: Loss = -10934.4033203125
Iteration 20900: Loss = -10934.40234375
Iteration 21000: Loss = -10934.40234375
Iteration 21100: Loss = -10934.40234375
Iteration 21200: Loss = -10934.40234375
Iteration 21300: Loss = -10934.4013671875
Iteration 21400: Loss = -10934.4013671875
Iteration 21500: Loss = -10934.4013671875
Iteration 21600: Loss = -10934.4013671875
Iteration 21700: Loss = -10934.4013671875
Iteration 21800: Loss = -10934.3994140625
Iteration 21900: Loss = -10934.4013671875
1
Iteration 22000: Loss = -10934.400390625
2
Iteration 22100: Loss = -10934.3994140625
Iteration 22200: Loss = -10934.3994140625
Iteration 22300: Loss = -10934.3994140625
Iteration 22400: Loss = -10934.400390625
1
Iteration 22500: Loss = -10934.400390625
2
Iteration 22600: Loss = -10934.400390625
3
Iteration 22700: Loss = -10934.400390625
4
Iteration 22800: Loss = -10934.400390625
5
Iteration 22900: Loss = -10934.3994140625
Iteration 23000: Loss = -10934.3994140625
Iteration 23100: Loss = -10934.400390625
1
Iteration 23200: Loss = -10934.400390625
2
Iteration 23300: Loss = -10934.3984375
Iteration 23400: Loss = -10934.4013671875
1
Iteration 23500: Loss = -10934.3994140625
2
Iteration 23600: Loss = -10934.3994140625
3
Iteration 23700: Loss = -10934.3994140625
4
Iteration 23800: Loss = -10934.3994140625
5
Iteration 23900: Loss = -10934.400390625
6
Iteration 24000: Loss = -10934.400390625
7
Iteration 24100: Loss = -10934.3994140625
8
Iteration 24200: Loss = -10934.400390625
9
Iteration 24300: Loss = -10934.3994140625
10
Iteration 24400: Loss = -10934.4013671875
11
Iteration 24500: Loss = -10934.3994140625
12
Iteration 24600: Loss = -10934.3994140625
13
Iteration 24700: Loss = -10934.4013671875
14
Iteration 24800: Loss = -10934.3994140625
15
Stopping early at iteration 24800 due to no improvement.
pi: tensor([[2.7996e-02, 9.7200e-01],
        [2.5648e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9898, 0.0102], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1544, 0.1501],
         [0.9900, 0.1620]],

        [[0.5516, 0.2382],
         [0.3326, 0.5851]],

        [[0.0554, 0.2021],
         [0.0204, 0.9081]],

        [[0.9919, 0.1156],
         [0.0481, 0.7904]],

        [[0.8856, 0.2036],
         [0.0532, 0.0281]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013122002774410336
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37919.69140625
Iteration 100: Loss = -25545.345703125
Iteration 200: Loss = -14314.216796875
Iteration 300: Loss = -12093.27734375
Iteration 400: Loss = -11575.287109375
Iteration 500: Loss = -11382.642578125
Iteration 600: Loss = -11250.265625
Iteration 700: Loss = -11199.7314453125
Iteration 800: Loss = -11171.3955078125
Iteration 900: Loss = -11152.740234375
Iteration 1000: Loss = -11138.6015625
Iteration 1100: Loss = -11128.619140625
Iteration 1200: Loss = -11120.5419921875
Iteration 1300: Loss = -11113.4267578125
Iteration 1400: Loss = -11107.0634765625
Iteration 1500: Loss = -11101.990234375
Iteration 1600: Loss = -11097.5625
Iteration 1700: Loss = -11092.9384765625
Iteration 1800: Loss = -11088.7333984375
Iteration 1900: Loss = -11084.421875
Iteration 2000: Loss = -11081.033203125
Iteration 2100: Loss = -11077.8203125
Iteration 2200: Loss = -11074.0087890625
Iteration 2300: Loss = -11070.1640625
Iteration 2400: Loss = -11067.958984375
Iteration 2500: Loss = -11065.80078125
Iteration 2600: Loss = -11063.392578125
Iteration 2700: Loss = -11060.423828125
Iteration 2800: Loss = -11058.4150390625
Iteration 2900: Loss = -11056.759765625
Iteration 3000: Loss = -11055.310546875
Iteration 3100: Loss = -11053.9560546875
Iteration 3200: Loss = -11052.7080078125
Iteration 3300: Loss = -11051.3427734375
Iteration 3400: Loss = -11049.6171875
Iteration 3500: Loss = -11048.2646484375
Iteration 3600: Loss = -11046.685546875
Iteration 3700: Loss = -11045.921875
Iteration 3800: Loss = -11045.2421875
Iteration 3900: Loss = -11044.2939453125
Iteration 4000: Loss = -11042.728515625
Iteration 4100: Loss = -11041.921875
Iteration 4200: Loss = -11041.359375
Iteration 4300: Loss = -11040.92578125
Iteration 4400: Loss = -11040.4287109375
Iteration 4500: Loss = -11039.9736328125
Iteration 4600: Loss = -11039.662109375
Iteration 4700: Loss = -11038.4228515625
Iteration 4800: Loss = -11038.2177734375
Iteration 4900: Loss = -11038.0341796875
Iteration 5000: Loss = -11037.8564453125
Iteration 5100: Loss = -11037.677734375
Iteration 5200: Loss = -11037.4873046875
Iteration 5300: Loss = -11037.26953125
Iteration 5400: Loss = -11037.0283203125
Iteration 5500: Loss = -11036.775390625
Iteration 5600: Loss = -11036.5009765625
Iteration 5700: Loss = -11036.201171875
Iteration 5800: Loss = -11035.982421875
Iteration 5900: Loss = -11035.8203125
Iteration 6000: Loss = -11035.6767578125
Iteration 6100: Loss = -11035.521484375
Iteration 6200: Loss = -11035.3544921875
Iteration 6300: Loss = -11035.208984375
Iteration 6400: Loss = -11035.091796875
Iteration 6500: Loss = -11035.0009765625
Iteration 6600: Loss = -11034.923828125
Iteration 6700: Loss = -11034.8564453125
Iteration 6800: Loss = -11034.796875
Iteration 6900: Loss = -11034.7421875
Iteration 7000: Loss = -11034.693359375
Iteration 7100: Loss = -11034.646484375
Iteration 7200: Loss = -11034.6044921875
Iteration 7300: Loss = -11034.564453125
Iteration 7400: Loss = -11034.52734375
Iteration 7500: Loss = -11034.490234375
Iteration 7600: Loss = -11034.4580078125
Iteration 7700: Loss = -11034.4287109375
Iteration 7800: Loss = -11034.3984375
Iteration 7900: Loss = -11034.369140625
Iteration 8000: Loss = -11034.34375
Iteration 8100: Loss = -11034.318359375
Iteration 8200: Loss = -11034.2958984375
Iteration 8300: Loss = -11034.2734375
Iteration 8400: Loss = -11034.2529296875
Iteration 8500: Loss = -11034.2314453125
Iteration 8600: Loss = -11034.212890625
Iteration 8700: Loss = -11034.1953125
Iteration 8800: Loss = -11034.177734375
Iteration 8900: Loss = -11034.1611328125
Iteration 9000: Loss = -11034.146484375
Iteration 9100: Loss = -11034.130859375
Iteration 9200: Loss = -11034.1142578125
Iteration 9300: Loss = -11033.662109375
Iteration 9400: Loss = -11033.6357421875
Iteration 9500: Loss = -11033.62109375
Iteration 9600: Loss = -11033.609375
Iteration 9700: Loss = -11033.595703125
Iteration 9800: Loss = -11033.5869140625
Iteration 9900: Loss = -11033.5771484375
Iteration 10000: Loss = -11033.5654296875
Iteration 10100: Loss = -11033.5576171875
Iteration 10200: Loss = -11033.548828125
Iteration 10300: Loss = -11033.541015625
Iteration 10400: Loss = -11033.5322265625
Iteration 10500: Loss = -11033.525390625
Iteration 10600: Loss = -11033.5185546875
Iteration 10700: Loss = -11033.51171875
Iteration 10800: Loss = -11033.505859375
Iteration 10900: Loss = -11033.5
Iteration 11000: Loss = -11033.494140625
Iteration 11100: Loss = -11033.4873046875
Iteration 11200: Loss = -11032.5810546875
Iteration 11300: Loss = -11032.5517578125
Iteration 11400: Loss = -11032.5419921875
Iteration 11500: Loss = -11032.537109375
Iteration 11600: Loss = -11032.53125
Iteration 11700: Loss = -11032.525390625
Iteration 11800: Loss = -11032.521484375
Iteration 11900: Loss = -11032.5166015625
Iteration 12000: Loss = -11032.513671875
Iteration 12100: Loss = -11032.509765625
Iteration 12200: Loss = -11032.5068359375
Iteration 12300: Loss = -11032.50390625
Iteration 12400: Loss = -11032.5
Iteration 12500: Loss = -11032.498046875
Iteration 12600: Loss = -11032.4951171875
Iteration 12700: Loss = -11032.4921875
Iteration 12800: Loss = -11032.490234375
Iteration 12900: Loss = -11032.4873046875
Iteration 13000: Loss = -11032.486328125
Iteration 13100: Loss = -11032.484375
Iteration 13200: Loss = -11032.4833984375
Iteration 13300: Loss = -11032.48046875
Iteration 13400: Loss = -11032.478515625
Iteration 13500: Loss = -11032.4765625
Iteration 13600: Loss = -11032.474609375
Iteration 13700: Loss = -11032.4736328125
Iteration 13800: Loss = -11032.4736328125
Iteration 13900: Loss = -11032.470703125
Iteration 14000: Loss = -11032.46875
Iteration 14100: Loss = -11032.4677734375
Iteration 14200: Loss = -11032.466796875
Iteration 14300: Loss = -11032.466796875
Iteration 14400: Loss = -11032.466796875
Iteration 14500: Loss = -11032.46484375
Iteration 14600: Loss = -11032.4638671875
Iteration 14700: Loss = -11032.4609375
Iteration 14800: Loss = -11032.4609375
Iteration 14900: Loss = -11032.4599609375
Iteration 15000: Loss = -11032.458984375
Iteration 15100: Loss = -11032.458984375
Iteration 15200: Loss = -11032.4580078125
Iteration 15300: Loss = -11032.45703125
Iteration 15400: Loss = -11032.45703125
Iteration 15500: Loss = -11032.45703125
Iteration 15600: Loss = -11032.455078125
Iteration 15700: Loss = -11032.455078125
Iteration 15800: Loss = -11032.453125
Iteration 15900: Loss = -11032.4541015625
1
Iteration 16000: Loss = -11032.453125
Iteration 16100: Loss = -11032.4541015625
1
Iteration 16200: Loss = -11031.421875
Iteration 16300: Loss = -11031.4052734375
Iteration 16400: Loss = -11031.40625
1
Iteration 16500: Loss = -11031.4033203125
Iteration 16600: Loss = -11031.404296875
1
Iteration 16700: Loss = -11031.4033203125
Iteration 16800: Loss = -11031.4033203125
Iteration 16900: Loss = -11031.40234375
Iteration 17000: Loss = -11031.40234375
Iteration 17100: Loss = -11031.40234375
Iteration 17200: Loss = -11031.4013671875
Iteration 17300: Loss = -11031.40234375
1
Iteration 17400: Loss = -11031.4013671875
Iteration 17500: Loss = -11031.4013671875
Iteration 17600: Loss = -11031.4013671875
Iteration 17700: Loss = -11031.3994140625
Iteration 17800: Loss = -11031.4013671875
1
Iteration 17900: Loss = -11031.400390625
2
Iteration 18000: Loss = -11031.4013671875
3
Iteration 18100: Loss = -11031.3994140625
Iteration 18200: Loss = -11031.4013671875
1
Iteration 18300: Loss = -11031.3994140625
Iteration 18400: Loss = -11031.3994140625
Iteration 18500: Loss = -11031.3984375
Iteration 18600: Loss = -11031.3994140625
1
Iteration 18700: Loss = -11031.400390625
2
Iteration 18800: Loss = -11031.3984375
Iteration 18900: Loss = -11031.4013671875
1
Iteration 19000: Loss = -11031.3994140625
2
Iteration 19100: Loss = -11031.3984375
Iteration 19200: Loss = -11031.3994140625
1
Iteration 19300: Loss = -11031.3984375
Iteration 19400: Loss = -11031.396484375
Iteration 19500: Loss = -11031.3984375
1
Iteration 19600: Loss = -11031.3984375
2
Iteration 19700: Loss = -11031.3984375
3
Iteration 19800: Loss = -11031.3974609375
4
Iteration 19900: Loss = -11031.3984375
5
Iteration 20000: Loss = -11031.396484375
Iteration 20100: Loss = -11031.396484375
Iteration 20200: Loss = -11031.3984375
1
Iteration 20300: Loss = -11031.3994140625
2
Iteration 20400: Loss = -11031.3984375
3
Iteration 20500: Loss = -11031.3984375
4
Iteration 20600: Loss = -11031.396484375
Iteration 20700: Loss = -11031.396484375
Iteration 20800: Loss = -11031.3974609375
1
Iteration 20900: Loss = -11031.396484375
Iteration 21000: Loss = -11031.396484375
Iteration 21100: Loss = -11031.3974609375
1
Iteration 21200: Loss = -11031.396484375
Iteration 21300: Loss = -11031.396484375
Iteration 21400: Loss = -11031.396484375
Iteration 21500: Loss = -11031.396484375
Iteration 21600: Loss = -11031.3974609375
1
Iteration 21700: Loss = -11031.396484375
Iteration 21800: Loss = -11031.396484375
Iteration 21900: Loss = -11031.396484375
Iteration 22000: Loss = -11031.3974609375
1
Iteration 22100: Loss = -11031.396484375
Iteration 22200: Loss = -11031.3974609375
1
Iteration 22300: Loss = -11031.3955078125
Iteration 22400: Loss = -11031.3955078125
Iteration 22500: Loss = -11031.400390625
1
Iteration 22600: Loss = -11031.396484375
2
Iteration 22700: Loss = -11031.3994140625
3
Iteration 22800: Loss = -11031.396484375
4
Iteration 22900: Loss = -11031.396484375
5
Iteration 23000: Loss = -11031.396484375
6
Iteration 23100: Loss = -11031.396484375
7
Iteration 23200: Loss = -11031.396484375
8
Iteration 23300: Loss = -11031.396484375
9
Iteration 23400: Loss = -11031.396484375
10
Iteration 23500: Loss = -11031.396484375
11
Iteration 23600: Loss = -11031.396484375
12
Iteration 23700: Loss = -11031.396484375
13
Iteration 23800: Loss = -11031.396484375
14
Iteration 23900: Loss = -11031.396484375
15
Stopping early at iteration 23900 due to no improvement.
pi: tensor([[5.7766e-01, 4.2234e-01],
        [2.4205e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 5.3814e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1589, 0.1549],
         [0.9637, 0.1638]],

        [[0.9594, 0.5689],
         [0.9602, 0.1156]],

        [[0.8088, 0.1593],
         [0.9822, 0.9258]],

        [[0.8250, 0.7138],
         [0.8871, 0.1106]],

        [[0.0184, 0.1810],
         [0.9826, 0.9168]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0012047342780249143
Average Adjusted Rand Index: 0.0
[-0.0013122002774410336, 0.0012047342780249143] [0.0, 0.0] [10934.3994140625, 11031.396484375]
-------------------------------------
This iteration is 32
True Objective function: Loss = -10979.160196049665
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42466.4140625
Iteration 100: Loss = -24132.921875
Iteration 200: Loss = -14130.3173828125
Iteration 300: Loss = -12179.7470703125
Iteration 400: Loss = -11756.48046875
Iteration 500: Loss = -11544.7822265625
Iteration 600: Loss = -11425.6650390625
Iteration 700: Loss = -11333.0947265625
Iteration 800: Loss = -11279.3828125
Iteration 900: Loss = -11247.76953125
Iteration 1000: Loss = -11224.4697265625
Iteration 1100: Loss = -11200.3955078125
Iteration 1200: Loss = -11184.525390625
Iteration 1300: Loss = -11166.9306640625
Iteration 1400: Loss = -11155.9853515625
Iteration 1500: Loss = -11149.71484375
Iteration 1600: Loss = -11144.6591796875
Iteration 1700: Loss = -11140.3916015625
Iteration 1800: Loss = -11136.701171875
Iteration 1900: Loss = -11126.7158203125
Iteration 2000: Loss = -11121.58203125
Iteration 2100: Loss = -11110.8359375
Iteration 2200: Loss = -11102.509765625
Iteration 2300: Loss = -11100.3291015625
Iteration 2400: Loss = -11098.494140625
Iteration 2500: Loss = -11096.935546875
Iteration 2600: Loss = -11095.599609375
Iteration 2700: Loss = -11094.4365234375
Iteration 2800: Loss = -11093.4384765625
Iteration 2900: Loss = -11092.5537109375
Iteration 3000: Loss = -11091.7587890625
Iteration 3100: Loss = -11091.0380859375
Iteration 3200: Loss = -11090.37890625
Iteration 3300: Loss = -11089.765625
Iteration 3400: Loss = -11085.4765625
Iteration 3500: Loss = -11084.6162109375
Iteration 3600: Loss = -11084.064453125
Iteration 3700: Loss = -11083.5810546875
Iteration 3800: Loss = -11083.138671875
Iteration 3900: Loss = -11082.6484375
Iteration 4000: Loss = -11075.310546875
Iteration 4100: Loss = -11074.939453125
Iteration 4200: Loss = -11074.640625
Iteration 4300: Loss = -11074.337890625
Iteration 4400: Loss = -11073.861328125
Iteration 4500: Loss = -11073.6328125
Iteration 4600: Loss = -11073.4228515625
Iteration 4700: Loss = -11073.2265625
Iteration 4800: Loss = -11073.0380859375
Iteration 4900: Loss = -11072.8603515625
Iteration 5000: Loss = -11072.6982421875
Iteration 5100: Loss = -11072.5458984375
Iteration 5200: Loss = -11066.5830078125
Iteration 5300: Loss = -11065.9990234375
Iteration 5400: Loss = -11065.8427734375
Iteration 5500: Loss = -11065.716796875
Iteration 5600: Loss = -11065.6044921875
Iteration 5700: Loss = -11062.2119140625
Iteration 5800: Loss = -11062.017578125
Iteration 5900: Loss = -11061.912109375
Iteration 6000: Loss = -11056.8330078125
Iteration 6100: Loss = -11056.1748046875
Iteration 6200: Loss = -11055.9912109375
Iteration 6300: Loss = -11055.861328125
Iteration 6400: Loss = -11055.751953125
Iteration 6500: Loss = -11055.6572265625
Iteration 6600: Loss = -11055.57421875
Iteration 6700: Loss = -11055.4970703125
Iteration 6800: Loss = -11055.4287109375
Iteration 6900: Loss = -11055.3642578125
Iteration 7000: Loss = -11055.3037109375
Iteration 7100: Loss = -11055.248046875
Iteration 7200: Loss = -11055.1982421875
Iteration 7300: Loss = -11055.1484375
Iteration 7400: Loss = -11055.1015625
Iteration 7500: Loss = -11055.060546875
Iteration 7600: Loss = -11055.017578125
Iteration 7700: Loss = -11054.9814453125
Iteration 7800: Loss = -11054.9453125
Iteration 7900: Loss = -11054.9111328125
Iteration 8000: Loss = -11054.8798828125
Iteration 8100: Loss = -11054.8486328125
Iteration 8200: Loss = -11054.8212890625
Iteration 8300: Loss = -11054.7939453125
Iteration 8400: Loss = -11054.7685546875
Iteration 8500: Loss = -11054.7431640625
Iteration 8600: Loss = -11054.7197265625
Iteration 8700: Loss = -11054.69921875
Iteration 8800: Loss = -11054.6787109375
Iteration 8900: Loss = -11054.658203125
Iteration 9000: Loss = -11054.6396484375
Iteration 9100: Loss = -11054.6220703125
Iteration 9200: Loss = -11054.6044921875
Iteration 9300: Loss = -11054.58984375
Iteration 9400: Loss = -11054.5732421875
Iteration 9500: Loss = -11054.55859375
Iteration 9600: Loss = -11054.544921875
Iteration 9700: Loss = -11054.533203125
Iteration 9800: Loss = -11054.5224609375
Iteration 9900: Loss = -11054.5078125
Iteration 10000: Loss = -11054.4970703125
Iteration 10100: Loss = -11054.4853515625
Iteration 10200: Loss = -11054.474609375
Iteration 10300: Loss = -11054.4619140625
Iteration 10400: Loss = -11054.4521484375
Iteration 10500: Loss = -11054.4404296875
Iteration 10600: Loss = -11054.431640625
Iteration 10700: Loss = -11054.421875
Iteration 10800: Loss = -11054.4130859375
Iteration 10900: Loss = -11054.404296875
Iteration 11000: Loss = -11054.3974609375
Iteration 11100: Loss = -11054.392578125
Iteration 11200: Loss = -11054.384765625
Iteration 11300: Loss = -11054.37890625
Iteration 11400: Loss = -11054.3740234375
Iteration 11500: Loss = -11054.3720703125
Iteration 11600: Loss = -11054.365234375
Iteration 11700: Loss = -11054.361328125
Iteration 11800: Loss = -11054.357421875
Iteration 11900: Loss = -11054.353515625
Iteration 12000: Loss = -11054.3505859375
Iteration 12100: Loss = -11054.3486328125
Iteration 12200: Loss = -11054.345703125
Iteration 12300: Loss = -11054.3427734375
Iteration 12400: Loss = -11054.33984375
Iteration 12500: Loss = -11054.3369140625
Iteration 12600: Loss = -11054.3349609375
Iteration 12700: Loss = -11054.33203125
Iteration 12800: Loss = -11054.330078125
Iteration 12900: Loss = -11054.3271484375
Iteration 13000: Loss = -11054.3271484375
Iteration 13100: Loss = -11054.322265625
Iteration 13200: Loss = -11054.3203125
Iteration 13300: Loss = -11054.31640625
Iteration 13400: Loss = -11054.314453125
Iteration 13500: Loss = -11054.30859375
Iteration 13600: Loss = -11054.3037109375
Iteration 13700: Loss = -11054.2900390625
Iteration 13800: Loss = -11054.2744140625
Iteration 13900: Loss = -11054.259765625
Iteration 14000: Loss = -11054.2509765625
Iteration 14100: Loss = -11054.2421875
Iteration 14200: Loss = -11054.232421875
Iteration 14300: Loss = -11054.220703125
Iteration 14400: Loss = -11054.2041015625
Iteration 14500: Loss = -11054.1875
Iteration 14600: Loss = -11054.169921875
Iteration 14700: Loss = -11054.14453125
Iteration 14800: Loss = -11054.109375
Iteration 14900: Loss = -11054.0546875
Iteration 15000: Loss = -11053.9716796875
Iteration 15100: Loss = -11053.8505859375
Iteration 15200: Loss = -11053.705078125
Iteration 15300: Loss = -11053.609375
Iteration 15400: Loss = -11053.5751953125
Iteration 15500: Loss = -11053.546875
Iteration 15600: Loss = -11053.51953125
Iteration 15700: Loss = -11053.5029296875
Iteration 15800: Loss = -11053.4951171875
Iteration 15900: Loss = -11053.490234375
Iteration 16000: Loss = -11053.48828125
Iteration 16100: Loss = -11053.48828125
Iteration 16200: Loss = -11053.4833984375
Iteration 16300: Loss = -11052.3154296875
Iteration 16400: Loss = -11052.1044921875
Iteration 16500: Loss = -11052.0146484375
Iteration 16600: Loss = -11051.955078125
Iteration 16700: Loss = -11051.919921875
Iteration 16800: Loss = -11051.9033203125
Iteration 16900: Loss = -11051.892578125
Iteration 17000: Loss = -11051.8857421875
Iteration 17100: Loss = -11051.8798828125
Iteration 17200: Loss = -11051.876953125
Iteration 17300: Loss = -11051.8740234375
Iteration 17400: Loss = -11051.873046875
Iteration 17500: Loss = -11051.8720703125
Iteration 17600: Loss = -11051.8701171875
Iteration 17700: Loss = -11051.87109375
1
Iteration 17800: Loss = -11051.8701171875
Iteration 17900: Loss = -11051.869140625
Iteration 18000: Loss = -11051.869140625
Iteration 18100: Loss = -11051.869140625
Iteration 18200: Loss = -11051.8681640625
Iteration 18300: Loss = -11051.8681640625
Iteration 18400: Loss = -11051.869140625
1
Iteration 18500: Loss = -11051.8671875
Iteration 18600: Loss = -11051.8681640625
1
Iteration 18700: Loss = -11051.8681640625
2
Iteration 18800: Loss = -11051.8671875
Iteration 18900: Loss = -11051.8662109375
Iteration 19000: Loss = -11051.8681640625
1
Iteration 19100: Loss = -11051.8671875
2
Iteration 19200: Loss = -11051.8662109375
Iteration 19300: Loss = -11051.5927734375
Iteration 19400: Loss = -11051.5888671875
Iteration 19500: Loss = -11051.587890625
Iteration 19600: Loss = -11051.578125
Iteration 19700: Loss = -11051.3369140625
Iteration 19800: Loss = -11051.3349609375
Iteration 19900: Loss = -11051.3076171875
Iteration 20000: Loss = -11051.14453125
Iteration 20100: Loss = -11051.1064453125
Iteration 20200: Loss = -11051.1015625
Iteration 20300: Loss = -11051.0986328125
Iteration 20400: Loss = -11051.0322265625
Iteration 20500: Loss = -11050.990234375
Iteration 20600: Loss = -11050.8955078125
Iteration 20700: Loss = -11050.8056640625
Iteration 20800: Loss = -11050.7060546875
Iteration 20900: Loss = -11050.373046875
Iteration 21000: Loss = -11047.8779296875
Iteration 21100: Loss = -11046.8154296875
Iteration 21200: Loss = -11046.6513671875
Iteration 21300: Loss = -11046.578125
Iteration 21400: Loss = -11046.4658203125
Iteration 21500: Loss = -11046.439453125
Iteration 21600: Loss = -11046.4111328125
Iteration 21700: Loss = -11046.3759765625
Iteration 21800: Loss = -11046.25
Iteration 21900: Loss = -11046.2373046875
Iteration 22000: Loss = -11046.232421875
Iteration 22100: Loss = -11046.2294921875
Iteration 22200: Loss = -11046.2265625
Iteration 22300: Loss = -11046.2236328125
Iteration 22400: Loss = -11046.22265625
Iteration 22500: Loss = -11046.220703125
Iteration 22600: Loss = -11046.21875
Iteration 22700: Loss = -11046.21875
Iteration 22800: Loss = -11046.216796875
Iteration 22900: Loss = -11046.2158203125
Iteration 23000: Loss = -11046.216796875
1
Iteration 23100: Loss = -11046.21484375
Iteration 23200: Loss = -11046.2138671875
Iteration 23300: Loss = -11046.21484375
1
Iteration 23400: Loss = -11046.21484375
2
Iteration 23500: Loss = -11046.21484375
3
Iteration 23600: Loss = -11046.21484375
4
Iteration 23700: Loss = -11046.212890625
Iteration 23800: Loss = -11046.2138671875
1
Iteration 23900: Loss = -11046.212890625
Iteration 24000: Loss = -11046.21484375
1
Iteration 24100: Loss = -11046.2119140625
Iteration 24200: Loss = -11046.2138671875
1
Iteration 24300: Loss = -11046.2138671875
2
Iteration 24400: Loss = -11046.2138671875
3
Iteration 24500: Loss = -11046.2138671875
4
Iteration 24600: Loss = -11046.21484375
5
Iteration 24700: Loss = -11046.2138671875
6
Iteration 24800: Loss = -11046.212890625
7
Iteration 24900: Loss = -11046.212890625
8
Iteration 25000: Loss = -11046.2138671875
9
Iteration 25100: Loss = -11046.2138671875
10
Iteration 25200: Loss = -11046.2138671875
11
Iteration 25300: Loss = -11046.2138671875
12
Iteration 25400: Loss = -11046.212890625
13
Iteration 25500: Loss = -11046.2138671875
14
Iteration 25600: Loss = -11046.212890625
15
Stopping early at iteration 25600 due to no improvement.
pi: tensor([[5.2006e-05, 9.9995e-01],
        [3.5512e-02, 9.6449e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0192, 0.9808], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3268, 0.0936],
         [0.9737, 0.1665]],

        [[0.9873, 0.2429],
         [0.0697, 0.1977]],

        [[0.9886, 0.1874],
         [0.0117, 0.9504]],

        [[0.9818, 0.0665],
         [0.0077, 0.9722]],

        [[0.4382, 0.0663],
         [0.6196, 0.9917]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.002821183194503557
Global Adjusted Rand Index: -0.00012218861508502565
Average Adjusted Rand Index: -0.002714155755075853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32617.3046875
Iteration 100: Loss = -20365.056640625
Iteration 200: Loss = -13328.2451171875
Iteration 300: Loss = -11666.5517578125
Iteration 400: Loss = -11381.6123046875
Iteration 500: Loss = -11287.916015625
Iteration 600: Loss = -11229.375
Iteration 700: Loss = -11189.2509765625
Iteration 800: Loss = -11159.33984375
Iteration 900: Loss = -11144.26171875
Iteration 1000: Loss = -11126.0830078125
Iteration 1100: Loss = -11113.58203125
Iteration 1200: Loss = -11100.78125
Iteration 1300: Loss = -11093.638671875
Iteration 1400: Loss = -11087.4609375
Iteration 1500: Loss = -11083.7177734375
Iteration 1600: Loss = -11080.71875
Iteration 1700: Loss = -11075.1298828125
Iteration 1800: Loss = -11071.2451171875
Iteration 1900: Loss = -11069.208984375
Iteration 2000: Loss = -11067.6083984375
Iteration 2100: Loss = -11066.2763671875
Iteration 2200: Loss = -11065.150390625
Iteration 2300: Loss = -11064.1806640625
Iteration 2400: Loss = -11063.3349609375
Iteration 2500: Loss = -11062.587890625
Iteration 2600: Loss = -11061.9248046875
Iteration 2700: Loss = -11061.33203125
Iteration 2800: Loss = -11060.7978515625
Iteration 2900: Loss = -11060.3154296875
Iteration 3000: Loss = -11059.87890625
Iteration 3100: Loss = -11059.4814453125
Iteration 3200: Loss = -11059.12109375
Iteration 3300: Loss = -11058.7900390625
Iteration 3400: Loss = -11058.4853515625
Iteration 3500: Loss = -11058.2080078125
Iteration 3600: Loss = -11057.951171875
Iteration 3700: Loss = -11057.7158203125
Iteration 3800: Loss = -11057.4951171875
Iteration 3900: Loss = -11057.29296875
Iteration 4000: Loss = -11057.10546875
Iteration 4100: Loss = -11056.931640625
Iteration 4200: Loss = -11056.7705078125
Iteration 4300: Loss = -11056.6181640625
Iteration 4400: Loss = -11056.4775390625
Iteration 4500: Loss = -11056.3466796875
Iteration 4600: Loss = -11056.2236328125
Iteration 4700: Loss = -11056.1083984375
Iteration 4800: Loss = -11056.0
Iteration 4900: Loss = -11055.8994140625
Iteration 5000: Loss = -11055.802734375
Iteration 5100: Loss = -11055.7119140625
Iteration 5200: Loss = -11055.6259765625
Iteration 5300: Loss = -11055.544921875
Iteration 5400: Loss = -11055.4697265625
Iteration 5500: Loss = -11055.3935546875
Iteration 5600: Loss = -11055.322265625
Iteration 5700: Loss = -11055.25390625
Iteration 5800: Loss = -11055.1845703125
Iteration 5900: Loss = -11055.1103515625
Iteration 6000: Loss = -11055.0302734375
Iteration 6100: Loss = -11054.9326171875
Iteration 6200: Loss = -11054.810546875
Iteration 6300: Loss = -11054.6630859375
Iteration 6400: Loss = -11054.5087890625
Iteration 6500: Loss = -11054.3740234375
Iteration 6600: Loss = -11054.2607421875
Iteration 6700: Loss = -11054.171875
Iteration 6800: Loss = -11054.0986328125
Iteration 6900: Loss = -11054.0419921875
Iteration 7000: Loss = -11053.998046875
Iteration 7100: Loss = -11053.9599609375
Iteration 7200: Loss = -11053.923828125
Iteration 7300: Loss = -11053.8916015625
Iteration 7400: Loss = -11053.8642578125
Iteration 7500: Loss = -11053.83203125
Iteration 7600: Loss = -11053.8046875
Iteration 7700: Loss = -11053.7734375
Iteration 7800: Loss = -11053.732421875
Iteration 7900: Loss = -11053.6376953125
Iteration 8000: Loss = -11053.5068359375
Iteration 8100: Loss = -11053.44140625
Iteration 8200: Loss = -11053.404296875
Iteration 8300: Loss = -11053.3828125
Iteration 8400: Loss = -11053.3662109375
Iteration 8500: Loss = -11053.3486328125
Iteration 8600: Loss = -11053.3369140625
Iteration 8700: Loss = -11053.32421875
Iteration 8800: Loss = -11053.3125
Iteration 8900: Loss = -11053.302734375
Iteration 9000: Loss = -11053.2939453125
Iteration 9100: Loss = -11053.283203125
Iteration 9200: Loss = -11053.275390625
Iteration 9300: Loss = -11053.2666015625
Iteration 9400: Loss = -11053.2578125
Iteration 9500: Loss = -11053.25
Iteration 9600: Loss = -11053.2421875
Iteration 9700: Loss = -11053.2294921875
Iteration 9800: Loss = -11053.21484375
Iteration 9900: Loss = -11053.197265625
Iteration 10000: Loss = -11053.171875
Iteration 10100: Loss = -11053.138671875
Iteration 10200: Loss = -11053.103515625
Iteration 10300: Loss = -11053.0732421875
Iteration 10400: Loss = -11053.0068359375
Iteration 10500: Loss = -11052.8779296875
Iteration 10600: Loss = -11052.84765625
Iteration 10700: Loss = -11052.8330078125
Iteration 10800: Loss = -11052.7958984375
Iteration 10900: Loss = -11052.78125
Iteration 11000: Loss = -11052.736328125
Iteration 11100: Loss = -11052.67578125
Iteration 11200: Loss = -11052.6259765625
Iteration 11300: Loss = -11052.5615234375
Iteration 11400: Loss = -11052.521484375
Iteration 11500: Loss = -11052.5087890625
Iteration 11600: Loss = -11052.482421875
Iteration 11700: Loss = -11052.4658203125
Iteration 11800: Loss = -11052.423828125
Iteration 11900: Loss = -11052.3515625
Iteration 12000: Loss = -11052.2919921875
Iteration 12100: Loss = -11052.2431640625
Iteration 12200: Loss = -11052.1591796875
Iteration 12300: Loss = -11052.0771484375
Iteration 12400: Loss = -11052.0224609375
Iteration 12500: Loss = -11051.9658203125
Iteration 12600: Loss = -11051.91796875
Iteration 12700: Loss = -11051.8466796875
Iteration 12800: Loss = -11051.783203125
Iteration 12900: Loss = -11051.701171875
Iteration 13000: Loss = -11051.1875
Iteration 13100: Loss = -11051.005859375
Iteration 13200: Loss = -11050.9921875
Iteration 13300: Loss = -11050.9033203125
Iteration 13400: Loss = -11050.6943359375
Iteration 13500: Loss = -11050.5224609375
Iteration 13600: Loss = -11049.0927734375
Iteration 13700: Loss = -11047.4140625
Iteration 13800: Loss = -11047.3134765625
Iteration 13900: Loss = -11047.0986328125
Iteration 14000: Loss = -11046.9150390625
Iteration 14100: Loss = -11046.7978515625
Iteration 14200: Loss = -11046.7626953125
Iteration 14300: Loss = -11046.720703125
Iteration 14400: Loss = -11046.64453125
Iteration 14500: Loss = -11046.5703125
Iteration 14600: Loss = -11046.404296875
Iteration 14700: Loss = -11046.341796875
Iteration 14800: Loss = -11046.224609375
Iteration 14900: Loss = -11046.1962890625
Iteration 15000: Loss = -11046.134765625
Iteration 15100: Loss = -11046.052734375
Iteration 15200: Loss = -11045.9833984375
Iteration 15300: Loss = -11045.8388671875
Iteration 15400: Loss = -11045.677734375
Iteration 15500: Loss = -11045.6064453125
Iteration 15600: Loss = -11045.5498046875
Iteration 15700: Loss = -11045.4833984375
Iteration 15800: Loss = -11045.458984375
Iteration 15900: Loss = -11045.4306640625
Iteration 16000: Loss = -11045.40625
Iteration 16100: Loss = -11045.3935546875
Iteration 16200: Loss = -11045.3857421875
Iteration 16300: Loss = -11045.3740234375
Iteration 16400: Loss = -11045.357421875
Iteration 16500: Loss = -11045.349609375
Iteration 16600: Loss = -11045.3447265625
Iteration 16700: Loss = -11045.33984375
Iteration 16800: Loss = -11045.3349609375
Iteration 16900: Loss = -11045.3349609375
Iteration 17000: Loss = -11045.3359375
1
Iteration 17100: Loss = -11045.3349609375
Iteration 17200: Loss = -11045.3330078125
Iteration 17300: Loss = -11045.33203125
Iteration 17400: Loss = -11045.3330078125
1
Iteration 17500: Loss = -11045.33203125
Iteration 17600: Loss = -11045.3330078125
1
Iteration 17700: Loss = -11045.3330078125
2
Iteration 17800: Loss = -11045.33203125
Iteration 17900: Loss = -11045.3310546875
Iteration 18000: Loss = -11045.3271484375
Iteration 18100: Loss = -11045.08984375
Iteration 18200: Loss = -11045.01953125
Iteration 18300: Loss = -11045.0087890625
Iteration 18400: Loss = -11045.005859375
Iteration 18500: Loss = -11044.9921875
Iteration 18600: Loss = -11044.9931640625
1
Iteration 18700: Loss = -11044.9921875
Iteration 18800: Loss = -11044.9931640625
1
Iteration 18900: Loss = -11044.9921875
Iteration 19000: Loss = -11044.9912109375
Iteration 19100: Loss = -11044.9921875
1
Iteration 19200: Loss = -11044.9921875
2
Iteration 19300: Loss = -11044.9921875
3
Iteration 19400: Loss = -11044.9921875
4
Iteration 19500: Loss = -11044.9921875
5
Iteration 19600: Loss = -11044.9921875
6
Iteration 19700: Loss = -11044.9931640625
7
Iteration 19800: Loss = -11044.9921875
8
Iteration 19900: Loss = -11044.9912109375
Iteration 20000: Loss = -11044.9921875
1
Iteration 20100: Loss = -11044.994140625
2
Iteration 20200: Loss = -11044.9921875
3
Iteration 20300: Loss = -11044.9921875
4
Iteration 20400: Loss = -11044.9921875
5
Iteration 20500: Loss = -11044.9921875
6
Iteration 20600: Loss = -11044.9912109375
Iteration 20700: Loss = -11044.9931640625
1
Iteration 20800: Loss = -11044.9921875
2
Iteration 20900: Loss = -11044.9921875
3
Iteration 21000: Loss = -11044.9931640625
4
Iteration 21100: Loss = -11044.9921875
5
Iteration 21200: Loss = -11044.9921875
6
Iteration 21300: Loss = -11044.9921875
7
Iteration 21400: Loss = -11044.9921875
8
Iteration 21500: Loss = -11044.9931640625
9
Iteration 21600: Loss = -11044.9912109375
Iteration 21700: Loss = -11044.994140625
1
Iteration 21800: Loss = -11044.9912109375
Iteration 21900: Loss = -11044.9921875
1
Iteration 22000: Loss = -11044.9921875
2
Iteration 22100: Loss = -11044.9921875
3
Iteration 22200: Loss = -11044.9921875
4
Iteration 22300: Loss = -11044.9921875
5
Iteration 22400: Loss = -11044.9892578125
Iteration 22500: Loss = -11044.9892578125
Iteration 22600: Loss = -11044.9892578125
Iteration 22700: Loss = -11044.9912109375
1
Iteration 22800: Loss = -11044.990234375
2
Iteration 22900: Loss = -11044.9892578125
Iteration 23000: Loss = -11044.9921875
1
Iteration 23100: Loss = -11044.990234375
2
Iteration 23200: Loss = -11044.9892578125
Iteration 23300: Loss = -11044.9892578125
Iteration 23400: Loss = -11044.990234375
1
Iteration 23500: Loss = -11044.9892578125
Iteration 23600: Loss = -11044.9892578125
Iteration 23700: Loss = -11044.9892578125
Iteration 23800: Loss = -11044.98828125
Iteration 23900: Loss = -11044.98828125
Iteration 24000: Loss = -11044.9892578125
1
Iteration 24100: Loss = -11044.9892578125
2
Iteration 24200: Loss = -11044.990234375
3
Iteration 24300: Loss = -11044.990234375
4
Iteration 24400: Loss = -11044.990234375
5
Iteration 24500: Loss = -11044.990234375
6
Iteration 24600: Loss = -11044.9892578125
7
Iteration 24700: Loss = -11044.9892578125
8
Iteration 24800: Loss = -11044.990234375
9
Iteration 24900: Loss = -11044.9892578125
10
Iteration 25000: Loss = -11044.9892578125
11
Iteration 25100: Loss = -11044.98828125
Iteration 25200: Loss = -11044.9892578125
1
Iteration 25300: Loss = -11044.98828125
Iteration 25400: Loss = -11044.98828125
Iteration 25500: Loss = -11044.9892578125
1
Iteration 25600: Loss = -11044.98828125
Iteration 25700: Loss = -11044.9873046875
Iteration 25800: Loss = -11044.98828125
1
Iteration 25900: Loss = -11044.9892578125
2
Iteration 26000: Loss = -11044.9892578125
3
Iteration 26100: Loss = -11044.98828125
4
Iteration 26200: Loss = -11044.9892578125
5
Iteration 26300: Loss = -11044.98828125
6
Iteration 26400: Loss = -11044.9892578125
7
Iteration 26500: Loss = -11044.9912109375
8
Iteration 26600: Loss = -11044.98828125
9
Iteration 26700: Loss = -11044.9892578125
10
Iteration 26800: Loss = -11044.9892578125
11
Iteration 26900: Loss = -11044.98828125
12
Iteration 27000: Loss = -11044.98828125
13
Iteration 27100: Loss = -11044.990234375
14
Iteration 27200: Loss = -11044.9892578125
15
Stopping early at iteration 27200 due to no improvement.
pi: tensor([[9.5613e-01, 4.3870e-02],
        [1.0000e+00, 4.1187e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9803, 0.0197], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1663, 0.0947],
         [0.8510, 0.3051]],

        [[0.2073, 0.2260],
         [0.2665, 0.8442]],

        [[0.0111, 0.1866],
         [0.1521, 0.8274]],

        [[0.9882, 0.0694],
         [0.0253, 0.4588]],

        [[0.0348, 0.0668],
         [0.5290, 0.3740]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
Global Adjusted Rand Index: -0.00012218861508502565
Average Adjusted Rand Index: -0.002714155755075853
[-0.00012218861508502565, -0.00012218861508502565] [-0.002714155755075853, -0.002714155755075853] [11046.212890625, 11044.9892578125]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11136.713475986695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27716.0546875
Iteration 100: Loss = -18332.8046875
Iteration 200: Loss = -12738.8173828125
Iteration 300: Loss = -11812.0732421875
Iteration 400: Loss = -11565.234375
Iteration 500: Loss = -11468.8095703125
Iteration 600: Loss = -11425.7568359375
Iteration 700: Loss = -11404.923828125
Iteration 800: Loss = -11390.525390625
Iteration 900: Loss = -11380.6259765625
Iteration 1000: Loss = -11372.9765625
Iteration 1100: Loss = -11365.119140625
Iteration 1200: Loss = -11358.2177734375
Iteration 1300: Loss = -11351.4638671875
Iteration 1400: Loss = -11346.6103515625
Iteration 1500: Loss = -11342.142578125
Iteration 1600: Loss = -11337.3251953125
Iteration 1700: Loss = -11333.5654296875
Iteration 1800: Loss = -11330.1962890625
Iteration 1900: Loss = -11326.3369140625
Iteration 2000: Loss = -11321.8056640625
Iteration 2100: Loss = -11317.578125
Iteration 2200: Loss = -11313.0830078125
Iteration 2300: Loss = -11310.771484375
Iteration 2400: Loss = -11308.6640625
Iteration 2500: Loss = -11306.4384765625
Iteration 2600: Loss = -11304.017578125
Iteration 2700: Loss = -11302.0244140625
Iteration 2800: Loss = -11300.6728515625
Iteration 2900: Loss = -11299.3759765625
Iteration 3000: Loss = -11298.0458984375
Iteration 3100: Loss = -11295.0927734375
Iteration 3200: Loss = -11294.150390625
Iteration 3300: Loss = -11293.076171875
Iteration 3400: Loss = -11291.6318359375
Iteration 3500: Loss = -11289.921875
Iteration 3600: Loss = -11288.111328125
Iteration 3700: Loss = -11287.3701171875
Iteration 3800: Loss = -11286.873046875
Iteration 3900: Loss = -11286.4609375
Iteration 4000: Loss = -11286.107421875
Iteration 4100: Loss = -11285.802734375
Iteration 4200: Loss = -11285.501953125
Iteration 4300: Loss = -11285.1796875
Iteration 4400: Loss = -11284.814453125
Iteration 4500: Loss = -11284.416015625
Iteration 4600: Loss = -11284.025390625
Iteration 4700: Loss = -11283.6875
Iteration 4800: Loss = -11283.3681640625
Iteration 4900: Loss = -11283.13671875
Iteration 5000: Loss = -11282.9404296875
Iteration 5100: Loss = -11282.7626953125
Iteration 5200: Loss = -11282.583984375
Iteration 5300: Loss = -11282.4033203125
Iteration 5400: Loss = -11282.2255859375
Iteration 5500: Loss = -11282.07421875
Iteration 5600: Loss = -11281.9501953125
Iteration 5700: Loss = -11281.8505859375
Iteration 5800: Loss = -11281.7685546875
Iteration 5900: Loss = -11281.6962890625
Iteration 6000: Loss = -11281.63671875
Iteration 6100: Loss = -11281.5859375
Iteration 6200: Loss = -11281.5390625
Iteration 6300: Loss = -11281.498046875
Iteration 6400: Loss = -11281.462890625
Iteration 6500: Loss = -11281.4287109375
Iteration 6600: Loss = -11281.3974609375
Iteration 6700: Loss = -11281.3701171875
Iteration 6800: Loss = -11281.3447265625
Iteration 6900: Loss = -11281.322265625
Iteration 7000: Loss = -11281.2998046875
Iteration 7100: Loss = -11281.2802734375
Iteration 7200: Loss = -11281.263671875
Iteration 7300: Loss = -11281.248046875
Iteration 7400: Loss = -11281.2314453125
Iteration 7500: Loss = -11281.21875
Iteration 7600: Loss = -11281.205078125
Iteration 7700: Loss = -11281.1943359375
Iteration 7800: Loss = -11281.1826171875
Iteration 7900: Loss = -11281.1728515625
Iteration 8000: Loss = -11281.162109375
Iteration 8100: Loss = -11281.146484375
Iteration 8200: Loss = -11280.96484375
Iteration 8300: Loss = -11280.935546875
Iteration 8400: Loss = -11280.9267578125
Iteration 8500: Loss = -11280.919921875
Iteration 8600: Loss = -11280.912109375
Iteration 8700: Loss = -11280.90625
Iteration 8800: Loss = -11280.9013671875
Iteration 8900: Loss = -11280.8935546875
Iteration 9000: Loss = -11280.8876953125
Iteration 9100: Loss = -11280.884765625
Iteration 9200: Loss = -11280.87890625
Iteration 9300: Loss = -11280.875
Iteration 9400: Loss = -11280.87109375
Iteration 9500: Loss = -11280.865234375
Iteration 9600: Loss = -11280.8642578125
Iteration 9700: Loss = -11280.8583984375
Iteration 9800: Loss = -11280.8564453125
Iteration 9900: Loss = -11280.853515625
Iteration 10000: Loss = -11280.8486328125
Iteration 10100: Loss = -11280.8466796875
Iteration 10200: Loss = -11280.84375
Iteration 10300: Loss = -11280.83984375
Iteration 10400: Loss = -11280.8369140625
Iteration 10500: Loss = -11280.8310546875
Iteration 10600: Loss = -11280.8212890625
Iteration 10700: Loss = -11280.8115234375
Iteration 10800: Loss = -11280.8017578125
Iteration 10900: Loss = -11280.7978515625
Iteration 11000: Loss = -11280.7939453125
Iteration 11100: Loss = -11280.7900390625
Iteration 11200: Loss = -11280.7880859375
Iteration 11300: Loss = -11280.787109375
Iteration 11400: Loss = -11280.7841796875
Iteration 11500: Loss = -11280.783203125
Iteration 11600: Loss = -11280.783203125
Iteration 11700: Loss = -11280.7802734375
Iteration 11800: Loss = -11280.779296875
Iteration 11900: Loss = -11280.779296875
Iteration 12000: Loss = -11280.77734375
Iteration 12100: Loss = -11280.775390625
Iteration 12200: Loss = -11280.7763671875
1
Iteration 12300: Loss = -11280.775390625
Iteration 12400: Loss = -11280.7724609375
Iteration 12500: Loss = -11280.7734375
1
Iteration 12600: Loss = -11280.771484375
Iteration 12700: Loss = -11280.76953125
Iteration 12800: Loss = -11280.76953125
Iteration 12900: Loss = -11280.767578125
Iteration 13000: Loss = -11280.767578125
Iteration 13100: Loss = -11280.7666015625
Iteration 13200: Loss = -11280.767578125
1
Iteration 13300: Loss = -11280.7666015625
Iteration 13400: Loss = -11280.7666015625
Iteration 13500: Loss = -11280.7666015625
Iteration 13600: Loss = -11280.7646484375
Iteration 13700: Loss = -11280.7646484375
Iteration 13800: Loss = -11280.763671875
Iteration 13900: Loss = -11280.763671875
Iteration 14000: Loss = -11280.7626953125
Iteration 14100: Loss = -11280.763671875
1
Iteration 14200: Loss = -11280.76171875
Iteration 14300: Loss = -11280.76171875
Iteration 14400: Loss = -11280.76171875
Iteration 14500: Loss = -11280.759765625
Iteration 14600: Loss = -11280.7607421875
1
Iteration 14700: Loss = -11280.759765625
Iteration 14800: Loss = -11280.759765625
Iteration 14900: Loss = -11280.759765625
Iteration 15000: Loss = -11280.759765625
Iteration 15100: Loss = -11280.7607421875
1
Iteration 15200: Loss = -11280.7587890625
Iteration 15300: Loss = -11280.7578125
Iteration 15400: Loss = -11280.7587890625
1
Iteration 15500: Loss = -11280.7587890625
2
Iteration 15600: Loss = -11280.7587890625
3
Iteration 15700: Loss = -11280.7587890625
4
Iteration 15800: Loss = -11280.7578125
Iteration 15900: Loss = -11280.7578125
Iteration 16000: Loss = -11280.7578125
Iteration 16100: Loss = -11280.7587890625
1
Iteration 16200: Loss = -11280.7568359375
Iteration 16300: Loss = -11280.7578125
1
Iteration 16400: Loss = -11280.7548828125
Iteration 16500: Loss = -11280.755859375
1
Iteration 16600: Loss = -11280.7568359375
2
Iteration 16700: Loss = -11280.7568359375
3
Iteration 16800: Loss = -11280.755859375
4
Iteration 16900: Loss = -11280.7587890625
5
Iteration 17000: Loss = -11280.755859375
6
Iteration 17100: Loss = -11280.7568359375
7
Iteration 17200: Loss = -11280.755859375
8
Iteration 17300: Loss = -11280.755859375
9
Iteration 17400: Loss = -11280.7548828125
Iteration 17500: Loss = -11280.7568359375
1
Iteration 17600: Loss = -11280.744140625
Iteration 17700: Loss = -11280.7431640625
Iteration 17800: Loss = -11280.7451171875
1
Iteration 17900: Loss = -11280.7431640625
Iteration 18000: Loss = -11280.7431640625
Iteration 18100: Loss = -11280.744140625
1
Iteration 18200: Loss = -11280.744140625
2
Iteration 18300: Loss = -11280.7431640625
Iteration 18400: Loss = -11280.744140625
1
Iteration 18500: Loss = -11280.744140625
2
Iteration 18600: Loss = -11280.744140625
3
Iteration 18700: Loss = -11280.744140625
4
Iteration 18800: Loss = -11280.7421875
Iteration 18900: Loss = -11280.7421875
Iteration 19000: Loss = -11280.744140625
1
Iteration 19100: Loss = -11280.744140625
2
Iteration 19200: Loss = -11280.7431640625
3
Iteration 19300: Loss = -11280.744140625
4
Iteration 19400: Loss = -11280.7431640625
5
Iteration 19500: Loss = -11280.7431640625
6
Iteration 19600: Loss = -11280.7431640625
7
Iteration 19700: Loss = -11280.7421875
Iteration 19800: Loss = -11280.744140625
1
Iteration 19900: Loss = -11280.7421875
Iteration 20000: Loss = -11280.7431640625
1
Iteration 20100: Loss = -11280.7412109375
Iteration 20200: Loss = -11280.7421875
1
Iteration 20300: Loss = -11280.7421875
2
Iteration 20400: Loss = -11280.7421875
3
Iteration 20500: Loss = -11280.7421875
4
Iteration 20600: Loss = -11280.7431640625
5
Iteration 20700: Loss = -11280.7431640625
6
Iteration 20800: Loss = -11280.7421875
7
Iteration 20900: Loss = -11280.7431640625
8
Iteration 21000: Loss = -11280.7412109375
Iteration 21100: Loss = -11280.7421875
1
Iteration 21200: Loss = -11280.7421875
2
Iteration 21300: Loss = -11280.740234375
Iteration 21400: Loss = -11280.7412109375
1
Iteration 21500: Loss = -11280.7412109375
2
Iteration 21600: Loss = -11280.7412109375
3
Iteration 21700: Loss = -11280.7412109375
4
Iteration 21800: Loss = -11280.7412109375
5
Iteration 21900: Loss = -11280.740234375
Iteration 22000: Loss = -11280.740234375
Iteration 22100: Loss = -11280.7412109375
1
Iteration 22200: Loss = -11280.7412109375
2
Iteration 22300: Loss = -11280.7412109375
3
Iteration 22400: Loss = -11280.740234375
Iteration 22500: Loss = -11280.740234375
Iteration 22600: Loss = -11280.7412109375
1
Iteration 22700: Loss = -11280.7421875
2
Iteration 22800: Loss = -11280.740234375
Iteration 22900: Loss = -11280.7421875
1
Iteration 23000: Loss = -11280.7412109375
2
Iteration 23100: Loss = -11280.740234375
Iteration 23200: Loss = -11280.740234375
Iteration 23300: Loss = -11280.7431640625
1
Iteration 23400: Loss = -11280.740234375
Iteration 23500: Loss = -11280.740234375
Iteration 23600: Loss = -11280.7412109375
1
Iteration 23700: Loss = -11280.73046875
Iteration 23800: Loss = -11280.7177734375
Iteration 23900: Loss = -11280.68359375
Iteration 24000: Loss = -11279.998046875
Iteration 24100: Loss = -11279.708984375
Iteration 24200: Loss = -11279.6103515625
Iteration 24300: Loss = -11279.6103515625
Iteration 24400: Loss = -11279.6123046875
1
Iteration 24500: Loss = -11279.544921875
Iteration 24600: Loss = -11279.5380859375
Iteration 24700: Loss = -11279.4658203125
Iteration 24800: Loss = -11279.458984375
Iteration 24900: Loss = -11279.4375
Iteration 25000: Loss = -11279.255859375
Iteration 25100: Loss = -11279.1083984375
Iteration 25200: Loss = -11279.0341796875
Iteration 25300: Loss = -11278.6181640625
Iteration 25400: Loss = -11278.0595703125
Iteration 25500: Loss = -11277.5849609375
Iteration 25600: Loss = -11275.95703125
Iteration 25700: Loss = -11274.9765625
Iteration 25800: Loss = -11241.072265625
Iteration 25900: Loss = -11201.4521484375
Iteration 26000: Loss = -11193.275390625
Iteration 26100: Loss = -11178.009765625
Iteration 26200: Loss = -11166.623046875
Iteration 26300: Loss = -11151.033203125
Iteration 26400: Loss = -11147.767578125
Iteration 26500: Loss = -11147.388671875
Iteration 26600: Loss = -11147.345703125
Iteration 26700: Loss = -11143.46484375
Iteration 26800: Loss = -11142.9921875
Iteration 26900: Loss = -11142.9716796875
Iteration 27000: Loss = -11142.9580078125
Iteration 27100: Loss = -11142.94921875
Iteration 27200: Loss = -11142.9443359375
Iteration 27300: Loss = -11142.9375
Iteration 27400: Loss = -11142.93359375
Iteration 27500: Loss = -11142.931640625
Iteration 27600: Loss = -11142.9287109375
Iteration 27700: Loss = -11142.92578125
Iteration 27800: Loss = -11142.7373046875
Iteration 27900: Loss = -11142.7294921875
Iteration 28000: Loss = -11142.728515625
Iteration 28100: Loss = -11142.7255859375
Iteration 28200: Loss = -11142.724609375
Iteration 28300: Loss = -11142.724609375
Iteration 28400: Loss = -11141.310546875
Iteration 28500: Loss = -11139.5517578125
Iteration 28600: Loss = -11139.5458984375
Iteration 28700: Loss = -11139.5439453125
Iteration 28800: Loss = -11139.5419921875
Iteration 28900: Loss = -11139.5419921875
Iteration 29000: Loss = -11139.541015625
Iteration 29100: Loss = -11139.541015625
Iteration 29200: Loss = -11139.541015625
Iteration 29300: Loss = -11139.541015625
Iteration 29400: Loss = -11139.5390625
Iteration 29500: Loss = -11139.5400390625
1
Iteration 29600: Loss = -11139.5390625
Iteration 29700: Loss = -11139.5400390625
1
Iteration 29800: Loss = -11139.5390625
Iteration 29900: Loss = -11139.5390625
pi: tensor([[0.6773, 0.3227],
        [0.4920, 0.5080]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3832, 0.6168], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2420, 0.0980],
         [0.8348, 0.2308]],

        [[0.0240, 0.0969],
         [0.0103, 0.9758]],

        [[0.0272, 0.0973],
         [0.0701, 0.0145]],

        [[0.0129, 0.1027],
         [0.9754, 0.6174]],

        [[0.7302, 0.0968],
         [0.2468, 0.0647]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8447901065451248
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080890789891884
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.6689365546862865
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6362859133983289
Global Adjusted Rand Index: 0.2565010898861454
Average Adjusted Rand Index: 0.7681015855270908
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42699.86328125
Iteration 100: Loss = -24085.427734375
Iteration 200: Loss = -14592.0947265625
Iteration 300: Loss = -12016.224609375
Iteration 400: Loss = -11646.5546875
Iteration 500: Loss = -11507.3740234375
Iteration 600: Loss = -11425.306640625
Iteration 700: Loss = -11390.2568359375
Iteration 800: Loss = -11364.9033203125
Iteration 900: Loss = -11350.3134765625
Iteration 1000: Loss = -11327.705078125
Iteration 1100: Loss = -11314.3134765625
Iteration 1200: Loss = -11309.427734375
Iteration 1300: Loss = -11306.0478515625
Iteration 1400: Loss = -11303.404296875
Iteration 1500: Loss = -11301.2451171875
Iteration 1600: Loss = -11299.431640625
Iteration 1700: Loss = -11297.8876953125
Iteration 1800: Loss = -11296.5576171875
Iteration 1900: Loss = -11295.396484375
Iteration 2000: Loss = -11294.373046875
Iteration 2100: Loss = -11293.4658203125
Iteration 2200: Loss = -11292.6611328125
Iteration 2300: Loss = -11291.9365234375
Iteration 2400: Loss = -11291.283203125
Iteration 2500: Loss = -11290.6943359375
Iteration 2600: Loss = -11290.158203125
Iteration 2700: Loss = -11289.6728515625
Iteration 2800: Loss = -11289.2421875
Iteration 2900: Loss = -11288.8564453125
Iteration 3000: Loss = -11288.509765625
Iteration 3100: Loss = -11288.193359375
Iteration 3200: Loss = -11287.90625
Iteration 3300: Loss = -11287.64453125
Iteration 3400: Loss = -11287.404296875
Iteration 3500: Loss = -11287.18359375
Iteration 3600: Loss = -11286.9814453125
Iteration 3700: Loss = -11286.7939453125
Iteration 3800: Loss = -11286.619140625
Iteration 3900: Loss = -11286.45703125
Iteration 4000: Loss = -11286.3076171875
Iteration 4100: Loss = -11286.1689453125
Iteration 4200: Loss = -11286.0390625
Iteration 4300: Loss = -11285.9169921875
Iteration 4400: Loss = -11285.8046875
Iteration 4500: Loss = -11285.697265625
Iteration 4600: Loss = -11285.5986328125
Iteration 4700: Loss = -11285.5048828125
Iteration 4800: Loss = -11285.41796875
Iteration 4900: Loss = -11285.3369140625
Iteration 5000: Loss = -11285.259765625
Iteration 5100: Loss = -11285.1884765625
Iteration 5200: Loss = -11285.1220703125
Iteration 5300: Loss = -11285.0576171875
Iteration 5400: Loss = -11284.9990234375
Iteration 5500: Loss = -11284.9443359375
Iteration 5600: Loss = -11284.8896484375
Iteration 5700: Loss = -11284.83984375
Iteration 5800: Loss = -11284.7939453125
Iteration 5900: Loss = -11284.7490234375
Iteration 6000: Loss = -11284.70703125
Iteration 6100: Loss = -11284.6669921875
Iteration 6200: Loss = -11284.630859375
Iteration 6300: Loss = -11284.5927734375
Iteration 6400: Loss = -11284.5615234375
Iteration 6500: Loss = -11284.5283203125
Iteration 6600: Loss = -11284.4990234375
Iteration 6700: Loss = -11284.4716796875
Iteration 6800: Loss = -11284.4423828125
Iteration 6900: Loss = -11284.4189453125
Iteration 7000: Loss = -11284.392578125
Iteration 7100: Loss = -11284.37109375
Iteration 7200: Loss = -11284.349609375
Iteration 7300: Loss = -11284.328125
Iteration 7400: Loss = -11284.3076171875
Iteration 7500: Loss = -11284.2890625
Iteration 7600: Loss = -11284.2734375
Iteration 7700: Loss = -11284.25390625
Iteration 7800: Loss = -11284.23828125
Iteration 7900: Loss = -11284.2236328125
Iteration 8000: Loss = -11284.208984375
Iteration 8100: Loss = -11284.1943359375
Iteration 8200: Loss = -11284.1796875
Iteration 8300: Loss = -11284.16796875
Iteration 8400: Loss = -11284.15625
Iteration 8500: Loss = -11284.14453125
Iteration 8600: Loss = -11284.1337890625
Iteration 8700: Loss = -11284.123046875
Iteration 8800: Loss = -11284.11328125
Iteration 8900: Loss = -11284.1044921875
Iteration 9000: Loss = -11284.0947265625
Iteration 9100: Loss = -11284.0849609375
Iteration 9200: Loss = -11284.076171875
Iteration 9300: Loss = -11284.068359375
Iteration 9400: Loss = -11284.0595703125
Iteration 9500: Loss = -11284.05078125
Iteration 9600: Loss = -11284.0419921875
Iteration 9700: Loss = -11284.037109375
Iteration 9800: Loss = -11284.0283203125
Iteration 9900: Loss = -11284.01953125
Iteration 10000: Loss = -11284.013671875
Iteration 10100: Loss = -11284.0048828125
Iteration 10200: Loss = -11283.9970703125
Iteration 10300: Loss = -11283.9912109375
Iteration 10400: Loss = -11283.9833984375
Iteration 10500: Loss = -11283.9755859375
Iteration 10600: Loss = -11283.96875
Iteration 10700: Loss = -11283.9619140625
Iteration 10800: Loss = -11283.9541015625
Iteration 10900: Loss = -11283.9482421875
Iteration 11000: Loss = -11283.94140625
Iteration 11100: Loss = -11283.9365234375
Iteration 11200: Loss = -11283.9287109375
Iteration 11300: Loss = -11283.9208984375
Iteration 11400: Loss = -11283.912109375
Iteration 11500: Loss = -11283.900390625
Iteration 11600: Loss = -11283.884765625
Iteration 11700: Loss = -11283.8525390625
Iteration 11800: Loss = -11283.748046875
Iteration 11900: Loss = -11281.80078125
Iteration 12000: Loss = -11281.4697265625
Iteration 12100: Loss = -11281.349609375
Iteration 12200: Loss = -11280.806640625
Iteration 12300: Loss = -11280.666015625
Iteration 12400: Loss = -11280.59375
Iteration 12500: Loss = -11280.5361328125
Iteration 12600: Loss = -11280.4814453125
Iteration 12700: Loss = -11280.4267578125
Iteration 12800: Loss = -11280.3671875
Iteration 12900: Loss = -11280.3017578125
Iteration 13000: Loss = -11280.2236328125
Iteration 13100: Loss = -11280.1376953125
Iteration 13200: Loss = -11280.0361328125
Iteration 13300: Loss = -11279.916015625
Iteration 13400: Loss = -11279.7763671875
Iteration 13500: Loss = -11279.609375
Iteration 13600: Loss = -11279.388671875
Iteration 13700: Loss = -11279.0634765625
Iteration 13800: Loss = -11278.56640625
Iteration 13900: Loss = -11278.0205078125
Iteration 14000: Loss = -11276.9794921875
Iteration 14100: Loss = -11276.3837890625
Iteration 14200: Loss = -11275.4462890625
Iteration 14300: Loss = -11221.0654296875
Iteration 14400: Loss = -11165.0283203125
Iteration 14500: Loss = -11148.390625
Iteration 14600: Loss = -11143.78125
Iteration 14700: Loss = -11142.9755859375
Iteration 14800: Loss = -11142.8310546875
Iteration 14900: Loss = -11142.7158203125
Iteration 15000: Loss = -11142.5966796875
Iteration 15100: Loss = -11142.48046875
Iteration 15200: Loss = -11140.3330078125
Iteration 15300: Loss = -11138.9248046875
Iteration 15400: Loss = -11138.88671875
Iteration 15500: Loss = -11138.8720703125
Iteration 15600: Loss = -11138.8564453125
Iteration 15700: Loss = -11138.849609375
Iteration 15800: Loss = -11138.8349609375
Iteration 15900: Loss = -11138.8203125
Iteration 16000: Loss = -11138.8017578125
Iteration 16100: Loss = -11138.79296875
Iteration 16200: Loss = -11138.7890625
Iteration 16300: Loss = -11138.73046875
Iteration 16400: Loss = -11138.7275390625
Iteration 16500: Loss = -11138.7255859375
Iteration 16600: Loss = -11138.724609375
Iteration 16700: Loss = -11138.7236328125
Iteration 16800: Loss = -11138.7216796875
Iteration 16900: Loss = -11138.7197265625
Iteration 17000: Loss = -11138.7177734375
Iteration 17100: Loss = -11138.6728515625
Iteration 17200: Loss = -11138.6728515625
Iteration 17300: Loss = -11138.671875
Iteration 17400: Loss = -11138.669921875
Iteration 17500: Loss = -11138.669921875
Iteration 17600: Loss = -11138.6708984375
1
Iteration 17700: Loss = -11138.66796875
Iteration 17800: Loss = -11138.66796875
Iteration 17900: Loss = -11138.66796875
Iteration 18000: Loss = -11138.66796875
Iteration 18100: Loss = -11138.6669921875
Iteration 18200: Loss = -11138.6650390625
Iteration 18300: Loss = -11138.6640625
Iteration 18400: Loss = -11138.6640625
Iteration 18500: Loss = -11138.6630859375
Iteration 18600: Loss = -11138.6630859375
Iteration 18700: Loss = -11138.6630859375
Iteration 18800: Loss = -11138.6630859375
Iteration 18900: Loss = -11138.662109375
Iteration 19000: Loss = -11138.654296875
Iteration 19100: Loss = -11138.5078125
Iteration 19200: Loss = -11138.5078125
Iteration 19300: Loss = -11138.505859375
Iteration 19400: Loss = -11138.5068359375
1
Iteration 19500: Loss = -11138.4990234375
Iteration 19600: Loss = -11138.4990234375
Iteration 19700: Loss = -11138.4990234375
Iteration 19800: Loss = -11138.4990234375
Iteration 19900: Loss = -11138.5
1
Iteration 20000: Loss = -11138.5
2
Iteration 20100: Loss = -11138.498046875
Iteration 20200: Loss = -11138.4970703125
Iteration 20300: Loss = -11138.4990234375
1
Iteration 20400: Loss = -11138.4990234375
2
Iteration 20500: Loss = -11138.4990234375
3
Iteration 20600: Loss = -11138.498046875
4
Iteration 20700: Loss = -11138.4970703125
Iteration 20800: Loss = -11138.5
1
Iteration 20900: Loss = -11138.49609375
Iteration 21000: Loss = -11138.49609375
Iteration 21100: Loss = -11138.49609375
Iteration 21200: Loss = -11138.49609375
Iteration 21300: Loss = -11138.4951171875
Iteration 21400: Loss = -11138.49609375
1
Iteration 21500: Loss = -11138.4951171875
Iteration 21600: Loss = -11138.494140625
Iteration 21700: Loss = -11138.4931640625
Iteration 21800: Loss = -11138.498046875
1
Iteration 21900: Loss = -11138.494140625
2
Iteration 22000: Loss = -11138.4931640625
Iteration 22100: Loss = -11138.4931640625
Iteration 22200: Loss = -11138.4931640625
Iteration 22300: Loss = -11138.4931640625
Iteration 22400: Loss = -11138.4931640625
Iteration 22500: Loss = -11138.4931640625
Iteration 22600: Loss = -11138.4921875
Iteration 22700: Loss = -11138.490234375
Iteration 22800: Loss = -11138.4921875
1
Iteration 22900: Loss = -11138.4912109375
2
Iteration 23000: Loss = -11138.4921875
3
Iteration 23100: Loss = -11138.4912109375
4
Iteration 23200: Loss = -11138.4912109375
5
Iteration 23300: Loss = -11138.490234375
Iteration 23400: Loss = -11138.4912109375
1
Iteration 23500: Loss = -11138.4912109375
2
Iteration 23600: Loss = -11138.4912109375
3
Iteration 23700: Loss = -11138.4912109375
4
Iteration 23800: Loss = -11138.4912109375
5
Iteration 23900: Loss = -11138.4912109375
6
Iteration 24000: Loss = -11138.490234375
Iteration 24100: Loss = -11138.4912109375
1
Iteration 24200: Loss = -11138.4892578125
Iteration 24300: Loss = -11138.4921875
1
Iteration 24400: Loss = -11138.4892578125
Iteration 24500: Loss = -11138.4912109375
1
Iteration 24600: Loss = -11138.490234375
2
Iteration 24700: Loss = -11138.4921875
3
Iteration 24800: Loss = -11138.490234375
4
Iteration 24900: Loss = -11138.490234375
5
Iteration 25000: Loss = -11138.490234375
6
Iteration 25100: Loss = -11138.490234375
7
Iteration 25200: Loss = -11138.4921875
8
Iteration 25300: Loss = -11138.4873046875
Iteration 25400: Loss = -11138.4873046875
Iteration 25500: Loss = -11138.48828125
1
Iteration 25600: Loss = -11138.4873046875
Iteration 25700: Loss = -11138.4873046875
Iteration 25800: Loss = -11138.4873046875
Iteration 25900: Loss = -11138.48828125
1
Iteration 26000: Loss = -11138.4873046875
Iteration 26100: Loss = -11138.4892578125
1
Iteration 26200: Loss = -11138.4873046875
Iteration 26300: Loss = -11138.4873046875
Iteration 26400: Loss = -11138.48828125
1
Iteration 26500: Loss = -11138.4873046875
Iteration 26600: Loss = -11138.4873046875
Iteration 26700: Loss = -11138.48828125
1
Iteration 26800: Loss = -11138.48828125
2
Iteration 26900: Loss = -11138.4873046875
Iteration 27000: Loss = -11138.4892578125
1
Iteration 27100: Loss = -11138.486328125
Iteration 27200: Loss = -11138.48828125
1
Iteration 27300: Loss = -11138.4873046875
2
Iteration 27400: Loss = -11138.4873046875
3
Iteration 27500: Loss = -11138.4873046875
4
Iteration 27600: Loss = -11138.4873046875
5
Iteration 27700: Loss = -11138.486328125
Iteration 27800: Loss = -11138.4794921875
Iteration 27900: Loss = -11138.4794921875
Iteration 28000: Loss = -11138.4794921875
Iteration 28100: Loss = -11138.4794921875
Iteration 28200: Loss = -11138.478515625
Iteration 28300: Loss = -11138.4794921875
1
Iteration 28400: Loss = -11138.4794921875
2
Iteration 28500: Loss = -11138.4794921875
3
Iteration 28600: Loss = -11138.48046875
4
Iteration 28700: Loss = -11138.48046875
5
Iteration 28800: Loss = -11138.4794921875
6
Iteration 28900: Loss = -11138.4794921875
7
Iteration 29000: Loss = -11138.478515625
Iteration 29100: Loss = -11138.4794921875
1
Iteration 29200: Loss = -11138.4794921875
2
Iteration 29300: Loss = -11138.4794921875
3
Iteration 29400: Loss = -11138.4794921875
4
Iteration 29500: Loss = -11138.478515625
Iteration 29600: Loss = -11138.478515625
Iteration 29700: Loss = -11138.478515625
Iteration 29800: Loss = -11138.4814453125
1
Iteration 29900: Loss = -11138.478515625
pi: tensor([[0.7379, 0.2621],
        [0.3508, 0.6492]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4201, 0.5799], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2061, 0.0989],
         [0.0101, 0.2613]],

        [[0.0239, 0.0982],
         [0.0068, 0.0996]],

        [[0.0090, 0.0949],
         [0.9886, 0.5288]],

        [[0.0249, 0.1085],
         [0.5125, 0.5337]],

        [[0.6930, 0.1097],
         [0.5818, 0.0071]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369583604949977
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.0009058703634804834
Global Adjusted Rand Index: 0.5232318125160491
Average Adjusted Rand Index: 0.6923390991222967
[0.2565010898861454, 0.5232318125160491] [0.7681015855270908, 0.6923390991222967] [11139.5380859375, 11138.48046875]
-------------------------------------
This iteration is 34
True Objective function: Loss = -10779.619951748846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21693.3515625
Iteration 100: Loss = -14862.716796875
Iteration 200: Loss = -11692.4150390625
Iteration 300: Loss = -11083.533203125
Iteration 400: Loss = -10983.556640625
Iteration 500: Loss = -10934.912109375
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [28:05:19<52:38:55, 2915.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [28:47:55<49:55:02, 2807.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [29:36:04<49:33:48, 2832.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [30:18:46<47:22:47, 2751.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [31:00:36<45:23:23, 2678.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 40%|████      | 40/100 [31:55:19<47:40:07, 2860.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [32:42:38<46:46:09, 2853.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [33:30:09<45:57:55, 2853.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [34:17:22<45:04:36, 2846.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [35:00:59<43:12:43, 2777.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [35:48:41<42:49:25, 2803.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [36:37:26<42:35:41, 2839.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 47%|████▋     | 47/100 [37:29:57<43:10:58, 2933.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [38:19:15<42:28:28, 2940.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [39:04:42<40:45:08, 2876.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [39:48:10<38:49:56, 2795.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [40:36:16<38:25:30, 2823.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [41:26:42<38:27:04, 2883.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [42:13:56<37:27:24, 2869.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [42:49:10<33:45:50, 2642.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [43:44:02<35:27:53, 2837.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [44:28:35<34:04:36, 2788.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [45:15:24<33:22:30, 2794.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [46:07:40<33:47:40, 2896.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [46:51:04<31:59:24, 2808.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [47:36:52<31:00:29, 2790.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [48:29:54<31:30:13, 2908.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [49:22:02<31:23:38, 2974.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [50:14:14<31:03:17, 3021.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [51:01:28<29:39:01, 2965.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [51:49:01<28:30:06, 2931.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [52:42:32<28:28:44, 3015.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [53:29:51<27:09:16, 2962.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [54:24:32<27:10:56, 3058.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 600: Loss = -10912.712890625
Iteration 700: Loss = -10901.1357421875
Iteration 800: Loss = -10893.927734375
Iteration 900: Loss = -10888.9453125
Iteration 1000: Loss = -10885.32421875
Iteration 1100: Loss = -10882.609375
Iteration 1200: Loss = -10880.509765625
Iteration 1300: Loss = -10878.84375
Iteration 1400: Loss = -10877.5048828125
Iteration 1500: Loss = -10876.4052734375
Iteration 1600: Loss = -10875.4921875
Iteration 1700: Loss = -10874.724609375
Iteration 1800: Loss = -10874.0712890625
Iteration 1900: Loss = -10873.5107421875
Iteration 2000: Loss = -10873.02734375
Iteration 2100: Loss = -10872.607421875
Iteration 2200: Loss = -10872.2392578125
Iteration 2300: Loss = -10871.9150390625
Iteration 2400: Loss = -10871.625
Iteration 2500: Loss = -10871.369140625
Iteration 2600: Loss = -10871.140625
Iteration 2700: Loss = -10870.93359375
Iteration 2800: Loss = -10870.7470703125
Iteration 2900: Loss = -10870.5810546875
Iteration 3000: Loss = -10870.4296875
Iteration 3100: Loss = -10870.2900390625
Iteration 3200: Loss = -10870.1630859375
Iteration 3300: Loss = -10870.048828125
Iteration 3400: Loss = -10869.9453125
Iteration 3500: Loss = -10869.8486328125
Iteration 3600: Loss = -10869.759765625
Iteration 3700: Loss = -10869.677734375
Iteration 3800: Loss = -10869.603515625
Iteration 3900: Loss = -10869.53515625
Iteration 4000: Loss = -10869.470703125
Iteration 4100: Loss = -10869.412109375
Iteration 4200: Loss = -10869.357421875
Iteration 4300: Loss = -10869.3046875
Iteration 4400: Loss = -10869.2578125
Iteration 4500: Loss = -10869.2119140625
Iteration 4600: Loss = -10869.1708984375
Iteration 4700: Loss = -10869.1318359375
Iteration 4800: Loss = -10869.09765625
Iteration 4900: Loss = -10869.0625
Iteration 5000: Loss = -10869.0302734375
Iteration 5100: Loss = -10869.0009765625
Iteration 5200: Loss = -10868.9716796875
Iteration 5300: Loss = -10868.9462890625
Iteration 5400: Loss = -10868.9208984375
Iteration 5500: Loss = -10868.8984375
Iteration 5600: Loss = -10868.8759765625
Iteration 5700: Loss = -10868.85546875
Iteration 5800: Loss = -10868.8359375
Iteration 5900: Loss = -10868.818359375
Iteration 6000: Loss = -10868.80078125
Iteration 6100: Loss = -10868.7861328125
Iteration 6200: Loss = -10868.76953125
Iteration 6300: Loss = -10868.75390625
Iteration 6400: Loss = -10868.7412109375
Iteration 6500: Loss = -10868.7275390625
Iteration 6600: Loss = -10868.71484375
Iteration 6700: Loss = -10868.7021484375
Iteration 6800: Loss = -10868.6904296875
Iteration 6900: Loss = -10868.6787109375
Iteration 7000: Loss = -10868.6689453125
Iteration 7100: Loss = -10868.66015625
Iteration 7200: Loss = -10868.6494140625
Iteration 7300: Loss = -10868.6416015625
Iteration 7400: Loss = -10868.630859375
Iteration 7500: Loss = -10868.6220703125
Iteration 7600: Loss = -10868.6123046875
Iteration 7700: Loss = -10868.60546875
Iteration 7800: Loss = -10868.595703125
Iteration 7900: Loss = -10868.5859375
Iteration 8000: Loss = -10868.57421875
Iteration 8100: Loss = -10868.5625
Iteration 8200: Loss = -10868.544921875
Iteration 8300: Loss = -10868.525390625
Iteration 8400: Loss = -10868.4921875
Iteration 8500: Loss = -10868.4453125
Iteration 8600: Loss = -10868.3798828125
Iteration 8700: Loss = -10868.3173828125
Iteration 8800: Loss = -10868.275390625
Iteration 8900: Loss = -10868.2412109375
Iteration 9000: Loss = -10868.2119140625
Iteration 9100: Loss = -10868.1796875
Iteration 9200: Loss = -10868.1494140625
Iteration 9300: Loss = -10868.11328125
Iteration 9400: Loss = -10868.078125
Iteration 9500: Loss = -10868.0322265625
Iteration 9600: Loss = -10867.9677734375
Iteration 9700: Loss = -10867.8291015625
Iteration 9800: Loss = -10867.501953125
Iteration 9900: Loss = -10867.3076171875
Iteration 10000: Loss = -10866.3076171875
Iteration 10100: Loss = -10865.9873046875
Iteration 10200: Loss = -10865.94140625
Iteration 10300: Loss = -10865.9140625
Iteration 10400: Loss = -10865.89453125
Iteration 10500: Loss = -10865.880859375
Iteration 10600: Loss = -10865.87109375
Iteration 10700: Loss = -10865.86328125
Iteration 10800: Loss = -10865.8564453125
Iteration 10900: Loss = -10865.8505859375
Iteration 11000: Loss = -10865.8466796875
Iteration 11100: Loss = -10865.841796875
Iteration 11200: Loss = -10865.83984375
Iteration 11300: Loss = -10865.8388671875
Iteration 11400: Loss = -10865.8369140625
Iteration 11500: Loss = -10865.833984375
Iteration 11600: Loss = -10865.8330078125
Iteration 11700: Loss = -10865.8310546875
Iteration 11800: Loss = -10865.8310546875
Iteration 11900: Loss = -10865.8271484375
Iteration 12000: Loss = -10865.828125
1
Iteration 12100: Loss = -10865.826171875
Iteration 12200: Loss = -10865.8251953125
Iteration 12300: Loss = -10865.8251953125
Iteration 12400: Loss = -10865.8251953125
Iteration 12500: Loss = -10865.82421875
Iteration 12600: Loss = -10865.822265625
Iteration 12700: Loss = -10865.8232421875
1
Iteration 12800: Loss = -10865.822265625
Iteration 12900: Loss = -10865.8212890625
Iteration 13000: Loss = -10865.8212890625
Iteration 13100: Loss = -10865.8203125
Iteration 13200: Loss = -10865.8212890625
1
Iteration 13300: Loss = -10865.818359375
Iteration 13400: Loss = -10865.8212890625
1
Iteration 13500: Loss = -10865.818359375
Iteration 13600: Loss = -10865.8173828125
Iteration 13700: Loss = -10865.8173828125
Iteration 13800: Loss = -10865.8173828125
Iteration 13900: Loss = -10865.818359375
1
Iteration 14000: Loss = -10865.8173828125
Iteration 14100: Loss = -10865.81640625
Iteration 14200: Loss = -10865.8173828125
1
Iteration 14300: Loss = -10865.8173828125
2
Iteration 14400: Loss = -10865.81640625
Iteration 14500: Loss = -10865.8173828125
1
Iteration 14600: Loss = -10865.8154296875
Iteration 14700: Loss = -10865.8173828125
1
Iteration 14800: Loss = -10865.8173828125
2
Iteration 14900: Loss = -10865.8154296875
Iteration 15000: Loss = -10865.8173828125
1
Iteration 15100: Loss = -10865.8173828125
2
Iteration 15200: Loss = -10865.814453125
Iteration 15300: Loss = -10865.81640625
1
Iteration 15400: Loss = -10865.8134765625
Iteration 15500: Loss = -10865.8154296875
1
Iteration 15600: Loss = -10865.814453125
2
Iteration 15700: Loss = -10865.8154296875
3
Iteration 15800: Loss = -10865.814453125
4
Iteration 15900: Loss = -10865.814453125
5
Iteration 16000: Loss = -10865.8134765625
Iteration 16100: Loss = -10865.814453125
1
Iteration 16200: Loss = -10865.814453125
2
Iteration 16300: Loss = -10865.8134765625
Iteration 16400: Loss = -10865.8154296875
1
Iteration 16500: Loss = -10865.8134765625
Iteration 16600: Loss = -10865.814453125
1
Iteration 16700: Loss = -10865.814453125
2
Iteration 16800: Loss = -10865.8134765625
Iteration 16900: Loss = -10865.8134765625
Iteration 17000: Loss = -10865.8134765625
Iteration 17100: Loss = -10865.814453125
1
Iteration 17200: Loss = -10865.8134765625
Iteration 17300: Loss = -10865.8134765625
Iteration 17400: Loss = -10865.814453125
1
Iteration 17500: Loss = -10865.8134765625
Iteration 17600: Loss = -10865.8125
Iteration 17700: Loss = -10865.8134765625
1
Iteration 17800: Loss = -10865.8134765625
2
Iteration 17900: Loss = -10865.8134765625
3
Iteration 18000: Loss = -10865.814453125
4
Iteration 18100: Loss = -10865.814453125
5
Iteration 18200: Loss = -10865.814453125
6
Iteration 18300: Loss = -10865.8125
Iteration 18400: Loss = -10865.814453125
1
Iteration 18500: Loss = -10865.8134765625
2
Iteration 18600: Loss = -10865.8134765625
3
Iteration 18700: Loss = -10865.8134765625
4
Iteration 18800: Loss = -10865.8134765625
5
Iteration 18900: Loss = -10865.814453125
6
Iteration 19000: Loss = -10865.8115234375
Iteration 19100: Loss = -10865.8125
1
Iteration 19200: Loss = -10865.814453125
2
Iteration 19300: Loss = -10865.8134765625
3
Iteration 19400: Loss = -10865.8134765625
4
Iteration 19500: Loss = -10865.8134765625
5
Iteration 19600: Loss = -10865.8134765625
6
Iteration 19700: Loss = -10865.814453125
7
Iteration 19800: Loss = -10865.8134765625
8
Iteration 19900: Loss = -10865.814453125
9
Iteration 20000: Loss = -10865.8125
10
Iteration 20100: Loss = -10865.8125
11
Iteration 20200: Loss = -10865.814453125
12
Iteration 20300: Loss = -10865.814453125
13
Iteration 20400: Loss = -10865.8134765625
14
Iteration 20500: Loss = -10865.8134765625
15
Stopping early at iteration 20500 due to no improvement.
pi: tensor([[9.3433e-01, 6.5673e-02],
        [9.9998e-01, 1.9378e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.1339e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1650, 0.1734],
         [0.6895, 0.1049]],

        [[0.7384, 0.1060],
         [0.2025, 0.9096]],

        [[0.5650, 0.1216],
         [0.9345, 0.0162]],

        [[0.0183, 0.1214],
         [0.7947, 0.9779]],

        [[0.0196, 0.0869],
         [0.9736, 0.0125]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.012864505300896736
Global Adjusted Rand Index: 0.00032091907810639935
Average Adjusted Rand Index: 0.0032686540678616197
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24098.9765625
Iteration 100: Loss = -15130.8671875
Iteration 200: Loss = -11807.6630859375
Iteration 300: Loss = -11219.5361328125
Iteration 400: Loss = -11097.9677734375
Iteration 500: Loss = -11036.6318359375
Iteration 600: Loss = -10989.2421875
Iteration 700: Loss = -10970.85546875
Iteration 800: Loss = -10947.927734375
Iteration 900: Loss = -10936.6064453125
Iteration 1000: Loss = -10925.177734375
Iteration 1100: Loss = -10914.1123046875
Iteration 1200: Loss = -10900.70703125
Iteration 1300: Loss = -10896.7685546875
Iteration 1400: Loss = -10894.1826171875
Iteration 1500: Loss = -10892.2939453125
Iteration 1600: Loss = -10890.76953125
Iteration 1700: Loss = -10887.5205078125
Iteration 1800: Loss = -10882.796875
Iteration 1900: Loss = -10881.0302734375
Iteration 2000: Loss = -10880.0341796875
Iteration 2100: Loss = -10879.263671875
Iteration 2200: Loss = -10878.6328125
Iteration 2300: Loss = -10878.1005859375
Iteration 2400: Loss = -10877.6416015625
Iteration 2500: Loss = -10877.2392578125
Iteration 2600: Loss = -10876.8837890625
Iteration 2700: Loss = -10876.5732421875
Iteration 2800: Loss = -10876.296875
Iteration 2900: Loss = -10876.0517578125
Iteration 3000: Loss = -10875.8291015625
Iteration 3100: Loss = -10875.626953125
Iteration 3200: Loss = -10875.4365234375
Iteration 3300: Loss = -10871.74609375
Iteration 3400: Loss = -10871.3740234375
Iteration 3500: Loss = -10871.1474609375
Iteration 3600: Loss = -10870.9580078125
Iteration 3700: Loss = -10870.791015625
Iteration 3800: Loss = -10870.642578125
Iteration 3900: Loss = -10870.5078125
Iteration 4000: Loss = -10870.3828125
Iteration 4100: Loss = -10870.26953125
Iteration 4200: Loss = -10870.1650390625
Iteration 4300: Loss = -10870.0693359375
Iteration 4400: Loss = -10869.9775390625
Iteration 4500: Loss = -10869.89453125
Iteration 4600: Loss = -10869.8173828125
Iteration 4700: Loss = -10869.7431640625
Iteration 4800: Loss = -10869.6748046875
Iteration 4900: Loss = -10869.6103515625
Iteration 5000: Loss = -10869.552734375
Iteration 5100: Loss = -10869.4970703125
Iteration 5200: Loss = -10869.4453125
Iteration 5300: Loss = -10869.396484375
Iteration 5400: Loss = -10869.349609375
Iteration 5500: Loss = -10869.3076171875
Iteration 5600: Loss = -10869.265625
Iteration 5700: Loss = -10869.2275390625
Iteration 5800: Loss = -10869.1904296875
Iteration 5900: Loss = -10869.158203125
Iteration 6000: Loss = -10869.1240234375
Iteration 6100: Loss = -10869.095703125
Iteration 6200: Loss = -10869.06640625
Iteration 6300: Loss = -10869.0380859375
Iteration 6400: Loss = -10869.0126953125
Iteration 6500: Loss = -10868.9892578125
Iteration 6600: Loss = -10868.96484375
Iteration 6700: Loss = -10868.9443359375
Iteration 6800: Loss = -10868.92578125
Iteration 6900: Loss = -10868.9052734375
Iteration 7000: Loss = -10868.88671875
Iteration 7100: Loss = -10868.8681640625
Iteration 7200: Loss = -10868.8525390625
Iteration 7300: Loss = -10868.8369140625
Iteration 7400: Loss = -10868.822265625
Iteration 7500: Loss = -10868.8095703125
Iteration 7600: Loss = -10868.796875
Iteration 7700: Loss = -10868.7822265625
Iteration 7800: Loss = -10868.7705078125
Iteration 7900: Loss = -10868.7587890625
Iteration 8000: Loss = -10868.7490234375
Iteration 8100: Loss = -10868.73828125
Iteration 8200: Loss = -10868.7275390625
Iteration 8300: Loss = -10868.71875
Iteration 8400: Loss = -10868.7109375
Iteration 8500: Loss = -10868.701171875
Iteration 8600: Loss = -10868.6943359375
Iteration 8700: Loss = -10868.6865234375
Iteration 8800: Loss = -10868.677734375
Iteration 8900: Loss = -10868.671875
Iteration 9000: Loss = -10868.6650390625
Iteration 9100: Loss = -10868.66015625
Iteration 9200: Loss = -10868.654296875
Iteration 9300: Loss = -10868.6474609375
Iteration 9400: Loss = -10868.64453125
Iteration 9500: Loss = -10868.63671875
Iteration 9600: Loss = -10868.630859375
Iteration 9700: Loss = -10868.6259765625
Iteration 9800: Loss = -10868.62109375
Iteration 9900: Loss = -10868.6162109375
Iteration 10000: Loss = -10868.6103515625
Iteration 10100: Loss = -10868.607421875
Iteration 10200: Loss = -10868.6005859375
Iteration 10300: Loss = -10868.59375
Iteration 10400: Loss = -10868.5888671875
Iteration 10500: Loss = -10868.583984375
Iteration 10600: Loss = -10868.5771484375
Iteration 10700: Loss = -10868.5693359375
Iteration 10800: Loss = -10868.55859375
Iteration 10900: Loss = -10868.529296875
Iteration 11000: Loss = -10868.1103515625
Iteration 11100: Loss = -10868.0224609375
Iteration 11200: Loss = -10867.9697265625
Iteration 11300: Loss = -10867.939453125
Iteration 11400: Loss = -10867.9189453125
Iteration 11500: Loss = -10867.8984375
Iteration 11600: Loss = -10867.8720703125
Iteration 11700: Loss = -10867.8369140625
Iteration 11800: Loss = -10867.7529296875
Iteration 11900: Loss = -10867.6259765625
Iteration 12000: Loss = -10867.515625
Iteration 12100: Loss = -10867.4169921875
Iteration 12200: Loss = -10867.361328125
Iteration 12300: Loss = -10867.0732421875
Iteration 12400: Loss = -10867.03125
Iteration 12500: Loss = -10866.9560546875
Iteration 12600: Loss = -10866.73046875
Iteration 12700: Loss = -10866.455078125
Iteration 12800: Loss = -10866.3505859375
Iteration 12900: Loss = -10866.306640625
Iteration 13000: Loss = -10866.28125
Iteration 13100: Loss = -10866.267578125
Iteration 13200: Loss = -10866.2548828125
Iteration 13300: Loss = -10866.2470703125
Iteration 13400: Loss = -10866.2412109375
Iteration 13500: Loss = -10866.236328125
Iteration 13600: Loss = -10866.232421875
Iteration 13700: Loss = -10866.2109375
Iteration 13800: Loss = -10866.19921875
Iteration 13900: Loss = -10866.1962890625
Iteration 14000: Loss = -10866.1962890625
Iteration 14100: Loss = -10866.1923828125
Iteration 14200: Loss = -10866.19140625
Iteration 14300: Loss = -10866.1884765625
Iteration 14400: Loss = -10866.1875
Iteration 14500: Loss = -10866.1865234375
Iteration 14600: Loss = -10866.185546875
Iteration 14700: Loss = -10866.185546875
Iteration 14800: Loss = -10866.1826171875
Iteration 14900: Loss = -10866.181640625
Iteration 15000: Loss = -10866.1806640625
Iteration 15100: Loss = -10866.173828125
Iteration 15200: Loss = -10866.173828125
Iteration 15300: Loss = -10866.1708984375
Iteration 15400: Loss = -10866.169921875
Iteration 15500: Loss = -10866.169921875
Iteration 15600: Loss = -10866.1689453125
Iteration 15700: Loss = -10866.1689453125
Iteration 15800: Loss = -10866.1689453125
Iteration 15900: Loss = -10866.16796875
Iteration 16000: Loss = -10866.169921875
1
Iteration 16100: Loss = -10866.16796875
Iteration 16200: Loss = -10866.16796875
Iteration 16300: Loss = -10866.166015625
Iteration 16400: Loss = -10866.16796875
1
Iteration 16500: Loss = -10866.16796875
2
Iteration 16600: Loss = -10866.166015625
Iteration 16700: Loss = -10866.1669921875
1
Iteration 16800: Loss = -10866.1650390625
Iteration 16900: Loss = -10866.1650390625
Iteration 17000: Loss = -10866.1650390625
Iteration 17100: Loss = -10866.1640625
Iteration 17200: Loss = -10866.1640625
Iteration 17300: Loss = -10866.1650390625
1
Iteration 17400: Loss = -10866.1650390625
2
Iteration 17500: Loss = -10866.1640625
Iteration 17600: Loss = -10866.1640625
Iteration 17700: Loss = -10866.1630859375
Iteration 17800: Loss = -10866.1640625
1
Iteration 17900: Loss = -10866.1630859375
Iteration 18000: Loss = -10866.1630859375
Iteration 18100: Loss = -10866.1630859375
Iteration 18200: Loss = -10866.1640625
1
Iteration 18300: Loss = -10866.1630859375
Iteration 18400: Loss = -10866.1630859375
Iteration 18500: Loss = -10866.162109375
Iteration 18600: Loss = -10866.1630859375
1
Iteration 18700: Loss = -10866.162109375
Iteration 18800: Loss = -10866.1630859375
1
Iteration 18900: Loss = -10866.1630859375
2
Iteration 19000: Loss = -10866.162109375
Iteration 19100: Loss = -10866.1630859375
1
Iteration 19200: Loss = -10866.1630859375
2
Iteration 19300: Loss = -10866.162109375
Iteration 19400: Loss = -10866.162109375
Iteration 19500: Loss = -10866.1611328125
Iteration 19600: Loss = -10866.162109375
1
Iteration 19700: Loss = -10866.162109375
2
Iteration 19800: Loss = -10866.1611328125
Iteration 19900: Loss = -10866.1630859375
1
Iteration 20000: Loss = -10866.1611328125
Iteration 20100: Loss = -10866.1630859375
1
Iteration 20200: Loss = -10866.1611328125
Iteration 20300: Loss = -10866.158203125
Iteration 20400: Loss = -10866.1572265625
Iteration 20500: Loss = -10866.1572265625
Iteration 20600: Loss = -10866.1552734375
Iteration 20700: Loss = -10866.15625
1
Iteration 20800: Loss = -10866.1416015625
Iteration 20900: Loss = -10866.1416015625
Iteration 21000: Loss = -10866.130859375
Iteration 21100: Loss = -10866.12890625
Iteration 21200: Loss = -10866.1259765625
Iteration 21300: Loss = -10866.1259765625
Iteration 21400: Loss = -10866.125
Iteration 21500: Loss = -10866.1220703125
Iteration 21600: Loss = -10866.1220703125
Iteration 21700: Loss = -10866.123046875
1
Iteration 21800: Loss = -10866.1220703125
Iteration 21900: Loss = -10866.1220703125
Iteration 22000: Loss = -10866.123046875
1
Iteration 22100: Loss = -10866.12109375
Iteration 22200: Loss = -10866.12109375
Iteration 22300: Loss = -10866.1220703125
1
Iteration 22400: Loss = -10866.1220703125
2
Iteration 22500: Loss = -10866.12109375
Iteration 22600: Loss = -10866.1201171875
Iteration 22700: Loss = -10866.12109375
1
Iteration 22800: Loss = -10866.1181640625
Iteration 22900: Loss = -10866.1171875
Iteration 23000: Loss = -10866.1171875
Iteration 23100: Loss = -10866.1162109375
Iteration 23200: Loss = -10866.1162109375
Iteration 23300: Loss = -10866.1171875
1
Iteration 23400: Loss = -10866.1162109375
Iteration 23500: Loss = -10866.1162109375
Iteration 23600: Loss = -10866.1162109375
Iteration 23700: Loss = -10866.115234375
Iteration 23800: Loss = -10866.1162109375
1
Iteration 23900: Loss = -10866.1142578125
Iteration 24000: Loss = -10866.1015625
Iteration 24100: Loss = -10866.0986328125
Iteration 24200: Loss = -10866.091796875
Iteration 24300: Loss = -10866.091796875
Iteration 24400: Loss = -10866.091796875
Iteration 24500: Loss = -10866.0927734375
1
Iteration 24600: Loss = -10866.091796875
Iteration 24700: Loss = -10866.0908203125
Iteration 24800: Loss = -10866.0927734375
1
Iteration 24900: Loss = -10866.0908203125
Iteration 25000: Loss = -10866.091796875
1
Iteration 25100: Loss = -10866.0888671875
Iteration 25200: Loss = -10866.08984375
1
Iteration 25300: Loss = -10866.0888671875
Iteration 25400: Loss = -10866.087890625
Iteration 25500: Loss = -10866.083984375
Iteration 25600: Loss = -10866.08203125
Iteration 25700: Loss = -10866.0634765625
Iteration 25800: Loss = -10866.064453125
1
Iteration 25900: Loss = -10866.064453125
2
Iteration 26000: Loss = -10866.0654296875
3
Iteration 26100: Loss = -10866.064453125
4
Iteration 26200: Loss = -10866.0615234375
Iteration 26300: Loss = -10866.0615234375
Iteration 26400: Loss = -10866.0615234375
Iteration 26500: Loss = -10866.0615234375
Iteration 26600: Loss = -10866.0615234375
Iteration 26700: Loss = -10866.0615234375
Iteration 26800: Loss = -10866.0615234375
Iteration 26900: Loss = -10866.05859375
Iteration 27000: Loss = -10866.05859375
Iteration 27100: Loss = -10866.0595703125
1
Iteration 27200: Loss = -10866.05859375
Iteration 27300: Loss = -10866.05859375
Iteration 27400: Loss = -10866.05859375
Iteration 27500: Loss = -10866.0595703125
1
Iteration 27600: Loss = -10866.0576171875
Iteration 27700: Loss = -10866.0595703125
1
Iteration 27800: Loss = -10866.060546875
2
Iteration 27900: Loss = -10866.0595703125
3
Iteration 28000: Loss = -10866.05859375
4
Iteration 28100: Loss = -10866.0595703125
5
Iteration 28200: Loss = -10866.0595703125
6
Iteration 28300: Loss = -10866.05859375
7
Iteration 28400: Loss = -10866.05859375
8
Iteration 28500: Loss = -10866.056640625
Iteration 28600: Loss = -10866.0576171875
1
Iteration 28700: Loss = -10866.0576171875
2
Iteration 28800: Loss = -10866.0556640625
Iteration 28900: Loss = -10866.056640625
1
Iteration 29000: Loss = -10866.056640625
2
Iteration 29100: Loss = -10866.056640625
3
Iteration 29200: Loss = -10866.0537109375
Iteration 29300: Loss = -10866.052734375
Iteration 29400: Loss = -10866.052734375
Iteration 29500: Loss = -10866.0537109375
1
Iteration 29600: Loss = -10866.0517578125
Iteration 29700: Loss = -10866.052734375
1
Iteration 29800: Loss = -10866.05078125
Iteration 29900: Loss = -10866.05078125
pi: tensor([[9.9999e-01, 8.4056e-06],
        [4.8281e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0382, 0.9618], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2983, 0.1580],
         [0.8851, 0.1588]],

        [[0.4905, 0.1016],
         [0.9679, 0.0188]],

        [[0.9726, 0.2185],
         [0.0114, 0.0082]],

        [[0.7125, 0.2074],
         [0.9703, 0.7407]],

        [[0.9699, 0.1564],
         [0.9768, 0.0159]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: 8.601727269170772e-06
Average Adjusted Rand Index: -0.0008526268427057344
[0.00032091907810639935, 8.601727269170772e-06] [0.0032686540678616197, -0.0008526268427057344] [10865.8134765625, 10866.05078125]
-------------------------------------
This iteration is 35
True Objective function: Loss = -10850.706619525363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34089.6875
Iteration 100: Loss = -20615.880859375
Iteration 200: Loss = -13073.09765625
Iteration 300: Loss = -11626.419921875
Iteration 400: Loss = -11319.173828125
Iteration 500: Loss = -11209.96875
Iteration 600: Loss = -11148.107421875
Iteration 700: Loss = -11116.005859375
Iteration 800: Loss = -11093.01171875
Iteration 900: Loss = -11078.3681640625
Iteration 1000: Loss = -11063.41015625
Iteration 1100: Loss = -11048.390625
Iteration 1200: Loss = -11036.361328125
Iteration 1300: Loss = -11026.58203125
Iteration 1400: Loss = -11019.0537109375
Iteration 1500: Loss = -11013.2119140625
Iteration 1600: Loss = -11005.8056640625
Iteration 1700: Loss = -11000.3203125
Iteration 1800: Loss = -10994.623046875
Iteration 1900: Loss = -10989.5947265625
Iteration 2000: Loss = -10986.671875
Iteration 2100: Loss = -10983.4296875
Iteration 2200: Loss = -10973.2333984375
Iteration 2300: Loss = -10963.091796875
Iteration 2400: Loss = -10960.1865234375
Iteration 2500: Loss = -10957.90234375
Iteration 2600: Loss = -10955.1884765625
Iteration 2700: Loss = -10953.494140625
Iteration 2800: Loss = -10952.083984375
Iteration 2900: Loss = -10950.8017578125
Iteration 3000: Loss = -10949.5546875
Iteration 3100: Loss = -10948.326171875
Iteration 3200: Loss = -10947.1337890625
Iteration 3300: Loss = -10945.9873046875
Iteration 3400: Loss = -10944.9365234375
Iteration 3500: Loss = -10944.0009765625
Iteration 3600: Loss = -10943.1083984375
Iteration 3700: Loss = -10942.1376953125
Iteration 3800: Loss = -10941.2236328125
Iteration 3900: Loss = -10940.57421875
Iteration 4000: Loss = -10940.1064453125
Iteration 4100: Loss = -10939.74609375
Iteration 4200: Loss = -10939.453125
Iteration 4300: Loss = -10939.208984375
Iteration 4400: Loss = -10938.9970703125
Iteration 4500: Loss = -10938.8115234375
Iteration 4600: Loss = -10938.6376953125
Iteration 4700: Loss = -10938.4658203125
Iteration 4800: Loss = -10938.2734375
Iteration 4900: Loss = -10938.04296875
Iteration 5000: Loss = -10937.8134765625
Iteration 5100: Loss = -10937.3330078125
Iteration 5200: Loss = -10936.0810546875
Iteration 5300: Loss = -10935.890625
Iteration 5400: Loss = -10935.7626953125
Iteration 5500: Loss = -10935.6572265625
Iteration 5600: Loss = -10935.564453125
Iteration 5700: Loss = -10935.4814453125
Iteration 5800: Loss = -10935.4033203125
Iteration 5900: Loss = -10935.33203125
Iteration 6000: Loss = -10935.265625
Iteration 6100: Loss = -10935.2060546875
Iteration 6200: Loss = -10935.154296875
Iteration 6300: Loss = -10935.1083984375
Iteration 6400: Loss = -10935.068359375
Iteration 6500: Loss = -10935.0302734375
Iteration 6600: Loss = -10934.99609375
Iteration 6700: Loss = -10934.9638671875
Iteration 6800: Loss = -10934.93359375
Iteration 6900: Loss = -10934.9013671875
Iteration 7000: Loss = -10934.8603515625
Iteration 7100: Loss = -10934.7685546875
Iteration 7200: Loss = -10934.63671875
Iteration 7300: Loss = -10934.5849609375
Iteration 7400: Loss = -10934.548828125
Iteration 7500: Loss = -10934.5205078125
Iteration 7600: Loss = -10934.494140625
Iteration 7700: Loss = -10934.4697265625
Iteration 7800: Loss = -10934.4453125
Iteration 7900: Loss = -10934.4228515625
Iteration 8000: Loss = -10934.400390625
Iteration 8100: Loss = -10934.37890625
Iteration 8200: Loss = -10934.3564453125
Iteration 8300: Loss = -10934.3359375
Iteration 8400: Loss = -10934.3154296875
Iteration 8500: Loss = -10934.2958984375
Iteration 8600: Loss = -10934.275390625
Iteration 8700: Loss = -10934.259765625
Iteration 8800: Loss = -10934.2412109375
Iteration 8900: Loss = -10934.2236328125
Iteration 9000: Loss = -10934.2060546875
Iteration 9100: Loss = -10934.1884765625
Iteration 9200: Loss = -10934.1748046875
Iteration 9300: Loss = -10934.16015625
Iteration 9400: Loss = -10934.14453125
Iteration 9500: Loss = -10934.130859375
Iteration 9600: Loss = -10934.1181640625
Iteration 9700: Loss = -10934.1064453125
Iteration 9800: Loss = -10934.0947265625
Iteration 9900: Loss = -10934.083984375
Iteration 10000: Loss = -10934.0732421875
Iteration 10100: Loss = -10934.0625
Iteration 10200: Loss = -10934.0537109375
Iteration 10300: Loss = -10934.0458984375
Iteration 10400: Loss = -10934.0380859375
Iteration 10500: Loss = -10934.029296875
Iteration 10600: Loss = -10934.0234375
Iteration 10700: Loss = -10934.0146484375
Iteration 10800: Loss = -10934.0087890625
Iteration 10900: Loss = -10934.0048828125
Iteration 11000: Loss = -10933.9990234375
Iteration 11100: Loss = -10933.994140625
Iteration 11200: Loss = -10933.9892578125
Iteration 11300: Loss = -10933.9833984375
Iteration 11400: Loss = -10933.98046875
Iteration 11500: Loss = -10933.978515625
Iteration 11600: Loss = -10933.974609375
Iteration 11700: Loss = -10933.9716796875
Iteration 11800: Loss = -10933.96875
Iteration 11900: Loss = -10933.9658203125
Iteration 12000: Loss = -10933.9638671875
Iteration 12100: Loss = -10933.9609375
Iteration 12200: Loss = -10933.9580078125
Iteration 12300: Loss = -10933.955078125
Iteration 12400: Loss = -10933.9541015625
Iteration 12500: Loss = -10933.9521484375
Iteration 12600: Loss = -10933.9501953125
Iteration 12700: Loss = -10933.9482421875
Iteration 12800: Loss = -10933.9462890625
Iteration 12900: Loss = -10933.9462890625
Iteration 13000: Loss = -10933.9453125
Iteration 13100: Loss = -10933.9443359375
Iteration 13200: Loss = -10933.943359375
Iteration 13300: Loss = -10933.94140625
Iteration 13400: Loss = -10933.9404296875
Iteration 13500: Loss = -10933.9404296875
Iteration 13600: Loss = -10933.939453125
Iteration 13700: Loss = -10933.9384765625
Iteration 13800: Loss = -10933.9384765625
Iteration 13900: Loss = -10933.9375
Iteration 14000: Loss = -10933.935546875
Iteration 14100: Loss = -10933.9365234375
1
Iteration 14200: Loss = -10933.935546875
Iteration 14300: Loss = -10933.935546875
Iteration 14400: Loss = -10933.9345703125
Iteration 14500: Loss = -10933.9326171875
Iteration 14600: Loss = -10933.93359375
1
Iteration 14700: Loss = -10933.93359375
2
Iteration 14800: Loss = -10933.9345703125
3
Iteration 14900: Loss = -10933.9326171875
Iteration 15000: Loss = -10933.9326171875
Iteration 15100: Loss = -10933.9326171875
Iteration 15200: Loss = -10933.9326171875
Iteration 15300: Loss = -10933.931640625
Iteration 15400: Loss = -10933.9306640625
Iteration 15500: Loss = -10933.931640625
1
Iteration 15600: Loss = -10933.931640625
2
Iteration 15700: Loss = -10933.931640625
3
Iteration 15800: Loss = -10933.9287109375
Iteration 15900: Loss = -10933.9306640625
1
Iteration 16000: Loss = -10933.9287109375
Iteration 16100: Loss = -10933.9306640625
1
Iteration 16200: Loss = -10933.9306640625
2
Iteration 16300: Loss = -10933.9287109375
Iteration 16400: Loss = -10933.9306640625
1
Iteration 16500: Loss = -10933.927734375
Iteration 16600: Loss = -10933.927734375
Iteration 16700: Loss = -10933.927734375
Iteration 16800: Loss = -10933.927734375
Iteration 16900: Loss = -10933.9287109375
1
Iteration 17000: Loss = -10933.927734375
Iteration 17100: Loss = -10933.927734375
Iteration 17200: Loss = -10933.927734375
Iteration 17300: Loss = -10933.927734375
Iteration 17400: Loss = -10933.9287109375
1
Iteration 17500: Loss = -10933.9267578125
Iteration 17600: Loss = -10933.9267578125
Iteration 17700: Loss = -10933.9287109375
1
Iteration 17800: Loss = -10933.9267578125
Iteration 17900: Loss = -10933.9267578125
Iteration 18000: Loss = -10933.9287109375
1
Iteration 18100: Loss = -10933.9267578125
Iteration 18200: Loss = -10933.9267578125
Iteration 18300: Loss = -10933.9287109375
1
Iteration 18400: Loss = -10933.927734375
2
Iteration 18500: Loss = -10933.9267578125
Iteration 18600: Loss = -10933.9267578125
Iteration 18700: Loss = -10933.9267578125
Iteration 18800: Loss = -10933.927734375
1
Iteration 18900: Loss = -10933.9267578125
Iteration 19000: Loss = -10933.927734375
1
Iteration 19100: Loss = -10933.9267578125
Iteration 19200: Loss = -10933.9267578125
Iteration 19300: Loss = -10933.9267578125
Iteration 19400: Loss = -10933.9267578125
Iteration 19500: Loss = -10933.92578125
Iteration 19600: Loss = -10933.9267578125
1
Iteration 19700: Loss = -10933.9267578125
2
Iteration 19800: Loss = -10933.9267578125
3
Iteration 19900: Loss = -10933.9267578125
4
Iteration 20000: Loss = -10933.9267578125
5
Iteration 20100: Loss = -10933.9267578125
6
Iteration 20200: Loss = -10933.9267578125
7
Iteration 20300: Loss = -10933.9267578125
8
Iteration 20400: Loss = -10933.9267578125
9
Iteration 20500: Loss = -10933.9267578125
10
Iteration 20600: Loss = -10933.92578125
Iteration 20700: Loss = -10933.9267578125
1
Iteration 20800: Loss = -10933.9267578125
2
Iteration 20900: Loss = -10933.9267578125
3
Iteration 21000: Loss = -10933.9267578125
4
Iteration 21100: Loss = -10933.9248046875
Iteration 21200: Loss = -10933.9267578125
1
Iteration 21300: Loss = -10933.927734375
2
Iteration 21400: Loss = -10933.9267578125
3
Iteration 21500: Loss = -10933.9267578125
4
Iteration 21600: Loss = -10933.9267578125
5
Iteration 21700: Loss = -10933.92578125
6
Iteration 21800: Loss = -10933.92578125
7
Iteration 21900: Loss = -10933.9248046875
Iteration 22000: Loss = -10933.9267578125
1
Iteration 22100: Loss = -10933.9267578125
2
Iteration 22200: Loss = -10933.9248046875
Iteration 22300: Loss = -10933.9267578125
1
Iteration 22400: Loss = -10933.9248046875
Iteration 22500: Loss = -10933.92578125
1
Iteration 22600: Loss = -10933.92578125
2
Iteration 22700: Loss = -10933.92578125
3
Iteration 22800: Loss = -10933.92578125
4
Iteration 22900: Loss = -10933.927734375
5
Iteration 23000: Loss = -10933.92578125
6
Iteration 23100: Loss = -10933.92578125
7
Iteration 23200: Loss = -10933.92578125
8
Iteration 23300: Loss = -10933.9248046875
Iteration 23400: Loss = -10933.9248046875
Iteration 23500: Loss = -10933.92578125
1
Iteration 23600: Loss = -10933.92578125
2
Iteration 23700: Loss = -10933.92578125
3
Iteration 23800: Loss = -10933.92578125
4
Iteration 23900: Loss = -10933.92578125
5
Iteration 24000: Loss = -10933.92578125
6
Iteration 24100: Loss = -10933.92578125
7
Iteration 24200: Loss = -10933.92578125
8
Iteration 24300: Loss = -10933.927734375
9
Iteration 24400: Loss = -10933.92578125
10
Iteration 24500: Loss = -10933.92578125
11
Iteration 24600: Loss = -10933.92578125
12
Iteration 24700: Loss = -10933.92578125
13
Iteration 24800: Loss = -10933.92578125
14
Iteration 24900: Loss = -10933.92578125
15
Stopping early at iteration 24900 due to no improvement.
pi: tensor([[4.0673e-08, 1.0000e+00],
        [2.2472e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9980, 0.0020], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1554, 0.1554],
         [0.9899, 0.1627]],

        [[0.0110, 0.6746],
         [0.6089, 0.2816]],

        [[0.9552, 0.1733],
         [0.2582, 0.3666]],

        [[0.6642, 0.1476],
         [0.0286, 0.4269]],

        [[0.9655, 0.3766],
         [0.0605, 0.6408]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004522532215895281
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25423.234375
Iteration 100: Loss = -15925.8603515625
Iteration 200: Loss = -12025.4130859375
Iteration 300: Loss = -11362.478515625
Iteration 400: Loss = -11192.7568359375
Iteration 500: Loss = -11125.541015625
Iteration 600: Loss = -11091.5087890625
Iteration 700: Loss = -11065.6640625
Iteration 800: Loss = -11049.08203125
Iteration 900: Loss = -11034.408203125
Iteration 1000: Loss = -11025.232421875
Iteration 1100: Loss = -11019.3017578125
Iteration 1200: Loss = -11012.1103515625
Iteration 1300: Loss = -11007.474609375
Iteration 1400: Loss = -11003.0
Iteration 1500: Loss = -10999.1787109375
Iteration 1600: Loss = -10991.9658203125
Iteration 1700: Loss = -10985.1015625
Iteration 1800: Loss = -10982.4970703125
Iteration 1900: Loss = -10980.203125
Iteration 2000: Loss = -10977.60546875
Iteration 2100: Loss = -10975.7509765625
Iteration 2200: Loss = -10972.8515625
Iteration 2300: Loss = -10968.251953125
Iteration 2400: Loss = -10966.7109375
Iteration 2500: Loss = -10965.4853515625
Iteration 2600: Loss = -10963.830078125
Iteration 2700: Loss = -10959.255859375
Iteration 2800: Loss = -10956.53515625
Iteration 2900: Loss = -10955.59765625
Iteration 3000: Loss = -10953.388671875
Iteration 3100: Loss = -10948.9501953125
Iteration 3200: Loss = -10947.9814453125
Iteration 3300: Loss = -10947.486328125
Iteration 3400: Loss = -10947.1171875
Iteration 3500: Loss = -10943.76171875
Iteration 3600: Loss = -10942.740234375
Iteration 3700: Loss = -10942.4208984375
Iteration 3800: Loss = -10942.1845703125
Iteration 3900: Loss = -10941.9931640625
Iteration 4000: Loss = -10941.8330078125
Iteration 4100: Loss = -10941.693359375
Iteration 4200: Loss = -10941.5693359375
Iteration 4300: Loss = -10941.45703125
Iteration 4400: Loss = -10941.345703125
Iteration 4500: Loss = -10941.216796875
Iteration 4600: Loss = -10941.1357421875
Iteration 4700: Loss = -10941.0615234375
Iteration 4800: Loss = -10940.9951171875
Iteration 4900: Loss = -10940.9345703125
Iteration 5000: Loss = -10940.87890625
Iteration 5100: Loss = -10940.8291015625
Iteration 5200: Loss = -10940.7802734375
Iteration 5300: Loss = -10940.736328125
Iteration 5400: Loss = -10940.697265625
Iteration 5500: Loss = -10940.6611328125
Iteration 5600: Loss = -10940.625
Iteration 5700: Loss = -10940.5927734375
Iteration 5800: Loss = -10940.5654296875
Iteration 5900: Loss = -10940.5380859375
Iteration 6000: Loss = -10940.5126953125
Iteration 6100: Loss = -10940.48828125
Iteration 6200: Loss = -10940.4638671875
Iteration 6300: Loss = -10940.4423828125
Iteration 6400: Loss = -10940.421875
Iteration 6500: Loss = -10940.400390625
Iteration 6600: Loss = -10940.380859375
Iteration 6700: Loss = -10940.3642578125
Iteration 6800: Loss = -10940.3486328125
Iteration 6900: Loss = -10940.333984375
Iteration 7000: Loss = -10940.3203125
Iteration 7100: Loss = -10940.3076171875
Iteration 7200: Loss = -10940.29296875
Iteration 7300: Loss = -10940.2822265625
Iteration 7400: Loss = -10940.2705078125
Iteration 7500: Loss = -10940.2607421875
Iteration 7600: Loss = -10940.2490234375
Iteration 7700: Loss = -10940.240234375
Iteration 7800: Loss = -10940.23046875
Iteration 7900: Loss = -10940.2216796875
Iteration 8000: Loss = -10940.21484375
Iteration 8100: Loss = -10940.2060546875
Iteration 8200: Loss = -10940.19921875
Iteration 8300: Loss = -10940.1904296875
Iteration 8400: Loss = -10940.1845703125
Iteration 8500: Loss = -10940.177734375
Iteration 8600: Loss = -10940.1728515625
Iteration 8700: Loss = -10940.1640625
Iteration 8800: Loss = -10939.0869140625
Iteration 8900: Loss = -10935.0966796875
Iteration 9000: Loss = -10934.9794921875
Iteration 9100: Loss = -10934.927734375
Iteration 9200: Loss = -10934.8955078125
Iteration 9300: Loss = -10934.873046875
Iteration 9400: Loss = -10934.8583984375
Iteration 9500: Loss = -10934.8466796875
Iteration 9600: Loss = -10934.833984375
Iteration 9700: Loss = -10934.8271484375
Iteration 9800: Loss = -10934.8203125
Iteration 9900: Loss = -10934.8125
Iteration 10000: Loss = -10934.806640625
Iteration 10100: Loss = -10934.802734375
Iteration 10200: Loss = -10934.7958984375
Iteration 10300: Loss = -10934.79296875
Iteration 10400: Loss = -10934.7890625
Iteration 10500: Loss = -10934.787109375
Iteration 10600: Loss = -10934.7841796875
Iteration 10700: Loss = -10934.7802734375
Iteration 10800: Loss = -10934.7783203125
Iteration 10900: Loss = -10934.775390625
Iteration 11000: Loss = -10934.7744140625
Iteration 11100: Loss = -10934.7705078125
Iteration 11200: Loss = -10934.7705078125
Iteration 11300: Loss = -10934.767578125
Iteration 11400: Loss = -10934.7666015625
Iteration 11500: Loss = -10934.763671875
Iteration 11600: Loss = -10934.7646484375
1
Iteration 11700: Loss = -10934.7626953125
Iteration 11800: Loss = -10934.7607421875
Iteration 11900: Loss = -10934.7587890625
Iteration 12000: Loss = -10934.759765625
1
Iteration 12100: Loss = -10934.7578125
Iteration 12200: Loss = -10934.755859375
Iteration 12300: Loss = -10934.755859375
Iteration 12400: Loss = -10934.7548828125
Iteration 12500: Loss = -10934.75390625
Iteration 12600: Loss = -10934.7529296875
Iteration 12700: Loss = -10934.751953125
Iteration 12800: Loss = -10934.7509765625
Iteration 12900: Loss = -10934.7509765625
Iteration 13000: Loss = -10934.75
Iteration 13100: Loss = -10934.75
Iteration 13200: Loss = -10934.7490234375
Iteration 13300: Loss = -10934.748046875
Iteration 13400: Loss = -10934.7470703125
Iteration 13500: Loss = -10934.7490234375
1
Iteration 13600: Loss = -10934.7470703125
Iteration 13700: Loss = -10934.74609375
Iteration 13800: Loss = -10934.7451171875
Iteration 13900: Loss = -10934.74609375
1
Iteration 14000: Loss = -10934.7451171875
Iteration 14100: Loss = -10934.744140625
Iteration 14200: Loss = -10934.7451171875
1
Iteration 14300: Loss = -10934.744140625
Iteration 14400: Loss = -10934.744140625
Iteration 14500: Loss = -10934.7431640625
Iteration 14600: Loss = -10934.7421875
Iteration 14700: Loss = -10934.7431640625
1
Iteration 14800: Loss = -10934.740234375
Iteration 14900: Loss = -10934.7412109375
1
Iteration 15000: Loss = -10934.7412109375
2
Iteration 15100: Loss = -10934.7421875
3
Iteration 15200: Loss = -10934.7421875
4
Iteration 15300: Loss = -10934.7421875
5
Iteration 15400: Loss = -10934.7421875
6
Iteration 15500: Loss = -10934.7421875
7
Iteration 15600: Loss = -10934.7421875
8
Iteration 15700: Loss = -10934.740234375
Iteration 15800: Loss = -10934.7392578125
Iteration 15900: Loss = -10934.7392578125
Iteration 16000: Loss = -10934.740234375
1
Iteration 16100: Loss = -10934.7412109375
2
Iteration 16200: Loss = -10934.73828125
Iteration 16300: Loss = -10934.7392578125
1
Iteration 16400: Loss = -10934.740234375
2
Iteration 16500: Loss = -10934.73828125
Iteration 16600: Loss = -10934.7392578125
1
Iteration 16700: Loss = -10934.740234375
2
Iteration 16800: Loss = -10934.7392578125
3
Iteration 16900: Loss = -10934.73828125
Iteration 17000: Loss = -10934.73828125
Iteration 17100: Loss = -10934.73828125
Iteration 17200: Loss = -10934.7392578125
1
Iteration 17300: Loss = -10934.7373046875
Iteration 17400: Loss = -10934.7373046875
Iteration 17500: Loss = -10934.7373046875
Iteration 17600: Loss = -10934.7333984375
Iteration 17700: Loss = -10934.69921875
Iteration 17800: Loss = -10933.03125
Iteration 17900: Loss = -10932.37890625
Iteration 18000: Loss = -10932.3701171875
Iteration 18100: Loss = -10932.3681640625
Iteration 18200: Loss = -10932.3642578125
Iteration 18300: Loss = -10932.36328125
Iteration 18400: Loss = -10932.36328125
Iteration 18500: Loss = -10932.3623046875
Iteration 18600: Loss = -10932.3623046875
Iteration 18700: Loss = -10932.361328125
Iteration 18800: Loss = -10932.361328125
Iteration 18900: Loss = -10932.36328125
1
Iteration 19000: Loss = -10932.3603515625
Iteration 19100: Loss = -10932.361328125
1
Iteration 19200: Loss = -10932.361328125
2
Iteration 19300: Loss = -10932.359375
Iteration 19400: Loss = -10932.3603515625
1
Iteration 19500: Loss = -10932.3603515625
2
Iteration 19600: Loss = -10932.359375
Iteration 19700: Loss = -10932.3583984375
Iteration 19800: Loss = -10932.3603515625
1
Iteration 19900: Loss = -10932.3583984375
Iteration 20000: Loss = -10932.359375
1
Iteration 20100: Loss = -10932.3603515625
2
Iteration 20200: Loss = -10932.3583984375
Iteration 20300: Loss = -10932.359375
1
Iteration 20400: Loss = -10932.359375
2
Iteration 20500: Loss = -10932.357421875
Iteration 20600: Loss = -10932.3583984375
1
Iteration 20700: Loss = -10932.3583984375
2
Iteration 20800: Loss = -10932.359375
3
Iteration 20900: Loss = -10932.359375
4
Iteration 21000: Loss = -10932.3583984375
5
Iteration 21100: Loss = -10932.3583984375
6
Iteration 21200: Loss = -10932.359375
7
Iteration 21300: Loss = -10932.3603515625
8
Iteration 21400: Loss = -10932.359375
9
Iteration 21500: Loss = -10932.3603515625
10
Iteration 21600: Loss = -10932.359375
11
Iteration 21700: Loss = -10932.359375
12
Iteration 21800: Loss = -10932.359375
13
Iteration 21900: Loss = -10932.3603515625
14
Iteration 22000: Loss = -10932.3583984375
15
Stopping early at iteration 22000 due to no improvement.
pi: tensor([[1.0000e+00, 1.7283e-06],
        [9.0381e-01, 9.6194e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1085, 0.8915], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1648, 0.1558],
         [0.9714, 0.1559]],

        [[0.5881, 0.0944],
         [0.0611, 0.1976]],

        [[0.0094, 0.2657],
         [0.9510, 0.8773]],

        [[0.9813, 0.1352],
         [0.0093, 0.9924]],

        [[0.4441, 0.3569],
         [0.6743, 0.1672]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0011225368650757482
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016327245338047514
Average Adjusted Rand Index: 0.0007352769003124079
[-0.0004522532215895281, -0.0016327245338047514] [0.0, 0.0007352769003124079] [10933.92578125, 10932.3583984375]
-------------------------------------
This iteration is 36
True Objective function: Loss = -10758.981156730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36220.26953125
Iteration 100: Loss = -20409.291015625
Iteration 200: Loss = -12742.6064453125
Iteration 300: Loss = -11243.212890625
Iteration 400: Loss = -11083.9365234375
Iteration 500: Loss = -11016.6962890625
Iteration 600: Loss = -10988.7646484375
Iteration 700: Loss = -10969.3115234375
Iteration 800: Loss = -10955.2099609375
Iteration 900: Loss = -10944.7958984375
Iteration 1000: Loss = -10933.3515625
Iteration 1100: Loss = -10926.8798828125
Iteration 1200: Loss = -10923.3349609375
Iteration 1300: Loss = -10920.6767578125
Iteration 1400: Loss = -10918.5439453125
Iteration 1500: Loss = -10916.779296875
Iteration 1600: Loss = -10915.2861328125
Iteration 1700: Loss = -10914.0068359375
Iteration 1800: Loss = -10912.896484375
Iteration 1900: Loss = -10911.923828125
Iteration 2000: Loss = -10911.064453125
Iteration 2100: Loss = -10910.3076171875
Iteration 2200: Loss = -10909.634765625
Iteration 2300: Loss = -10909.03515625
Iteration 2400: Loss = -10908.494140625
Iteration 2500: Loss = -10908.0048828125
Iteration 2600: Loss = -10907.560546875
Iteration 2700: Loss = -10907.158203125
Iteration 2800: Loss = -10906.7900390625
Iteration 2900: Loss = -10906.4501953125
Iteration 3000: Loss = -10906.1396484375
Iteration 3100: Loss = -10905.85546875
Iteration 3200: Loss = -10905.58984375
Iteration 3300: Loss = -10905.3466796875
Iteration 3400: Loss = -10905.1201171875
Iteration 3500: Loss = -10904.91015625
Iteration 3600: Loss = -10904.71484375
Iteration 3700: Loss = -10904.53515625
Iteration 3800: Loss = -10904.3642578125
Iteration 3900: Loss = -10904.20703125
Iteration 4000: Loss = -10904.05859375
Iteration 4100: Loss = -10903.9208984375
Iteration 4200: Loss = -10903.791015625
Iteration 4300: Loss = -10903.6708984375
Iteration 4400: Loss = -10903.556640625
Iteration 4500: Loss = -10903.451171875
Iteration 4600: Loss = -10903.3525390625
Iteration 4700: Loss = -10903.259765625
Iteration 4800: Loss = -10903.171875
Iteration 4900: Loss = -10903.0888671875
Iteration 5000: Loss = -10903.0126953125
Iteration 5100: Loss = -10902.9384765625
Iteration 5200: Loss = -10902.8701171875
Iteration 5300: Loss = -10902.8037109375
Iteration 5400: Loss = -10902.7451171875
Iteration 5500: Loss = -10902.6865234375
Iteration 5600: Loss = -10902.6318359375
Iteration 5700: Loss = -10902.5791015625
Iteration 5800: Loss = -10902.53125
Iteration 5900: Loss = -10902.4853515625
Iteration 6000: Loss = -10902.44140625
Iteration 6100: Loss = -10902.3994140625
Iteration 6200: Loss = -10902.359375
Iteration 6300: Loss = -10902.3232421875
Iteration 6400: Loss = -10902.2890625
Iteration 6500: Loss = -10902.2548828125
Iteration 6600: Loss = -10902.2236328125
Iteration 6700: Loss = -10902.193359375
Iteration 6800: Loss = -10902.166015625
Iteration 6900: Loss = -10902.1376953125
Iteration 7000: Loss = -10902.11328125
Iteration 7100: Loss = -10902.087890625
Iteration 7200: Loss = -10902.0634765625
Iteration 7300: Loss = -10902.04296875
Iteration 7400: Loss = -10902.021484375
Iteration 7500: Loss = -10902.0009765625
Iteration 7600: Loss = -10901.9814453125
Iteration 7700: Loss = -10901.962890625
Iteration 7800: Loss = -10901.9453125
Iteration 7900: Loss = -10901.9287109375
Iteration 8000: Loss = -10901.9140625
Iteration 8100: Loss = -10901.8974609375
Iteration 8200: Loss = -10901.8818359375
Iteration 8300: Loss = -10901.865234375
Iteration 8400: Loss = -10901.8505859375
Iteration 8500: Loss = -10901.8359375
Iteration 8600: Loss = -10901.822265625
Iteration 8700: Loss = -10901.8076171875
Iteration 8800: Loss = -10901.794921875
Iteration 8900: Loss = -10901.7822265625
Iteration 9000: Loss = -10901.7685546875
Iteration 9100: Loss = -10901.7607421875
Iteration 9200: Loss = -10901.7470703125
Iteration 9300: Loss = -10901.73828125
Iteration 9400: Loss = -10901.7275390625
Iteration 9500: Loss = -10901.7177734375
Iteration 9600: Loss = -10901.708984375
Iteration 9700: Loss = -10901.7001953125
Iteration 9800: Loss = -10901.69140625
Iteration 9900: Loss = -10901.685546875
Iteration 10000: Loss = -10901.677734375
Iteration 10100: Loss = -10901.6689453125
Iteration 10200: Loss = -10901.6630859375
Iteration 10300: Loss = -10901.6572265625
Iteration 10400: Loss = -10901.650390625
Iteration 10500: Loss = -10901.6455078125
Iteration 10600: Loss = -10901.638671875
Iteration 10700: Loss = -10901.634765625
Iteration 10800: Loss = -10901.6279296875
Iteration 10900: Loss = -10901.623046875
Iteration 11000: Loss = -10901.6181640625
Iteration 11100: Loss = -10901.6123046875
Iteration 11200: Loss = -10901.6083984375
Iteration 11300: Loss = -10901.6025390625
Iteration 11400: Loss = -10901.599609375
Iteration 11500: Loss = -10901.5947265625
Iteration 11600: Loss = -10901.5908203125
Iteration 11700: Loss = -10901.5849609375
Iteration 11800: Loss = -10901.5810546875
Iteration 11900: Loss = -10901.5771484375
Iteration 12000: Loss = -10901.572265625
Iteration 12100: Loss = -10901.5693359375
Iteration 12200: Loss = -10901.5634765625
Iteration 12300: Loss = -10901.560546875
Iteration 12400: Loss = -10901.5556640625
Iteration 12500: Loss = -10901.5517578125
Iteration 12600: Loss = -10901.546875
Iteration 12700: Loss = -10901.5439453125
Iteration 12800: Loss = -10901.5380859375
Iteration 12900: Loss = -10901.533203125
Iteration 13000: Loss = -10901.5283203125
Iteration 13100: Loss = -10901.5234375
Iteration 13200: Loss = -10901.517578125
Iteration 13300: Loss = -10901.509765625
Iteration 13400: Loss = -10901.5048828125
Iteration 13500: Loss = -10901.4951171875
Iteration 13600: Loss = -10901.48828125
Iteration 13700: Loss = -10901.4775390625
Iteration 13800: Loss = -10901.466796875
Iteration 13900: Loss = -10901.451171875
Iteration 14000: Loss = -10901.435546875
Iteration 14100: Loss = -10901.4091796875
Iteration 14200: Loss = -10901.37890625
Iteration 14300: Loss = -10901.3369140625
Iteration 14400: Loss = -10901.267578125
Iteration 14500: Loss = -10901.140625
Iteration 14600: Loss = -10900.8330078125
Iteration 14700: Loss = -10899.96875
Iteration 14800: Loss = -10899.3349609375
Iteration 14900: Loss = -10899.0087890625
Iteration 15000: Loss = -10898.7255859375
Iteration 15100: Loss = -10898.640625
Iteration 15200: Loss = -10898.603515625
Iteration 15300: Loss = -10898.5751953125
Iteration 15400: Loss = -10898.55078125
Iteration 15500: Loss = -10898.53125
Iteration 15600: Loss = -10898.5244140625
Iteration 15700: Loss = -10898.5224609375
Iteration 15800: Loss = -10898.5185546875
Iteration 15900: Loss = -10898.5166015625
Iteration 16000: Loss = -10898.513671875
Iteration 16100: Loss = -10898.513671875
Iteration 16200: Loss = -10898.5107421875
Iteration 16300: Loss = -10898.5107421875
Iteration 16400: Loss = -10898.5107421875
Iteration 16500: Loss = -10898.509765625
Iteration 16600: Loss = -10898.5078125
Iteration 16700: Loss = -10898.5078125
Iteration 16800: Loss = -10898.5068359375
Iteration 16900: Loss = -10898.5078125
1
Iteration 17000: Loss = -10898.5068359375
Iteration 17100: Loss = -10898.5068359375
Iteration 17200: Loss = -10898.5068359375
Iteration 17300: Loss = -10898.505859375
Iteration 17400: Loss = -10898.505859375
Iteration 17500: Loss = -10898.5078125
1
Iteration 17600: Loss = -10898.5048828125
Iteration 17700: Loss = -10898.5048828125
Iteration 17800: Loss = -10898.505859375
1
Iteration 17900: Loss = -10898.5048828125
Iteration 18000: Loss = -10898.50390625
Iteration 18100: Loss = -10898.50390625
Iteration 18200: Loss = -10898.505859375
1
Iteration 18300: Loss = -10898.505859375
2
Iteration 18400: Loss = -10898.50390625
Iteration 18500: Loss = -10898.5048828125
1
Iteration 18600: Loss = -10898.5029296875
Iteration 18700: Loss = -10898.5048828125
1
Iteration 18800: Loss = -10898.50390625
2
Iteration 18900: Loss = -10898.5029296875
Iteration 19000: Loss = -10898.50390625
1
Iteration 19100: Loss = -10898.50390625
2
Iteration 19200: Loss = -10898.5048828125
3
Iteration 19300: Loss = -10898.5029296875
Iteration 19400: Loss = -10898.50390625
1
Iteration 19500: Loss = -10898.50390625
2
Iteration 19600: Loss = -10898.5029296875
Iteration 19700: Loss = -10898.50390625
1
Iteration 19800: Loss = -10898.505859375
2
Iteration 19900: Loss = -10898.5029296875
Iteration 20000: Loss = -10898.50390625
1
Iteration 20100: Loss = -10898.50390625
2
Iteration 20200: Loss = -10898.50390625
3
Iteration 20300: Loss = -10898.5029296875
Iteration 20400: Loss = -10898.50390625
1
Iteration 20500: Loss = -10898.5029296875
Iteration 20600: Loss = -10898.5029296875
Iteration 20700: Loss = -10898.5029296875
Iteration 20800: Loss = -10898.5029296875
Iteration 20900: Loss = -10898.50390625
1
Iteration 21000: Loss = -10898.5029296875
Iteration 21100: Loss = -10898.50390625
1
Iteration 21200: Loss = -10898.50390625
2
Iteration 21300: Loss = -10898.50390625
3
Iteration 21400: Loss = -10898.501953125
Iteration 21500: Loss = -10898.5029296875
1
Iteration 21600: Loss = -10898.50390625
2
Iteration 21700: Loss = -10898.5029296875
3
Iteration 21800: Loss = -10898.5029296875
4
Iteration 21900: Loss = -10898.5029296875
5
Iteration 22000: Loss = -10898.501953125
Iteration 22100: Loss = -10898.501953125
Iteration 22200: Loss = -10898.501953125
Iteration 22300: Loss = -10898.5029296875
1
Iteration 22400: Loss = -10898.5029296875
2
Iteration 22500: Loss = -10898.5029296875
3
Iteration 22600: Loss = -10898.5029296875
4
Iteration 22700: Loss = -10898.5029296875
5
Iteration 22800: Loss = -10898.5048828125
6
Iteration 22900: Loss = -10898.50390625
7
Iteration 23000: Loss = -10898.5029296875
8
Iteration 23100: Loss = -10898.50390625
9
Iteration 23200: Loss = -10898.5029296875
10
Iteration 23300: Loss = -10898.501953125
Iteration 23400: Loss = -10898.501953125
Iteration 23500: Loss = -10898.5029296875
1
Iteration 23600: Loss = -10898.50390625
2
Iteration 23700: Loss = -10898.501953125
Iteration 23800: Loss = -10898.501953125
Iteration 23900: Loss = -10898.5009765625
Iteration 24000: Loss = -10898.501953125
1
Iteration 24100: Loss = -10898.5029296875
2
Iteration 24200: Loss = -10898.5029296875
3
Iteration 24300: Loss = -10898.5029296875
4
Iteration 24400: Loss = -10898.50390625
5
Iteration 24500: Loss = -10898.50390625
6
Iteration 24600: Loss = -10898.5029296875
7
Iteration 24700: Loss = -10898.5029296875
8
Iteration 24800: Loss = -10898.5029296875
9
Iteration 24900: Loss = -10898.5029296875
10
Iteration 25000: Loss = -10898.5029296875
11
Iteration 25100: Loss = -10898.5029296875
12
Iteration 25200: Loss = -10898.50390625
13
Iteration 25300: Loss = -10898.501953125
14
Iteration 25400: Loss = -10898.501953125
15
Stopping early at iteration 25400 due to no improvement.
pi: tensor([[8.1758e-05, 9.9992e-01],
        [5.6502e-02, 9.4350e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0275, 0.9725], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1269, 0.2031],
         [0.8951, 0.1611]],

        [[0.0894, 0.2350],
         [0.9502, 0.8101]],

        [[0.9769, 0.1374],
         [0.9894, 0.1132]],

        [[0.9190, 0.1141],
         [0.1287, 0.0107]],

        [[0.9537, 0.1218],
         [0.0411, 0.1359]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008735738497905159
Average Adjusted Rand Index: -0.0015521208230856337
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -46072.1171875
Iteration 100: Loss = -25799.986328125
Iteration 200: Loss = -14201.6376953125
Iteration 300: Loss = -11926.4306640625
Iteration 400: Loss = -11483.7421875
Iteration 500: Loss = -11259.1005859375
Iteration 600: Loss = -11152.77734375
Iteration 700: Loss = -11076.7890625
Iteration 800: Loss = -11034.85546875
Iteration 900: Loss = -11001.2470703125
Iteration 1000: Loss = -10982.458984375
Iteration 1100: Loss = -10967.4248046875
Iteration 1200: Loss = -10953.1103515625
Iteration 1300: Loss = -10946.0380859375
Iteration 1400: Loss = -10940.77734375
Iteration 1500: Loss = -10936.759765625
Iteration 1600: Loss = -10933.482421875
Iteration 1700: Loss = -10930.7294921875
Iteration 1800: Loss = -10928.384765625
Iteration 1900: Loss = -10926.3544921875
Iteration 2000: Loss = -10924.57421875
Iteration 2100: Loss = -10923.015625
Iteration 2200: Loss = -10921.6611328125
Iteration 2300: Loss = -10920.4580078125
Iteration 2400: Loss = -10919.3916015625
Iteration 2500: Loss = -10918.40234375
Iteration 2600: Loss = -10917.3994140625
Iteration 2700: Loss = -10916.22265625
Iteration 2800: Loss = -10915.1904296875
Iteration 2900: Loss = -10914.453125
Iteration 3000: Loss = -10913.8515625
Iteration 3100: Loss = -10913.337890625
Iteration 3200: Loss = -10912.8896484375
Iteration 3300: Loss = -10912.494140625
Iteration 3400: Loss = -10912.142578125
Iteration 3500: Loss = -10911.8271484375
Iteration 3600: Loss = -10911.54296875
Iteration 3700: Loss = -10911.28515625
Iteration 3800: Loss = -10911.0498046875
Iteration 3900: Loss = -10910.8349609375
Iteration 4000: Loss = -10910.63671875
Iteration 4100: Loss = -10910.453125
Iteration 4200: Loss = -10910.28515625
Iteration 4300: Loss = -10910.12890625
Iteration 4400: Loss = -10909.982421875
Iteration 4500: Loss = -10909.845703125
Iteration 4600: Loss = -10909.716796875
Iteration 4700: Loss = -10909.59765625
Iteration 4800: Loss = -10909.486328125
Iteration 4900: Loss = -10909.380859375
Iteration 5000: Loss = -10909.279296875
Iteration 5100: Loss = -10909.185546875
Iteration 5200: Loss = -10909.095703125
Iteration 5300: Loss = -10909.0107421875
Iteration 5400: Loss = -10908.9326171875
Iteration 5500: Loss = -10908.8583984375
Iteration 5600: Loss = -10908.7861328125
Iteration 5700: Loss = -10908.7197265625
Iteration 5800: Loss = -10908.654296875
Iteration 5900: Loss = -10908.591796875
Iteration 6000: Loss = -10908.5361328125
Iteration 6100: Loss = -10908.48046875
Iteration 6200: Loss = -10908.4248046875
Iteration 6300: Loss = -10908.3720703125
Iteration 6400: Loss = -10908.322265625
Iteration 6500: Loss = -10908.2724609375
Iteration 6600: Loss = -10908.23046875
Iteration 6700: Loss = -10908.1875
Iteration 6800: Loss = -10908.1484375
Iteration 6900: Loss = -10908.1103515625
Iteration 7000: Loss = -10908.0751953125
Iteration 7100: Loss = -10908.0419921875
Iteration 7200: Loss = -10908.0078125
Iteration 7300: Loss = -10907.9775390625
Iteration 7400: Loss = -10907.9501953125
Iteration 7500: Loss = -10907.9208984375
Iteration 7600: Loss = -10907.8935546875
Iteration 7700: Loss = -10907.8671875
Iteration 7800: Loss = -10907.84375
Iteration 7900: Loss = -10907.8203125
Iteration 8000: Loss = -10907.7958984375
Iteration 8100: Loss = -10907.7734375
Iteration 8200: Loss = -10907.751953125
Iteration 8300: Loss = -10907.7314453125
Iteration 8400: Loss = -10907.3994140625
Iteration 8500: Loss = -10902.255859375
Iteration 8600: Loss = -10902.123046875
Iteration 8700: Loss = -10902.0595703125
Iteration 8800: Loss = -10902.01953125
Iteration 8900: Loss = -10901.9873046875
Iteration 9000: Loss = -10901.9609375
Iteration 9100: Loss = -10901.939453125
Iteration 9200: Loss = -10901.919921875
Iteration 9300: Loss = -10901.904296875
Iteration 9400: Loss = -10901.8896484375
Iteration 9500: Loss = -10901.8740234375
Iteration 9600: Loss = -10901.861328125
Iteration 9700: Loss = -10901.8486328125
Iteration 9800: Loss = -10901.8359375
Iteration 9900: Loss = -10901.82421875
Iteration 10000: Loss = -10901.8134765625
Iteration 10100: Loss = -10901.8037109375
Iteration 10200: Loss = -10901.794921875
Iteration 10300: Loss = -10901.7841796875
Iteration 10400: Loss = -10901.775390625
Iteration 10500: Loss = -10901.767578125
Iteration 10600: Loss = -10901.759765625
Iteration 10700: Loss = -10901.7509765625
Iteration 10800: Loss = -10901.7451171875
Iteration 10900: Loss = -10901.7373046875
Iteration 11000: Loss = -10901.7314453125
Iteration 11100: Loss = -10901.7255859375
Iteration 11200: Loss = -10901.71875
Iteration 11300: Loss = -10901.7138671875
Iteration 11400: Loss = -10901.7080078125
Iteration 11500: Loss = -10901.7041015625
Iteration 11600: Loss = -10901.697265625
Iteration 11700: Loss = -10901.693359375
Iteration 11800: Loss = -10901.6875
Iteration 11900: Loss = -10901.68359375
Iteration 12000: Loss = -10901.6806640625
Iteration 12100: Loss = -10901.67578125
Iteration 12200: Loss = -10901.671875
Iteration 12300: Loss = -10901.6669921875
Iteration 12400: Loss = -10901.662109375
Iteration 12500: Loss = -10901.6591796875
Iteration 12600: Loss = -10901.6533203125
Iteration 12700: Loss = -10901.646484375
Iteration 12800: Loss = -10901.640625
Iteration 12900: Loss = -10901.6328125
Iteration 13000: Loss = -10901.626953125
Iteration 13100: Loss = -10901.619140625
Iteration 13200: Loss = -10901.6064453125
Iteration 13300: Loss = -10901.595703125
Iteration 13400: Loss = -10901.5830078125
Iteration 13500: Loss = -10901.560546875
Iteration 13600: Loss = -10901.5322265625
Iteration 13700: Loss = -10901.4873046875
Iteration 13800: Loss = -10901.4072265625
Iteration 13900: Loss = -10901.25390625
Iteration 14000: Loss = -10901.0146484375
Iteration 14100: Loss = -10900.8173828125
Iteration 14200: Loss = -10900.6904296875
Iteration 14300: Loss = -10900.6103515625
Iteration 14400: Loss = -10900.5283203125
Iteration 14500: Loss = -10900.4287109375
Iteration 14600: Loss = -10899.8984375
Iteration 14700: Loss = -10899.1962890625
Iteration 14800: Loss = -10898.8798828125
Iteration 14900: Loss = -10898.8388671875
Iteration 15000: Loss = -10898.8212890625
Iteration 15100: Loss = -10898.8017578125
Iteration 15200: Loss = -10898.79296875
Iteration 15300: Loss = -10898.787109375
Iteration 15400: Loss = -10898.7822265625
Iteration 15500: Loss = -10898.7783203125
Iteration 15600: Loss = -10898.7744140625
Iteration 15700: Loss = -10898.7705078125
Iteration 15800: Loss = -10898.7666015625
Iteration 15900: Loss = -10898.7666015625
Iteration 16000: Loss = -10898.763671875
Iteration 16100: Loss = -10898.7626953125
Iteration 16200: Loss = -10898.759765625
Iteration 16300: Loss = -10898.755859375
Iteration 16400: Loss = -10898.7119140625
Iteration 16500: Loss = -10898.6123046875
Iteration 16600: Loss = -10898.607421875
Iteration 16700: Loss = -10898.60546875
Iteration 16800: Loss = -10898.60546875
Iteration 16900: Loss = -10898.60546875
Iteration 17000: Loss = -10898.603515625
Iteration 17100: Loss = -10898.6044921875
1
Iteration 17200: Loss = -10898.6044921875
2
Iteration 17300: Loss = -10898.603515625
Iteration 17400: Loss = -10898.6025390625
Iteration 17500: Loss = -10898.6025390625
Iteration 17600: Loss = -10898.6015625
Iteration 17700: Loss = -10898.603515625
1
Iteration 17800: Loss = -10898.6025390625
2
Iteration 17900: Loss = -10898.6015625
Iteration 18000: Loss = -10898.6025390625
1
Iteration 18100: Loss = -10898.6015625
Iteration 18200: Loss = -10898.6005859375
Iteration 18300: Loss = -10898.6005859375
Iteration 18400: Loss = -10898.6005859375
Iteration 18500: Loss = -10898.6015625
1
Iteration 18600: Loss = -10898.6005859375
Iteration 18700: Loss = -10898.6005859375
Iteration 18800: Loss = -10898.6005859375
Iteration 18900: Loss = -10898.6005859375
Iteration 19000: Loss = -10898.599609375
Iteration 19100: Loss = -10898.6005859375
1
Iteration 19200: Loss = -10898.599609375
Iteration 19300: Loss = -10898.5986328125
Iteration 19400: Loss = -10898.599609375
1
Iteration 19500: Loss = -10898.599609375
2
Iteration 19600: Loss = -10898.59765625
Iteration 19700: Loss = -10898.5986328125
1
Iteration 19800: Loss = -10898.5986328125
2
Iteration 19900: Loss = -10898.5986328125
3
Iteration 20000: Loss = -10898.5986328125
4
Iteration 20100: Loss = -10898.5986328125
5
Iteration 20200: Loss = -10898.5986328125
6
Iteration 20300: Loss = -10898.59765625
Iteration 20400: Loss = -10898.5986328125
1
Iteration 20500: Loss = -10898.59765625
Iteration 20600: Loss = -10898.599609375
1
Iteration 20700: Loss = -10898.59765625
Iteration 20800: Loss = -10898.59765625
Iteration 20900: Loss = -10898.59765625
Iteration 21000: Loss = -10898.5986328125
1
Iteration 21100: Loss = -10898.5966796875
Iteration 21200: Loss = -10898.5986328125
1
Iteration 21300: Loss = -10898.59765625
2
Iteration 21400: Loss = -10898.59765625
3
Iteration 21500: Loss = -10898.5986328125
4
Iteration 21600: Loss = -10898.59765625
5
Iteration 21700: Loss = -10898.59765625
6
Iteration 21800: Loss = -10898.5986328125
7
Iteration 21900: Loss = -10898.5966796875
Iteration 22000: Loss = -10898.59765625
1
Iteration 22100: Loss = -10898.59765625
2
Iteration 22200: Loss = -10898.59765625
3
Iteration 22300: Loss = -10898.59765625
4
Iteration 22400: Loss = -10898.59765625
5
Iteration 22500: Loss = -10898.5986328125
6
Iteration 22600: Loss = -10898.59765625
7
Iteration 22700: Loss = -10898.5966796875
Iteration 22800: Loss = -10898.59765625
1
Iteration 22900: Loss = -10898.5966796875
Iteration 23000: Loss = -10898.59765625
1
Iteration 23100: Loss = -10898.59765625
2
Iteration 23200: Loss = -10898.59765625
3
Iteration 23300: Loss = -10898.59765625
4
Iteration 23400: Loss = -10898.5966796875
Iteration 23500: Loss = -10898.5986328125
1
Iteration 23600: Loss = -10898.5986328125
2
Iteration 23700: Loss = -10898.59765625
3
Iteration 23800: Loss = -10898.59765625
4
Iteration 23900: Loss = -10898.595703125
Iteration 24000: Loss = -10898.599609375
1
Iteration 24100: Loss = -10898.59765625
2
Iteration 24200: Loss = -10898.59765625
3
Iteration 24300: Loss = -10898.5986328125
4
Iteration 24400: Loss = -10898.5966796875
5
Iteration 24500: Loss = -10898.5986328125
6
Iteration 24600: Loss = -10898.59765625
7
Iteration 24700: Loss = -10898.59765625
8
Iteration 24800: Loss = -10898.59765625
9
Iteration 24900: Loss = -10898.5986328125
10
Iteration 25000: Loss = -10898.59765625
11
Iteration 25100: Loss = -10898.5986328125
12
Iteration 25200: Loss = -10898.59765625
13
Iteration 25300: Loss = -10898.595703125
Iteration 25400: Loss = -10898.5966796875
1
Iteration 25500: Loss = -10898.59765625
2
Iteration 25600: Loss = -10898.5966796875
3
Iteration 25700: Loss = -10898.5966796875
4
Iteration 25800: Loss = -10898.5966796875
5
Iteration 25900: Loss = -10898.599609375
6
Iteration 26000: Loss = -10898.59765625
7
Iteration 26100: Loss = -10898.59765625
8
Iteration 26200: Loss = -10898.5966796875
9
Iteration 26300: Loss = -10898.5966796875
10
Iteration 26400: Loss = -10898.59765625
11
Iteration 26500: Loss = -10898.5966796875
12
Iteration 26600: Loss = -10898.59765625
13
Iteration 26700: Loss = -10898.5966796875
14
Iteration 26800: Loss = -10898.59765625
15
Stopping early at iteration 26800 due to no improvement.
pi: tensor([[6.9625e-02, 9.3038e-01],
        [5.0012e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9550, 0.0450], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1715, 0.1218],
         [0.0524, 0.1566]],

        [[0.0088, 0.2264],
         [0.0844, 0.0531]],

        [[0.0272, 0.1664],
         [0.7225, 0.9660]],

        [[0.3786, 0.1311],
         [0.9657, 0.0158]],

        [[0.9651, 0.1678],
         [0.5700, 0.0316]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00024772455344179066
Average Adjusted Rand Index: -0.0020854854324299475
[-0.0008735738497905159, -0.00024772455344179066] [-0.0015521208230856337, -0.0020854854324299475] [10898.501953125, 10898.59765625]
-------------------------------------
This iteration is 37
True Objective function: Loss = -10912.303545498846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48131.45703125
Iteration 100: Loss = -29792.380859375
Iteration 200: Loss = -16728.669921875
Iteration 300: Loss = -12677.0
Iteration 400: Loss = -11749.197265625
Iteration 500: Loss = -11456.806640625
Iteration 600: Loss = -11297.71484375
Iteration 700: Loss = -11238.9853515625
Iteration 800: Loss = -11203.7119140625
Iteration 900: Loss = -11177.333984375
Iteration 1000: Loss = -11149.1787109375
Iteration 1100: Loss = -11122.20703125
Iteration 1200: Loss = -11101.85546875
Iteration 1300: Loss = -11091.185546875
Iteration 1400: Loss = -11073.3193359375
Iteration 1500: Loss = -11068.4248046875
Iteration 1600: Loss = -11061.7880859375
Iteration 1700: Loss = -11059.701171875
Iteration 1800: Loss = -11058.078125
Iteration 1900: Loss = -11056.7138671875
Iteration 2000: Loss = -11055.5
Iteration 2100: Loss = -11052.7890625
Iteration 2200: Loss = -11046.107421875
Iteration 2300: Loss = -11045.130859375
Iteration 2400: Loss = -11044.376953125
Iteration 2500: Loss = -11043.7373046875
Iteration 2600: Loss = -11043.1728515625
Iteration 2700: Loss = -11042.6669921875
Iteration 2800: Loss = -11042.2109375
Iteration 2900: Loss = -11041.796875
Iteration 3000: Loss = -11041.421875
Iteration 3100: Loss = -11041.0771484375
Iteration 3200: Loss = -11040.763671875
Iteration 3300: Loss = -11040.4736328125
Iteration 3400: Loss = -11040.2080078125
Iteration 3500: Loss = -11039.96484375
Iteration 3600: Loss = -11039.7373046875
Iteration 3700: Loss = -11039.5283203125
Iteration 3800: Loss = -11039.3349609375
Iteration 3900: Loss = -11039.1533203125
Iteration 4000: Loss = -11038.984375
Iteration 4100: Loss = -11038.8271484375
Iteration 4200: Loss = -11038.6806640625
Iteration 4300: Loss = -11038.5419921875
Iteration 4400: Loss = -11038.412109375
Iteration 4500: Loss = -11038.2890625
Iteration 4600: Loss = -11038.171875
Iteration 4700: Loss = -11038.05859375
Iteration 4800: Loss = -11033.7626953125
Iteration 4900: Loss = -11033.3955078125
Iteration 5000: Loss = -11033.2119140625
Iteration 5100: Loss = -11033.078125
Iteration 5200: Loss = -11032.966796875
Iteration 5300: Loss = -11032.8671875
Iteration 5400: Loss = -11032.78125
Iteration 5500: Loss = -11032.703125
Iteration 5600: Loss = -11032.630859375
Iteration 5700: Loss = -11032.5654296875
Iteration 5800: Loss = -11032.5029296875
Iteration 5900: Loss = -11032.4453125
Iteration 6000: Loss = -11032.392578125
Iteration 6100: Loss = -11032.3427734375
Iteration 6200: Loss = -11032.2978515625
Iteration 6300: Loss = -11032.25390625
Iteration 6400: Loss = -11032.2099609375
Iteration 6500: Loss = -11032.171875
Iteration 6600: Loss = -11032.134765625
Iteration 6700: Loss = -11032.099609375
Iteration 6800: Loss = -11032.068359375
Iteration 6900: Loss = -11032.0380859375
Iteration 7000: Loss = -11032.0087890625
Iteration 7100: Loss = -11031.98046875
Iteration 7200: Loss = -11031.9541015625
Iteration 7300: Loss = -11031.9287109375
Iteration 7400: Loss = -11031.9052734375
Iteration 7500: Loss = -11031.8828125
Iteration 7600: Loss = -11031.861328125
Iteration 7700: Loss = -11031.8408203125
Iteration 7800: Loss = -11031.8212890625
Iteration 7900: Loss = -11031.802734375
Iteration 8000: Loss = -11031.78515625
Iteration 8100: Loss = -11031.7685546875
Iteration 8200: Loss = -11031.7529296875
Iteration 8300: Loss = -11031.73828125
Iteration 8400: Loss = -11031.7236328125
Iteration 8500: Loss = -11031.708984375
Iteration 8600: Loss = -11031.6953125
Iteration 8700: Loss = -11031.681640625
Iteration 8800: Loss = -11031.669921875
Iteration 8900: Loss = -11031.658203125
Iteration 9000: Loss = -11031.646484375
Iteration 9100: Loss = -11031.634765625
Iteration 9200: Loss = -11031.6259765625
Iteration 9300: Loss = -11031.6162109375
Iteration 9400: Loss = -11031.607421875
Iteration 9500: Loss = -11031.599609375
Iteration 9600: Loss = -11031.5927734375
Iteration 9700: Loss = -11031.583984375
Iteration 9800: Loss = -11031.576171875
Iteration 9900: Loss = -11031.5703125
Iteration 10000: Loss = -11031.5634765625
Iteration 10100: Loss = -11031.5595703125
Iteration 10200: Loss = -11031.552734375
Iteration 10300: Loss = -11031.5498046875
Iteration 10400: Loss = -11031.5439453125
Iteration 10500: Loss = -11031.541015625
Iteration 10600: Loss = -11031.5361328125
Iteration 10700: Loss = -11031.53125
Iteration 10800: Loss = -11031.5263671875
Iteration 10900: Loss = -11031.5244140625
Iteration 11000: Loss = -11031.5205078125
Iteration 11100: Loss = -11031.5166015625
Iteration 11200: Loss = -11031.513671875
Iteration 11300: Loss = -11031.51171875
Iteration 11400: Loss = -11031.5078125
Iteration 11500: Loss = -11031.5048828125
Iteration 11600: Loss = -11031.5029296875
Iteration 11700: Loss = -11031.5
Iteration 11800: Loss = -11031.4970703125
Iteration 11900: Loss = -11031.49609375
Iteration 12000: Loss = -11031.4931640625
Iteration 12100: Loss = -11031.4921875
Iteration 12200: Loss = -11031.4892578125
Iteration 12300: Loss = -11031.4873046875
Iteration 12400: Loss = -11031.486328125
Iteration 12500: Loss = -11031.482421875
Iteration 12600: Loss = -11031.4814453125
Iteration 12700: Loss = -11031.478515625
Iteration 12800: Loss = -11031.4765625
Iteration 12900: Loss = -11031.4755859375
Iteration 13000: Loss = -11031.474609375
Iteration 13100: Loss = -11031.4736328125
Iteration 13200: Loss = -11031.470703125
Iteration 13300: Loss = -11031.4697265625
Iteration 13400: Loss = -11031.46875
Iteration 13500: Loss = -11031.466796875
Iteration 13600: Loss = -11031.4658203125
Iteration 13700: Loss = -11031.46484375
Iteration 13800: Loss = -11031.462890625
Iteration 13900: Loss = -11031.4609375
Iteration 14000: Loss = -11031.4599609375
Iteration 14100: Loss = -11031.458984375
Iteration 14200: Loss = -11031.45703125
Iteration 14300: Loss = -11031.455078125
Iteration 14400: Loss = -11031.455078125
Iteration 14500: Loss = -11031.451171875
Iteration 14600: Loss = -11031.4501953125
Iteration 14700: Loss = -11031.4462890625
Iteration 14800: Loss = -11031.4443359375
Iteration 14900: Loss = -11031.443359375
Iteration 15000: Loss = -11031.4404296875
Iteration 15100: Loss = -11031.4375
Iteration 15200: Loss = -11031.4326171875
Iteration 15300: Loss = -11031.4287109375
Iteration 15400: Loss = -11031.4248046875
Iteration 15500: Loss = -11031.4189453125
Iteration 15600: Loss = -11031.4111328125
Iteration 15700: Loss = -11031.4013671875
Iteration 15800: Loss = -11031.3896484375
Iteration 15900: Loss = -11031.37109375
Iteration 16000: Loss = -11031.349609375
Iteration 16100: Loss = -11031.314453125
Iteration 16200: Loss = -11031.275390625
Iteration 16300: Loss = -11031.23828125
Iteration 16400: Loss = -11031.216796875
Iteration 16500: Loss = -11031.20703125
Iteration 16600: Loss = -11031.201171875
Iteration 16700: Loss = -11031.19921875
Iteration 16800: Loss = -11031.1962890625
Iteration 16900: Loss = -11031.1953125
Iteration 17000: Loss = -11031.1953125
Iteration 17100: Loss = -11031.1953125
Iteration 17200: Loss = -11031.193359375
Iteration 17300: Loss = -11031.1943359375
1
Iteration 17400: Loss = -11031.1943359375
2
Iteration 17500: Loss = -11031.1943359375
3
Iteration 17600: Loss = -11031.19140625
Iteration 17700: Loss = -11031.193359375
1
Iteration 17800: Loss = -11031.1923828125
2
Iteration 17900: Loss = -11031.1943359375
3
Iteration 18000: Loss = -11031.193359375
4
Iteration 18100: Loss = -11031.1923828125
5
Iteration 18200: Loss = -11031.1904296875
Iteration 18300: Loss = -11031.19140625
1
Iteration 18400: Loss = -11031.1923828125
2
Iteration 18500: Loss = -11031.1923828125
3
Iteration 18600: Loss = -11031.193359375
4
Iteration 18700: Loss = -11031.1923828125
5
Iteration 18800: Loss = -11031.19140625
6
Iteration 18900: Loss = -11031.1923828125
7
Iteration 19000: Loss = -11031.19140625
8
Iteration 19100: Loss = -11031.19140625
9
Iteration 19200: Loss = -11031.1923828125
10
Iteration 19300: Loss = -11031.19140625
11
Iteration 19400: Loss = -11031.193359375
12
Iteration 19500: Loss = -11031.19140625
13
Iteration 19600: Loss = -11031.189453125
Iteration 19700: Loss = -11031.19140625
1
Iteration 19800: Loss = -11031.19140625
2
Iteration 19900: Loss = -11031.19140625
3
Iteration 20000: Loss = -11031.1904296875
4
Iteration 20100: Loss = -11031.1904296875
5
Iteration 20200: Loss = -11031.1904296875
6
Iteration 20300: Loss = -11031.19140625
7
Iteration 20400: Loss = -11031.19140625
8
Iteration 20500: Loss = -11031.19140625
9
Iteration 20600: Loss = -11031.19140625
10
Iteration 20700: Loss = -11031.189453125
Iteration 20800: Loss = -11031.19140625
1
Iteration 20900: Loss = -11031.19140625
2
Iteration 21000: Loss = -11031.19140625
3
Iteration 21100: Loss = -11031.1904296875
4
Iteration 21200: Loss = -11031.189453125
Iteration 21300: Loss = -11031.1884765625
Iteration 21400: Loss = -11031.1904296875
1
Iteration 21500: Loss = -11031.19140625
2
Iteration 21600: Loss = -11031.1904296875
3
Iteration 21700: Loss = -11031.19140625
4
Iteration 21800: Loss = -11031.19140625
5
Iteration 21900: Loss = -11031.19140625
6
Iteration 22000: Loss = -11031.0791015625
Iteration 22100: Loss = -11031.0703125
Iteration 22200: Loss = -11031.0673828125
Iteration 22300: Loss = -11031.0478515625
Iteration 22400: Loss = -11031.0322265625
Iteration 22500: Loss = -11031.033203125
1
Iteration 22600: Loss = -11031.033203125
2
Iteration 22700: Loss = -11031.0341796875
3
Iteration 22800: Loss = -11031.0009765625
Iteration 22900: Loss = -11030.8837890625
Iteration 23000: Loss = -11030.744140625
Iteration 23100: Loss = -11030.568359375
Iteration 23200: Loss = -11030.470703125
Iteration 23300: Loss = -11030.4130859375
Iteration 23400: Loss = -11030.3681640625
Iteration 23500: Loss = -11030.3193359375
Iteration 23600: Loss = -11030.302734375
Iteration 23700: Loss = -11030.3037109375
1
Iteration 23800: Loss = -11030.2998046875
Iteration 23900: Loss = -11030.2529296875
Iteration 24000: Loss = -11030.0517578125
Iteration 24100: Loss = -11029.771484375
Iteration 24200: Loss = -11028.9482421875
Iteration 24300: Loss = -11028.8984375
Iteration 24400: Loss = -11028.763671875
Iteration 24500: Loss = -11028.7412109375
Iteration 24600: Loss = -11028.740234375
Iteration 24700: Loss = -11028.740234375
Iteration 24800: Loss = -11028.7373046875
Iteration 24900: Loss = -11028.7373046875
Iteration 25000: Loss = -11028.7333984375
Iteration 25100: Loss = -11028.732421875
Iteration 25200: Loss = -11028.732421875
Iteration 25300: Loss = -11028.7041015625
Iteration 25400: Loss = -11028.703125
Iteration 25500: Loss = -11028.705078125
1
Iteration 25600: Loss = -11028.703125
Iteration 25700: Loss = -11028.703125
Iteration 25800: Loss = -11028.703125
Iteration 25900: Loss = -11028.705078125
1
Iteration 26000: Loss = -11028.7041015625
2
Iteration 26100: Loss = -11028.7021484375
Iteration 26200: Loss = -11028.7021484375
Iteration 26300: Loss = -11028.6875
Iteration 26400: Loss = -11028.6884765625
1
Iteration 26500: Loss = -11028.6875
Iteration 26600: Loss = -11028.6875
Iteration 26700: Loss = -11028.6884765625
1
Iteration 26800: Loss = -11028.689453125
2
Iteration 26900: Loss = -11028.6865234375
Iteration 27000: Loss = -11028.6865234375
Iteration 27100: Loss = -11028.6875
1
Iteration 27200: Loss = -11028.6865234375
Iteration 27300: Loss = -11028.6787109375
Iteration 27400: Loss = -11028.6767578125
Iteration 27500: Loss = -11028.677734375
1
Iteration 27600: Loss = -11028.677734375
2
Iteration 27700: Loss = -11028.67578125
Iteration 27800: Loss = -11028.677734375
1
Iteration 27900: Loss = -11028.6767578125
2
Iteration 28000: Loss = -11028.673828125
Iteration 28100: Loss = -11028.6669921875
Iteration 28200: Loss = -11028.6533203125
Iteration 28300: Loss = -11028.654296875
1
Iteration 28400: Loss = -11028.6533203125
Iteration 28500: Loss = -11028.6513671875
Iteration 28600: Loss = -11028.6494140625
Iteration 28700: Loss = -11028.6435546875
Iteration 28800: Loss = -11028.6396484375
Iteration 28900: Loss = -11028.6015625
Iteration 29000: Loss = -11028.603515625
1
Iteration 29100: Loss = -11028.5673828125
Iteration 29200: Loss = -11028.5654296875
Iteration 29300: Loss = -11028.544921875
Iteration 29400: Loss = -11028.544921875
Iteration 29500: Loss = -11028.5439453125
Iteration 29600: Loss = -11028.5400390625
Iteration 29700: Loss = -11028.541015625
1
Iteration 29800: Loss = -11028.53515625
Iteration 29900: Loss = -11028.537109375
1
pi: tensor([[9.8752e-01, 1.2478e-02],
        [1.0119e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9910, 0.0090], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1635, 0.1960],
         [0.7660, 0.0798]],

        [[0.9773, 0.2257],
         [0.0137, 0.0131]],

        [[0.9918, 0.1583],
         [0.0619, 0.0437]],

        [[0.2525, 0.2266],
         [0.2782, 0.4439]],

        [[0.4008, 0.1045],
         [0.9927, 0.9196]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.007262881945936654
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: 0.0017279605936875195
Average Adjusted Rand Index: 0.0007376909272542833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23638.5703125
Iteration 100: Loss = -16434.927734375
Iteration 200: Loss = -12728.7490234375
Iteration 300: Loss = -11663.1640625
Iteration 400: Loss = -11336.9912109375
Iteration 500: Loss = -11220.9033203125
Iteration 600: Loss = -11168.376953125
Iteration 700: Loss = -11138.78515625
Iteration 800: Loss = -11117.9931640625
Iteration 900: Loss = -11101.9951171875
Iteration 1000: Loss = -11091.865234375
Iteration 1100: Loss = -11084.34765625
Iteration 1200: Loss = -11079.189453125
Iteration 1300: Loss = -11074.591796875
Iteration 1400: Loss = -11069.3701171875
Iteration 1500: Loss = -11066.3994140625
Iteration 1600: Loss = -11063.54296875
Iteration 1700: Loss = -11060.791015625
Iteration 1800: Loss = -11058.3349609375
Iteration 1900: Loss = -11054.3037109375
Iteration 2000: Loss = -11051.5400390625
Iteration 2100: Loss = -11049.1494140625
Iteration 2200: Loss = -11045.55078125
Iteration 2300: Loss = -11043.376953125
Iteration 2400: Loss = -11042.09765625
Iteration 2500: Loss = -11041.177734375
Iteration 2600: Loss = -11040.4716796875
Iteration 2700: Loss = -11039.9013671875
Iteration 2800: Loss = -11039.4267578125
Iteration 2900: Loss = -11039.0126953125
Iteration 3000: Loss = -11038.580078125
Iteration 3100: Loss = -11035.98046875
Iteration 3200: Loss = -11034.158203125
Iteration 3300: Loss = -11033.5595703125
Iteration 3400: Loss = -11033.140625
Iteration 3500: Loss = -11032.8056640625
Iteration 3600: Loss = -11032.529296875
Iteration 3700: Loss = -11032.29296875
Iteration 3800: Loss = -11032.0888671875
Iteration 3900: Loss = -11031.9091796875
Iteration 4000: Loss = -11031.75
Iteration 4100: Loss = -11031.607421875
Iteration 4200: Loss = -11031.4794921875
Iteration 4300: Loss = -11031.36328125
Iteration 4400: Loss = -11031.2607421875
Iteration 4500: Loss = -11031.1640625
Iteration 4600: Loss = -11031.0771484375
Iteration 4700: Loss = -11030.998046875
Iteration 4800: Loss = -11030.92578125
Iteration 4900: Loss = -11030.857421875
Iteration 5000: Loss = -11030.7958984375
Iteration 5100: Loss = -11030.73828125
Iteration 5200: Loss = -11030.6845703125
Iteration 5300: Loss = -11030.6337890625
Iteration 5400: Loss = -11030.587890625
Iteration 5500: Loss = -11030.5458984375
Iteration 5600: Loss = -11030.505859375
Iteration 5700: Loss = -11030.466796875
Iteration 5800: Loss = -11030.4296875
Iteration 5900: Loss = -11030.3916015625
Iteration 6000: Loss = -11030.34765625
Iteration 6100: Loss = -11030.2841796875
Iteration 6200: Loss = -11030.2265625
Iteration 6300: Loss = -11030.185546875
Iteration 6400: Loss = -11030.15625
Iteration 6500: Loss = -11030.1279296875
Iteration 6600: Loss = -11030.107421875
Iteration 6700: Loss = -11030.0859375
Iteration 6800: Loss = -11030.068359375
Iteration 6900: Loss = -11030.052734375
Iteration 7000: Loss = -11030.0361328125
Iteration 7100: Loss = -11030.0205078125
Iteration 7200: Loss = -11030.0087890625
Iteration 7300: Loss = -11029.9970703125
Iteration 7400: Loss = -11029.9833984375
Iteration 7500: Loss = -11029.97265625
Iteration 7600: Loss = -11029.9609375
Iteration 7700: Loss = -11029.953125
Iteration 7800: Loss = -11029.943359375
Iteration 7900: Loss = -11029.9365234375
Iteration 8000: Loss = -11029.927734375
Iteration 8100: Loss = -11029.9189453125
Iteration 8200: Loss = -11029.9140625
Iteration 8300: Loss = -11029.9072265625
Iteration 8400: Loss = -11029.9013671875
Iteration 8500: Loss = -11029.89453125
Iteration 8600: Loss = -11029.890625
Iteration 8700: Loss = -11029.884765625
Iteration 8800: Loss = -11029.8798828125
Iteration 8900: Loss = -11029.875
Iteration 9000: Loss = -11029.87109375
Iteration 9100: Loss = -11029.865234375
Iteration 9200: Loss = -11029.861328125
Iteration 9300: Loss = -11029.8583984375
Iteration 9400: Loss = -11029.8544921875
Iteration 9500: Loss = -11029.8525390625
Iteration 9600: Loss = -11029.8486328125
Iteration 9700: Loss = -11029.8447265625
Iteration 9800: Loss = -11029.841796875
Iteration 9900: Loss = -11029.841796875
Iteration 10000: Loss = -11029.8369140625
Iteration 10100: Loss = -11029.8369140625
Iteration 10200: Loss = -11029.83203125
Iteration 10300: Loss = -11029.83203125
Iteration 10400: Loss = -11029.830078125
Iteration 10500: Loss = -11029.8271484375
Iteration 10600: Loss = -11029.826171875
Iteration 10700: Loss = -11029.8232421875
Iteration 10800: Loss = -11029.8212890625
Iteration 10900: Loss = -11029.8203125
Iteration 11000: Loss = -11029.8203125
Iteration 11100: Loss = -11029.8173828125
Iteration 11200: Loss = -11029.8173828125
Iteration 11300: Loss = -11029.81640625
Iteration 11400: Loss = -11029.814453125
Iteration 11500: Loss = -11029.8125
Iteration 11600: Loss = -11029.8115234375
Iteration 11700: Loss = -11029.8115234375
Iteration 11800: Loss = -11029.80859375
Iteration 11900: Loss = -11029.810546875
1
Iteration 12000: Loss = -11029.8095703125
2
Iteration 12100: Loss = -11029.80859375
Iteration 12200: Loss = -11029.8056640625
Iteration 12300: Loss = -11029.8076171875
1
Iteration 12400: Loss = -11029.806640625
2
Iteration 12500: Loss = -11029.8056640625
Iteration 12600: Loss = -11029.8037109375
Iteration 12700: Loss = -11029.8046875
1
Iteration 12800: Loss = -11029.802734375
Iteration 12900: Loss = -11029.8037109375
1
Iteration 13000: Loss = -11029.8017578125
Iteration 13100: Loss = -11029.80078125
Iteration 13200: Loss = -11029.802734375
1
Iteration 13300: Loss = -11029.8017578125
2
Iteration 13400: Loss = -11029.8017578125
3
Iteration 13500: Loss = -11029.7998046875
Iteration 13600: Loss = -11029.7998046875
Iteration 13700: Loss = -11029.7998046875
Iteration 13800: Loss = -11029.7998046875
Iteration 13900: Loss = -11029.7998046875
Iteration 14000: Loss = -11029.798828125
Iteration 14100: Loss = -11029.7978515625
Iteration 14200: Loss = -11029.796875
Iteration 14300: Loss = -11029.798828125
1
Iteration 14400: Loss = -11029.7978515625
2
Iteration 14500: Loss = -11029.7978515625
3
Iteration 14600: Loss = -11029.796875
Iteration 14700: Loss = -11029.7978515625
1
Iteration 14800: Loss = -11029.7978515625
2
Iteration 14900: Loss = -11029.7958984375
Iteration 15000: Loss = -11029.7978515625
1
Iteration 15100: Loss = -11029.794921875
Iteration 15200: Loss = -11029.7958984375
1
Iteration 15300: Loss = -11029.7958984375
2
Iteration 15400: Loss = -11029.796875
3
Iteration 15500: Loss = -11029.7958984375
4
Iteration 15600: Loss = -11029.7958984375
5
Iteration 15700: Loss = -11029.7958984375
6
Iteration 15800: Loss = -11029.79296875
Iteration 15900: Loss = -11029.7939453125
1
Iteration 16000: Loss = -11029.7958984375
2
Iteration 16100: Loss = -11029.7958984375
3
Iteration 16200: Loss = -11029.794921875
4
Iteration 16300: Loss = -11029.7958984375
5
Iteration 16400: Loss = -11029.7939453125
6
Iteration 16500: Loss = -11029.7958984375
7
Iteration 16600: Loss = -11029.794921875
8
Iteration 16700: Loss = -11029.794921875
9
Iteration 16800: Loss = -11029.794921875
10
Iteration 16900: Loss = -11029.7958984375
11
Iteration 17000: Loss = -11029.794921875
12
Iteration 17100: Loss = -11029.7939453125
13
Iteration 17200: Loss = -11029.794921875
14
Iteration 17300: Loss = -11029.7939453125
15
Stopping early at iteration 17300 due to no improvement.
pi: tensor([[2.9078e-06, 1.0000e+00],
        [9.9761e-01, 2.3941e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.3636e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1686, 0.1710],
         [0.9248, 0.1605]],

        [[0.9898, 0.1916],
         [0.0797, 0.0773]],

        [[0.0072, 0.4151],
         [0.0202, 0.9808]],

        [[0.9287, 0.2308],
         [0.1628, 0.4328]],

        [[0.8447, 0.0817],
         [0.4318, 0.9311]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00012517982953244724
Average Adjusted Rand Index: 0.0
[0.0017279605936875195, -0.00012517982953244724] [0.0007376909272542833, 0.0] [11028.537109375, 11029.7939453125]
-------------------------------------
This iteration is 38
True Objective function: Loss = -10748.386368596859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15596.9208984375
Iteration 100: Loss = -12046.4150390625
Iteration 200: Loss = -10996.0166015625
Iteration 300: Loss = -10836.279296875
Iteration 400: Loss = -10807.6875
Iteration 500: Loss = -10797.4560546875
Iteration 600: Loss = -10793.0068359375
Iteration 700: Loss = -10790.6025390625
Iteration 800: Loss = -10789.1103515625
Iteration 900: Loss = -10788.115234375
Iteration 1000: Loss = -10787.412109375
Iteration 1100: Loss = -10786.8984375
Iteration 1200: Loss = -10786.5009765625
Iteration 1300: Loss = -10786.17578125
Iteration 1400: Loss = -10785.890625
Iteration 1500: Loss = -10785.62890625
Iteration 1600: Loss = -10785.390625
Iteration 1700: Loss = -10785.1689453125
Iteration 1800: Loss = -10784.955078125
Iteration 1900: Loss = -10784.74609375
Iteration 2000: Loss = -10784.529296875
Iteration 2100: Loss = -10784.2822265625
Iteration 2200: Loss = -10784.0517578125
Iteration 2300: Loss = -10783.859375
Iteration 2400: Loss = -10783.671875
Iteration 2500: Loss = -10783.484375
Iteration 2600: Loss = -10783.2978515625
Iteration 2700: Loss = -10783.1162109375
Iteration 2800: Loss = -10782.9375
Iteration 2900: Loss = -10782.7646484375
Iteration 3000: Loss = -10782.595703125
Iteration 3100: Loss = -10782.4296875
Iteration 3200: Loss = -10782.2626953125
Iteration 3300: Loss = -10782.0947265625
Iteration 3400: Loss = -10781.923828125
Iteration 3500: Loss = -10781.7431640625
Iteration 3600: Loss = -10781.521484375
Iteration 3700: Loss = -10781.1123046875
Iteration 3800: Loss = -10777.623046875
Iteration 3900: Loss = -10708.0615234375
Iteration 4000: Loss = -10685.3984375
Iteration 4100: Loss = -10679.99609375
Iteration 4200: Loss = -10678.9990234375
Iteration 4300: Loss = -10678.650390625
Iteration 4400: Loss = -10678.455078125
Iteration 4500: Loss = -10678.328125
Iteration 4600: Loss = -10678.2392578125
Iteration 4700: Loss = -10678.171875
Iteration 4800: Loss = -10678.12109375
Iteration 4900: Loss = -10678.0830078125
Iteration 5000: Loss = -10678.0517578125
Iteration 5100: Loss = -10678.0283203125
Iteration 5200: Loss = -10678.009765625
Iteration 5300: Loss = -10677.9921875
Iteration 5400: Loss = -10677.9794921875
Iteration 5500: Loss = -10677.9677734375
Iteration 5600: Loss = -10677.95703125
Iteration 5700: Loss = -10677.94921875
Iteration 5800: Loss = -10677.9404296875
Iteration 5900: Loss = -10677.9345703125
Iteration 6000: Loss = -10677.927734375
Iteration 6100: Loss = -10677.9228515625
Iteration 6200: Loss = -10677.9169921875
Iteration 6300: Loss = -10677.912109375
Iteration 6400: Loss = -10677.908203125
Iteration 6500: Loss = -10677.9052734375
Iteration 6600: Loss = -10677.9013671875
Iteration 6700: Loss = -10677.8974609375
Iteration 6800: Loss = -10677.8955078125
Iteration 6900: Loss = -10677.8935546875
Iteration 7000: Loss = -10677.8896484375
Iteration 7100: Loss = -10677.88671875
Iteration 7200: Loss = -10677.884765625
Iteration 7300: Loss = -10677.8818359375
Iteration 7400: Loss = -10677.8798828125
Iteration 7500: Loss = -10677.875
Iteration 7600: Loss = -10677.87109375
Iteration 7700: Loss = -10677.8681640625
Iteration 7800: Loss = -10677.8671875
Iteration 7900: Loss = -10677.865234375
Iteration 8000: Loss = -10677.8642578125
Iteration 8100: Loss = -10677.8623046875
Iteration 8200: Loss = -10677.8623046875
Iteration 8300: Loss = -10677.8603515625
Iteration 8400: Loss = -10677.8583984375
Iteration 8500: Loss = -10677.8525390625
Iteration 8600: Loss = -10677.84765625
Iteration 8700: Loss = -10677.8466796875
Iteration 8800: Loss = -10677.845703125
Iteration 8900: Loss = -10677.8447265625
Iteration 9000: Loss = -10677.84375
Iteration 9100: Loss = -10677.8447265625
1
Iteration 9200: Loss = -10677.8427734375
Iteration 9300: Loss = -10677.841796875
Iteration 9400: Loss = -10677.8408203125
Iteration 9500: Loss = -10677.83984375
Iteration 9600: Loss = -10677.8408203125
1
Iteration 9700: Loss = -10677.8388671875
Iteration 9800: Loss = -10677.8388671875
Iteration 9900: Loss = -10677.8388671875
Iteration 10000: Loss = -10677.8388671875
Iteration 10100: Loss = -10677.837890625
Iteration 10200: Loss = -10677.8388671875
1
Iteration 10300: Loss = -10677.8369140625
Iteration 10400: Loss = -10677.837890625
1
Iteration 10500: Loss = -10677.8359375
Iteration 10600: Loss = -10677.8369140625
1
Iteration 10700: Loss = -10677.8359375
Iteration 10800: Loss = -10677.8369140625
1
Iteration 10900: Loss = -10677.8359375
Iteration 11000: Loss = -10677.8359375
Iteration 11100: Loss = -10677.8359375
Iteration 11200: Loss = -10677.8349609375
Iteration 11300: Loss = -10677.8359375
1
Iteration 11400: Loss = -10677.833984375
Iteration 11500: Loss = -10677.8349609375
1
Iteration 11600: Loss = -10677.8349609375
2
Iteration 11700: Loss = -10677.833984375
Iteration 11800: Loss = -10677.833984375
Iteration 11900: Loss = -10677.8349609375
1
Iteration 12000: Loss = -10677.8349609375
2
Iteration 12100: Loss = -10677.833984375
Iteration 12200: Loss = -10677.8349609375
1
Iteration 12300: Loss = -10677.8349609375
2
Iteration 12400: Loss = -10677.8349609375
3
Iteration 12500: Loss = -10677.833984375
Iteration 12600: Loss = -10677.8349609375
1
Iteration 12700: Loss = -10677.833984375
Iteration 12800: Loss = -10677.833984375
Iteration 12900: Loss = -10677.8330078125
Iteration 13000: Loss = -10677.833984375
1
Iteration 13100: Loss = -10677.833984375
2
Iteration 13200: Loss = -10677.8330078125
Iteration 13300: Loss = -10677.8330078125
Iteration 13400: Loss = -10677.8330078125
Iteration 13500: Loss = -10677.833984375
1
Iteration 13600: Loss = -10677.8330078125
Iteration 13700: Loss = -10677.8330078125
Iteration 13800: Loss = -10677.8330078125
Iteration 13900: Loss = -10677.8330078125
Iteration 14000: Loss = -10677.8330078125
Iteration 14100: Loss = -10677.83203125
Iteration 14200: Loss = -10677.8330078125
1
Iteration 14300: Loss = -10677.8330078125
2
Iteration 14400: Loss = -10677.8330078125
3
Iteration 14500: Loss = -10677.8330078125
4
Iteration 14600: Loss = -10677.8330078125
5
Iteration 14700: Loss = -10677.833984375
6
Iteration 14800: Loss = -10677.8310546875
Iteration 14900: Loss = -10677.8330078125
1
Iteration 15000: Loss = -10677.8330078125
2
Iteration 15100: Loss = -10677.83203125
3
Iteration 15200: Loss = -10677.83203125
4
Iteration 15300: Loss = -10677.83203125
5
Iteration 15400: Loss = -10677.83203125
6
Iteration 15500: Loss = -10677.83203125
7
Iteration 15600: Loss = -10677.83203125
8
Iteration 15700: Loss = -10677.83203125
9
Iteration 15800: Loss = -10677.8310546875
Iteration 15900: Loss = -10677.83203125
1
Iteration 16000: Loss = -10677.8310546875
Iteration 16100: Loss = -10677.8310546875
Iteration 16200: Loss = -10677.83203125
1
Iteration 16300: Loss = -10677.8330078125
2
Iteration 16400: Loss = -10677.83203125
3
Iteration 16500: Loss = -10677.83203125
4
Iteration 16600: Loss = -10677.83203125
5
Iteration 16700: Loss = -10677.8310546875
Iteration 16800: Loss = -10677.83203125
1
Iteration 16900: Loss = -10677.83203125
2
Iteration 17000: Loss = -10677.8310546875
Iteration 17100: Loss = -10677.83203125
1
Iteration 17200: Loss = -10677.8310546875
Iteration 17300: Loss = -10677.830078125
Iteration 17400: Loss = -10677.8310546875
1
Iteration 17500: Loss = -10677.83203125
2
Iteration 17600: Loss = -10677.83203125
3
Iteration 17700: Loss = -10677.8330078125
4
Iteration 17800: Loss = -10677.8310546875
5
Iteration 17900: Loss = -10677.83203125
6
Iteration 18000: Loss = -10677.83203125
7
Iteration 18100: Loss = -10677.8310546875
8
Iteration 18200: Loss = -10677.83203125
9
Iteration 18300: Loss = -10677.8330078125
10
Iteration 18400: Loss = -10677.8310546875
11
Iteration 18500: Loss = -10677.83203125
12
Iteration 18600: Loss = -10677.8310546875
13
Iteration 18700: Loss = -10677.83203125
14
Iteration 18800: Loss = -10677.7333984375
Iteration 18900: Loss = -10677.7236328125
Iteration 19000: Loss = -10677.72265625
Iteration 19100: Loss = -10677.7236328125
1
Iteration 19200: Loss = -10677.72265625
Iteration 19300: Loss = -10677.7236328125
1
Iteration 19400: Loss = -10677.7236328125
2
Iteration 19500: Loss = -10677.7236328125
3
Iteration 19600: Loss = -10677.72265625
Iteration 19700: Loss = -10677.7216796875
Iteration 19800: Loss = -10677.7236328125
1
Iteration 19900: Loss = -10677.7236328125
2
Iteration 20000: Loss = -10677.72265625
3
Iteration 20100: Loss = -10677.72265625
4
Iteration 20200: Loss = -10677.72265625
5
Iteration 20300: Loss = -10677.72265625
6
Iteration 20400: Loss = -10677.72265625
7
Iteration 20500: Loss = -10677.7236328125
8
Iteration 20600: Loss = -10677.72265625
9
Iteration 20700: Loss = -10677.7236328125
10
Iteration 20800: Loss = -10677.7236328125
11
Iteration 20900: Loss = -10677.7255859375
12
Iteration 21000: Loss = -10677.7236328125
13
Iteration 21100: Loss = -10677.72265625
14
Iteration 21200: Loss = -10677.72265625
15
Stopping early at iteration 21200 due to no improvement.
pi: tensor([[0.7862, 0.2138],
        [0.2268, 0.7732]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2402, 0.0986],
         [0.4246, 0.1936]],

        [[0.9109, 0.1091],
         [0.2251, 0.0455]],

        [[0.0268, 0.0993],
         [0.1708, 0.9906]],

        [[0.0751, 0.0936],
         [0.9669, 0.5806]],

        [[0.5055, 0.0961],
         [0.9315, 0.9711]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.772104805341358
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.6363374553319541
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
Global Adjusted Rand Index: 0.7599032260472534
Average Adjusted Rand Index: 0.7609569305582697
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43175.57421875
Iteration 100: Loss = -23926.9453125
Iteration 200: Loss = -13428.2978515625
Iteration 300: Loss = -11536.373046875
Iteration 400: Loss = -11238.2314453125
Iteration 500: Loss = -11111.357421875
Iteration 600: Loss = -11012.8583984375
Iteration 700: Loss = -10949.93359375
Iteration 800: Loss = -10915.2626953125
Iteration 900: Loss = -10891.5751953125
Iteration 1000: Loss = -10872.853515625
Iteration 1100: Loss = -10856.826171875
Iteration 1200: Loss = -10847.8896484375
Iteration 1300: Loss = -10840.4638671875
Iteration 1400: Loss = -10832.5205078125
Iteration 1500: Loss = -10824.8994140625
Iteration 1600: Loss = -10820.3583984375
Iteration 1700: Loss = -10816.9931640625
Iteration 1800: Loss = -10814.2890625
Iteration 1900: Loss = -10812.037109375
Iteration 2000: Loss = -10810.1201171875
Iteration 2100: Loss = -10808.4619140625
Iteration 2200: Loss = -10807.015625
Iteration 2300: Loss = -10805.740234375
Iteration 2400: Loss = -10804.6083984375
Iteration 2500: Loss = -10803.5986328125
Iteration 2600: Loss = -10802.6923828125
Iteration 2700: Loss = -10801.876953125
Iteration 2800: Loss = -10801.1396484375
Iteration 2900: Loss = -10800.46875
Iteration 3000: Loss = -10799.8583984375
Iteration 3100: Loss = -10799.3017578125
Iteration 3200: Loss = -10798.79296875
Iteration 3300: Loss = -10798.326171875
Iteration 3400: Loss = -10797.89453125
Iteration 3500: Loss = -10797.48828125
Iteration 3600: Loss = -10797.1171875
Iteration 3700: Loss = -10796.775390625
Iteration 3800: Loss = -10796.455078125
Iteration 3900: Loss = -10796.16015625
Iteration 4000: Loss = -10795.88671875
Iteration 4100: Loss = -10795.6298828125
Iteration 4200: Loss = -10795.392578125
Iteration 4300: Loss = -10795.16796875
Iteration 4400: Loss = -10794.9580078125
Iteration 4500: Loss = -10794.7607421875
Iteration 4600: Loss = -10794.5791015625
Iteration 4700: Loss = -10794.41015625
Iteration 4800: Loss = -10794.2509765625
Iteration 4900: Loss = -10794.1005859375
Iteration 5000: Loss = -10793.958984375
Iteration 5100: Loss = -10793.82421875
Iteration 5200: Loss = -10793.7021484375
Iteration 5300: Loss = -10793.5859375
Iteration 5400: Loss = -10793.4794921875
Iteration 5500: Loss = -10793.3818359375
Iteration 5600: Loss = -10793.2890625
Iteration 5700: Loss = -10793.2041015625
Iteration 5800: Loss = -10793.1240234375
Iteration 5900: Loss = -10793.0517578125
Iteration 6000: Loss = -10792.982421875
Iteration 6100: Loss = -10792.9189453125
Iteration 6200: Loss = -10792.861328125
Iteration 6300: Loss = -10792.8046875
Iteration 6400: Loss = -10792.7529296875
Iteration 6500: Loss = -10792.7041015625
Iteration 6600: Loss = -10792.658203125
Iteration 6700: Loss = -10792.615234375
Iteration 6800: Loss = -10792.5732421875
Iteration 6900: Loss = -10792.53515625
Iteration 7000: Loss = -10792.5009765625
Iteration 7100: Loss = -10792.4658203125
Iteration 7200: Loss = -10792.4345703125
Iteration 7300: Loss = -10792.404296875
Iteration 7400: Loss = -10792.3759765625
Iteration 7500: Loss = -10792.3486328125
Iteration 7600: Loss = -10792.3232421875
Iteration 7700: Loss = -10792.298828125
Iteration 7800: Loss = -10792.2744140625
Iteration 7900: Loss = -10792.25390625
Iteration 8000: Loss = -10792.232421875
Iteration 8100: Loss = -10792.2138671875
Iteration 8200: Loss = -10792.1943359375
Iteration 8300: Loss = -10792.1767578125
Iteration 8400: Loss = -10792.16015625
Iteration 8500: Loss = -10792.14453125
Iteration 8600: Loss = -10792.1298828125
Iteration 8700: Loss = -10792.115234375
Iteration 8800: Loss = -10792.1025390625
Iteration 8900: Loss = -10792.0888671875
Iteration 9000: Loss = -10792.078125
Iteration 9100: Loss = -10792.0654296875
Iteration 9200: Loss = -10792.052734375
Iteration 9300: Loss = -10792.03515625
Iteration 9400: Loss = -10792.021484375
Iteration 9500: Loss = -10792.0048828125
Iteration 9600: Loss = -10791.9873046875
Iteration 9700: Loss = -10791.96484375
Iteration 9800: Loss = -10791.9384765625
Iteration 9900: Loss = -10791.9072265625
Iteration 10000: Loss = -10791.8798828125
Iteration 10100: Loss = -10791.859375
Iteration 10200: Loss = -10791.8505859375
Iteration 10300: Loss = -10791.84375
Iteration 10400: Loss = -10791.83984375
Iteration 10500: Loss = -10791.8388671875
Iteration 10600: Loss = -10791.8349609375
Iteration 10700: Loss = -10791.8330078125
Iteration 10800: Loss = -10791.830078125
Iteration 10900: Loss = -10791.8271484375
Iteration 11000: Loss = -10791.826171875
Iteration 11100: Loss = -10791.8203125
Iteration 11200: Loss = -10791.818359375
Iteration 11300: Loss = -10791.814453125
Iteration 11400: Loss = -10791.8125
Iteration 11500: Loss = -10791.806640625
Iteration 11600: Loss = -10791.802734375
Iteration 11700: Loss = -10791.798828125
Iteration 11800: Loss = -10791.7958984375
Iteration 11900: Loss = -10791.791015625
Iteration 12000: Loss = -10791.787109375
Iteration 12100: Loss = -10791.7822265625
Iteration 12200: Loss = -10791.77734375
Iteration 12300: Loss = -10791.7724609375
Iteration 12400: Loss = -10791.767578125
Iteration 12500: Loss = -10791.763671875
Iteration 12600: Loss = -10791.7607421875
Iteration 12700: Loss = -10791.7568359375
Iteration 12800: Loss = -10791.7529296875
Iteration 12900: Loss = -10791.7490234375
Iteration 13000: Loss = -10791.7470703125
Iteration 13100: Loss = -10791.7431640625
Iteration 13200: Loss = -10791.7412109375
Iteration 13300: Loss = -10791.7392578125
Iteration 13400: Loss = -10791.7333984375
Iteration 13500: Loss = -10791.732421875
Iteration 13600: Loss = -10791.7275390625
Iteration 13700: Loss = -10791.7255859375
Iteration 13800: Loss = -10791.7197265625
Iteration 13900: Loss = -10791.7216796875
1
Iteration 14000: Loss = -10791.712890625
Iteration 14100: Loss = -10791.70703125
Iteration 14200: Loss = -10791.7041015625
Iteration 14300: Loss = -10791.6982421875
Iteration 14400: Loss = -10791.693359375
Iteration 14500: Loss = -10791.6875
Iteration 14600: Loss = -10791.681640625
Iteration 14700: Loss = -10791.673828125
Iteration 14800: Loss = -10791.6640625
Iteration 14900: Loss = -10791.654296875
Iteration 15000: Loss = -10791.6416015625
Iteration 15100: Loss = -10791.625
Iteration 15200: Loss = -10791.6064453125
Iteration 15300: Loss = -10791.5771484375
Iteration 15400: Loss = -10791.53515625
Iteration 15500: Loss = -10791.470703125
Iteration 15600: Loss = -10791.36328125
Iteration 15700: Loss = -10791.177734375
Iteration 15800: Loss = -10790.9326171875
Iteration 15900: Loss = -10790.771484375
Iteration 16000: Loss = -10790.6982421875
Iteration 16100: Loss = -10790.638671875
Iteration 16200: Loss = -10790.5810546875
Iteration 16300: Loss = -10790.53515625
Iteration 16400: Loss = -10790.4912109375
Iteration 16500: Loss = -10790.4462890625
Iteration 16600: Loss = -10790.3984375
Iteration 16700: Loss = -10790.3486328125
Iteration 16800: Loss = -10790.330078125
Iteration 16900: Loss = -10790.31640625
Iteration 17000: Loss = -10790.3046875
Iteration 17100: Loss = -10790.296875
Iteration 17200: Loss = -10790.2841796875
Iteration 17300: Loss = -10790.275390625
Iteration 17400: Loss = -10790.2724609375
Iteration 17500: Loss = -10790.2685546875
Iteration 17600: Loss = -10790.2685546875
Iteration 17700: Loss = -10790.2666015625
Iteration 17800: Loss = -10790.2666015625
Iteration 17900: Loss = -10790.2646484375
Iteration 18000: Loss = -10790.265625
1
Iteration 18100: Loss = -10790.263671875
Iteration 18200: Loss = -10790.2646484375
1
Iteration 18300: Loss = -10790.2646484375
2
Iteration 18400: Loss = -10790.2626953125
Iteration 18500: Loss = -10790.2607421875
Iteration 18600: Loss = -10790.26171875
1
Iteration 18700: Loss = -10790.26171875
2
Iteration 18800: Loss = -10790.2626953125
3
Iteration 18900: Loss = -10790.2626953125
4
Iteration 19000: Loss = -10790.2607421875
Iteration 19100: Loss = -10790.259765625
Iteration 19200: Loss = -10790.2607421875
1
Iteration 19300: Loss = -10790.2607421875
2
Iteration 19400: Loss = -10790.2607421875
3
Iteration 19500: Loss = -10790.26171875
4
Iteration 19600: Loss = -10790.2607421875
5
Iteration 19700: Loss = -10790.2607421875
6
Iteration 19800: Loss = -10790.259765625
Iteration 19900: Loss = -10790.259765625
Iteration 20000: Loss = -10790.259765625
Iteration 20100: Loss = -10790.2607421875
1
Iteration 20200: Loss = -10790.259765625
Iteration 20300: Loss = -10790.2607421875
1
Iteration 20400: Loss = -10790.259765625
Iteration 20500: Loss = -10790.2607421875
1
Iteration 20600: Loss = -10790.259765625
Iteration 20700: Loss = -10790.2607421875
1
Iteration 20800: Loss = -10790.2587890625
Iteration 20900: Loss = -10790.259765625
1
Iteration 21000: Loss = -10790.259765625
2
Iteration 21100: Loss = -10790.2587890625
Iteration 21200: Loss = -10790.259765625
1
Iteration 21300: Loss = -10790.2587890625
Iteration 21400: Loss = -10790.2587890625
Iteration 21500: Loss = -10790.259765625
1
Iteration 21600: Loss = -10790.2587890625
Iteration 21700: Loss = -10790.2587890625
Iteration 21800: Loss = -10790.259765625
1
Iteration 21900: Loss = -10790.259765625
2
Iteration 22000: Loss = -10790.259765625
3
Iteration 22100: Loss = -10790.259765625
4
Iteration 22200: Loss = -10790.2578125
Iteration 22300: Loss = -10790.259765625
1
Iteration 22400: Loss = -10790.259765625
2
Iteration 22500: Loss = -10790.2587890625
3
Iteration 22600: Loss = -10790.259765625
4
Iteration 22700: Loss = -10790.259765625
5
Iteration 22800: Loss = -10790.2587890625
6
Iteration 22900: Loss = -10790.2587890625
7
Iteration 23000: Loss = -10790.2587890625
8
Iteration 23100: Loss = -10790.259765625
9
Iteration 23200: Loss = -10790.2587890625
10
Iteration 23300: Loss = -10790.2607421875
11
Iteration 23400: Loss = -10790.2587890625
12
Iteration 23500: Loss = -10790.259765625
13
Iteration 23600: Loss = -10790.259765625
14
Iteration 23700: Loss = -10790.259765625
15
Stopping early at iteration 23700 due to no improvement.
pi: tensor([[4.1974e-05, 9.9996e-01],
        [2.8955e-02, 9.7105e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1063, 0.8937], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1191, 0.1349],
         [0.9929, 0.1587]],

        [[0.2730, 0.2096],
         [0.6828, 0.7857]],

        [[0.9347, 0.1064],
         [0.7728, 0.2312]],

        [[0.7266, 0.2232],
         [0.9681, 0.9609]],

        [[0.9577, 0.1224],
         [0.0143, 0.9331]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002304549185215683
Average Adjusted Rand Index: -0.00015692302765368048
[0.7599032260472534, 0.0002304549185215683] [0.7609569305582697, -0.00015692302765368048] [10677.72265625, 10790.259765625]
-------------------------------------
This iteration is 39
True Objective function: Loss = -10989.528216445342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32817.890625
Iteration 100: Loss = -22645.12109375
Iteration 200: Loss = -15219.388671875
Iteration 300: Loss = -13119.7373046875
Iteration 400: Loss = -12198.1767578125
Iteration 500: Loss = -11772.7041015625
Iteration 600: Loss = -11568.1640625
Iteration 700: Loss = -11466.0712890625
Iteration 800: Loss = -11412.5947265625
Iteration 900: Loss = -11383.6513671875
Iteration 1000: Loss = -11364.13671875
Iteration 1100: Loss = -11351.4306640625
Iteration 1200: Loss = -11341.9501953125
Iteration 1300: Loss = -11332.80859375
Iteration 1400: Loss = -11323.9130859375
Iteration 1500: Loss = -11317.1015625
Iteration 1600: Loss = -11309.634765625
Iteration 1700: Loss = -11304.15625
Iteration 1800: Loss = -11296.60546875
Iteration 1900: Loss = -11289.080078125
Iteration 2000: Loss = -11283.17578125
Iteration 2100: Loss = -11278.283203125
Iteration 2200: Loss = -11272.0732421875
Iteration 2300: Loss = -11267.4228515625
Iteration 2400: Loss = -11262.75390625
Iteration 2500: Loss = -11258.4482421875
Iteration 2600: Loss = -11253.5361328125
Iteration 2700: Loss = -11249.1650390625
Iteration 2800: Loss = -11244.748046875
Iteration 2900: Loss = -11239.3076171875
Iteration 3000: Loss = -11233.853515625
Iteration 3100: Loss = -11224.267578125
Iteration 3200: Loss = -11219.4404296875
Iteration 3300: Loss = -11216.4072265625
Iteration 3400: Loss = -11214.2041015625
Iteration 3500: Loss = -11212.7890625
Iteration 3600: Loss = -11211.7216796875
Iteration 3700: Loss = -11210.8798828125
Iteration 3800: Loss = -11210.1943359375
Iteration 3900: Loss = -11209.625
Iteration 4000: Loss = -11209.1455078125
Iteration 4100: Loss = -11208.73046875
Iteration 4200: Loss = -11208.37109375
Iteration 4300: Loss = -11208.0576171875
Iteration 4400: Loss = -11207.77734375
Iteration 4500: Loss = -11207.5283203125
Iteration 4600: Loss = -11207.306640625
Iteration 4700: Loss = -11207.10546875
Iteration 4800: Loss = -11206.9228515625
Iteration 4900: Loss = -11206.7578125
Iteration 5000: Loss = -11206.6064453125
Iteration 5100: Loss = -11206.4697265625
Iteration 5200: Loss = -11206.34375
Iteration 5300: Loss = -11206.2275390625
Iteration 5400: Loss = -11206.119140625
Iteration 5500: Loss = -11206.0205078125
Iteration 5600: Loss = -11205.9306640625
Iteration 5700: Loss = -11205.84375
Iteration 5800: Loss = -11205.765625
Iteration 5900: Loss = -11205.69140625
Iteration 6000: Loss = -11205.6240234375
Iteration 6100: Loss = -11205.5595703125
Iteration 6200: Loss = -11205.5009765625
Iteration 6300: Loss = -11205.4462890625
Iteration 6400: Loss = -11205.3916015625
Iteration 6500: Loss = -11204.9111328125
Iteration 6600: Loss = -11204.8583984375
Iteration 6700: Loss = -11204.8154296875
Iteration 6800: Loss = -11204.7724609375
Iteration 6900: Loss = -11204.7353515625
Iteration 7000: Loss = -11204.7001953125
Iteration 7100: Loss = -11204.6689453125
Iteration 7200: Loss = -11204.634765625
Iteration 7300: Loss = -11204.607421875
Iteration 7400: Loss = -11204.5791015625
Iteration 7500: Loss = -11204.5517578125
Iteration 7600: Loss = -11204.5263671875
Iteration 7700: Loss = -11204.5048828125
Iteration 7800: Loss = -11204.4814453125
Iteration 7900: Loss = -11204.4619140625
Iteration 8000: Loss = -11204.443359375
Iteration 8100: Loss = -11204.4248046875
Iteration 8200: Loss = -11204.4052734375
Iteration 8300: Loss = -11204.390625
Iteration 8400: Loss = -11204.3740234375
Iteration 8500: Loss = -11204.3583984375
Iteration 8600: Loss = -11204.345703125
Iteration 8700: Loss = -11204.33203125
Iteration 8800: Loss = -11204.318359375
Iteration 8900: Loss = -11204.3076171875
Iteration 9000: Loss = -11204.2958984375
Iteration 9100: Loss = -11204.2861328125
Iteration 9200: Loss = -11204.2744140625
Iteration 9300: Loss = -11204.2666015625
Iteration 9400: Loss = -11204.2587890625
Iteration 9500: Loss = -11204.248046875
Iteration 9600: Loss = -11204.240234375
Iteration 9700: Loss = -11204.2333984375
Iteration 9800: Loss = -11204.2265625
Iteration 9900: Loss = -11204.2197265625
Iteration 10000: Loss = -11204.2119140625
Iteration 10100: Loss = -11204.20703125
Iteration 10200: Loss = -11204.2021484375
Iteration 10300: Loss = -11204.1943359375
Iteration 10400: Loss = -11204.189453125
Iteration 10500: Loss = -11204.18359375
Iteration 10600: Loss = -11204.1806640625
Iteration 10700: Loss = -11204.1748046875
Iteration 10800: Loss = -11204.1708984375
Iteration 10900: Loss = -11204.1650390625
Iteration 11000: Loss = -11204.162109375
Iteration 11100: Loss = -11204.1591796875
Iteration 11200: Loss = -11204.1572265625
Iteration 11300: Loss = -11204.1533203125
Iteration 11400: Loss = -11204.1474609375
Iteration 11500: Loss = -11204.1416015625
Iteration 11600: Loss = -11204.109375
Iteration 11700: Loss = -11200.873046875
Iteration 11800: Loss = -11196.8310546875
Iteration 11900: Loss = -11196.109375
Iteration 12000: Loss = -11193.0166015625
Iteration 12100: Loss = -11191.2919921875
Iteration 12200: Loss = -11191.18359375
Iteration 12300: Loss = -11190.908203125
Iteration 12400: Loss = -11190.876953125
Iteration 12500: Loss = -11188.134765625
Iteration 12600: Loss = -11184.59765625
Iteration 12700: Loss = -11183.201171875
Iteration 12800: Loss = -11181.927734375
Iteration 12900: Loss = -11180.9052734375
Iteration 13000: Loss = -11179.6923828125
Iteration 13100: Loss = -11177.5048828125
Iteration 13200: Loss = -11176.7353515625
Iteration 13300: Loss = -11175.9033203125
Iteration 13400: Loss = -11171.9970703125
Iteration 13500: Loss = -11170.400390625
Iteration 13600: Loss = -11166.7568359375
Iteration 13700: Loss = -11161.865234375
Iteration 13800: Loss = -11152.3525390625
Iteration 13900: Loss = -11143.322265625
Iteration 14000: Loss = -11131.1708984375
Iteration 14100: Loss = -11121.8935546875
Iteration 14200: Loss = -11116.52734375
Iteration 14300: Loss = -11112.7734375
Iteration 14400: Loss = -11112.669921875
Iteration 14500: Loss = -11112.64453125
Iteration 14600: Loss = -11112.630859375
Iteration 14700: Loss = -11112.626953125
Iteration 14800: Loss = -11112.4658203125
Iteration 14900: Loss = -11111.4892578125
Iteration 15000: Loss = -11111.47265625
Iteration 15100: Loss = -11111.4658203125
Iteration 15200: Loss = -11111.4619140625
Iteration 15300: Loss = -11111.4599609375
Iteration 15400: Loss = -11111.4560546875
Iteration 15500: Loss = -11111.412109375
Iteration 15600: Loss = -11109.6708984375
Iteration 15700: Loss = -11109.5673828125
Iteration 15800: Loss = -11108.634765625
Iteration 15900: Loss = -11108.494140625
Iteration 16000: Loss = -11108.3720703125
Iteration 16100: Loss = -11108.2041015625
Iteration 16200: Loss = -11108.05859375
Iteration 16300: Loss = -11108.0029296875
Iteration 16400: Loss = -11107.9970703125
Iteration 16500: Loss = -11107.9892578125
Iteration 16600: Loss = -11107.9755859375
Iteration 16700: Loss = -11107.970703125
Iteration 16800: Loss = -11107.9677734375
Iteration 16900: Loss = -11107.9677734375
Iteration 17000: Loss = -11107.9658203125
Iteration 17100: Loss = -11107.9638671875
Iteration 17200: Loss = -11107.962890625
Iteration 17300: Loss = -11107.9609375
Iteration 17400: Loss = -11107.962890625
1
Iteration 17500: Loss = -11107.9619140625
2
Iteration 17600: Loss = -11107.9609375
Iteration 17700: Loss = -11107.9619140625
1
Iteration 17800: Loss = -11107.9609375
Iteration 17900: Loss = -11107.9599609375
Iteration 18000: Loss = -11107.9599609375
Iteration 18100: Loss = -11107.9599609375
Iteration 18200: Loss = -11107.958984375
Iteration 18300: Loss = -11107.958984375
Iteration 18400: Loss = -11107.958984375
Iteration 18500: Loss = -11107.9599609375
1
Iteration 18600: Loss = -11107.9580078125
Iteration 18700: Loss = -11107.9580078125
Iteration 18800: Loss = -11107.9580078125
Iteration 18900: Loss = -11107.9580078125
Iteration 19000: Loss = -11107.958984375
1
Iteration 19100: Loss = -11107.958984375
2
Iteration 19200: Loss = -11107.958984375
3
Iteration 19300: Loss = -11107.958984375
4
Iteration 19400: Loss = -11107.9580078125
Iteration 19500: Loss = -11107.9560546875
Iteration 19600: Loss = -11107.95703125
1
Iteration 19700: Loss = -11107.9599609375
2
Iteration 19800: Loss = -11107.95703125
3
Iteration 19900: Loss = -11107.9580078125
4
Iteration 20000: Loss = -11107.95703125
5
Iteration 20100: Loss = -11107.95703125
6
Iteration 20200: Loss = -11107.95703125
7
Iteration 20300: Loss = -11107.9560546875
Iteration 20400: Loss = -11107.9560546875
Iteration 20500: Loss = -11107.958984375
1
Iteration 20600: Loss = -11107.9560546875
Iteration 20700: Loss = -11107.95703125
1
Iteration 20800: Loss = -11107.95703125
2
Iteration 20900: Loss = -11107.95703125
3
Iteration 21000: Loss = -11107.95703125
4
Iteration 21100: Loss = -11107.9560546875
Iteration 21200: Loss = -11107.9580078125
1
Iteration 21300: Loss = -11107.95703125
2
Iteration 21400: Loss = -11107.95703125
3
Iteration 21500: Loss = -11107.9580078125
4
Iteration 21600: Loss = -11107.95703125
5
Iteration 21700: Loss = -11107.95703125
6
Iteration 21800: Loss = -11107.9580078125
7
Iteration 21900: Loss = -11107.95703125
8
Iteration 22000: Loss = -11107.95703125
9
Iteration 22100: Loss = -11107.9580078125
10
Iteration 22200: Loss = -11107.9560546875
Iteration 22300: Loss = -11107.95703125
1
Iteration 22400: Loss = -11107.9580078125
2
Iteration 22500: Loss = -11107.95703125
3
Iteration 22600: Loss = -11107.95703125
4
Iteration 22700: Loss = -11107.9580078125
5
Iteration 22800: Loss = -11107.95703125
6
Iteration 22900: Loss = -11107.9580078125
7
Iteration 23000: Loss = -11107.9560546875
Iteration 23100: Loss = -11107.9580078125
1
Iteration 23200: Loss = -11107.9580078125
2
Iteration 23300: Loss = -11107.9580078125
3
Iteration 23400: Loss = -11107.9560546875
Iteration 23500: Loss = -11107.9580078125
1
Iteration 23600: Loss = -11107.9560546875
Iteration 23700: Loss = -11107.9560546875
Iteration 23800: Loss = -11107.9580078125
1
Iteration 23900: Loss = -11107.9560546875
Iteration 24000: Loss = -11107.95703125
1
Iteration 24100: Loss = -11107.9580078125
2
Iteration 24200: Loss = -11107.95703125
3
Iteration 24300: Loss = -11107.95703125
4
Iteration 24400: Loss = -11107.95703125
5
Iteration 24500: Loss = -11107.95703125
6
Iteration 24600: Loss = -11107.9580078125
7
Iteration 24700: Loss = -11107.95703125
8
Iteration 24800: Loss = -11107.9560546875
Iteration 24900: Loss = -11107.95703125
1
Iteration 25000: Loss = -11107.9580078125
2
Iteration 25100: Loss = -11107.95703125
3
Iteration 25200: Loss = -11107.9560546875
Iteration 25300: Loss = -11107.9580078125
1
Iteration 25400: Loss = -11107.95703125
2
Iteration 25500: Loss = -11107.95703125
3
Iteration 25600: Loss = -11107.95703125
4
Iteration 25700: Loss = -11107.95703125
5
Iteration 25800: Loss = -11107.9560546875
Iteration 25900: Loss = -11107.9560546875
Iteration 26000: Loss = -11107.9580078125
1
Iteration 26100: Loss = -11107.95703125
2
Iteration 26200: Loss = -11107.9580078125
3
Iteration 26300: Loss = -11107.95703125
4
Iteration 26400: Loss = -11107.95703125
5
Iteration 26500: Loss = -11107.9560546875
Iteration 26600: Loss = -11107.9580078125
1
Iteration 26700: Loss = -11107.9560546875
Iteration 26800: Loss = -11107.95703125
1
Iteration 26900: Loss = -11107.958984375
2
Iteration 27000: Loss = -11107.95703125
3
Iteration 27100: Loss = -11107.9580078125
4
Iteration 27200: Loss = -11107.95703125
5
Iteration 27300: Loss = -11107.9560546875
Iteration 27400: Loss = -11107.9580078125
1
Iteration 27500: Loss = -11107.9580078125
2
Iteration 27600: Loss = -11107.9560546875
Iteration 27700: Loss = -11107.9580078125
1
Iteration 27800: Loss = -11107.9580078125
2
Iteration 27900: Loss = -11107.9580078125
3
Iteration 28000: Loss = -11107.955078125
Iteration 28100: Loss = -11107.95703125
1
Iteration 28200: Loss = -11107.958984375
2
Iteration 28300: Loss = -11107.94921875
Iteration 28400: Loss = -11107.9404296875
Iteration 28500: Loss = -11107.943359375
1
Iteration 28600: Loss = -11107.9423828125
2
Iteration 28700: Loss = -11107.9423828125
3
Iteration 28800: Loss = -11107.9404296875
Iteration 28900: Loss = -11107.9423828125
1
Iteration 29000: Loss = -11107.94140625
2
Iteration 29100: Loss = -11107.9404296875
Iteration 29200: Loss = -11107.9404296875
Iteration 29300: Loss = -11107.9404296875
Iteration 29400: Loss = -11107.9423828125
1
Iteration 29500: Loss = -11107.94140625
2
Iteration 29600: Loss = -11107.94140625
3
Iteration 29700: Loss = -11107.9423828125
4
Iteration 29800: Loss = -11107.9423828125
5
Iteration 29900: Loss = -11107.9423828125
6
pi: tensor([[9.9997e-01, 3.2424e-05],
        [7.6320e-03, 9.9237e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.6061e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2675, 0.1568],
         [0.9918, 0.1647]],

        [[0.4013, 0.2525],
         [0.9589, 0.9895]],

        [[0.6713, 0.0872],
         [0.0255, 0.9926]],

        [[0.9135, 0.2167],
         [0.2181, 0.0105]],

        [[0.2930, 0.2332],
         [0.8638, 0.0398]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.0004061078008667804
Average Adjusted Rand Index: -0.0014931528688998344
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -54876.59375
Iteration 100: Loss = -39782.8203125
Iteration 200: Loss = -28014.173828125
Iteration 300: Loss = -19897.63671875
Iteration 400: Loss = -15132.08984375
Iteration 500: Loss = -12701.953125
Iteration 600: Loss = -11685.19140625
Iteration 700: Loss = -11349.607421875
Iteration 800: Loss = -11240.1298828125
Iteration 900: Loss = -11189.6083984375
Iteration 1000: Loss = -11170.283203125
Iteration 1100: Loss = -11155.201171875
Iteration 1200: Loss = -11146.9169921875
Iteration 1300: Loss = -11140.9326171875
Iteration 1400: Loss = -11137.6650390625
Iteration 1500: Loss = -11134.7568359375
Iteration 1600: Loss = -11127.7109375
Iteration 1700: Loss = -11124.9814453125
Iteration 1800: Loss = -11123.0146484375
Iteration 1900: Loss = -11121.3935546875
Iteration 2000: Loss = -11118.6181640625
Iteration 2100: Loss = -11116.2822265625
Iteration 2200: Loss = -11115.3896484375
Iteration 2300: Loss = -11114.845703125
Iteration 2400: Loss = -11114.4453125
Iteration 2500: Loss = -11114.1259765625
Iteration 2600: Loss = -11113.84375
Iteration 2700: Loss = -11113.498046875
Iteration 2800: Loss = -11112.9609375
Iteration 2900: Loss = -11112.470703125
Iteration 3000: Loss = -11112.2509765625
Iteration 3100: Loss = -11112.0537109375
Iteration 3200: Loss = -11111.8720703125
Iteration 3300: Loss = -11111.7021484375
Iteration 3400: Loss = -11111.5322265625
Iteration 3500: Loss = -11111.359375
Iteration 3600: Loss = -11111.16796875
Iteration 3700: Loss = -11110.9453125
Iteration 3800: Loss = -11110.7490234375
Iteration 3900: Loss = -11110.5849609375
Iteration 4000: Loss = -11110.458984375
Iteration 4100: Loss = -11110.3447265625
Iteration 4200: Loss = -11110.2353515625
Iteration 4300: Loss = -11110.1279296875
Iteration 4400: Loss = -11110.033203125
Iteration 4500: Loss = -11109.9453125
Iteration 4600: Loss = -11109.8603515625
Iteration 4700: Loss = -11109.779296875
Iteration 4800: Loss = -11109.69140625
Iteration 4900: Loss = -11109.6123046875
Iteration 5000: Loss = -11109.5390625
Iteration 5100: Loss = -11109.466796875
Iteration 5200: Loss = -11109.3916015625
Iteration 5300: Loss = -11109.3076171875
Iteration 5400: Loss = -11109.2158203125
Iteration 5500: Loss = -11109.1181640625
Iteration 5600: Loss = -11109.0009765625
Iteration 5700: Loss = -11108.890625
Iteration 5800: Loss = -11108.7509765625
Iteration 5900: Loss = -11108.5888671875
Iteration 6000: Loss = -11108.134765625
Iteration 6100: Loss = -11107.802734375
Iteration 6200: Loss = -11107.5849609375
Iteration 6300: Loss = -11107.447265625
Iteration 6400: Loss = -11107.345703125
Iteration 6500: Loss = -11107.2587890625
Iteration 6600: Loss = -11107.169921875
Iteration 6700: Loss = -11107.078125
Iteration 6800: Loss = -11107.00390625
Iteration 6900: Loss = -11106.927734375
Iteration 7000: Loss = -11106.82421875
Iteration 7100: Loss = -11106.4736328125
Iteration 7200: Loss = -11105.662109375
Iteration 7300: Loss = -11103.5859375
Iteration 7400: Loss = -11100.1650390625
Iteration 7500: Loss = -11099.1318359375
Iteration 7600: Loss = -11095.6474609375
Iteration 7700: Loss = -11090.3017578125
Iteration 7800: Loss = -11073.8310546875
Iteration 7900: Loss = -11066.19921875
Iteration 8000: Loss = -11061.419921875
Iteration 8100: Loss = -11024.7060546875
Iteration 8200: Loss = -10993.7685546875
Iteration 8300: Loss = -10969.4697265625
Iteration 8400: Loss = -10964.654296875
Iteration 8500: Loss = -10958.51953125
Iteration 8600: Loss = -10955.33984375
Iteration 8700: Loss = -10953.8642578125
Iteration 8800: Loss = -10953.80078125
Iteration 8900: Loss = -10953.7412109375
Iteration 9000: Loss = -10953.7119140625
Iteration 9100: Loss = -10953.6943359375
Iteration 9200: Loss = -10953.6806640625
Iteration 9300: Loss = -10953.6689453125
Iteration 9400: Loss = -10953.658203125
Iteration 9500: Loss = -10953.642578125
Iteration 9600: Loss = -10953.63671875
Iteration 9700: Loss = -10953.6328125
Iteration 9800: Loss = -10953.6279296875
Iteration 9900: Loss = -10953.623046875
Iteration 10000: Loss = -10953.61328125
Iteration 10100: Loss = -10953.29296875
Iteration 10200: Loss = -10953.287109375
Iteration 10300: Loss = -10953.28515625
Iteration 10400: Loss = -10952.43359375
Iteration 10500: Loss = -10950.5400390625
Iteration 10600: Loss = -10950.537109375
Iteration 10700: Loss = -10950.5322265625
Iteration 10800: Loss = -10950.5302734375
Iteration 10900: Loss = -10950.52734375
Iteration 11000: Loss = -10950.525390625
Iteration 11100: Loss = -10950.521484375
Iteration 11200: Loss = -10950.51953125
Iteration 11300: Loss = -10950.517578125
Iteration 11400: Loss = -10950.5166015625
Iteration 11500: Loss = -10950.5146484375
Iteration 11600: Loss = -10950.513671875
Iteration 11700: Loss = -10950.5126953125
Iteration 11800: Loss = -10950.51171875
Iteration 11900: Loss = -10950.50390625
Iteration 12000: Loss = -10950.48828125
Iteration 12100: Loss = -10950.482421875
Iteration 12200: Loss = -10950.48046875
Iteration 12300: Loss = -10950.4697265625
Iteration 12400: Loss = -10950.46875
Iteration 12500: Loss = -10950.466796875
Iteration 12600: Loss = -10950.462890625
Iteration 12700: Loss = -10950.4599609375
Iteration 12800: Loss = -10950.3642578125
Iteration 12900: Loss = -10950.361328125
Iteration 13000: Loss = -10950.3603515625
Iteration 13100: Loss = -10950.359375
Iteration 13200: Loss = -10950.3603515625
1
Iteration 13300: Loss = -10950.3603515625
2
Iteration 13400: Loss = -10950.3603515625
3
Iteration 13500: Loss = -10950.359375
Iteration 13600: Loss = -10950.359375
Iteration 13700: Loss = -10950.3603515625
1
Iteration 13800: Loss = -10950.3583984375
Iteration 13900: Loss = -10950.3583984375
Iteration 14000: Loss = -10950.357421875
Iteration 14100: Loss = -10950.341796875
Iteration 14200: Loss = -10950.3388671875
Iteration 14300: Loss = -10950.33984375
1
Iteration 14400: Loss = -10950.3388671875
Iteration 14500: Loss = -10950.33984375
1
Iteration 14600: Loss = -10950.3388671875
Iteration 14700: Loss = -10950.18359375
Iteration 14800: Loss = -10950.181640625
Iteration 14900: Loss = -10950.1796875
Iteration 15000: Loss = -10950.1796875
Iteration 15100: Loss = -10950.1796875
Iteration 15200: Loss = -10950.1796875
Iteration 15300: Loss = -10950.1796875
Iteration 15400: Loss = -10950.1796875
Iteration 15500: Loss = -10950.1787109375
Iteration 15600: Loss = -10950.173828125
Iteration 15700: Loss = -10950.1708984375
Iteration 15800: Loss = -10950.1708984375
Iteration 15900: Loss = -10950.169921875
Iteration 16000: Loss = -10950.1484375
Iteration 16100: Loss = -10950.1474609375
Iteration 16200: Loss = -10950.1474609375
Iteration 16300: Loss = -10950.1455078125
Iteration 16400: Loss = -10950.14453125
Iteration 16500: Loss = -10950.1455078125
1
Iteration 16600: Loss = -10950.1455078125
2
Iteration 16700: Loss = -10950.14453125
Iteration 16800: Loss = -10950.1396484375
Iteration 16900: Loss = -10950.1396484375
Iteration 17000: Loss = -10950.1396484375
Iteration 17100: Loss = -10950.138671875
Iteration 17200: Loss = -10950.138671875
Iteration 17300: Loss = -10950.138671875
Iteration 17400: Loss = -10950.140625
1
Iteration 17500: Loss = -10950.1376953125
Iteration 17600: Loss = -10950.138671875
1
Iteration 17700: Loss = -10950.1318359375
Iteration 17800: Loss = -10950.115234375
Iteration 17900: Loss = -10950.115234375
Iteration 18000: Loss = -10950.1142578125
Iteration 18100: Loss = -10950.10546875
Iteration 18200: Loss = -10950.0849609375
Iteration 18300: Loss = -10950.083984375
Iteration 18400: Loss = -10950.0849609375
1
Iteration 18500: Loss = -10950.0830078125
Iteration 18600: Loss = -10950.08203125
Iteration 18700: Loss = -10950.0830078125
1
Iteration 18800: Loss = -10950.083984375
2
Iteration 18900: Loss = -10950.08203125
Iteration 19000: Loss = -10950.08203125
Iteration 19100: Loss = -10950.0810546875
Iteration 19200: Loss = -10950.0810546875
Iteration 19300: Loss = -10950.0810546875
Iteration 19400: Loss = -10950.0380859375
Iteration 19500: Loss = -10950.03515625
Iteration 19600: Loss = -10950.03515625
Iteration 19700: Loss = -10950.0341796875
Iteration 19800: Loss = -10950.0341796875
Iteration 19900: Loss = -10950.0341796875
Iteration 20000: Loss = -10950.0341796875
Iteration 20100: Loss = -10950.0302734375
Iteration 20200: Loss = -10950.0322265625
1
Iteration 20300: Loss = -10949.7802734375
Iteration 20400: Loss = -10949.77734375
Iteration 20500: Loss = -10949.7783203125
1
Iteration 20600: Loss = -10949.7802734375
2
Iteration 20700: Loss = -10949.77734375
Iteration 20800: Loss = -10949.7783203125
1
Iteration 20900: Loss = -10949.7734375
Iteration 21000: Loss = -10949.7568359375
Iteration 21100: Loss = -10949.7568359375
Iteration 21200: Loss = -10949.755859375
Iteration 21300: Loss = -10949.755859375
Iteration 21400: Loss = -10949.755859375
Iteration 21500: Loss = -10949.755859375
Iteration 21600: Loss = -10949.7568359375
1
Iteration 21700: Loss = -10949.7548828125
Iteration 21800: Loss = -10949.755859375
1
Iteration 21900: Loss = -10949.7568359375
2
Iteration 22000: Loss = -10949.7548828125
Iteration 22100: Loss = -10949.751953125
Iteration 22200: Loss = -10949.75
Iteration 22300: Loss = -10949.75
Iteration 22400: Loss = -10949.7509765625
1
Iteration 22500: Loss = -10949.748046875
Iteration 22600: Loss = -10949.75
1
Iteration 22700: Loss = -10949.7490234375
2
Iteration 22800: Loss = -10949.7490234375
3
Iteration 22900: Loss = -10949.748046875
Iteration 23000: Loss = -10949.7470703125
Iteration 23100: Loss = -10949.7451171875
Iteration 23200: Loss = -10949.74609375
1
Iteration 23300: Loss = -10949.7373046875
Iteration 23400: Loss = -10949.736328125
Iteration 23500: Loss = -10949.7373046875
1
Iteration 23600: Loss = -10949.7373046875
2
Iteration 23700: Loss = -10949.7353515625
Iteration 23800: Loss = -10949.7353515625
Iteration 23900: Loss = -10949.734375
Iteration 24000: Loss = -10949.73046875
Iteration 24100: Loss = -10948.2568359375
Iteration 24200: Loss = -10947.9521484375
Iteration 24300: Loss = -10947.951171875
Iteration 24400: Loss = -10947.9501953125
Iteration 24500: Loss = -10947.94921875
Iteration 24600: Loss = -10947.9365234375
Iteration 24700: Loss = -10947.935546875
Iteration 24800: Loss = -10947.9345703125
Iteration 24900: Loss = -10947.93359375
Iteration 25000: Loss = -10947.9248046875
Iteration 25100: Loss = -10947.9189453125
Iteration 25200: Loss = -10947.91796875
Iteration 25300: Loss = -10947.91796875
Iteration 25400: Loss = -10947.919921875
1
Iteration 25500: Loss = -10947.91796875
Iteration 25600: Loss = -10947.919921875
1
Iteration 25700: Loss = -10947.91796875
Iteration 25800: Loss = -10947.91796875
Iteration 25900: Loss = -10947.9169921875
Iteration 26000: Loss = -10947.9169921875
Iteration 26100: Loss = -10947.9169921875
Iteration 26200: Loss = -10947.9169921875
Iteration 26300: Loss = -10947.8994140625
Iteration 26400: Loss = -10947.900390625
1
Iteration 26500: Loss = -10947.900390625
2
Iteration 26600: Loss = -10947.9013671875
3
Iteration 26700: Loss = -10947.8994140625
Iteration 26800: Loss = -10947.900390625
1
Iteration 26900: Loss = -10947.900390625
2
Iteration 27000: Loss = -10947.900390625
3
Iteration 27100: Loss = -10947.900390625
4
Iteration 27200: Loss = -10947.900390625
5
Iteration 27300: Loss = -10947.9013671875
6
Iteration 27400: Loss = -10947.900390625
7
Iteration 27500: Loss = -10947.9033203125
8
Iteration 27600: Loss = -10947.8984375
Iteration 27700: Loss = -10947.8984375
Iteration 27800: Loss = -10947.8984375
Iteration 27900: Loss = -10947.8994140625
1
Iteration 28000: Loss = -10947.896484375
Iteration 28100: Loss = -10947.8974609375
1
Iteration 28200: Loss = -10947.896484375
Iteration 28300: Loss = -10947.8955078125
Iteration 28400: Loss = -10947.8955078125
Iteration 28500: Loss = -10947.896484375
1
Iteration 28600: Loss = -10947.896484375
2
Iteration 28700: Loss = -10947.896484375
3
Iteration 28800: Loss = -10947.8955078125
Iteration 28900: Loss = -10947.8955078125
Iteration 29000: Loss = -10947.896484375
1
Iteration 29100: Loss = -10947.896484375
2
Iteration 29200: Loss = -10947.8955078125
Iteration 29300: Loss = -10947.8955078125
Iteration 29400: Loss = -10947.896484375
1
Iteration 29500: Loss = -10947.896484375
2
Iteration 29600: Loss = -10947.8955078125
Iteration 29700: Loss = -10947.8984375
1
Iteration 29800: Loss = -10947.8955078125
Iteration 29900: Loss = -10947.896484375
1
pi: tensor([[0.7781, 0.2219],
        [0.2143, 0.7857]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5391, 0.4609], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2619, 0.0951],
         [0.1582, 0.1916]],

        [[0.8012, 0.0964],
         [0.3205, 0.1179]],

        [[0.0082, 0.1116],
         [0.4971, 0.9469]],

        [[0.2593, 0.1019],
         [0.0164, 0.8787]],

        [[0.8387, 0.1110],
         [0.3134, 0.0275]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369635135591801
Global Adjusted Rand Index: 0.846092490511178
Average Adjusted Rand Index: 0.8464607663419516
[-0.0004061078008667804, 0.846092490511178] [-0.0014931528688998344, 0.8464607663419516] [11107.94140625, 10947.8955078125]
-------------------------------------
This iteration is 40
True Objective function: Loss = -10892.700698578867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47218.08984375
Iteration 100: Loss = -28561.505859375
Iteration 200: Loss = -16340.408203125
Iteration 300: Loss = -12342.1650390625
Iteration 400: Loss = -11443.068359375
Iteration 500: Loss = -11193.0029296875
Iteration 600: Loss = -11118.01953125
Iteration 700: Loss = -11076.5908203125
Iteration 800: Loss = -11048.50390625
Iteration 900: Loss = -11037.447265625
Iteration 1000: Loss = -11030.25390625
Iteration 1100: Loss = -11024.8818359375
Iteration 1200: Loss = -11020.7119140625
Iteration 1300: Loss = -11017.33203125
Iteration 1400: Loss = -11014.6162109375
Iteration 1500: Loss = -11012.37890625
Iteration 1600: Loss = -11010.5048828125
Iteration 1700: Loss = -11008.9208984375
Iteration 1800: Loss = -11007.5654296875
Iteration 1900: Loss = -11006.40234375
Iteration 2000: Loss = -11005.3935546875
Iteration 2100: Loss = -11004.5166015625
Iteration 2200: Loss = -11003.748046875
Iteration 2300: Loss = -11003.07421875
Iteration 2400: Loss = -11002.474609375
Iteration 2500: Loss = -11001.9423828125
Iteration 2600: Loss = -11001.46484375
Iteration 2700: Loss = -11001.0390625
Iteration 2800: Loss = -11000.658203125
Iteration 2900: Loss = -11000.3134765625
Iteration 3000: Loss = -11000.005859375
Iteration 3100: Loss = -10999.728515625
Iteration 3200: Loss = -10999.4794921875
Iteration 3300: Loss = -10999.2529296875
Iteration 3400: Loss = -10999.0478515625
Iteration 3500: Loss = -10998.861328125
Iteration 3600: Loss = -10998.6943359375
Iteration 3700: Loss = -10998.5400390625
Iteration 3800: Loss = -10998.3974609375
Iteration 3900: Loss = -10998.2685546875
Iteration 4000: Loss = -10998.15234375
Iteration 4100: Loss = -10998.04296875
Iteration 4200: Loss = -10997.9462890625
Iteration 4300: Loss = -10997.853515625
Iteration 4400: Loss = -10997.7666015625
Iteration 4500: Loss = -10997.6884765625
Iteration 4600: Loss = -10997.619140625
Iteration 4700: Loss = -10997.5517578125
Iteration 4800: Loss = -10997.4873046875
Iteration 4900: Loss = -10997.4306640625
Iteration 5000: Loss = -10997.376953125
Iteration 5100: Loss = -10997.3251953125
Iteration 5200: Loss = -10997.2783203125
Iteration 5300: Loss = -10997.234375
Iteration 5400: Loss = -10997.1904296875
Iteration 5500: Loss = -10997.15234375
Iteration 5600: Loss = -10997.11328125
Iteration 5700: Loss = -10997.078125
Iteration 5800: Loss = -10997.0439453125
Iteration 5900: Loss = -10997.0126953125
Iteration 6000: Loss = -10996.9833984375
Iteration 6100: Loss = -10996.953125
Iteration 6200: Loss = -10996.92578125
Iteration 6300: Loss = -10996.8994140625
Iteration 6400: Loss = -10996.5859375
Iteration 6500: Loss = -10991.572265625
Iteration 6600: Loss = -10991.34765625
Iteration 6700: Loss = -10991.2412109375
Iteration 6800: Loss = -10991.1748046875
Iteration 6900: Loss = -10991.1240234375
Iteration 7000: Loss = -10991.0849609375
Iteration 7100: Loss = -10991.0517578125
Iteration 7200: Loss = -10991.0244140625
Iteration 7300: Loss = -10990.9990234375
Iteration 7400: Loss = -10990.9765625
Iteration 7500: Loss = -10990.955078125
Iteration 7600: Loss = -10990.9365234375
Iteration 7700: Loss = -10990.919921875
Iteration 7800: Loss = -10990.904296875
Iteration 7900: Loss = -10990.8896484375
Iteration 8000: Loss = -10990.875
Iteration 8100: Loss = -10990.86328125
Iteration 8200: Loss = -10990.8486328125
Iteration 8300: Loss = -10990.837890625
Iteration 8400: Loss = -10990.8271484375
Iteration 8500: Loss = -10990.81640625
Iteration 8600: Loss = -10990.8076171875
Iteration 8700: Loss = -10990.7978515625
Iteration 8800: Loss = -10990.7880859375
Iteration 8900: Loss = -10990.78125
Iteration 9000: Loss = -10990.7724609375
Iteration 9100: Loss = -10990.763671875
Iteration 9200: Loss = -10990.7568359375
Iteration 9300: Loss = -10990.7490234375
Iteration 9400: Loss = -10990.740234375
Iteration 9500: Loss = -10990.732421875
Iteration 9600: Loss = -10990.724609375
Iteration 9700: Loss = -10990.716796875
Iteration 9800: Loss = -10990.703125
Iteration 9900: Loss = -10990.681640625
Iteration 10000: Loss = -10990.634765625
Iteration 10100: Loss = -10990.57421875
Iteration 10200: Loss = -10990.552734375
Iteration 10300: Loss = -10990.5390625
Iteration 10400: Loss = -10990.52734375
Iteration 10500: Loss = -10990.5185546875
Iteration 10600: Loss = -10990.51171875
Iteration 10700: Loss = -10990.5048828125
Iteration 10800: Loss = -10990.4990234375
Iteration 10900: Loss = -10990.4921875
Iteration 11000: Loss = -10990.4873046875
Iteration 11100: Loss = -10990.4814453125
Iteration 11200: Loss = -10990.4755859375
Iteration 11300: Loss = -10990.46875
Iteration 11400: Loss = -10990.46484375
Iteration 11500: Loss = -10990.458984375
Iteration 11600: Loss = -10990.4541015625
Iteration 11700: Loss = -10990.4482421875
Iteration 11800: Loss = -10990.44140625
Iteration 11900: Loss = -10990.4365234375
Iteration 12000: Loss = -10990.427734375
Iteration 12100: Loss = -10990.421875
Iteration 12200: Loss = -10990.4150390625
Iteration 12300: Loss = -10990.4072265625
Iteration 12400: Loss = -10990.400390625
Iteration 12500: Loss = -10990.388671875
Iteration 12600: Loss = -10990.37890625
Iteration 12700: Loss = -10990.3681640625
Iteration 12800: Loss = -10990.35546875
Iteration 12900: Loss = -10990.33984375
Iteration 13000: Loss = -10990.322265625
Iteration 13100: Loss = -10990.3017578125
Iteration 13200: Loss = -10990.27734375
Iteration 13300: Loss = -10990.2509765625
Iteration 13400: Loss = -10990.2197265625
Iteration 13500: Loss = -10990.181640625
Iteration 13600: Loss = -10990.1396484375
Iteration 13700: Loss = -10990.087890625
Iteration 13800: Loss = -10990.037109375
Iteration 13900: Loss = -10989.9892578125
Iteration 14000: Loss = -10989.9541015625
Iteration 14100: Loss = -10989.9326171875
Iteration 14200: Loss = -10989.9208984375
Iteration 14300: Loss = -10989.9130859375
Iteration 14400: Loss = -10989.8974609375
Iteration 14500: Loss = -10989.8681640625
Iteration 14600: Loss = -10989.7421875
Iteration 14700: Loss = -10989.287109375
Iteration 14800: Loss = -10989.2568359375
Iteration 14900: Loss = -10989.251953125
Iteration 15000: Loss = -10989.2470703125
Iteration 15100: Loss = -10989.236328125
Iteration 15200: Loss = -10989.2333984375
Iteration 15300: Loss = -10989.232421875
Iteration 15400: Loss = -10989.23046875
Iteration 15500: Loss = -10989.2265625
Iteration 15600: Loss = -10989.2255859375
Iteration 15700: Loss = -10989.224609375
Iteration 15800: Loss = -10989.2236328125
Iteration 15900: Loss = -10989.216796875
Iteration 16000: Loss = -10989.2158203125
Iteration 16100: Loss = -10989.21484375
Iteration 16200: Loss = -10989.21484375
Iteration 16300: Loss = -10989.212890625
Iteration 16400: Loss = -10989.212890625
Iteration 16500: Loss = -10989.2109375
Iteration 16600: Loss = -10989.212890625
1
Iteration 16700: Loss = -10989.2099609375
Iteration 16800: Loss = -10989.208984375
Iteration 16900: Loss = -10989.20703125
Iteration 17000: Loss = -10989.2080078125
1
Iteration 17100: Loss = -10989.20703125
Iteration 17200: Loss = -10989.2080078125
1
Iteration 17300: Loss = -10989.205078125
Iteration 17400: Loss = -10989.2060546875
1
Iteration 17500: Loss = -10989.2021484375
Iteration 17600: Loss = -10989.2041015625
1
Iteration 17700: Loss = -10989.201171875
Iteration 17800: Loss = -10989.2001953125
Iteration 17900: Loss = -10989.19921875
Iteration 18000: Loss = -10989.197265625
Iteration 18100: Loss = -10989.1962890625
Iteration 18200: Loss = -10989.1953125
Iteration 18300: Loss = -10989.193359375
Iteration 18400: Loss = -10989.1943359375
1
Iteration 18500: Loss = -10989.193359375
Iteration 18600: Loss = -10989.1923828125
Iteration 18700: Loss = -10989.1943359375
1
Iteration 18800: Loss = -10989.19140625
Iteration 18900: Loss = -10989.1923828125
1
Iteration 19000: Loss = -10989.1923828125
2
Iteration 19100: Loss = -10989.19140625
Iteration 19200: Loss = -10989.1904296875
Iteration 19300: Loss = -10989.19140625
1
Iteration 19400: Loss = -10989.1904296875
Iteration 19500: Loss = -10989.1904296875
Iteration 19600: Loss = -10989.189453125
Iteration 19700: Loss = -10989.193359375
1
Iteration 19800: Loss = -10989.189453125
Iteration 19900: Loss = -10989.19140625
1
Iteration 20000: Loss = -10989.1904296875
2
Iteration 20100: Loss = -10989.189453125
Iteration 20200: Loss = -10989.1875
Iteration 20300: Loss = -10989.1884765625
1
Iteration 20400: Loss = -10989.1845703125
Iteration 20500: Loss = -10989.18359375
Iteration 20600: Loss = -10989.18359375
Iteration 20700: Loss = -10989.1826171875
Iteration 20800: Loss = -10989.18359375
1
Iteration 20900: Loss = -10989.18359375
2
Iteration 21000: Loss = -10989.18359375
3
Iteration 21100: Loss = -10989.1826171875
Iteration 21200: Loss = -10989.1826171875
Iteration 21300: Loss = -10989.185546875
1
Iteration 21400: Loss = -10989.18359375
2
Iteration 21500: Loss = -10989.1845703125
3
Iteration 21600: Loss = -10989.1826171875
Iteration 21700: Loss = -10989.18359375
1
Iteration 21800: Loss = -10989.181640625
Iteration 21900: Loss = -10989.1826171875
1
Iteration 22000: Loss = -10989.181640625
Iteration 22100: Loss = -10989.1826171875
1
Iteration 22200: Loss = -10989.181640625
Iteration 22300: Loss = -10989.1826171875
1
Iteration 22400: Loss = -10989.181640625
Iteration 22500: Loss = -10989.18359375
1
Iteration 22600: Loss = -10989.181640625
Iteration 22700: Loss = -10989.181640625
Iteration 22800: Loss = -10989.181640625
Iteration 22900: Loss = -10989.18359375
1
Iteration 23000: Loss = -10989.1806640625
Iteration 23100: Loss = -10989.18359375
1
Iteration 23200: Loss = -10989.1826171875
2
Iteration 23300: Loss = -10989.181640625
3
Iteration 23400: Loss = -10989.181640625
4
Iteration 23500: Loss = -10989.181640625
5
Iteration 23600: Loss = -10989.1806640625
Iteration 23700: Loss = -10989.1826171875
1
Iteration 23800: Loss = -10989.1826171875
2
Iteration 23900: Loss = -10989.1806640625
Iteration 24000: Loss = -10989.181640625
1
Iteration 24100: Loss = -10989.181640625
2
Iteration 24200: Loss = -10989.181640625
3
Iteration 24300: Loss = -10989.1826171875
4
Iteration 24400: Loss = -10989.1826171875
5
Iteration 24500: Loss = -10989.1845703125
6
Iteration 24600: Loss = -10989.1806640625
Iteration 24700: Loss = -10989.1845703125
1
Iteration 24800: Loss = -10989.1806640625
Iteration 24900: Loss = -10989.1826171875
1
Iteration 25000: Loss = -10989.1826171875
2
Iteration 25100: Loss = -10989.181640625
3
Iteration 25200: Loss = -10989.1826171875
4
Iteration 25300: Loss = -10989.1826171875
5
Iteration 25400: Loss = -10989.181640625
6
Iteration 25500: Loss = -10989.1826171875
7
Iteration 25600: Loss = -10989.18359375
8
Iteration 25700: Loss = -10989.18359375
9
Iteration 25800: Loss = -10989.181640625
10
Iteration 25900: Loss = -10989.18359375
11
Iteration 26000: Loss = -10989.181640625
12
Iteration 26100: Loss = -10989.181640625
13
Iteration 26200: Loss = -10989.181640625
14
Iteration 26300: Loss = -10989.181640625
15
Stopping early at iteration 26300 due to no improvement.
pi: tensor([[7.5367e-01, 2.4633e-01],
        [5.1673e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0343, 0.9657], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3129, 0.2313],
         [0.3347, 0.1607]],

        [[0.0956, 0.1772],
         [0.8210, 0.5357]],

        [[0.0346, 0.1892],
         [0.9333, 0.9669]],

        [[0.8997, 0.2400],
         [0.1839, 0.9576]],

        [[0.0248, 0.1840],
         [0.0871, 0.9816]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.00028937871409835203
Average Adjusted Rand Index: 0.00027824544884903036
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16734.00390625
Iteration 100: Loss = -12885.5712890625
Iteration 200: Loss = -11497.251953125
Iteration 300: Loss = -11149.9716796875
Iteration 400: Loss = -11055.802734375
Iteration 500: Loss = -11021.9990234375
Iteration 600: Loss = -11007.4375
Iteration 700: Loss = -11000.80859375
Iteration 800: Loss = -10997.0556640625
Iteration 900: Loss = -10995.115234375
Iteration 1000: Loss = -10993.8818359375
Iteration 1100: Loss = -10993.0693359375
Iteration 1200: Loss = -10992.484375
Iteration 1300: Loss = -10992.0419921875
Iteration 1400: Loss = -10991.693359375
Iteration 1500: Loss = -10991.4296875
Iteration 1600: Loss = -10991.220703125
Iteration 1700: Loss = -10991.037109375
Iteration 1800: Loss = -10990.8681640625
Iteration 1900: Loss = -10990.703125
Iteration 2000: Loss = -10990.5810546875
Iteration 2100: Loss = -10990.466796875
Iteration 2200: Loss = -10990.3623046875
Iteration 2300: Loss = -10990.267578125
Iteration 2400: Loss = -10990.177734375
Iteration 2500: Loss = -10990.0927734375
Iteration 2600: Loss = -10990.0126953125
Iteration 2700: Loss = -10989.9365234375
Iteration 2800: Loss = -10989.84375
Iteration 2900: Loss = -10989.763671875
Iteration 3000: Loss = -10989.6767578125
Iteration 3100: Loss = -10989.5927734375
Iteration 3200: Loss = -10989.509765625
Iteration 3300: Loss = -10989.421875
Iteration 3400: Loss = -10989.328125
Iteration 3500: Loss = -10989.2197265625
Iteration 3600: Loss = -10989.091796875
Iteration 3700: Loss = -10988.9375
Iteration 3800: Loss = -10988.7509765625
Iteration 3900: Loss = -10988.5302734375
Iteration 4000: Loss = -10988.291015625
Iteration 4100: Loss = -10988.0595703125
Iteration 4200: Loss = -10987.85546875
Iteration 4300: Loss = -10987.6806640625
Iteration 4400: Loss = -10987.5224609375
Iteration 4500: Loss = -10987.37109375
Iteration 4600: Loss = -10987.2177734375
Iteration 4700: Loss = -10987.05859375
Iteration 4800: Loss = -10986.89453125
Iteration 4900: Loss = -10986.7353515625
Iteration 5000: Loss = -10986.587890625
Iteration 5100: Loss = -10986.4482421875
Iteration 5200: Loss = -10986.31640625
Iteration 5300: Loss = -10986.1884765625
Iteration 5400: Loss = -10986.0654296875
Iteration 5500: Loss = -10985.94921875
Iteration 5600: Loss = -10985.83203125
Iteration 5700: Loss = -10985.7216796875
Iteration 5800: Loss = -10985.6142578125
Iteration 5900: Loss = -10985.5224609375
Iteration 6000: Loss = -10985.4404296875
Iteration 6100: Loss = -10985.3701171875
Iteration 6200: Loss = -10985.3134765625
Iteration 6300: Loss = -10985.265625
Iteration 6400: Loss = -10985.2255859375
Iteration 6500: Loss = -10985.19140625
Iteration 6600: Loss = -10985.1611328125
Iteration 6700: Loss = -10985.1328125
Iteration 6800: Loss = -10985.111328125
Iteration 6900: Loss = -10985.0908203125
Iteration 7000: Loss = -10985.0712890625
Iteration 7100: Loss = -10985.0537109375
Iteration 7200: Loss = -10985.037109375
Iteration 7300: Loss = -10985.0244140625
Iteration 7400: Loss = -10985.009765625
Iteration 7500: Loss = -10984.9990234375
Iteration 7600: Loss = -10984.990234375
Iteration 7700: Loss = -10984.978515625
Iteration 7800: Loss = -10984.96875
Iteration 7900: Loss = -10984.9619140625
Iteration 8000: Loss = -10984.9541015625
Iteration 8100: Loss = -10984.9462890625
Iteration 8200: Loss = -10984.9384765625
Iteration 8300: Loss = -10984.931640625
Iteration 8400: Loss = -10984.9267578125
Iteration 8500: Loss = -10984.9208984375
Iteration 8600: Loss = -10984.9150390625
Iteration 8700: Loss = -10984.912109375
Iteration 8800: Loss = -10984.9072265625
Iteration 8900: Loss = -10984.9052734375
Iteration 9000: Loss = -10984.8984375
Iteration 9100: Loss = -10984.8955078125
Iteration 9200: Loss = -10984.892578125
Iteration 9300: Loss = -10984.888671875
Iteration 9400: Loss = -10984.8857421875
Iteration 9500: Loss = -10984.8818359375
Iteration 9600: Loss = -10984.87890625
Iteration 9700: Loss = -10984.8779296875
Iteration 9800: Loss = -10984.876953125
Iteration 9900: Loss = -10984.8720703125
Iteration 10000: Loss = -10984.87109375
Iteration 10100: Loss = -10984.8671875
Iteration 10200: Loss = -10984.865234375
Iteration 10300: Loss = -10984.8623046875
Iteration 10400: Loss = -10984.8623046875
Iteration 10500: Loss = -10984.861328125
Iteration 10600: Loss = -10984.8583984375
Iteration 10700: Loss = -10984.8564453125
Iteration 10800: Loss = -10984.8564453125
Iteration 10900: Loss = -10984.853515625
Iteration 11000: Loss = -10984.853515625
Iteration 11100: Loss = -10984.853515625
Iteration 11200: Loss = -10984.8515625
Iteration 11300: Loss = -10984.8505859375
Iteration 11400: Loss = -10984.8486328125
Iteration 11500: Loss = -10984.84765625
Iteration 11600: Loss = -10984.8486328125
1
Iteration 11700: Loss = -10984.8466796875
Iteration 11800: Loss = -10984.8447265625
Iteration 11900: Loss = -10984.845703125
1
Iteration 12000: Loss = -10984.8447265625
Iteration 12100: Loss = -10984.84375
Iteration 12200: Loss = -10984.84375
Iteration 12300: Loss = -10984.8408203125
Iteration 12400: Loss = -10984.8427734375
1
Iteration 12500: Loss = -10984.841796875
2
Iteration 12600: Loss = -10984.83984375
Iteration 12700: Loss = -10984.83984375
Iteration 12800: Loss = -10984.8388671875
Iteration 12900: Loss = -10984.83984375
1
Iteration 13000: Loss = -10984.837890625
Iteration 13100: Loss = -10984.837890625
Iteration 13200: Loss = -10984.8369140625
Iteration 13300: Loss = -10984.8359375
Iteration 13400: Loss = -10984.8359375
Iteration 13500: Loss = -10984.837890625
1
Iteration 13600: Loss = -10984.8388671875
2
Iteration 13700: Loss = -10984.8359375
Iteration 13800: Loss = -10984.8369140625
1
Iteration 13900: Loss = -10984.8359375
Iteration 14000: Loss = -10984.8349609375
Iteration 14100: Loss = -10984.8349609375
Iteration 14200: Loss = -10984.8349609375
Iteration 14300: Loss = -10984.833984375
Iteration 14400: Loss = -10984.8359375
1
Iteration 14500: Loss = -10984.833984375
Iteration 14600: Loss = -10984.833984375
Iteration 14700: Loss = -10984.8349609375
1
Iteration 14800: Loss = -10984.8349609375
2
Iteration 14900: Loss = -10984.8330078125
Iteration 15000: Loss = -10984.833984375
1
Iteration 15100: Loss = -10984.8330078125
Iteration 15200: Loss = -10984.8330078125
Iteration 15300: Loss = -10984.8330078125
Iteration 15400: Loss = -10984.83203125
Iteration 15500: Loss = -10984.8330078125
1
Iteration 15600: Loss = -10984.83203125
Iteration 15700: Loss = -10984.8330078125
1
Iteration 15800: Loss = -10984.83203125
Iteration 15900: Loss = -10984.83203125
Iteration 16000: Loss = -10984.83203125
Iteration 16100: Loss = -10984.83203125
Iteration 16200: Loss = -10984.83203125
Iteration 16300: Loss = -10984.8310546875
Iteration 16400: Loss = -10984.8310546875
Iteration 16500: Loss = -10984.830078125
Iteration 16600: Loss = -10984.83203125
1
Iteration 16700: Loss = -10984.83203125
2
Iteration 16800: Loss = -10984.8310546875
3
Iteration 16900: Loss = -10984.8310546875
4
Iteration 17000: Loss = -10984.8310546875
5
Iteration 17100: Loss = -10984.8310546875
6
Iteration 17200: Loss = -10984.8310546875
7
Iteration 17300: Loss = -10984.8310546875
8
Iteration 17400: Loss = -10984.830078125
Iteration 17500: Loss = -10984.8310546875
1
Iteration 17600: Loss = -10984.830078125
Iteration 17700: Loss = -10984.830078125
Iteration 17800: Loss = -10984.8310546875
1
Iteration 17900: Loss = -10984.830078125
Iteration 18000: Loss = -10984.830078125
Iteration 18100: Loss = -10984.8310546875
1
Iteration 18200: Loss = -10984.830078125
Iteration 18300: Loss = -10984.830078125
Iteration 18400: Loss = -10984.8310546875
1
Iteration 18500: Loss = -10984.8310546875
2
Iteration 18600: Loss = -10984.830078125
Iteration 18700: Loss = -10984.830078125
Iteration 18800: Loss = -10984.830078125
Iteration 18900: Loss = -10984.8291015625
Iteration 19000: Loss = -10984.830078125
1
Iteration 19100: Loss = -10984.83203125
2
Iteration 19200: Loss = -10984.8310546875
3
Iteration 19300: Loss = -10984.8291015625
Iteration 19400: Loss = -10984.8291015625
Iteration 19500: Loss = -10984.8310546875
1
Iteration 19600: Loss = -10984.8310546875
2
Iteration 19700: Loss = -10984.8291015625
Iteration 19800: Loss = -10984.830078125
1
Iteration 19900: Loss = -10984.8291015625
Iteration 20000: Loss = -10984.830078125
1
Iteration 20100: Loss = -10984.830078125
2
Iteration 20200: Loss = -10984.830078125
3
Iteration 20300: Loss = -10984.8310546875
4
Iteration 20400: Loss = -10984.8291015625
Iteration 20500: Loss = -10984.8310546875
1
Iteration 20600: Loss = -10984.8291015625
Iteration 20700: Loss = -10984.830078125
1
Iteration 20800: Loss = -10984.830078125
2
Iteration 20900: Loss = -10984.8310546875
3
Iteration 21000: Loss = -10984.830078125
4
Iteration 21100: Loss = -10984.830078125
5
Iteration 21200: Loss = -10984.828125
Iteration 21300: Loss = -10984.830078125
1
Iteration 21400: Loss = -10984.828125
Iteration 21500: Loss = -10984.8291015625
1
Iteration 21600: Loss = -10984.828125
Iteration 21700: Loss = -10984.8291015625
1
Iteration 21800: Loss = -10984.8291015625
2
Iteration 21900: Loss = -10984.8291015625
3
Iteration 22000: Loss = -10984.828125
Iteration 22100: Loss = -10984.830078125
1
Iteration 22200: Loss = -10984.8291015625
2
Iteration 22300: Loss = -10984.8291015625
3
Iteration 22400: Loss = -10984.8271484375
Iteration 22500: Loss = -10984.826171875
Iteration 22600: Loss = -10984.826171875
Iteration 22700: Loss = -10984.826171875
Iteration 22800: Loss = -10984.8271484375
1
Iteration 22900: Loss = -10984.8251953125
Iteration 23000: Loss = -10984.8271484375
1
Iteration 23100: Loss = -10984.8251953125
Iteration 23200: Loss = -10984.826171875
1
Iteration 23300: Loss = -10984.828125
2
Iteration 23400: Loss = -10984.826171875
3
Iteration 23500: Loss = -10984.828125
4
Iteration 23600: Loss = -10984.826171875
5
Iteration 23700: Loss = -10984.8271484375
6
Iteration 23800: Loss = -10984.8271484375
7
Iteration 23900: Loss = -10984.8271484375
8
Iteration 24000: Loss = -10984.8271484375
9
Iteration 24100: Loss = -10984.826171875
10
Iteration 24200: Loss = -10984.8271484375
11
Iteration 24300: Loss = -10984.8271484375
12
Iteration 24400: Loss = -10984.826171875
13
Iteration 24500: Loss = -10984.8271484375
14
Iteration 24600: Loss = -10984.826171875
15
Stopping early at iteration 24600 due to no improvement.
pi: tensor([[9.9998e-01, 1.5611e-05],
        [2.6985e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0664, 0.9336], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2129, 0.1233],
         [0.7597, 0.1654]],

        [[0.0097, 0.1459],
         [0.0387, 0.5226]],

        [[0.8719, 0.0863],
         [0.9257, 0.1030]],

        [[0.7270, 0.1552],
         [0.0575, 0.2696]],

        [[0.9855, 0.1991],
         [0.9901, 0.3049]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0028959952356207414
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0014658390783536795
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0008263300397341679
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
Global Adjusted Rand Index: 0.001852733420159495
Average Adjusted Rand Index: -6.042322890778538e-05
[0.00028937871409835203, 0.001852733420159495] [0.00027824544884903036, -6.042322890778538e-05] [10989.181640625, 10984.826171875]
-------------------------------------
This iteration is 41
True Objective function: Loss = -10818.401201748846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32465.591796875
Iteration 100: Loss = -21504.41796875
Iteration 200: Loss = -13446.40234375
Iteration 300: Loss = -11697.970703125
Iteration 400: Loss = -11366.115234375
Iteration 500: Loss = -11260.34375
Iteration 600: Loss = -11214.55078125
Iteration 700: Loss = -11188.904296875
Iteration 800: Loss = -11169.2900390625
Iteration 900: Loss = -11155.3408203125
Iteration 1000: Loss = -11143.27734375
Iteration 1100: Loss = -11133.5537109375
Iteration 1200: Loss = -11124.1357421875
Iteration 1300: Loss = -11114.2421875
Iteration 1400: Loss = -11105.6787109375
Iteration 1500: Loss = -11097.638671875
Iteration 1600: Loss = -11090.6328125
Iteration 1700: Loss = -11084.654296875
Iteration 1800: Loss = -11079.162109375
Iteration 1900: Loss = -11074.44140625
Iteration 2000: Loss = -11070.015625
Iteration 2100: Loss = -11064.7900390625
Iteration 2200: Loss = -11059.7353515625
Iteration 2300: Loss = -11053.578125
Iteration 2400: Loss = -11046.498046875
Iteration 2500: Loss = -11038.4326171875
Iteration 2600: Loss = -11033.0224609375
Iteration 2700: Loss = -11028.638671875
Iteration 2800: Loss = -11019.6005859375
Iteration 2900: Loss = -11013.1337890625
Iteration 3000: Loss = -11009.0126953125
Iteration 3100: Loss = -11006.4326171875
Iteration 3200: Loss = -11004.8447265625
Iteration 3300: Loss = -11003.66015625
Iteration 3400: Loss = -11002.73046875
Iteration 3500: Loss = -11001.9775390625
Iteration 3600: Loss = -11001.353515625
Iteration 3700: Loss = -11000.8251953125
Iteration 3800: Loss = -11000.12109375
Iteration 3900: Loss = -10999.0078125
Iteration 4000: Loss = -10998.5283203125
Iteration 4100: Loss = -10997.4072265625
Iteration 4200: Loss = -10997.115234375
Iteration 4300: Loss = -10996.8720703125
Iteration 4400: Loss = -10996.6591796875
Iteration 4500: Loss = -10996.4677734375
Iteration 4600: Loss = -10996.2958984375
Iteration 4700: Loss = -10996.1396484375
Iteration 4800: Loss = -10995.9951171875
Iteration 4900: Loss = -10995.8642578125
Iteration 5000: Loss = -10995.73828125
Iteration 5100: Loss = -10995.6103515625
Iteration 5200: Loss = -10995.4091796875
Iteration 5300: Loss = -10994.4423828125
Iteration 5400: Loss = -10994.341796875
Iteration 5500: Loss = -10994.259765625
Iteration 5600: Loss = -10994.185546875
Iteration 5700: Loss = -10994.1181640625
Iteration 5800: Loss = -10994.0546875
Iteration 5900: Loss = -10993.99609375
Iteration 6000: Loss = -10993.9404296875
Iteration 6100: Loss = -10993.888671875
Iteration 6200: Loss = -10993.841796875
Iteration 6300: Loss = -10993.79296875
Iteration 6400: Loss = -10993.728515625
Iteration 6500: Loss = -10992.2333984375
Iteration 6600: Loss = -10991.744140625
Iteration 6700: Loss = -10991.6748046875
Iteration 6800: Loss = -10991.626953125
Iteration 6900: Loss = -10991.5888671875
Iteration 7000: Loss = -10991.5556640625
Iteration 7100: Loss = -10991.525390625
Iteration 7200: Loss = -10991.4970703125
Iteration 7300: Loss = -10991.4716796875
Iteration 7400: Loss = -10991.44921875
Iteration 7500: Loss = -10991.42578125
Iteration 7600: Loss = -10991.404296875
Iteration 7700: Loss = -10991.3857421875
Iteration 7800: Loss = -10991.3662109375
Iteration 7900: Loss = -10991.3486328125
Iteration 8000: Loss = -10991.3310546875
Iteration 8100: Loss = -10991.3173828125
Iteration 8200: Loss = -10991.302734375
Iteration 8300: Loss = -10991.2900390625
Iteration 8400: Loss = -10991.275390625
Iteration 8500: Loss = -10991.263671875
Iteration 8600: Loss = -10991.2509765625
Iteration 8700: Loss = -10991.2412109375
Iteration 8800: Loss = -10991.232421875
Iteration 8900: Loss = -10991.2216796875
Iteration 9000: Loss = -10991.212890625
Iteration 9100: Loss = -10991.203125
Iteration 9200: Loss = -10991.1953125
Iteration 9300: Loss = -10991.1875
Iteration 9400: Loss = -10991.1796875
Iteration 9500: Loss = -10991.1728515625
Iteration 9600: Loss = -10991.16796875
Iteration 9700: Loss = -10991.16015625
Iteration 9800: Loss = -10991.154296875
Iteration 9900: Loss = -10991.1474609375
Iteration 10000: Loss = -10991.142578125
Iteration 10100: Loss = -10991.1376953125
Iteration 10200: Loss = -10991.1318359375
Iteration 10300: Loss = -10991.1279296875
Iteration 10400: Loss = -10991.1240234375
Iteration 10500: Loss = -10991.1181640625
Iteration 10600: Loss = -10991.1162109375
Iteration 10700: Loss = -10991.1123046875
Iteration 10800: Loss = -10991.109375
Iteration 10900: Loss = -10991.10546875
Iteration 11000: Loss = -10991.103515625
Iteration 11100: Loss = -10991.099609375
Iteration 11200: Loss = -10991.095703125
Iteration 11300: Loss = -10991.09375
Iteration 11400: Loss = -10991.091796875
Iteration 11500: Loss = -10991.0888671875
Iteration 11600: Loss = -10991.0869140625
Iteration 11700: Loss = -10991.083984375
Iteration 11800: Loss = -10991.0810546875
Iteration 11900: Loss = -10991.080078125
Iteration 12000: Loss = -10991.080078125
Iteration 12100: Loss = -10991.076171875
Iteration 12200: Loss = -10991.07421875
Iteration 12300: Loss = -10991.0732421875
Iteration 12400: Loss = -10991.0712890625
Iteration 12500: Loss = -10991.0703125
Iteration 12600: Loss = -10991.0693359375
Iteration 12700: Loss = -10991.068359375
Iteration 12800: Loss = -10991.06640625
Iteration 12900: Loss = -10991.0654296875
Iteration 13000: Loss = -10991.0634765625
Iteration 13100: Loss = -10991.0634765625
Iteration 13200: Loss = -10991.0625
Iteration 13300: Loss = -10991.0615234375
Iteration 13400: Loss = -10991.0595703125
Iteration 13500: Loss = -10991.05859375
Iteration 13600: Loss = -10991.05859375
Iteration 13700: Loss = -10991.0576171875
Iteration 13800: Loss = -10991.056640625
Iteration 13900: Loss = -10991.056640625
Iteration 14000: Loss = -10991.0546875
Iteration 14100: Loss = -10991.0546875
Iteration 14200: Loss = -10991.0537109375
Iteration 14300: Loss = -10991.052734375
Iteration 14400: Loss = -10991.052734375
Iteration 14500: Loss = -10991.05078125
Iteration 14600: Loss = -10991.0546875
1
Iteration 14700: Loss = -10991.05078125
Iteration 14800: Loss = -10991.052734375
1
Iteration 14900: Loss = -10991.05078125
Iteration 15000: Loss = -10991.0498046875
Iteration 15100: Loss = -10991.0498046875
Iteration 15200: Loss = -10991.0498046875
Iteration 15300: Loss = -10991.0498046875
Iteration 15400: Loss = -10991.0498046875
Iteration 15500: Loss = -10991.048828125
Iteration 15600: Loss = -10991.0478515625
Iteration 15700: Loss = -10991.046875
Iteration 15800: Loss = -10991.048828125
1
Iteration 15900: Loss = -10991.046875
Iteration 16000: Loss = -10991.0458984375
Iteration 16100: Loss = -10991.046875
1
Iteration 16200: Loss = -10991.046875
2
Iteration 16300: Loss = -10991.046875
3
Iteration 16400: Loss = -10991.048828125
4
Iteration 16500: Loss = -10991.046875
5
Iteration 16600: Loss = -10991.046875
6
Iteration 16700: Loss = -10991.0458984375
Iteration 16800: Loss = -10991.044921875
Iteration 16900: Loss = -10991.044921875
Iteration 17000: Loss = -10991.0439453125
Iteration 17100: Loss = -10991.044921875
1
Iteration 17200: Loss = -10991.044921875
2
Iteration 17300: Loss = -10991.044921875
3
Iteration 17400: Loss = -10991.0439453125
Iteration 17500: Loss = -10991.0458984375
1
Iteration 17600: Loss = -10991.0439453125
Iteration 17700: Loss = -10988.7431640625
Iteration 17800: Loss = -10988.6533203125
Iteration 17900: Loss = -10986.5966796875
Iteration 18000: Loss = -10985.2431640625
Iteration 18100: Loss = -10985.22265625
Iteration 18200: Loss = -10982.8134765625
Iteration 18300: Loss = -10979.42578125
Iteration 18400: Loss = -10974.76171875
Iteration 18500: Loss = -10972.8525390625
Iteration 18600: Loss = -10972.11328125
Iteration 18700: Loss = -10970.392578125
Iteration 18800: Loss = -10964.8994140625
Iteration 18900: Loss = -10957.3974609375
Iteration 19000: Loss = -10950.875
Iteration 19100: Loss = -10947.1904296875
Iteration 19200: Loss = -10932.7763671875
Iteration 19300: Loss = -10926.115234375
Iteration 19400: Loss = -10918.16796875
Iteration 19500: Loss = -10913.9677734375
Iteration 19600: Loss = -10909.8740234375
Iteration 19700: Loss = -10903.6376953125
Iteration 19800: Loss = -10899.2001953125
Iteration 19900: Loss = -10898.837890625
Iteration 20000: Loss = -10895.6982421875
Iteration 20100: Loss = -10895.560546875
Iteration 20200: Loss = -10895.533203125
Iteration 20300: Loss = -10895.501953125
Iteration 20400: Loss = -10895.490234375
Iteration 20500: Loss = -10895.25
Iteration 20600: Loss = -10895.244140625
Iteration 20700: Loss = -10895.2392578125
Iteration 20800: Loss = -10895.2373046875
Iteration 20900: Loss = -10895.234375
Iteration 21000: Loss = -10895.234375
Iteration 21100: Loss = -10895.228515625
Iteration 21200: Loss = -10895.2216796875
Iteration 21300: Loss = -10895.2197265625
Iteration 21400: Loss = -10895.216796875
Iteration 21500: Loss = -10895.21484375
Iteration 21600: Loss = -10895.21484375
Iteration 21700: Loss = -10895.2138671875
Iteration 21800: Loss = -10895.212890625
Iteration 21900: Loss = -10895.2119140625
Iteration 22000: Loss = -10895.2099609375
Iteration 22100: Loss = -10895.2080078125
Iteration 22200: Loss = -10895.2041015625
Iteration 22300: Loss = -10895.1962890625
Iteration 22400: Loss = -10895.19140625
Iteration 22500: Loss = -10895.189453125
Iteration 22600: Loss = -10895.1884765625
Iteration 22700: Loss = -10895.1884765625
Iteration 22800: Loss = -10895.1875
Iteration 22900: Loss = -10895.1865234375
Iteration 23000: Loss = -10895.1865234375
Iteration 23100: Loss = -10895.1865234375
Iteration 23200: Loss = -10895.1865234375
Iteration 23300: Loss = -10895.1865234375
Iteration 23400: Loss = -10895.1865234375
Iteration 23500: Loss = -10895.1474609375
Iteration 23600: Loss = -10895.1474609375
Iteration 23700: Loss = -10895.1474609375
Iteration 23800: Loss = -10895.1484375
1
Iteration 23900: Loss = -10895.146484375
Iteration 24000: Loss = -10895.146484375
Iteration 24100: Loss = -10895.1494140625
1
Iteration 24200: Loss = -10895.1474609375
2
Iteration 24300: Loss = -10895.1474609375
3
Iteration 24400: Loss = -10895.1455078125
Iteration 24500: Loss = -10895.1455078125
Iteration 24600: Loss = -10895.1455078125
Iteration 24700: Loss = -10895.1455078125
Iteration 24800: Loss = -10895.1455078125
Iteration 24900: Loss = -10895.14453125
Iteration 25000: Loss = -10895.1455078125
1
Iteration 25100: Loss = -10895.1455078125
2
Iteration 25200: Loss = -10895.1455078125
3
Iteration 25300: Loss = -10895.1181640625
Iteration 25400: Loss = -10894.84765625
Iteration 25500: Loss = -10894.31640625
Iteration 25600: Loss = -10893.591796875
Iteration 25700: Loss = -10892.4482421875
Iteration 25800: Loss = -10892.1650390625
Iteration 25900: Loss = -10892.10546875
Iteration 26000: Loss = -10892.10546875
Iteration 26100: Loss = -10892.1044921875
Iteration 26200: Loss = -10891.6298828125
Iteration 26300: Loss = -10891.49609375
Iteration 26400: Loss = -10891.490234375
Iteration 26500: Loss = -10891.4853515625
Iteration 26600: Loss = -10891.4833984375
Iteration 26700: Loss = -10891.484375
1
Iteration 26800: Loss = -10891.4677734375
Iteration 26900: Loss = -10891.466796875
Iteration 27000: Loss = -10891.4638671875
Iteration 27100: Loss = -10891.4482421875
Iteration 27200: Loss = -10891.4482421875
Iteration 27300: Loss = -10891.4482421875
Iteration 27400: Loss = -10891.4443359375
Iteration 27500: Loss = -10891.4384765625
Iteration 27600: Loss = -10891.4375
Iteration 27700: Loss = -10891.4375
Iteration 27800: Loss = -10891.3984375
Iteration 27900: Loss = -10891.3974609375
Iteration 28000: Loss = -10891.3994140625
1
Iteration 28100: Loss = -10891.3974609375
Iteration 28200: Loss = -10891.388671875
Iteration 28300: Loss = -10891.390625
1
Iteration 28400: Loss = -10891.3896484375
2
Iteration 28500: Loss = -10890.7353515625
Iteration 28600: Loss = -10890.6982421875
Iteration 28700: Loss = -10890.6982421875
Iteration 28800: Loss = -10888.607421875
Iteration 28900: Loss = -10888.2021484375
Iteration 29000: Loss = -10888.203125
1
Iteration 29100: Loss = -10888.203125
2
Iteration 29200: Loss = -10888.2021484375
Iteration 29300: Loss = -10888.201171875
Iteration 29400: Loss = -10888.203125
1
Iteration 29500: Loss = -10888.203125
2
Iteration 29600: Loss = -10888.2021484375
3
Iteration 29700: Loss = -10888.2021484375
4
Iteration 29800: Loss = -10888.2021484375
5
Iteration 29900: Loss = -10888.1982421875
pi: tensor([[0.8696, 0.1304],
        [0.0605, 0.9395]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 4.7257e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1711, 0.1615],
         [0.9912, 0.2309]],

        [[0.1296, 0.1281],
         [0.9232, 0.0403]],

        [[0.8279, 0.0889],
         [0.0121, 0.3537]],

        [[0.8574, 0.0991],
         [0.7574, 0.7460]],

        [[0.0158, 0.1511],
         [0.0106, 0.0664]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 32
Adjusted Rand Index: 0.1234375
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 17
Adjusted Rand Index: 0.4301535108653031
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.06807631890145967
Global Adjusted Rand Index: 0.06233900241413372
Average Adjusted Rand Index: 0.12298572744392207
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45028.0234375
Iteration 100: Loss = -28917.1171875
Iteration 200: Loss = -17436.462890625
Iteration 300: Loss = -13296.6357421875
Iteration 400: Loss = -11600.65625
Iteration 500: Loss = -11221.125
Iteration 600: Loss = -11126.1064453125
Iteration 700: Loss = -11070.4404296875
Iteration 800: Loss = -11036.7900390625
Iteration 900: Loss = -11009.232421875
Iteration 1000: Loss = -10984.046875
Iteration 1100: Loss = -10971.923828125
Iteration 1200: Loss = -10962.134765625
Iteration 1300: Loss = -10951.6826171875
Iteration 1400: Loss = -10946.0146484375
Iteration 1500: Loss = -10941.091796875
Iteration 1600: Loss = -10936.5634765625
Iteration 1700: Loss = -10931.0615234375
Iteration 1800: Loss = -10927.19140625
Iteration 1900: Loss = -10923.771484375
Iteration 2000: Loss = -10920.3046875
Iteration 2100: Loss = -10917.1552734375
Iteration 2200: Loss = -10914.4345703125
Iteration 2300: Loss = -10911.654296875
Iteration 2400: Loss = -10909.2939453125
Iteration 2500: Loss = -10907.587890625
Iteration 2600: Loss = -10906.1962890625
Iteration 2700: Loss = -10905.2216796875
Iteration 2800: Loss = -10904.390625
Iteration 2900: Loss = -10903.623046875
Iteration 3000: Loss = -10902.90625
Iteration 3100: Loss = -10902.345703125
Iteration 3200: Loss = -10901.837890625
Iteration 3300: Loss = -10901.3828125
Iteration 3400: Loss = -10900.970703125
Iteration 3500: Loss = -10900.60546875
Iteration 3600: Loss = -10900.279296875
Iteration 3700: Loss = -10899.9853515625
Iteration 3800: Loss = -10899.7177734375
Iteration 3900: Loss = -10899.4755859375
Iteration 4000: Loss = -10899.255859375
Iteration 4100: Loss = -10899.0546875
Iteration 4200: Loss = -10898.8681640625
Iteration 4300: Loss = -10898.69921875
Iteration 4400: Loss = -10898.541015625
Iteration 4500: Loss = -10898.3955078125
Iteration 4600: Loss = -10898.26171875
Iteration 4700: Loss = -10898.1376953125
Iteration 4800: Loss = -10898.0205078125
Iteration 4900: Loss = -10897.912109375
Iteration 5000: Loss = -10897.810546875
Iteration 5100: Loss = -10897.7177734375
Iteration 5200: Loss = -10897.6279296875
Iteration 5300: Loss = -10897.546875
Iteration 5400: Loss = -10897.470703125
Iteration 5500: Loss = -10897.396484375
Iteration 5600: Loss = -10897.328125
Iteration 5700: Loss = -10897.2646484375
Iteration 5800: Loss = -10897.203125
Iteration 5900: Loss = -10897.1474609375
Iteration 6000: Loss = -10897.0947265625
Iteration 6100: Loss = -10897.0439453125
Iteration 6200: Loss = -10896.998046875
Iteration 6300: Loss = -10896.953125
Iteration 6400: Loss = -10896.912109375
Iteration 6500: Loss = -10896.873046875
Iteration 6600: Loss = -10896.8359375
Iteration 6700: Loss = -10896.7998046875
Iteration 6800: Loss = -10896.7666015625
Iteration 6900: Loss = -10896.7353515625
Iteration 7000: Loss = -10896.7041015625
Iteration 7100: Loss = -10896.67578125
Iteration 7200: Loss = -10896.6494140625
Iteration 7300: Loss = -10896.625
Iteration 7400: Loss = -10896.6005859375
Iteration 7500: Loss = -10896.578125
Iteration 7600: Loss = -10896.556640625
Iteration 7700: Loss = -10896.5361328125
Iteration 7800: Loss = -10896.515625
Iteration 7900: Loss = -10896.498046875
Iteration 8000: Loss = -10896.4814453125
Iteration 8100: Loss = -10896.4638671875
Iteration 8200: Loss = -10896.4482421875
Iteration 8300: Loss = -10896.43359375
Iteration 8400: Loss = -10896.41796875
Iteration 8500: Loss = -10896.40625
Iteration 8600: Loss = -10896.3935546875
Iteration 8700: Loss = -10896.380859375
Iteration 8800: Loss = -10896.3701171875
Iteration 8900: Loss = -10896.3583984375
Iteration 9000: Loss = -10896.3486328125
Iteration 9100: Loss = -10896.3369140625
Iteration 9200: Loss = -10896.328125
Iteration 9300: Loss = -10896.3173828125
Iteration 9400: Loss = -10896.30859375
Iteration 9500: Loss = -10896.2978515625
Iteration 9600: Loss = -10896.287109375
Iteration 9700: Loss = -10896.271484375
Iteration 9800: Loss = -10896.2529296875
Iteration 9900: Loss = -10896.2421875
Iteration 10000: Loss = -10896.2333984375
Iteration 10100: Loss = -10896.2255859375
Iteration 10200: Loss = -10896.21875
Iteration 10300: Loss = -10896.2138671875
Iteration 10400: Loss = -10896.20703125
Iteration 10500: Loss = -10896.2021484375
Iteration 10600: Loss = -10896.197265625
Iteration 10700: Loss = -10896.1904296875
Iteration 10800: Loss = -10896.1884765625
Iteration 10900: Loss = -10896.1826171875
Iteration 11000: Loss = -10896.1796875
Iteration 11100: Loss = -10896.173828125
Iteration 11200: Loss = -10896.171875
Iteration 11300: Loss = -10896.16796875
Iteration 11400: Loss = -10896.1640625
Iteration 11500: Loss = -10896.1611328125
Iteration 11600: Loss = -10896.1572265625
Iteration 11700: Loss = -10896.1552734375
Iteration 11800: Loss = -10896.1513671875
Iteration 11900: Loss = -10896.1494140625
Iteration 12000: Loss = -10896.1474609375
Iteration 12100: Loss = -10896.14453125
Iteration 12200: Loss = -10896.140625
Iteration 12300: Loss = -10896.140625
Iteration 12400: Loss = -10896.1376953125
Iteration 12500: Loss = -10896.1357421875
Iteration 12600: Loss = -10896.1337890625
Iteration 12700: Loss = -10896.1318359375
Iteration 12800: Loss = -10896.1298828125
Iteration 12900: Loss = -10896.12890625
Iteration 13000: Loss = -10896.12890625
Iteration 13100: Loss = -10896.126953125
Iteration 13200: Loss = -10896.1240234375
Iteration 13300: Loss = -10896.1240234375
Iteration 13400: Loss = -10896.1220703125
Iteration 13500: Loss = -10896.12109375
Iteration 13600: Loss = -10896.12109375
Iteration 13700: Loss = -10896.119140625
Iteration 13800: Loss = -10896.1171875
Iteration 13900: Loss = -10896.1171875
Iteration 14000: Loss = -10896.1162109375
Iteration 14100: Loss = -10896.1162109375
Iteration 14200: Loss = -10896.1142578125
Iteration 14300: Loss = -10896.115234375
1
Iteration 14400: Loss = -10896.1142578125
Iteration 14500: Loss = -10896.11328125
Iteration 14600: Loss = -10896.1123046875
Iteration 14700: Loss = -10896.1123046875
Iteration 14800: Loss = -10896.111328125
Iteration 14900: Loss = -10896.111328125
Iteration 15000: Loss = -10896.1083984375
Iteration 15100: Loss = -10896.1083984375
Iteration 15200: Loss = -10896.109375
1
Iteration 15300: Loss = -10896.1083984375
Iteration 15400: Loss = -10896.1083984375
Iteration 15500: Loss = -10896.1083984375
Iteration 15600: Loss = -10896.107421875
Iteration 15700: Loss = -10896.1064453125
Iteration 15800: Loss = -10896.1064453125
Iteration 15900: Loss = -10896.107421875
1
Iteration 16000: Loss = -10896.10546875
Iteration 16100: Loss = -10896.10546875
Iteration 16200: Loss = -10896.1044921875
Iteration 16300: Loss = -10896.10546875
1
Iteration 16400: Loss = -10896.103515625
Iteration 16500: Loss = -10896.1044921875
1
Iteration 16600: Loss = -10896.103515625
Iteration 16700: Loss = -10896.1044921875
1
Iteration 16800: Loss = -10896.1025390625
Iteration 16900: Loss = -10896.103515625
1
Iteration 17000: Loss = -10896.103515625
2
Iteration 17100: Loss = -10896.1015625
Iteration 17200: Loss = -10896.103515625
1
Iteration 17300: Loss = -10896.1025390625
2
Iteration 17400: Loss = -10896.1025390625
3
Iteration 17500: Loss = -10896.1025390625
4
Iteration 17600: Loss = -10896.103515625
5
Iteration 17700: Loss = -10896.1015625
Iteration 17800: Loss = -10896.1025390625
1
Iteration 17900: Loss = -10896.1015625
Iteration 18000: Loss = -10896.1015625
Iteration 18100: Loss = -10896.1025390625
1
Iteration 18200: Loss = -10896.1005859375
Iteration 18300: Loss = -10896.1015625
1
Iteration 18400: Loss = -10896.1005859375
Iteration 18500: Loss = -10896.1005859375
Iteration 18600: Loss = -10896.1015625
1
Iteration 18700: Loss = -10896.1005859375
Iteration 18800: Loss = -10896.1005859375
Iteration 18900: Loss = -10896.1025390625
1
Iteration 19000: Loss = -10896.1025390625
2
Iteration 19100: Loss = -10896.1005859375
Iteration 19200: Loss = -10896.1015625
1
Iteration 19300: Loss = -10896.1005859375
Iteration 19400: Loss = -10896.099609375
Iteration 19500: Loss = -10896.099609375
Iteration 19600: Loss = -10896.1005859375
1
Iteration 19700: Loss = -10896.099609375
Iteration 19800: Loss = -10896.099609375
Iteration 19900: Loss = -10896.1005859375
1
Iteration 20000: Loss = -10896.0986328125
Iteration 20100: Loss = -10896.103515625
1
Iteration 20200: Loss = -10896.1005859375
2
Iteration 20300: Loss = -10896.099609375
3
Iteration 20400: Loss = -10896.0986328125
Iteration 20500: Loss = -10896.099609375
1
Iteration 20600: Loss = -10896.1015625
2
Iteration 20700: Loss = -10896.1005859375
3
Iteration 20800: Loss = -10896.1005859375
4
Iteration 20900: Loss = -10896.099609375
5
Iteration 21000: Loss = -10896.1015625
6
Iteration 21100: Loss = -10896.099609375
7
Iteration 21200: Loss = -10896.1005859375
8
Iteration 21300: Loss = -10896.1015625
9
Iteration 21400: Loss = -10896.0986328125
Iteration 21500: Loss = -10896.099609375
1
Iteration 21600: Loss = -10896.1015625
2
Iteration 21700: Loss = -10896.1005859375
3
Iteration 21800: Loss = -10896.1005859375
4
Iteration 21900: Loss = -10896.1015625
5
Iteration 22000: Loss = -10896.107421875
6
Iteration 22100: Loss = -10896.099609375
7
Iteration 22200: Loss = -10896.1005859375
8
Iteration 22300: Loss = -10896.1005859375
9
Iteration 22400: Loss = -10896.1015625
10
Iteration 22500: Loss = -10896.099609375
11
Iteration 22600: Loss = -10896.1005859375
12
Iteration 22700: Loss = -10896.1005859375
13
Iteration 22800: Loss = -10896.1005859375
14
Iteration 22900: Loss = -10896.1005859375
15
Stopping early at iteration 22900 due to no improvement.
pi: tensor([[1.2513e-06, 1.0000e+00],
        [1.2517e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0451, 0.9549], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2719, 0.2194],
         [0.8976, 0.1593]],

        [[0.0366, 0.3280],
         [0.0786, 0.6275]],

        [[0.8266, 0.8296],
         [0.9669, 0.9924]],

        [[0.0250, 0.1609],
         [0.0159, 0.7561]],

        [[0.9876, 0.1796],
         [0.0076, 0.0134]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.06233900241413372, 0.0] [0.12298572744392207, 0.0] [10888.2001953125, 10896.1005859375]
-------------------------------------
This iteration is 42
True Objective function: Loss = -10860.43023181403
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36382.1171875
Iteration 100: Loss = -21119.322265625
Iteration 200: Loss = -13083.546875
Iteration 300: Loss = -11660.828125
Iteration 400: Loss = -11381.599609375
Iteration 500: Loss = -11267.0439453125
Iteration 600: Loss = -11204.1455078125
Iteration 700: Loss = -11164.65234375
Iteration 800: Loss = -11133.7431640625
Iteration 900: Loss = -11108.0927734375
Iteration 1000: Loss = -11075.92578125
Iteration 1100: Loss = -11054.892578125
Iteration 1200: Loss = -11043.2783203125
Iteration 1300: Loss = -11036.5419921875
Iteration 1400: Loss = -11027.4208984375
Iteration 1500: Loss = -11023.6025390625
Iteration 1600: Loss = -11020.564453125
Iteration 1700: Loss = -11018.0390625
Iteration 1800: Loss = -11011.154296875
Iteration 1900: Loss = -11009.2880859375
Iteration 2000: Loss = -11007.7509765625
Iteration 2100: Loss = -11006.419921875
Iteration 2200: Loss = -11005.2548828125
Iteration 2300: Loss = -11004.224609375
Iteration 2400: Loss = -11003.310546875
Iteration 2500: Loss = -11002.4951171875
Iteration 2600: Loss = -11001.7646484375
Iteration 2700: Loss = -11001.10546875
Iteration 2800: Loss = -11000.5068359375
Iteration 2900: Loss = -10999.95703125
Iteration 3000: Loss = -10999.455078125
Iteration 3100: Loss = -10998.9951171875
Iteration 3200: Loss = -10998.572265625
Iteration 3300: Loss = -10998.1796875
Iteration 3400: Loss = -10997.814453125
Iteration 3500: Loss = -10997.4716796875
Iteration 3600: Loss = -10997.150390625
Iteration 3700: Loss = -10996.8408203125
Iteration 3800: Loss = -10996.3818359375
Iteration 3900: Loss = -10993.8056640625
Iteration 4000: Loss = -10993.5126953125
Iteration 4100: Loss = -10993.2763671875
Iteration 4200: Loss = -10993.06640625
Iteration 4300: Loss = -10992.876953125
Iteration 4400: Loss = -10992.69921875
Iteration 4500: Loss = -10992.533203125
Iteration 4600: Loss = -10992.3681640625
Iteration 4700: Loss = -10991.09375
Iteration 4800: Loss = -10990.0361328125
Iteration 4900: Loss = -10988.978515625
Iteration 5000: Loss = -10987.0126953125
Iteration 5100: Loss = -10986.76953125
Iteration 5200: Loss = -10986.626953125
Iteration 5300: Loss = -10986.5126953125
Iteration 5400: Loss = -10986.4091796875
Iteration 5500: Loss = -10986.3154296875
Iteration 5600: Loss = -10986.2275390625
Iteration 5700: Loss = -10986.1435546875
Iteration 5800: Loss = -10986.0615234375
Iteration 5900: Loss = -10985.9873046875
Iteration 6000: Loss = -10985.9140625
Iteration 6100: Loss = -10985.8486328125
Iteration 6200: Loss = -10985.7822265625
Iteration 6300: Loss = -10985.7197265625
Iteration 6400: Loss = -10985.66015625
Iteration 6500: Loss = -10985.5986328125
Iteration 6600: Loss = -10985.537109375
Iteration 6700: Loss = -10985.4755859375
Iteration 6800: Loss = -10985.4111328125
Iteration 6900: Loss = -10985.3232421875
Iteration 7000: Loss = -10983.5947265625
Iteration 7100: Loss = -10983.4599609375
Iteration 7200: Loss = -10981.84375
Iteration 7300: Loss = -10979.17578125
Iteration 7400: Loss = -10978.98046875
Iteration 7500: Loss = -10978.8759765625
Iteration 7600: Loss = -10978.798828125
Iteration 7700: Loss = -10978.7353515625
Iteration 7800: Loss = -10978.6787109375
Iteration 7900: Loss = -10978.630859375
Iteration 8000: Loss = -10978.5888671875
Iteration 8100: Loss = -10978.5537109375
Iteration 8200: Loss = -10978.521484375
Iteration 8300: Loss = -10978.4921875
Iteration 8400: Loss = -10978.466796875
Iteration 8500: Loss = -10978.4443359375
Iteration 8600: Loss = -10978.423828125
Iteration 8700: Loss = -10978.4052734375
Iteration 8800: Loss = -10978.388671875
Iteration 8900: Loss = -10978.373046875
Iteration 9000: Loss = -10978.357421875
Iteration 9100: Loss = -10978.345703125
Iteration 9200: Loss = -10978.3330078125
Iteration 9300: Loss = -10978.3203125
Iteration 9400: Loss = -10978.3076171875
Iteration 9500: Loss = -10978.29296875
Iteration 9600: Loss = -10978.2783203125
Iteration 9700: Loss = -10978.2666015625
Iteration 9800: Loss = -10978.2578125
Iteration 9900: Loss = -10978.248046875
Iteration 10000: Loss = -10978.2353515625
Iteration 10100: Loss = -10978.2275390625
Iteration 10200: Loss = -10978.2197265625
Iteration 10300: Loss = -10978.212890625
Iteration 10400: Loss = -10978.20703125
Iteration 10500: Loss = -10978.19921875
Iteration 10600: Loss = -10978.1943359375
Iteration 10700: Loss = -10978.19140625
Iteration 10800: Loss = -10978.1865234375
Iteration 10900: Loss = -10978.123046875
Iteration 11000: Loss = -10974.4013671875
Iteration 11100: Loss = -10974.3349609375
Iteration 11200: Loss = -10974.3076171875
Iteration 11300: Loss = -10974.2919921875
Iteration 11400: Loss = -10974.2822265625
Iteration 11500: Loss = -10974.2724609375
Iteration 11600: Loss = -10974.2666015625
Iteration 11700: Loss = -10974.259765625
Iteration 11800: Loss = -10974.25390625
Iteration 11900: Loss = -10974.2490234375
Iteration 12000: Loss = -10974.24609375
Iteration 12100: Loss = -10974.2412109375
Iteration 12200: Loss = -10974.23828125
Iteration 12300: Loss = -10974.2333984375
Iteration 12400: Loss = -10974.23046875
Iteration 12500: Loss = -10974.228515625
Iteration 12600: Loss = -10974.224609375
Iteration 12700: Loss = -10974.220703125
Iteration 12800: Loss = -10974.2158203125
Iteration 12900: Loss = -10974.20703125
Iteration 13000: Loss = -10974.1689453125
Iteration 13100: Loss = -10974.14453125
Iteration 13200: Loss = -10974.13671875
Iteration 13300: Loss = -10974.1171875
Iteration 13400: Loss = -10974.052734375
Iteration 13500: Loss = -10973.9462890625
Iteration 13600: Loss = -10973.8291015625
Iteration 13700: Loss = -10973.7119140625
Iteration 13800: Loss = -10973.666015625
Iteration 13900: Loss = -10973.6298828125
Iteration 14000: Loss = -10973.603515625
Iteration 14100: Loss = -10973.5859375
Iteration 14200: Loss = -10973.5673828125
Iteration 14300: Loss = -10973.552734375
Iteration 14400: Loss = -10973.4453125
Iteration 14500: Loss = -10972.03515625
Iteration 14600: Loss = -10971.041015625
Iteration 14700: Loss = -10966.873046875
Iteration 14800: Loss = -10962.8984375
Iteration 14900: Loss = -10961.9404296875
Iteration 15000: Loss = -10954.5458984375
Iteration 15100: Loss = -10952.62109375
Iteration 15200: Loss = -10943.7705078125
Iteration 15300: Loss = -10943.0595703125
Iteration 15400: Loss = -10942.1435546875
Iteration 15500: Loss = -10941.9140625
Iteration 15600: Loss = -10940.376953125
Iteration 15700: Loss = -10940.3603515625
Iteration 15800: Loss = -10940.326171875
Iteration 15900: Loss = -10940.2099609375
Iteration 16000: Loss = -10934.271484375
Iteration 16100: Loss = -10933.966796875
Iteration 16200: Loss = -10933.9453125
Iteration 16300: Loss = -10933.9365234375
Iteration 16400: Loss = -10933.931640625
Iteration 16500: Loss = -10933.9267578125
Iteration 16600: Loss = -10933.923828125
Iteration 16700: Loss = -10933.921875
Iteration 16800: Loss = -10933.919921875
Iteration 16900: Loss = -10933.919921875
Iteration 17000: Loss = -10933.8798828125
Iteration 17100: Loss = -10933.8779296875
Iteration 17200: Loss = -10933.8779296875
Iteration 17300: Loss = -10933.8759765625
Iteration 17400: Loss = -10933.8759765625
Iteration 17500: Loss = -10933.8759765625
Iteration 17600: Loss = -10933.873046875
Iteration 17700: Loss = -10933.51171875
Iteration 17800: Loss = -10933.478515625
Iteration 17900: Loss = -10933.4462890625
Iteration 18000: Loss = -10933.4443359375
Iteration 18100: Loss = -10933.4296875
Iteration 18200: Loss = -10933.427734375
Iteration 18300: Loss = -10933.42578125
Iteration 18400: Loss = -10933.423828125
Iteration 18500: Loss = -10933.4228515625
Iteration 18600: Loss = -10933.4189453125
Iteration 18700: Loss = -10933.416015625
Iteration 18800: Loss = -10933.40625
Iteration 18900: Loss = -10933.376953125
Iteration 19000: Loss = -10933.365234375
Iteration 19100: Loss = -10933.32421875
Iteration 19200: Loss = -10933.31640625
Iteration 19300: Loss = -10933.3017578125
Iteration 19400: Loss = -10933.2880859375
Iteration 19500: Loss = -10933.2509765625
Iteration 19600: Loss = -10933.2158203125
Iteration 19700: Loss = -10933.1904296875
Iteration 19800: Loss = -10933.1806640625
Iteration 19900: Loss = -10933.1748046875
Iteration 20000: Loss = -10933.1728515625
Iteration 20100: Loss = -10933.171875
Iteration 20200: Loss = -10933.150390625
Iteration 20300: Loss = -10933.0966796875
Iteration 20400: Loss = -10933.0888671875
Iteration 20500: Loss = -10933.0654296875
Iteration 20600: Loss = -10933.048828125
Iteration 20700: Loss = -10933.0478515625
Iteration 20800: Loss = -10933.0439453125
Iteration 20900: Loss = -10933.0419921875
Iteration 21000: Loss = -10933.0400390625
Iteration 21100: Loss = -10933.037109375
Iteration 21200: Loss = -10933.037109375
Iteration 21300: Loss = -10933.037109375
Iteration 21400: Loss = -10933.037109375
Iteration 21500: Loss = -10933.0361328125
Iteration 21600: Loss = -10933.03515625
Iteration 21700: Loss = -10933.03515625
Iteration 21800: Loss = -10933.03515625
Iteration 21900: Loss = -10933.0341796875
Iteration 22000: Loss = -10933.033203125
Iteration 22100: Loss = -10933.013671875
Iteration 22200: Loss = -10932.3408203125
Iteration 22300: Loss = -10932.3408203125
Iteration 22400: Loss = -10932.33984375
Iteration 22500: Loss = -10932.3408203125
1
Iteration 22600: Loss = -10932.3408203125
2
Iteration 22700: Loss = -10932.33984375
Iteration 22800: Loss = -10932.3408203125
1
Iteration 22900: Loss = -10932.33984375
Iteration 23000: Loss = -10932.33984375
Iteration 23100: Loss = -10932.33984375
Iteration 23200: Loss = -10932.341796875
1
Iteration 23300: Loss = -10932.3408203125
2
Iteration 23400: Loss = -10932.33984375
Iteration 23500: Loss = -10932.33984375
Iteration 23600: Loss = -10932.3408203125
1
Iteration 23700: Loss = -10932.33984375
Iteration 23800: Loss = -10932.33984375
Iteration 23900: Loss = -10932.33984375
Iteration 24000: Loss = -10932.33984375
Iteration 24100: Loss = -10932.3408203125
1
Iteration 24200: Loss = -10932.33984375
Iteration 24300: Loss = -10932.33984375
Iteration 24400: Loss = -10932.33984375
Iteration 24500: Loss = -10932.3408203125
1
Iteration 24600: Loss = -10932.3388671875
Iteration 24700: Loss = -10932.3359375
Iteration 24800: Loss = -10932.3369140625
1
Iteration 24900: Loss = -10932.337890625
2
Iteration 25000: Loss = -10932.3271484375
Iteration 25100: Loss = -10932.326171875
Iteration 25200: Loss = -10932.326171875
Iteration 25300: Loss = -10932.326171875
Iteration 25400: Loss = -10932.3251953125
Iteration 25500: Loss = -10932.3271484375
1
Iteration 25600: Loss = -10932.326171875
2
Iteration 25700: Loss = -10932.3271484375
3
Iteration 25800: Loss = -10932.3251953125
Iteration 25900: Loss = -10932.3271484375
1
Iteration 26000: Loss = -10932.326171875
2
Iteration 26100: Loss = -10932.3291015625
3
Iteration 26200: Loss = -10932.3271484375
4
Iteration 26300: Loss = -10932.326171875
5
Iteration 26400: Loss = -10932.326171875
6
Iteration 26500: Loss = -10932.3271484375
7
Iteration 26600: Loss = -10932.3271484375
8
Iteration 26700: Loss = -10932.326171875
9
Iteration 26800: Loss = -10932.3271484375
10
Iteration 26900: Loss = -10932.3271484375
11
Iteration 27000: Loss = -10932.326171875
12
Iteration 27100: Loss = -10932.3271484375
13
Iteration 27200: Loss = -10932.3251953125
Iteration 27300: Loss = -10932.3271484375
1
Iteration 27400: Loss = -10932.328125
2
Iteration 27500: Loss = -10932.326171875
3
Iteration 27600: Loss = -10932.3271484375
4
Iteration 27700: Loss = -10932.326171875
5
Iteration 27800: Loss = -10932.3271484375
6
Iteration 27900: Loss = -10932.3271484375
7
Iteration 28000: Loss = -10932.326171875
8
Iteration 28100: Loss = -10932.3271484375
9
Iteration 28200: Loss = -10932.3271484375
10
Iteration 28300: Loss = -10932.326171875
11
Iteration 28400: Loss = -10932.326171875
12
Iteration 28500: Loss = -10932.326171875
13
Iteration 28600: Loss = -10932.328125
14
Iteration 28700: Loss = -10932.326171875
15
Stopping early at iteration 28700 due to no improvement.
pi: tensor([[0.0386, 0.9614],
        [0.0104, 0.9896]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4709, 0.5291], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2933, 0.0976],
         [0.5856, 0.1619]],

        [[0.0341, 0.2597],
         [0.0717, 0.0111]],

        [[0.0250, 0.2210],
         [0.5047, 0.6964]],

        [[0.0578, 0.2459],
         [0.8867, 0.0242]],

        [[0.9856, 0.0839],
         [0.9519, 0.0661]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721116882917585
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.03013938036385293
Average Adjusted Rand Index: 0.154265414630698
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35767.578125
Iteration 100: Loss = -25588.654296875
Iteration 200: Loss = -15737.98046875
Iteration 300: Loss = -12331.16015625
Iteration 400: Loss = -11475.9326171875
Iteration 500: Loss = -11299.76171875
Iteration 600: Loss = -11222.177734375
Iteration 700: Loss = -11175.1845703125
Iteration 800: Loss = -11138.421875
Iteration 900: Loss = -11108.8828125
Iteration 1000: Loss = -11087.4482421875
Iteration 1100: Loss = -11074.142578125
Iteration 1200: Loss = -11065.3427734375
Iteration 1300: Loss = -11058.1884765625
Iteration 1400: Loss = -11052.1943359375
Iteration 1500: Loss = -11046.044921875
Iteration 1600: Loss = -11042.3818359375
Iteration 1700: Loss = -11039.578125
Iteration 1800: Loss = -11037.248046875
Iteration 1900: Loss = -11035.2666015625
Iteration 2000: Loss = -11033.5615234375
Iteration 2100: Loss = -11032.08984375
Iteration 2200: Loss = -11030.8095703125
Iteration 2300: Loss = -11029.685546875
Iteration 2400: Loss = -11028.69140625
Iteration 2500: Loss = -11027.806640625
Iteration 2600: Loss = -11027.0185546875
Iteration 2700: Loss = -11026.310546875
Iteration 2800: Loss = -11025.671875
Iteration 2900: Loss = -11025.095703125
Iteration 3000: Loss = -11024.572265625
Iteration 3100: Loss = -11024.0947265625
Iteration 3200: Loss = -11023.6591796875
Iteration 3300: Loss = -11023.263671875
Iteration 3400: Loss = -11022.8974609375
Iteration 3500: Loss = -11022.5625
Iteration 3600: Loss = -11022.25390625
Iteration 3700: Loss = -11021.9697265625
Iteration 3800: Loss = -11021.70703125
Iteration 3900: Loss = -11021.462890625
Iteration 4000: Loss = -11021.2373046875
Iteration 4100: Loss = -11021.02734375
Iteration 4200: Loss = -11020.833984375
Iteration 4300: Loss = -11020.65234375
Iteration 4400: Loss = -11020.4833984375
Iteration 4500: Loss = -11020.3251953125
Iteration 4600: Loss = -11020.177734375
Iteration 4700: Loss = -11020.0322265625
Iteration 4800: Loss = -11019.9013671875
Iteration 4900: Loss = -11019.771484375
Iteration 5000: Loss = -11019.5771484375
Iteration 5100: Loss = -11019.4404296875
Iteration 5200: Loss = -11019.337890625
Iteration 5300: Loss = -11019.2431640625
Iteration 5400: Loss = -11019.1533203125
Iteration 5500: Loss = -11019.072265625
Iteration 5600: Loss = -11018.994140625
Iteration 5700: Loss = -11018.919921875
Iteration 5800: Loss = -11018.8525390625
Iteration 5900: Loss = -11018.7880859375
Iteration 6000: Loss = -11018.7255859375
Iteration 6100: Loss = -11018.669921875
Iteration 6200: Loss = -11018.615234375
Iteration 6300: Loss = -11018.5625
Iteration 6400: Loss = -11018.5166015625
Iteration 6500: Loss = -11018.470703125
Iteration 6600: Loss = -11018.427734375
Iteration 6700: Loss = -11018.38671875
Iteration 6800: Loss = -11018.3486328125
Iteration 6900: Loss = -11018.3125
Iteration 7000: Loss = -11018.279296875
Iteration 7100: Loss = -11018.2470703125
Iteration 7200: Loss = -11018.216796875
Iteration 7300: Loss = -11018.1865234375
Iteration 7400: Loss = -11018.158203125
Iteration 7500: Loss = -11018.1337890625
Iteration 7600: Loss = -11018.1103515625
Iteration 7700: Loss = -11018.0849609375
Iteration 7800: Loss = -11018.064453125
Iteration 7900: Loss = -11018.04296875
Iteration 8000: Loss = -11018.0234375
Iteration 8100: Loss = -11018.00390625
Iteration 8200: Loss = -11017.986328125
Iteration 8300: Loss = -11017.9677734375
Iteration 8400: Loss = -11017.953125
Iteration 8500: Loss = -11017.9375
Iteration 8600: Loss = -11017.9228515625
Iteration 8700: Loss = -11017.9111328125
Iteration 8800: Loss = -11017.896484375
Iteration 8900: Loss = -11017.884765625
Iteration 9000: Loss = -11017.87109375
Iteration 9100: Loss = -11017.8603515625
Iteration 9200: Loss = -11017.8505859375
Iteration 9300: Loss = -11017.8388671875
Iteration 9400: Loss = -11017.8310546875
Iteration 9500: Loss = -11017.8203125
Iteration 9600: Loss = -11017.8095703125
Iteration 9700: Loss = -11017.80078125
Iteration 9800: Loss = -11017.7900390625
Iteration 9900: Loss = -11017.76953125
Iteration 10000: Loss = -11017.662109375
Iteration 10100: Loss = -11014.033203125
Iteration 10200: Loss = -11013.984375
Iteration 10300: Loss = -11013.09765625
Iteration 10400: Loss = -11013.0576171875
Iteration 10500: Loss = -11013.046875
Iteration 10600: Loss = -11012.755859375
Iteration 10700: Loss = -11012.7109375
Iteration 10800: Loss = -11012.1083984375
Iteration 10900: Loss = -11010.513671875
Iteration 11000: Loss = -11010.0478515625
Iteration 11100: Loss = -11008.814453125
Iteration 11200: Loss = -11008.181640625
Iteration 11300: Loss = -11007.595703125
Iteration 11400: Loss = -11007.4150390625
Iteration 11500: Loss = -11006.876953125
Iteration 11600: Loss = -11006.2001953125
Iteration 11700: Loss = -11005.3544921875
Iteration 11800: Loss = -11004.4609375
Iteration 11900: Loss = -11004.123046875
Iteration 12000: Loss = -11002.943359375
Iteration 12100: Loss = -11002.291015625
Iteration 12200: Loss = -11000.6845703125
Iteration 12300: Loss = -11000.1328125
Iteration 12400: Loss = -10998.2822265625
Iteration 12500: Loss = -10996.833984375
Iteration 12600: Loss = -10994.3876953125
Iteration 12700: Loss = -10990.2783203125
Iteration 12800: Loss = -10989.9072265625
Iteration 12900: Loss = -10987.8671875
Iteration 13000: Loss = -10985.853515625
Iteration 13100: Loss = -10985.82421875
Iteration 13200: Loss = -10985.21484375
Iteration 13300: Loss = -10985.1669921875
Iteration 13400: Loss = -10985.162109375
Iteration 13500: Loss = -10985.0419921875
Iteration 13600: Loss = -10985.0185546875
Iteration 13700: Loss = -10984.90625
Iteration 13800: Loss = -10984.8359375
Iteration 13900: Loss = -10984.833984375
Iteration 14000: Loss = -10984.8310546875
Iteration 14100: Loss = -10984.830078125
Iteration 14200: Loss = -10984.826171875
Iteration 14300: Loss = -10984.8134765625
Iteration 14400: Loss = -10984.8076171875
Iteration 14500: Loss = -10984.8046875
Iteration 14600: Loss = -10984.7705078125
Iteration 14700: Loss = -10983.1123046875
Iteration 14800: Loss = -10983.109375
Iteration 14900: Loss = -10983.1103515625
1
Iteration 15000: Loss = -10983.109375
Iteration 15100: Loss = -10983.1083984375
Iteration 15200: Loss = -10983.107421875
Iteration 15300: Loss = -10983.107421875
Iteration 15400: Loss = -10983.107421875
Iteration 15500: Loss = -10983.0986328125
Iteration 15600: Loss = -10980.916015625
Iteration 15700: Loss = -10980.916015625
Iteration 15800: Loss = -10980.9150390625
Iteration 15900: Loss = -10980.9150390625
Iteration 16000: Loss = -10980.9150390625
Iteration 16100: Loss = -10980.8583984375
Iteration 16200: Loss = -10977.4150390625
Iteration 16300: Loss = -10977.4033203125
Iteration 16400: Loss = -10974.931640625
Iteration 16500: Loss = -10974.91796875
Iteration 16600: Loss = -10974.9169921875
Iteration 16700: Loss = -10974.916015625
Iteration 16800: Loss = -10974.916015625
Iteration 16900: Loss = -10974.916015625
Iteration 17000: Loss = -10974.916015625
Iteration 17100: Loss = -10974.916015625
Iteration 17200: Loss = -10974.916015625
Iteration 17300: Loss = -10974.916015625
Iteration 17400: Loss = -10974.916015625
Iteration 17500: Loss = -10974.9150390625
Iteration 17600: Loss = -10974.9150390625
Iteration 17700: Loss = -10974.9150390625
Iteration 17800: Loss = -10974.9150390625
Iteration 17900: Loss = -10974.9150390625
Iteration 18000: Loss = -10974.86328125
Iteration 18100: Loss = -10973.8427734375
Iteration 18200: Loss = -10970.8505859375
Iteration 18300: Loss = -10970.8388671875
Iteration 18400: Loss = -10970.8359375
Iteration 18500: Loss = -10970.8349609375
Iteration 18600: Loss = -10970.833984375
Iteration 18700: Loss = -10970.83203125
Iteration 18800: Loss = -10970.8330078125
1
Iteration 18900: Loss = -10970.8330078125
2
Iteration 19000: Loss = -10968.2001953125
Iteration 19100: Loss = -10968.1572265625
Iteration 19200: Loss = -10968.1572265625
Iteration 19300: Loss = -10968.1572265625
Iteration 19400: Loss = -10968.1572265625
Iteration 19500: Loss = -10968.1572265625
Iteration 19600: Loss = -10968.158203125
1
Iteration 19700: Loss = -10968.1572265625
Iteration 19800: Loss = -10968.15234375
Iteration 19900: Loss = -10968.1533203125
1
Iteration 20000: Loss = -10968.1513671875
Iteration 20100: Loss = -10968.15234375
1
Iteration 20200: Loss = -10968.1533203125
2
Iteration 20300: Loss = -10968.15234375
3
Iteration 20400: Loss = -10968.1533203125
4
Iteration 20500: Loss = -10968.15234375
5
Iteration 20600: Loss = -10968.15234375
6
Iteration 20700: Loss = -10968.15234375
7
Iteration 20800: Loss = -10968.150390625
Iteration 20900: Loss = -10968.1513671875
1
Iteration 21000: Loss = -10968.1533203125
2
Iteration 21100: Loss = -10968.15234375
3
Iteration 21200: Loss = -10968.1513671875
4
Iteration 21300: Loss = -10968.1533203125
5
Iteration 21400: Loss = -10968.15234375
6
Iteration 21500: Loss = -10968.15234375
7
Iteration 21600: Loss = -10968.1533203125
8
Iteration 21700: Loss = -10968.1533203125
9
Iteration 21800: Loss = -10968.15234375
10
Iteration 21900: Loss = -10968.154296875
11
Iteration 22000: Loss = -10968.150390625
Iteration 22100: Loss = -10968.1533203125
1
Iteration 22200: Loss = -10968.154296875
2
Iteration 22300: Loss = -10968.15234375
3
Iteration 22400: Loss = -10968.154296875
4
Iteration 22500: Loss = -10968.15234375
5
Iteration 22600: Loss = -10968.15234375
6
Iteration 22700: Loss = -10968.15234375
7
Iteration 22800: Loss = -10968.15234375
8
Iteration 22900: Loss = -10968.1533203125
9
Iteration 23000: Loss = -10968.15234375
10
Iteration 23100: Loss = -10968.1533203125
11
Iteration 23200: Loss = -10968.15234375
12
Iteration 23300: Loss = -10968.154296875
13
Iteration 23400: Loss = -10968.1513671875
14
Iteration 23500: Loss = -10968.1533203125
15
Stopping early at iteration 23500 due to no improvement.
pi: tensor([[4.4628e-01, 5.5372e-01],
        [1.0000e+00, 1.7289e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.9239e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1836, 0.2121],
         [0.9817, 0.1854]],

        [[0.0130, 0.7264],
         [0.9823, 0.0148]],

        [[0.0481, 0.1186],
         [0.0525, 0.0333]],

        [[0.8072, 0.1005],
         [0.6292, 0.9829]],

        [[0.7950, 0.1004],
         [0.2312, 0.7940]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 14
Adjusted Rand Index: 0.513657303929177
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 17
Adjusted Rand Index: 0.4305549236948734
Global Adjusted Rand Index: 0.005089434398834984
Average Adjusted Rand Index: 0.3578092244028239
[0.03013938036385293, 0.005089434398834984] [0.154265414630698, 0.3578092244028239] [10932.326171875, 10968.1533203125]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11052.085874680504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25252.142578125
Iteration 100: Loss = -17273.998046875
Iteration 200: Loss = -12335.7705078125
Iteration 300: Loss = -11506.2568359375
Iteration 400: Loss = -11343.9375
Iteration 500: Loss = -11280.2392578125
Iteration 600: Loss = -11248.1416015625
Iteration 700: Loss = -11228.123046875
Iteration 800: Loss = -11213.6796875
Iteration 900: Loss = -11201.6005859375
Iteration 1000: Loss = -11190.96484375
Iteration 1100: Loss = -11181.447265625
Iteration 1200: Loss = -11175.8671875
Iteration 1300: Loss = -11171.8935546875
Iteration 1400: Loss = -11168.912109375
Iteration 1500: Loss = -11165.41796875
Iteration 1600: Loss = -11162.296875
Iteration 1700: Loss = -11159.14453125
Iteration 1800: Loss = -11155.8271484375
Iteration 1900: Loss = -11152.6142578125
Iteration 2000: Loss = -11149.83203125
Iteration 2100: Loss = -11146.931640625
Iteration 2200: Loss = -11141.5302734375
Iteration 2300: Loss = -11138.330078125
Iteration 2400: Loss = -11136.5458984375
Iteration 2500: Loss = -11135.673828125
Iteration 2600: Loss = -11134.990234375
Iteration 2700: Loss = -11133.93359375
Iteration 2800: Loss = -11132.0458984375
Iteration 2900: Loss = -11131.4814453125
Iteration 3000: Loss = -11130.5234375
Iteration 3100: Loss = -11129.529296875
Iteration 3200: Loss = -11128.625
Iteration 3300: Loss = -11127.8505859375
Iteration 3400: Loss = -11127.1708984375
Iteration 3500: Loss = -11126.814453125
Iteration 3600: Loss = -11126.529296875
Iteration 3700: Loss = -11126.228515625
Iteration 3800: Loss = -11125.7626953125
Iteration 3900: Loss = -11125.1494140625
Iteration 4000: Loss = -11124.7763671875
Iteration 4100: Loss = -11122.447265625
Iteration 4200: Loss = -11122.224609375
Iteration 4300: Loss = -11122.076171875
Iteration 4400: Loss = -11121.9375
Iteration 4500: Loss = -11119.7451171875
Iteration 4600: Loss = -11119.421875
Iteration 4700: Loss = -11119.2685546875
Iteration 4800: Loss = -11119.1435546875
Iteration 4900: Loss = -11119.0361328125
Iteration 5000: Loss = -11118.9462890625
Iteration 5100: Loss = -11118.8681640625
Iteration 5200: Loss = -11118.7978515625
Iteration 5300: Loss = -11118.7314453125
Iteration 5400: Loss = -11118.669921875
Iteration 5500: Loss = -11118.6103515625
Iteration 5600: Loss = -11118.5517578125
Iteration 5700: Loss = -11118.486328125
Iteration 5800: Loss = -11118.1630859375
Iteration 5900: Loss = -11117.9794921875
Iteration 6000: Loss = -11117.884765625
Iteration 6100: Loss = -11117.8017578125
Iteration 6200: Loss = -11117.7275390625
Iteration 6300: Loss = -11117.66796875
Iteration 6400: Loss = -11117.626953125
Iteration 6500: Loss = -11117.591796875
Iteration 6600: Loss = -11117.564453125
Iteration 6700: Loss = -11117.5419921875
Iteration 6800: Loss = -11117.521484375
Iteration 6900: Loss = -11117.50390625
Iteration 7000: Loss = -11117.48828125
Iteration 7100: Loss = -11117.4755859375
Iteration 7200: Loss = -11117.4619140625
Iteration 7300: Loss = -11117.4521484375
Iteration 7400: Loss = -11117.439453125
Iteration 7500: Loss = -11117.4306640625
Iteration 7600: Loss = -11117.421875
Iteration 7700: Loss = -11117.4140625
Iteration 7800: Loss = -11117.4052734375
Iteration 7900: Loss = -11117.3994140625
Iteration 8000: Loss = -11117.3916015625
Iteration 8100: Loss = -11117.3857421875
Iteration 8200: Loss = -11117.37890625
Iteration 8300: Loss = -11117.3740234375
Iteration 8400: Loss = -11117.3681640625
Iteration 8500: Loss = -11117.36328125
Iteration 8600: Loss = -11117.3583984375
Iteration 8700: Loss = -11117.3525390625
Iteration 8800: Loss = -11117.3505859375
Iteration 8900: Loss = -11117.345703125
Iteration 9000: Loss = -11117.3408203125
Iteration 9100: Loss = -11117.3388671875
Iteration 9200: Loss = -11117.3349609375
Iteration 9300: Loss = -11117.3330078125
Iteration 9400: Loss = -11117.3291015625
Iteration 9500: Loss = -11117.328125
Iteration 9600: Loss = -11117.32421875
Iteration 9700: Loss = -11117.3212890625
Iteration 9800: Loss = -11117.3203125
Iteration 9900: Loss = -11117.31640625
Iteration 10000: Loss = -11117.3134765625
Iteration 10100: Loss = -11117.3115234375
Iteration 10200: Loss = -11117.3115234375
Iteration 10300: Loss = -11117.30859375
Iteration 10400: Loss = -11117.3076171875
Iteration 10500: Loss = -11117.3056640625
Iteration 10600: Loss = -11117.302734375
Iteration 10700: Loss = -11117.3017578125
Iteration 10800: Loss = -11117.30078125
Iteration 10900: Loss = -11117.2998046875
Iteration 11000: Loss = -11117.2978515625
Iteration 11100: Loss = -11117.2958984375
Iteration 11200: Loss = -11117.2958984375
Iteration 11300: Loss = -11117.294921875
Iteration 11400: Loss = -11117.2939453125
Iteration 11500: Loss = -11117.2939453125
Iteration 11600: Loss = -11117.2919921875
Iteration 11700: Loss = -11117.291015625
Iteration 11800: Loss = -11117.2900390625
Iteration 11900: Loss = -11117.2900390625
Iteration 12000: Loss = -11117.2900390625
Iteration 12100: Loss = -11117.2890625
Iteration 12200: Loss = -11117.287109375
Iteration 12300: Loss = -11117.2861328125
Iteration 12400: Loss = -11117.2861328125
Iteration 12500: Loss = -11117.287109375
1
Iteration 12600: Loss = -11117.28515625
Iteration 12700: Loss = -11117.28515625
Iteration 12800: Loss = -11117.2841796875
Iteration 12900: Loss = -11117.28515625
1
Iteration 13000: Loss = -11117.2841796875
Iteration 13100: Loss = -11117.2841796875
Iteration 13200: Loss = -11117.28515625
1
Iteration 13300: Loss = -11117.2822265625
Iteration 13400: Loss = -11117.28125
Iteration 13500: Loss = -11117.283203125
1
Iteration 13600: Loss = -11117.2822265625
2
Iteration 13700: Loss = -11117.2802734375
Iteration 13800: Loss = -11117.28125
1
Iteration 13900: Loss = -11117.28125
2
Iteration 14000: Loss = -11117.28125
3
Iteration 14100: Loss = -11117.2822265625
4
Iteration 14200: Loss = -11117.28125
5
Iteration 14300: Loss = -11117.28125
6
Iteration 14400: Loss = -11117.28125
7
Iteration 14500: Loss = -11117.28125
8
Iteration 14600: Loss = -11117.2802734375
Iteration 14700: Loss = -11117.279296875
Iteration 14800: Loss = -11117.279296875
Iteration 14900: Loss = -11117.2802734375
1
Iteration 15000: Loss = -11117.2783203125
Iteration 15100: Loss = -11117.2802734375
1
Iteration 15200: Loss = -11117.2783203125
Iteration 15300: Loss = -11117.279296875
1
Iteration 15400: Loss = -11117.279296875
2
Iteration 15500: Loss = -11117.279296875
3
Iteration 15600: Loss = -11117.2783203125
Iteration 15700: Loss = -11117.279296875
1
Iteration 15800: Loss = -11117.279296875
2
Iteration 15900: Loss = -11117.2783203125
Iteration 16000: Loss = -11117.2783203125
Iteration 16100: Loss = -11117.27734375
Iteration 16200: Loss = -11117.279296875
1
Iteration 16300: Loss = -11117.2763671875
Iteration 16400: Loss = -11117.275390625
Iteration 16500: Loss = -11117.2734375
Iteration 16600: Loss = -11117.275390625
1
Iteration 16700: Loss = -11117.2763671875
2
Iteration 16800: Loss = -11117.2734375
Iteration 16900: Loss = -11117.2744140625
1
Iteration 17000: Loss = -11117.2744140625
2
Iteration 17100: Loss = -11117.2744140625
3
Iteration 17200: Loss = -11117.275390625
4
Iteration 17300: Loss = -11117.2744140625
5
Iteration 17400: Loss = -11117.2734375
Iteration 17500: Loss = -11117.2744140625
1
Iteration 17600: Loss = -11117.2744140625
2
Iteration 17700: Loss = -11117.2744140625
3
Iteration 17800: Loss = -11117.2734375
Iteration 17900: Loss = -11117.2744140625
1
Iteration 18000: Loss = -11117.275390625
2
Iteration 18100: Loss = -11117.2744140625
3
Iteration 18200: Loss = -11117.275390625
4
Iteration 18300: Loss = -11117.2734375
Iteration 18400: Loss = -11117.2744140625
1
Iteration 18500: Loss = -11117.2744140625
2
Iteration 18600: Loss = -11117.2724609375
Iteration 18700: Loss = -11117.2744140625
1
Iteration 18800: Loss = -11117.2744140625
2
Iteration 18900: Loss = -11117.2744140625
3
Iteration 19000: Loss = -11117.2744140625
4
Iteration 19100: Loss = -11117.275390625
5
Iteration 19200: Loss = -11117.2734375
6
Iteration 19300: Loss = -11117.2734375
7
Iteration 19400: Loss = -11117.2763671875
8
Iteration 19500: Loss = -11117.2724609375
Iteration 19600: Loss = -11117.26953125
Iteration 19700: Loss = -11117.26953125
Iteration 19800: Loss = -11117.26953125
Iteration 19900: Loss = -11117.267578125
Iteration 20000: Loss = -11117.2685546875
1
Iteration 20100: Loss = -11117.267578125
Iteration 20200: Loss = -11117.2685546875
1
Iteration 20300: Loss = -11117.267578125
Iteration 20400: Loss = -11117.265625
Iteration 20500: Loss = -11117.265625
Iteration 20600: Loss = -11117.2666015625
1
Iteration 20700: Loss = -11117.2666015625
2
Iteration 20800: Loss = -11117.265625
Iteration 20900: Loss = -11117.2646484375
Iteration 21000: Loss = -11117.2666015625
1
Iteration 21100: Loss = -11117.2646484375
Iteration 21200: Loss = -11117.265625
1
Iteration 21300: Loss = -11117.2646484375
Iteration 21400: Loss = -11117.2646484375
Iteration 21500: Loss = -11117.265625
1
Iteration 21600: Loss = -11117.2666015625
2
Iteration 21700: Loss = -11117.2646484375
Iteration 21800: Loss = -11117.265625
1
Iteration 21900: Loss = -11117.263671875
Iteration 22000: Loss = -11117.25
Iteration 22100: Loss = -11117.2451171875
Iteration 22200: Loss = -11117.2392578125
Iteration 22300: Loss = -11117.234375
Iteration 22400: Loss = -11117.2353515625
1
Iteration 22500: Loss = -11117.234375
Iteration 22600: Loss = -11117.2353515625
1
Iteration 22700: Loss = -11117.236328125
2
Iteration 22800: Loss = -11117.236328125
3
Iteration 22900: Loss = -11117.234375
Iteration 23000: Loss = -11117.234375
Iteration 23100: Loss = -11117.236328125
1
Iteration 23200: Loss = -11117.236328125
2
Iteration 23300: Loss = -11117.2353515625
3
Iteration 23400: Loss = -11117.2353515625
4
Iteration 23500: Loss = -11117.2353515625
5
Iteration 23600: Loss = -11117.2353515625
6
Iteration 23700: Loss = -11117.2353515625
7
Iteration 23800: Loss = -11117.2353515625
8
Iteration 23900: Loss = -11117.236328125
9
Iteration 24000: Loss = -11117.236328125
10
Iteration 24100: Loss = -11117.236328125
11
Iteration 24200: Loss = -11117.236328125
12
Iteration 24300: Loss = -11117.2353515625
13
Iteration 24400: Loss = -11117.2353515625
14
Iteration 24500: Loss = -11117.236328125
15
Stopping early at iteration 24500 due to no improvement.
pi: tensor([[0.0332, 0.9668],
        [0.0032, 0.9968]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9732, 0.0268], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1575, 0.1840],
         [0.5134, 0.1682]],

        [[0.9887, 0.1015],
         [0.5496, 0.0265]],

        [[0.9793, 0.2368],
         [0.3862, 0.0201]],

        [[0.9548, 0.3907],
         [0.3751, 0.2939]],

        [[0.9712, 0.2872],
         [0.5175, 0.0589]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0040355891165133745
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -47578.5546875
Iteration 100: Loss = -29950.251953125
Iteration 200: Loss = -17621.763671875
Iteration 300: Loss = -12873.533203125
Iteration 400: Loss = -11680.505859375
Iteration 500: Loss = -11365.330078125
Iteration 600: Loss = -11256.197265625
Iteration 700: Loss = -11209.306640625
Iteration 800: Loss = -11185.76171875
Iteration 900: Loss = -11172.1513671875
Iteration 1000: Loss = -11162.810546875
Iteration 1100: Loss = -11155.4287109375
Iteration 1200: Loss = -11149.5546875
Iteration 1300: Loss = -11144.7958984375
Iteration 1400: Loss = -11140.6474609375
Iteration 1500: Loss = -11137.2529296875
Iteration 1600: Loss = -11134.7353515625
Iteration 1700: Loss = -11132.8095703125
Iteration 1800: Loss = -11131.2607421875
Iteration 1900: Loss = -11129.98046875
Iteration 2000: Loss = -11128.900390625
Iteration 2100: Loss = -11127.9765625
Iteration 2200: Loss = -11127.177734375
Iteration 2300: Loss = -11126.478515625
Iteration 2400: Loss = -11125.86328125
Iteration 2500: Loss = -11125.3203125
Iteration 2600: Loss = -11124.8359375
Iteration 2700: Loss = -11124.40234375
Iteration 2800: Loss = -11124.01171875
Iteration 2900: Loss = -11123.6591796875
Iteration 3000: Loss = -11123.3408203125
Iteration 3100: Loss = -11123.0478515625
Iteration 3200: Loss = -11122.7822265625
Iteration 3300: Loss = -11122.541015625
Iteration 3400: Loss = -11122.318359375
Iteration 3500: Loss = -11122.1162109375
Iteration 3600: Loss = -11121.927734375
Iteration 3700: Loss = -11121.75390625
Iteration 3800: Loss = -11121.5947265625
Iteration 3900: Loss = -11121.4453125
Iteration 4000: Loss = -11121.3095703125
Iteration 4100: Loss = -11121.181640625
Iteration 4200: Loss = -11121.0634765625
Iteration 4300: Loss = -11120.953125
Iteration 4400: Loss = -11120.849609375
Iteration 4500: Loss = -11120.75390625
Iteration 4600: Loss = -11120.6630859375
Iteration 4700: Loss = -11120.580078125
Iteration 4800: Loss = -11120.5
Iteration 4900: Loss = -11120.42578125
Iteration 5000: Loss = -11120.3564453125
Iteration 5100: Loss = -11120.2919921875
Iteration 5200: Loss = -11120.23046875
Iteration 5300: Loss = -11120.173828125
Iteration 5400: Loss = -11120.1181640625
Iteration 5500: Loss = -11120.0673828125
Iteration 5600: Loss = -11120.0205078125
Iteration 5700: Loss = -11119.9736328125
Iteration 5800: Loss = -11119.9306640625
Iteration 5900: Loss = -11119.888671875
Iteration 6000: Loss = -11119.8525390625
Iteration 6100: Loss = -11119.8173828125
Iteration 6200: Loss = -11119.7822265625
Iteration 6300: Loss = -11119.7509765625
Iteration 6400: Loss = -11119.71875
Iteration 6500: Loss = -11119.6884765625
Iteration 6600: Loss = -11119.662109375
Iteration 6700: Loss = -11119.63671875
Iteration 6800: Loss = -11119.6103515625
Iteration 6900: Loss = -11119.5869140625
Iteration 7000: Loss = -11119.5654296875
Iteration 7100: Loss = -11119.5439453125
Iteration 7200: Loss = -11119.5244140625
Iteration 7300: Loss = -11119.5048828125
Iteration 7400: Loss = -11119.4853515625
Iteration 7500: Loss = -11119.4697265625
Iteration 7600: Loss = -11119.4521484375
Iteration 7700: Loss = -11119.4375
Iteration 7800: Loss = -11119.421875
Iteration 7900: Loss = -11119.4091796875
Iteration 8000: Loss = -11119.3935546875
Iteration 8100: Loss = -11119.3837890625
Iteration 8200: Loss = -11119.3701171875
Iteration 8300: Loss = -11119.3583984375
Iteration 8400: Loss = -11119.34765625
Iteration 8500: Loss = -11119.3369140625
Iteration 8600: Loss = -11119.328125
Iteration 8700: Loss = -11119.3173828125
Iteration 8800: Loss = -11119.30859375
Iteration 8900: Loss = -11119.298828125
Iteration 9000: Loss = -11119.2900390625
Iteration 9100: Loss = -11119.2822265625
Iteration 9200: Loss = -11119.2724609375
Iteration 9300: Loss = -11119.2626953125
Iteration 9400: Loss = -11119.251953125
Iteration 9500: Loss = -11119.236328125
Iteration 9600: Loss = -11119.2001953125
Iteration 9700: Loss = -11119.1669921875
Iteration 9800: Loss = -11119.1376953125
Iteration 9900: Loss = -11119.1083984375
Iteration 10000: Loss = -11119.0869140625
Iteration 10100: Loss = -11119.0703125
Iteration 10200: Loss = -11119.05859375
Iteration 10300: Loss = -11119.0458984375
Iteration 10400: Loss = -11119.0341796875
Iteration 10500: Loss = -11119.0263671875
Iteration 10600: Loss = -11119.0166015625
Iteration 10700: Loss = -11119.0087890625
Iteration 10800: Loss = -11119.0
Iteration 10900: Loss = -11118.9912109375
Iteration 11000: Loss = -11118.978515625
Iteration 11100: Loss = -11118.9619140625
Iteration 11200: Loss = -11118.9365234375
Iteration 11300: Loss = -11118.9111328125
Iteration 11400: Loss = -11118.892578125
Iteration 11500: Loss = -11118.8740234375
Iteration 11600: Loss = -11118.537109375
Iteration 11700: Loss = -11117.8203125
Iteration 11800: Loss = -11117.755859375
Iteration 11900: Loss = -11117.7021484375
Iteration 12000: Loss = -11117.638671875
Iteration 12100: Loss = -11117.5869140625
Iteration 12200: Loss = -11117.5361328125
Iteration 12300: Loss = -11117.4833984375
Iteration 12400: Loss = -11117.4296875
Iteration 12500: Loss = -11117.375
Iteration 12600: Loss = -11117.3193359375
Iteration 12700: Loss = -11117.2646484375
Iteration 12800: Loss = -11117.212890625
Iteration 12900: Loss = -11117.1611328125
Iteration 13000: Loss = -11117.11328125
Iteration 13100: Loss = -11117.068359375
Iteration 13200: Loss = -11117.025390625
Iteration 13300: Loss = -11116.9833984375
Iteration 13400: Loss = -11116.9384765625
Iteration 13500: Loss = -11116.880859375
Iteration 13600: Loss = -11116.767578125
Iteration 13700: Loss = -11116.587890625
Iteration 13800: Loss = -11116.5302734375
Iteration 13900: Loss = -11116.51171875
Iteration 14000: Loss = -11116.4912109375
Iteration 14100: Loss = -11116.4814453125
Iteration 14200: Loss = -11116.4716796875
Iteration 14300: Loss = -11116.4306640625
Iteration 14400: Loss = -11116.3994140625
Iteration 14500: Loss = -11116.373046875
Iteration 14600: Loss = -11116.353515625
Iteration 14700: Loss = -11116.294921875
Iteration 14800: Loss = -11116.265625
Iteration 14900: Loss = -11116.2294921875
Iteration 15000: Loss = -11116.1123046875
Iteration 15100: Loss = -11116.1044921875
Iteration 15200: Loss = -11116.0546875
Iteration 15300: Loss = -11116.044921875
Iteration 15400: Loss = -11116.0205078125
Iteration 15500: Loss = -11116.0009765625
Iteration 15600: Loss = -11115.9677734375
Iteration 15700: Loss = -11115.9521484375
Iteration 15800: Loss = -11115.94921875
Iteration 15900: Loss = -11115.92578125
Iteration 16000: Loss = -11115.90625
Iteration 16100: Loss = -11115.890625
Iteration 16200: Loss = -11115.8740234375
Iteration 16300: Loss = -11115.845703125
Iteration 16400: Loss = -11115.806640625
Iteration 16500: Loss = -11115.7880859375
Iteration 16600: Loss = -11115.775390625
Iteration 16700: Loss = -11115.7490234375
Iteration 16800: Loss = -11115.716796875
Iteration 16900: Loss = -11115.6884765625
Iteration 17000: Loss = -11115.6591796875
Iteration 17100: Loss = -11115.6376953125
Iteration 17200: Loss = -11115.623046875
Iteration 17300: Loss = -11115.603515625
Iteration 17400: Loss = -11115.5927734375
Iteration 17500: Loss = -11115.591796875
Iteration 17600: Loss = -11115.5869140625
Iteration 17700: Loss = -11115.578125
Iteration 17800: Loss = -11115.576171875
Iteration 17900: Loss = -11115.568359375
Iteration 18000: Loss = -11115.5654296875
Iteration 18100: Loss = -11115.5615234375
Iteration 18200: Loss = -11115.5595703125
Iteration 18300: Loss = -11115.560546875
1
Iteration 18400: Loss = -11115.560546875
2
Iteration 18500: Loss = -11115.55859375
Iteration 18600: Loss = -11115.560546875
1
Iteration 18700: Loss = -11115.5595703125
2
Iteration 18800: Loss = -11115.55859375
Iteration 18900: Loss = -11115.5595703125
1
Iteration 19000: Loss = -11115.55859375
Iteration 19100: Loss = -11115.560546875
1
Iteration 19200: Loss = -11115.560546875
2
Iteration 19300: Loss = -11115.560546875
3
Iteration 19400: Loss = -11115.5595703125
4
Iteration 19500: Loss = -11115.560546875
5
Iteration 19600: Loss = -11115.560546875
6
Iteration 19700: Loss = -11115.5595703125
7
Iteration 19800: Loss = -11115.5595703125
8
Iteration 19900: Loss = -11115.5595703125
9
Iteration 20000: Loss = -11115.5595703125
10
Iteration 20100: Loss = -11115.560546875
11
Iteration 20200: Loss = -11115.5595703125
12
Iteration 20300: Loss = -11115.55859375
Iteration 20400: Loss = -11115.5595703125
1
Iteration 20500: Loss = -11115.55859375
Iteration 20600: Loss = -11115.5595703125
1
Iteration 20700: Loss = -11115.5595703125
2
Iteration 20800: Loss = -11115.560546875
3
Iteration 20900: Loss = -11115.5595703125
4
Iteration 21000: Loss = -11115.55859375
Iteration 21100: Loss = -11115.5615234375
1
Iteration 21200: Loss = -11115.5595703125
2
Iteration 21300: Loss = -11115.5595703125
3
Iteration 21400: Loss = -11115.55859375
Iteration 21500: Loss = -11115.5595703125
1
Iteration 21600: Loss = -11115.5595703125
2
Iteration 21700: Loss = -11115.560546875
3
Iteration 21800: Loss = -11115.5595703125
4
Iteration 21900: Loss = -11115.5400390625
Iteration 22000: Loss = -11115.5400390625
Iteration 22100: Loss = -11115.5390625
Iteration 22200: Loss = -11115.5400390625
1
Iteration 22300: Loss = -11115.5400390625
2
Iteration 22400: Loss = -11115.541015625
3
Iteration 22500: Loss = -11115.5400390625
4
Iteration 22600: Loss = -11115.541015625
5
Iteration 22700: Loss = -11115.5400390625
6
Iteration 22800: Loss = -11115.541015625
7
Iteration 22900: Loss = -11115.5400390625
8
Iteration 23000: Loss = -11115.5400390625
9
Iteration 23100: Loss = -11115.541015625
10
Iteration 23200: Loss = -11115.541015625
11
Iteration 23300: Loss = -11115.5400390625
12
Iteration 23400: Loss = -11115.5419921875
13
Iteration 23500: Loss = -11115.541015625
14
Iteration 23600: Loss = -11115.541015625
15
Stopping early at iteration 23600 due to no improvement.
pi: tensor([[2.6339e-04, 9.9974e-01],
        [2.4275e-02, 9.7573e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0512, 0.9488], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2399, 0.0808],
         [0.3603, 0.1658]],

        [[0.0424, 0.2014],
         [0.9912, 0.9745]],

        [[0.1463, 0.2333],
         [0.0213, 0.0104]],

        [[0.0075, 0.1429],
         [0.1946, 0.7969]],

        [[0.9510, 0.2506],
         [0.9781, 0.8244]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: 0.0009813649942087015
Average Adjusted Rand Index: -0.0027963837626397885
[0.0040355891165133745, 0.0009813649942087015] [0.0, -0.0027963837626397885] [11117.236328125, 11115.541015625]
-------------------------------------
This iteration is 44
True Objective function: Loss = -10799.19909435901
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28462.1953125
Iteration 100: Loss = -18716.470703125
Iteration 200: Loss = -12834.7177734375
Iteration 300: Loss = -11512.3740234375
Iteration 400: Loss = -11259.779296875
Iteration 500: Loss = -11163.646484375
Iteration 600: Loss = -11114.5751953125
Iteration 700: Loss = -11083.28125
Iteration 800: Loss = -11061.5908203125
Iteration 900: Loss = -11042.7470703125
Iteration 1000: Loss = -11025.865234375
Iteration 1100: Loss = -11009.79296875
Iteration 1200: Loss = -10997.751953125
Iteration 1300: Loss = -10988.6181640625
Iteration 1400: Loss = -10981.27734375
Iteration 1500: Loss = -10975.56640625
Iteration 1600: Loss = -10969.224609375
Iteration 1700: Loss = -10961.6337890625
Iteration 1800: Loss = -10955.2001953125
Iteration 1900: Loss = -10949.56640625
Iteration 2000: Loss = -10944.171875
Iteration 2100: Loss = -10941.5634765625
Iteration 2200: Loss = -10939.1005859375
Iteration 2300: Loss = -10936.744140625
Iteration 2400: Loss = -10934.4072265625
Iteration 2500: Loss = -10932.6181640625
Iteration 2600: Loss = -10931.31640625
Iteration 2700: Loss = -10930.224609375
Iteration 2800: Loss = -10929.3173828125
Iteration 2900: Loss = -10928.4599609375
Iteration 3000: Loss = -10927.6826171875
Iteration 3100: Loss = -10926.96875
Iteration 3200: Loss = -10926.0419921875
Iteration 3300: Loss = -10924.943359375
Iteration 3400: Loss = -10924.2490234375
Iteration 3500: Loss = -10923.150390625
Iteration 3600: Loss = -10921.4345703125
Iteration 3700: Loss = -10920.2626953125
Iteration 3800: Loss = -10918.732421875
Iteration 3900: Loss = -10917.7890625
Iteration 4000: Loss = -10917.3154296875
Iteration 4100: Loss = -10916.9697265625
Iteration 4200: Loss = -10916.62109375
Iteration 4300: Loss = -10916.34375
Iteration 4400: Loss = -10916.115234375
Iteration 4500: Loss = -10915.9130859375
Iteration 4600: Loss = -10915.732421875
Iteration 4700: Loss = -10915.56640625
Iteration 4800: Loss = -10915.4111328125
Iteration 4900: Loss = -10915.2685546875
Iteration 5000: Loss = -10915.13671875
Iteration 5100: Loss = -10915.0166015625
Iteration 5200: Loss = -10914.8974609375
Iteration 5300: Loss = -10914.7412109375
Iteration 5400: Loss = -10913.5341796875
Iteration 5500: Loss = -10913.4423828125
Iteration 5600: Loss = -10913.3583984375
Iteration 5700: Loss = -10913.28125
Iteration 5800: Loss = -10913.2060546875
Iteration 5900: Loss = -10913.12890625
Iteration 6000: Loss = -10912.892578125
Iteration 6100: Loss = -10911.8603515625
Iteration 6200: Loss = -10911.6728515625
Iteration 6300: Loss = -10911.578125
Iteration 6400: Loss = -10911.505859375
Iteration 6500: Loss = -10911.4404296875
Iteration 6600: Loss = -10911.353515625
Iteration 6700: Loss = -10910.0869140625
Iteration 6800: Loss = -10910.0341796875
Iteration 6900: Loss = -10909.990234375
Iteration 7000: Loss = -10909.9521484375
Iteration 7100: Loss = -10909.9140625
Iteration 7200: Loss = -10909.8818359375
Iteration 7300: Loss = -10909.8505859375
Iteration 7400: Loss = -10908.5400390625
Iteration 7500: Loss = -10908.498046875
Iteration 7600: Loss = -10908.47265625
Iteration 7700: Loss = -10908.4482421875
Iteration 7800: Loss = -10908.427734375
Iteration 7900: Loss = -10908.40625
Iteration 8000: Loss = -10908.3857421875
Iteration 8100: Loss = -10908.369140625
Iteration 8200: Loss = -10908.349609375
Iteration 8300: Loss = -10908.3330078125
Iteration 8400: Loss = -10908.3173828125
Iteration 8500: Loss = -10908.3037109375
Iteration 8600: Loss = -10908.2880859375
Iteration 8700: Loss = -10908.275390625
Iteration 8800: Loss = -10908.2626953125
Iteration 8900: Loss = -10908.2509765625
Iteration 9000: Loss = -10908.2392578125
Iteration 9100: Loss = -10908.228515625
Iteration 9200: Loss = -10908.2177734375
Iteration 9300: Loss = -10908.208984375
Iteration 9400: Loss = -10908.197265625
Iteration 9500: Loss = -10908.189453125
Iteration 9600: Loss = -10908.1806640625
Iteration 9700: Loss = -10908.171875
Iteration 9800: Loss = -10908.1630859375
Iteration 9900: Loss = -10908.15234375
Iteration 10000: Loss = -10908.1396484375
Iteration 10100: Loss = -10908.125
Iteration 10200: Loss = -10908.107421875
Iteration 10300: Loss = -10908.0927734375
Iteration 10400: Loss = -10908.078125
Iteration 10500: Loss = -10906.8837890625
Iteration 10600: Loss = -10906.8369140625
Iteration 10700: Loss = -10906.830078125
Iteration 10800: Loss = -10906.8203125
Iteration 10900: Loss = -10906.81640625
Iteration 11000: Loss = -10906.8076171875
Iteration 11100: Loss = -10906.8017578125
Iteration 11200: Loss = -10904.6865234375
Iteration 11300: Loss = -10904.1220703125
Iteration 11400: Loss = -10904.10546875
Iteration 11500: Loss = -10902.7646484375
Iteration 11600: Loss = -10902.7333984375
Iteration 11700: Loss = -10902.7236328125
Iteration 11800: Loss = -10902.716796875
Iteration 11900: Loss = -10902.712890625
Iteration 12000: Loss = -10902.708984375
Iteration 12100: Loss = -10902.7060546875
Iteration 12200: Loss = -10902.6982421875
Iteration 12300: Loss = -10901.3388671875
Iteration 12400: Loss = -10899.875
Iteration 12500: Loss = -10899.8583984375
Iteration 12600: Loss = -10898.38671875
Iteration 12700: Loss = -10898.376953125
Iteration 12800: Loss = -10898.37109375
Iteration 12900: Loss = -10898.3681640625
Iteration 13000: Loss = -10898.365234375
Iteration 13100: Loss = -10898.3642578125
Iteration 13200: Loss = -10898.361328125
Iteration 13300: Loss = -10898.359375
Iteration 13400: Loss = -10898.3583984375
Iteration 13500: Loss = -10898.35546875
Iteration 13600: Loss = -10898.3544921875
Iteration 13700: Loss = -10898.3525390625
Iteration 13800: Loss = -10898.3544921875
1
Iteration 13900: Loss = -10898.34765625
Iteration 14000: Loss = -10896.6396484375
Iteration 14100: Loss = -10896.630859375
Iteration 14200: Loss = -10896.6279296875
Iteration 14300: Loss = -10896.626953125
Iteration 14400: Loss = -10896.626953125
Iteration 14500: Loss = -10896.6279296875
1
Iteration 14600: Loss = -10896.6240234375
Iteration 14700: Loss = -10896.625
1
Iteration 14800: Loss = -10896.3779296875
Iteration 14900: Loss = -10895.080078125
Iteration 15000: Loss = -10895.0791015625
Iteration 15100: Loss = -10895.078125
Iteration 15200: Loss = -10895.078125
Iteration 15300: Loss = -10895.078125
Iteration 15400: Loss = -10895.0771484375
Iteration 15500: Loss = -10895.0771484375
Iteration 15600: Loss = -10895.076171875
Iteration 15700: Loss = -10895.0771484375
1
Iteration 15800: Loss = -10895.076171875
Iteration 15900: Loss = -10895.076171875
Iteration 16000: Loss = -10895.07421875
Iteration 16100: Loss = -10895.07421875
Iteration 16200: Loss = -10895.0751953125
1
Iteration 16300: Loss = -10895.0732421875
Iteration 16400: Loss = -10895.072265625
Iteration 16500: Loss = -10895.07421875
1
Iteration 16600: Loss = -10895.0732421875
2
Iteration 16700: Loss = -10895.072265625
Iteration 16800: Loss = -10895.0732421875
1
Iteration 16900: Loss = -10895.072265625
Iteration 17000: Loss = -10895.0732421875
1
Iteration 17100: Loss = -10895.0712890625
Iteration 17200: Loss = -10895.0712890625
Iteration 17300: Loss = -10895.0712890625
Iteration 17400: Loss = -10895.0712890625
Iteration 17500: Loss = -10893.3681640625
Iteration 17600: Loss = -10893.3662109375
Iteration 17700: Loss = -10893.3671875
1
Iteration 17800: Loss = -10893.36328125
Iteration 17900: Loss = -10891.2275390625
Iteration 18000: Loss = -10891.2119140625
Iteration 18100: Loss = -10891.20703125
Iteration 18200: Loss = -10891.2021484375
Iteration 18300: Loss = -10891.1923828125
Iteration 18400: Loss = -10891.1865234375
Iteration 18500: Loss = -10891.181640625
Iteration 18600: Loss = -10891.1826171875
1
Iteration 18700: Loss = -10891.181640625
Iteration 18800: Loss = -10891.1806640625
Iteration 18900: Loss = -10891.1806640625
Iteration 19000: Loss = -10891.1806640625
Iteration 19100: Loss = -10891.1806640625
Iteration 19200: Loss = -10891.181640625
1
Iteration 19300: Loss = -10891.181640625
2
Iteration 19400: Loss = -10891.1806640625
Iteration 19500: Loss = -10891.181640625
1
Iteration 19600: Loss = -10891.1826171875
2
Iteration 19700: Loss = -10891.181640625
3
Iteration 19800: Loss = -10891.1796875
Iteration 19900: Loss = -10891.1796875
Iteration 20000: Loss = -10891.1796875
Iteration 20100: Loss = -10891.1806640625
1
Iteration 20200: Loss = -10891.1806640625
2
Iteration 20300: Loss = -10891.1796875
Iteration 20400: Loss = -10891.1796875
Iteration 20500: Loss = -10891.1806640625
1
Iteration 20600: Loss = -10891.1796875
Iteration 20700: Loss = -10891.1826171875
1
Iteration 20800: Loss = -10891.1796875
Iteration 20900: Loss = -10891.1806640625
1
Iteration 21000: Loss = -10891.1796875
Iteration 21100: Loss = -10891.1796875
Iteration 21200: Loss = -10891.181640625
1
Iteration 21300: Loss = -10891.1796875
Iteration 21400: Loss = -10891.1796875
Iteration 21500: Loss = -10891.1806640625
1
Iteration 21600: Loss = -10891.1796875
Iteration 21700: Loss = -10891.1796875
Iteration 21800: Loss = -10891.1796875
Iteration 21900: Loss = -10891.1796875
Iteration 22000: Loss = -10891.181640625
1
Iteration 22100: Loss = -10891.1796875
Iteration 22200: Loss = -10891.1796875
Iteration 22300: Loss = -10891.1806640625
1
Iteration 22400: Loss = -10891.1806640625
2
Iteration 22500: Loss = -10891.1796875
Iteration 22600: Loss = -10891.1796875
Iteration 22700: Loss = -10891.181640625
1
Iteration 22800: Loss = -10891.1796875
Iteration 22900: Loss = -10891.1796875
Iteration 23000: Loss = -10891.1796875
Iteration 23100: Loss = -10891.181640625
1
Iteration 23200: Loss = -10891.1806640625
2
Iteration 23300: Loss = -10891.1796875
Iteration 23400: Loss = -10891.1796875
Iteration 23500: Loss = -10891.1796875
Iteration 23600: Loss = -10891.1796875
Iteration 23700: Loss = -10891.1806640625
1
Iteration 23800: Loss = -10891.181640625
2
Iteration 23900: Loss = -10891.1796875
Iteration 24000: Loss = -10891.1796875
Iteration 24100: Loss = -10891.1796875
Iteration 24200: Loss = -10891.1806640625
1
Iteration 24300: Loss = -10891.1806640625
2
Iteration 24400: Loss = -10891.1796875
Iteration 24500: Loss = -10891.1796875
Iteration 24600: Loss = -10891.1806640625
1
Iteration 24700: Loss = -10891.1796875
Iteration 24800: Loss = -10891.1806640625
1
Iteration 24900: Loss = -10891.1806640625
2
Iteration 25000: Loss = -10891.1796875
Iteration 25100: Loss = -10891.1796875
Iteration 25200: Loss = -10891.1796875
Iteration 25300: Loss = -10891.1796875
Iteration 25400: Loss = -10891.1796875
Iteration 25500: Loss = -10891.1796875
Iteration 25600: Loss = -10891.1796875
Iteration 25700: Loss = -10891.1796875
Iteration 25800: Loss = -10891.1796875
Iteration 25900: Loss = -10891.1806640625
1
Iteration 26000: Loss = -10891.1796875
Iteration 26100: Loss = -10891.1796875
Iteration 26200: Loss = -10891.1806640625
1
Iteration 26300: Loss = -10891.1806640625
2
Iteration 26400: Loss = -10891.181640625
3
Iteration 26500: Loss = -10891.1806640625
4
Iteration 26600: Loss = -10891.1806640625
5
Iteration 26700: Loss = -10891.1806640625
6
Iteration 26800: Loss = -10891.181640625
7
Iteration 26900: Loss = -10891.1806640625
8
Iteration 27000: Loss = -10891.1806640625
9
Iteration 27100: Loss = -10891.1806640625
10
Iteration 27200: Loss = -10891.1806640625
11
Iteration 27300: Loss = -10891.1796875
Iteration 27400: Loss = -10891.1806640625
1
Iteration 27500: Loss = -10891.1796875
Iteration 27600: Loss = -10891.1796875
Iteration 27700: Loss = -10891.1796875
Iteration 27800: Loss = -10891.181640625
1
Iteration 27900: Loss = -10891.1796875
Iteration 28000: Loss = -10891.1796875
Iteration 28100: Loss = -10891.1826171875
1
Iteration 28200: Loss = -10891.1806640625
2
Iteration 28300: Loss = -10891.1806640625
3
Iteration 28400: Loss = -10891.1806640625
4
Iteration 28500: Loss = -10891.1796875
Iteration 28600: Loss = -10891.1796875
Iteration 28700: Loss = -10891.1796875
Iteration 28800: Loss = -10891.181640625
1
Iteration 28900: Loss = -10891.1806640625
2
Iteration 29000: Loss = -10891.181640625
3
Iteration 29100: Loss = -10891.18359375
4
Iteration 29200: Loss = -10891.181640625
5
Iteration 29300: Loss = -10891.181640625
6
Iteration 29400: Loss = -10891.1806640625
7
Iteration 29500: Loss = -10891.1806640625
8
Iteration 29600: Loss = -10891.1796875
Iteration 29700: Loss = -10891.1796875
Iteration 29800: Loss = -10891.1796875
Iteration 29900: Loss = -10891.181640625
1
pi: tensor([[1.0365e-02, 9.8963e-01],
        [8.9542e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 7.6395e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1653, 0.1654],
         [0.3032, 0.1587]],

        [[0.0105, 0.1997],
         [0.0216, 0.0166]],

        [[0.9063, 0.2120],
         [0.8604, 0.3088]],

        [[0.5090, 0.1618],
         [0.4304, 0.9730]],

        [[0.1018, 0.1742],
         [0.0224, 0.7997]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011199360300759335
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48941.2421875
Iteration 100: Loss = -28812.775390625
Iteration 200: Loss = -15942.8212890625
Iteration 300: Loss = -12645.94140625
Iteration 400: Loss = -11771.0498046875
Iteration 500: Loss = -11445.298828125
Iteration 600: Loss = -11280.3671875
Iteration 700: Loss = -11190.51171875
Iteration 800: Loss = -11132.572265625
Iteration 900: Loss = -11075.3486328125
Iteration 1000: Loss = -11036.4140625
Iteration 1100: Loss = -11009.3720703125
Iteration 1200: Loss = -10996.3251953125
Iteration 1300: Loss = -10986.041015625
Iteration 1400: Loss = -10975.6220703125
Iteration 1500: Loss = -10966.841796875
Iteration 1600: Loss = -10957.69921875
Iteration 1700: Loss = -10952.375
Iteration 1800: Loss = -10943.8134765625
Iteration 1900: Loss = -10940.1064453125
Iteration 2000: Loss = -10935.96484375
Iteration 2100: Loss = -10932.2919921875
Iteration 2200: Loss = -10928.5029296875
Iteration 2300: Loss = -10926.70703125
Iteration 2400: Loss = -10921.712890625
Iteration 2500: Loss = -10920.0576171875
Iteration 2600: Loss = -10918.9912109375
Iteration 2700: Loss = -10917.3994140625
Iteration 2800: Loss = -10907.9052734375
Iteration 2900: Loss = -10906.2939453125
Iteration 3000: Loss = -10905.4248046875
Iteration 3100: Loss = -10904.7578125
Iteration 3200: Loss = -10904.2021484375
Iteration 3300: Loss = -10903.7197265625
Iteration 3400: Loss = -10903.2919921875
Iteration 3500: Loss = -10902.912109375
Iteration 3600: Loss = -10902.568359375
Iteration 3700: Loss = -10902.255859375
Iteration 3800: Loss = -10901.97265625
Iteration 3900: Loss = -10901.693359375
Iteration 4000: Loss = -10896.1904296875
Iteration 4100: Loss = -10895.6171875
Iteration 4200: Loss = -10895.2783203125
Iteration 4300: Loss = -10895.009765625
Iteration 4400: Loss = -10894.779296875
Iteration 4500: Loss = -10894.5732421875
Iteration 4600: Loss = -10894.3876953125
Iteration 4700: Loss = -10894.21875
Iteration 4800: Loss = -10894.0625
Iteration 4900: Loss = -10893.9189453125
Iteration 5000: Loss = -10893.787109375
Iteration 5100: Loss = -10893.662109375
Iteration 5200: Loss = -10893.5498046875
Iteration 5300: Loss = -10893.4423828125
Iteration 5400: Loss = -10893.3408203125
Iteration 5500: Loss = -10893.248046875
Iteration 5600: Loss = -10893.1572265625
Iteration 5700: Loss = -10893.07421875
Iteration 5800: Loss = -10892.998046875
Iteration 5900: Loss = -10892.9248046875
Iteration 6000: Loss = -10892.859375
Iteration 6100: Loss = -10892.794921875
Iteration 6200: Loss = -10892.736328125
Iteration 6300: Loss = -10892.6796875
Iteration 6400: Loss = -10892.6259765625
Iteration 6500: Loss = -10892.57421875
Iteration 6600: Loss = -10892.52734375
Iteration 6700: Loss = -10892.4833984375
Iteration 6800: Loss = -10892.44140625
Iteration 6900: Loss = -10892.40234375
Iteration 7000: Loss = -10892.36328125
Iteration 7100: Loss = -10892.3271484375
Iteration 7200: Loss = -10892.2939453125
Iteration 7300: Loss = -10892.263671875
Iteration 7400: Loss = -10892.232421875
Iteration 7500: Loss = -10892.2021484375
Iteration 7600: Loss = -10892.1748046875
Iteration 7700: Loss = -10892.1494140625
Iteration 7800: Loss = -10892.1259765625
Iteration 7900: Loss = -10892.1025390625
Iteration 8000: Loss = -10892.0810546875
Iteration 8100: Loss = -10892.0595703125
Iteration 8200: Loss = -10892.0390625
Iteration 8300: Loss = -10892.021484375
Iteration 8400: Loss = -10892.0029296875
Iteration 8500: Loss = -10891.986328125
Iteration 8600: Loss = -10891.9677734375
Iteration 8700: Loss = -10891.9521484375
Iteration 8800: Loss = -10891.935546875
Iteration 8900: Loss = -10891.9228515625
Iteration 9000: Loss = -10891.908203125
Iteration 9100: Loss = -10891.8955078125
Iteration 9200: Loss = -10891.8818359375
Iteration 9300: Loss = -10891.869140625
Iteration 9400: Loss = -10891.857421875
Iteration 9500: Loss = -10891.845703125
Iteration 9600: Loss = -10891.83203125
Iteration 9700: Loss = -10891.822265625
Iteration 9800: Loss = -10891.8095703125
Iteration 9900: Loss = -10891.7978515625
Iteration 10000: Loss = -10891.787109375
Iteration 10100: Loss = -10891.7744140625
Iteration 10200: Loss = -10891.7646484375
Iteration 10300: Loss = -10891.751953125
Iteration 10400: Loss = -10891.73828125
Iteration 10500: Loss = -10891.7275390625
Iteration 10600: Loss = -10891.712890625
Iteration 10700: Loss = -10891.69921875
Iteration 10800: Loss = -10891.6845703125
Iteration 10900: Loss = -10891.6689453125
Iteration 11000: Loss = -10891.6533203125
Iteration 11100: Loss = -10891.63671875
Iteration 11200: Loss = -10891.6162109375
Iteration 11300: Loss = -10891.59765625
Iteration 11400: Loss = -10891.5771484375
Iteration 11500: Loss = -10891.5537109375
Iteration 11600: Loss = -10891.53125
Iteration 11700: Loss = -10891.5107421875
Iteration 11800: Loss = -10891.48828125
Iteration 11900: Loss = -10891.466796875
Iteration 12000: Loss = -10891.4443359375
Iteration 12100: Loss = -10891.4189453125
Iteration 12200: Loss = -10891.3916015625
Iteration 12300: Loss = -10891.369140625
Iteration 12400: Loss = -10891.3466796875
Iteration 12500: Loss = -10891.3310546875
Iteration 12600: Loss = -10891.3154296875
Iteration 12700: Loss = -10891.2998046875
Iteration 12800: Loss = -10891.2919921875
Iteration 12900: Loss = -10891.2802734375
Iteration 13000: Loss = -10891.271484375
Iteration 13100: Loss = -10891.2666015625
Iteration 13200: Loss = -10891.2578125
Iteration 13300: Loss = -10891.2529296875
Iteration 13400: Loss = -10891.2490234375
Iteration 13500: Loss = -10891.2451171875
Iteration 13600: Loss = -10891.2412109375
Iteration 13700: Loss = -10891.23828125
Iteration 13800: Loss = -10891.234375
Iteration 13900: Loss = -10891.232421875
Iteration 14000: Loss = -10891.23046875
Iteration 14100: Loss = -10891.228515625
Iteration 14200: Loss = -10891.2255859375
Iteration 14300: Loss = -10891.2236328125
Iteration 14400: Loss = -10891.2216796875
Iteration 14500: Loss = -10891.220703125
Iteration 14600: Loss = -10891.2197265625
Iteration 14700: Loss = -10891.2177734375
Iteration 14800: Loss = -10891.21875
1
Iteration 14900: Loss = -10891.2158203125
Iteration 15000: Loss = -10891.21484375
Iteration 15100: Loss = -10891.21484375
Iteration 15200: Loss = -10891.212890625
Iteration 15300: Loss = -10891.21484375
1
Iteration 15400: Loss = -10891.2119140625
Iteration 15500: Loss = -10891.2109375
Iteration 15600: Loss = -10891.2109375
Iteration 15700: Loss = -10891.2109375
Iteration 15800: Loss = -10891.208984375
Iteration 15900: Loss = -10891.2080078125
Iteration 16000: Loss = -10891.208984375
1
Iteration 16100: Loss = -10891.2060546875
Iteration 16200: Loss = -10891.2060546875
Iteration 16300: Loss = -10891.20703125
1
Iteration 16400: Loss = -10891.2060546875
Iteration 16500: Loss = -10891.2060546875
Iteration 16600: Loss = -10891.205078125
Iteration 16700: Loss = -10891.2041015625
Iteration 16800: Loss = -10891.205078125
1
Iteration 16900: Loss = -10891.205078125
2
Iteration 17000: Loss = -10891.2041015625
Iteration 17100: Loss = -10891.2041015625
Iteration 17200: Loss = -10891.2041015625
Iteration 17300: Loss = -10891.2041015625
Iteration 17400: Loss = -10891.2021484375
Iteration 17500: Loss = -10891.203125
1
Iteration 17600: Loss = -10891.203125
2
Iteration 17700: Loss = -10891.2041015625
3
Iteration 17800: Loss = -10891.203125
4
Iteration 17900: Loss = -10891.2021484375
Iteration 18000: Loss = -10891.2021484375
Iteration 18100: Loss = -10891.203125
1
Iteration 18200: Loss = -10891.2021484375
Iteration 18300: Loss = -10891.201171875
Iteration 18400: Loss = -10891.203125
1
Iteration 18500: Loss = -10891.203125
2
Iteration 18600: Loss = -10891.201171875
Iteration 18700: Loss = -10891.201171875
Iteration 18800: Loss = -10891.2021484375
1
Iteration 18900: Loss = -10891.203125
2
Iteration 19000: Loss = -10891.19921875
Iteration 19100: Loss = -10891.2001953125
1
Iteration 19200: Loss = -10891.2021484375
2
Iteration 19300: Loss = -10891.201171875
3
Iteration 19400: Loss = -10891.201171875
4
Iteration 19500: Loss = -10891.2001953125
5
Iteration 19600: Loss = -10891.201171875
6
Iteration 19700: Loss = -10891.2001953125
7
Iteration 19800: Loss = -10891.201171875
8
Iteration 19900: Loss = -10891.201171875
9
Iteration 20000: Loss = -10891.2001953125
10
Iteration 20100: Loss = -10891.19921875
Iteration 20200: Loss = -10891.2001953125
1
Iteration 20300: Loss = -10891.201171875
2
Iteration 20400: Loss = -10891.2001953125
3
Iteration 20500: Loss = -10891.19921875
Iteration 20600: Loss = -10891.19921875
Iteration 20700: Loss = -10891.19921875
Iteration 20800: Loss = -10891.19921875
Iteration 20900: Loss = -10891.201171875
1
Iteration 21000: Loss = -10891.2001953125
2
Iteration 21100: Loss = -10891.19921875
Iteration 21200: Loss = -10891.2001953125
1
Iteration 21300: Loss = -10891.2001953125
2
Iteration 21400: Loss = -10891.2001953125
3
Iteration 21500: Loss = -10891.2001953125
4
Iteration 21600: Loss = -10891.19921875
Iteration 21700: Loss = -10891.2001953125
1
Iteration 21800: Loss = -10891.197265625
Iteration 21900: Loss = -10891.201171875
1
Iteration 22000: Loss = -10891.2001953125
2
Iteration 22100: Loss = -10891.2001953125
3
Iteration 22200: Loss = -10891.19921875
4
Iteration 22300: Loss = -10891.1982421875
5
Iteration 22400: Loss = -10891.19921875
6
Iteration 22500: Loss = -10891.2001953125
7
Iteration 22600: Loss = -10891.1982421875
8
Iteration 22700: Loss = -10891.2001953125
9
Iteration 22800: Loss = -10891.1982421875
10
Iteration 22900: Loss = -10891.19921875
11
Iteration 23000: Loss = -10891.1982421875
12
Iteration 23100: Loss = -10891.1982421875
13
Iteration 23200: Loss = -10891.2001953125
14
Iteration 23300: Loss = -10891.19921875
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[1.0000e+00, 2.6821e-06],
        [1.0000e+00, 1.7882e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1847, 0.8153], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1590, 0.1627],
         [0.5645, 0.1667]],

        [[0.1196, 0.2535],
         [0.0846, 0.0339]],

        [[0.9915, 0.1594],
         [0.4545, 0.6639]],

        [[0.9882, 0.1483],
         [0.0088, 0.8690]],

        [[0.0190, 0.5742],
         [0.1585, 0.9887]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011199360300759335
Average Adjusted Rand Index: 0.0
[-0.0011199360300759335, -0.0011199360300759335] [0.0, 0.0] [10891.1796875, 10891.19921875]
-------------------------------------
This iteration is 45
True Objective function: Loss = -10836.0673815645
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -60240.13671875
Iteration 100: Loss = -37655.40234375
Iteration 200: Loss = -20725.1484375
Iteration 300: Loss = -13747.2041015625
Iteration 400: Loss = -11803.828125
Iteration 500: Loss = -11292.0478515625
Iteration 600: Loss = -11122.2998046875
Iteration 700: Loss = -11047.384765625
Iteration 800: Loss = -11002.22265625
Iteration 900: Loss = -10955.5625
Iteration 1000: Loss = -10941.8349609375
Iteration 1100: Loss = -10924.44921875
Iteration 1200: Loss = -10916.4794921875
Iteration 1300: Loss = -10910.470703125
Iteration 1400: Loss = -10905.875
Iteration 1500: Loss = -10902.21484375
Iteration 1600: Loss = -10899.205078125
Iteration 1700: Loss = -10896.681640625
Iteration 1800: Loss = -10894.53515625
Iteration 1900: Loss = -10892.68359375
Iteration 2000: Loss = -10891.078125
Iteration 2100: Loss = -10889.6728515625
Iteration 2200: Loss = -10888.435546875
Iteration 2300: Loss = -10887.337890625
Iteration 2400: Loss = -10886.3603515625
Iteration 2500: Loss = -10885.4873046875
Iteration 2600: Loss = -10884.7001953125
Iteration 2700: Loss = -10883.9931640625
Iteration 2800: Loss = -10883.3505859375
Iteration 2900: Loss = -10882.7685546875
Iteration 3000: Loss = -10882.2353515625
Iteration 3100: Loss = -10881.7509765625
Iteration 3200: Loss = -10881.3095703125
Iteration 3300: Loss = -10880.9013671875
Iteration 3400: Loss = -10880.529296875
Iteration 3500: Loss = -10880.1845703125
Iteration 3600: Loss = -10879.8662109375
Iteration 3700: Loss = -10879.57421875
Iteration 3800: Loss = -10879.2998046875
Iteration 3900: Loss = -10879.048828125
Iteration 4000: Loss = -10878.8115234375
Iteration 4100: Loss = -10878.595703125
Iteration 4200: Loss = -10878.392578125
Iteration 4300: Loss = -10878.2041015625
Iteration 4400: Loss = -10878.0283203125
Iteration 4500: Loss = -10877.8623046875
Iteration 4600: Loss = -10877.7099609375
Iteration 4700: Loss = -10877.5673828125
Iteration 4800: Loss = -10877.43359375
Iteration 4900: Loss = -10877.306640625
Iteration 5000: Loss = -10877.1904296875
Iteration 5100: Loss = -10877.0791015625
Iteration 5200: Loss = -10876.9755859375
Iteration 5300: Loss = -10876.8779296875
Iteration 5400: Loss = -10876.7880859375
Iteration 5500: Loss = -10876.7021484375
Iteration 5600: Loss = -10876.62109375
Iteration 5700: Loss = -10876.5439453125
Iteration 5800: Loss = -10876.4736328125
Iteration 5900: Loss = -10876.4052734375
Iteration 6000: Loss = -10876.3427734375
Iteration 6100: Loss = -10876.28125
Iteration 6200: Loss = -10876.2265625
Iteration 6300: Loss = -10876.1708984375
Iteration 6400: Loss = -10876.1220703125
Iteration 6500: Loss = -10876.0751953125
Iteration 6600: Loss = -10876.0283203125
Iteration 6700: Loss = -10875.986328125
Iteration 6800: Loss = -10875.9453125
Iteration 6900: Loss = -10875.9072265625
Iteration 7000: Loss = -10875.8701171875
Iteration 7100: Loss = -10875.8359375
Iteration 7200: Loss = -10875.8056640625
Iteration 7300: Loss = -10875.7734375
Iteration 7400: Loss = -10875.7451171875
Iteration 7500: Loss = -10875.716796875
Iteration 7600: Loss = -10875.6923828125
Iteration 7700: Loss = -10875.666015625
Iteration 7800: Loss = -10875.6435546875
Iteration 7900: Loss = -10875.62109375
Iteration 8000: Loss = -10875.599609375
Iteration 8100: Loss = -10875.580078125
Iteration 8200: Loss = -10875.560546875
Iteration 8300: Loss = -10875.5419921875
Iteration 8400: Loss = -10875.5263671875
Iteration 8500: Loss = -10875.5087890625
Iteration 8600: Loss = -10875.494140625
Iteration 8700: Loss = -10875.4794921875
Iteration 8800: Loss = -10875.4658203125
Iteration 8900: Loss = -10875.451171875
Iteration 9000: Loss = -10875.4384765625
Iteration 9100: Loss = -10875.4267578125
Iteration 9200: Loss = -10875.4150390625
Iteration 9300: Loss = -10875.404296875
Iteration 9400: Loss = -10875.39453125
Iteration 9500: Loss = -10875.3837890625
Iteration 9600: Loss = -10875.3740234375
Iteration 9700: Loss = -10875.365234375
Iteration 9800: Loss = -10875.359375
Iteration 9900: Loss = -10875.349609375
Iteration 10000: Loss = -10875.3427734375
Iteration 10100: Loss = -10875.3349609375
Iteration 10200: Loss = -10875.328125
Iteration 10300: Loss = -10875.3203125
Iteration 10400: Loss = -10875.314453125
Iteration 10500: Loss = -10875.3095703125
Iteration 10600: Loss = -10875.3017578125
Iteration 10700: Loss = -10875.2978515625
Iteration 10800: Loss = -10875.2919921875
Iteration 10900: Loss = -10875.2890625
Iteration 11000: Loss = -10875.2822265625
Iteration 11100: Loss = -10875.279296875
Iteration 11200: Loss = -10875.2744140625
Iteration 11300: Loss = -10875.26953125
Iteration 11400: Loss = -10875.2666015625
Iteration 11500: Loss = -10875.2626953125
Iteration 11600: Loss = -10875.259765625
Iteration 11700: Loss = -10875.255859375
Iteration 11800: Loss = -10875.2529296875
Iteration 11900: Loss = -10875.25
Iteration 12000: Loss = -10875.2470703125
Iteration 12100: Loss = -10875.2451171875
Iteration 12200: Loss = -10875.2421875
Iteration 12300: Loss = -10875.240234375
Iteration 12400: Loss = -10875.23828125
Iteration 12500: Loss = -10875.234375
Iteration 12600: Loss = -10875.232421875
Iteration 12700: Loss = -10875.2314453125
Iteration 12800: Loss = -10875.228515625
Iteration 12900: Loss = -10875.2255859375
Iteration 13000: Loss = -10875.2236328125
Iteration 13100: Loss = -10875.21875
Iteration 13200: Loss = -10875.111328125
Iteration 13300: Loss = -10873.25390625
Iteration 13400: Loss = -10872.4033203125
Iteration 13500: Loss = -10872.2666015625
Iteration 13600: Loss = -10872.015625
Iteration 13700: Loss = -10870.9384765625
Iteration 13800: Loss = -10870.8564453125
Iteration 13900: Loss = -10870.76953125
Iteration 14000: Loss = -10870.6044921875
Iteration 14100: Loss = -10867.67578125
Iteration 14200: Loss = -10867.42578125
Iteration 14300: Loss = -10866.0791015625
Iteration 14400: Loss = -10865.576171875
Iteration 14500: Loss = -10865.337890625
Iteration 14600: Loss = -10864.26953125
Iteration 14700: Loss = -10862.81640625
Iteration 14800: Loss = -10862.6875
Iteration 14900: Loss = -10862.6591796875
Iteration 15000: Loss = -10862.6298828125
Iteration 15100: Loss = -10862.6064453125
Iteration 15200: Loss = -10862.5927734375
Iteration 15300: Loss = -10862.5517578125
Iteration 15400: Loss = -10862.5458984375
Iteration 15500: Loss = -10862.5390625
Iteration 15600: Loss = -10862.537109375
Iteration 15700: Loss = -10862.53515625
Iteration 15800: Loss = -10862.53515625
Iteration 15900: Loss = -10862.533203125
Iteration 16000: Loss = -10862.533203125
Iteration 16100: Loss = -10862.5322265625
Iteration 16200: Loss = -10862.53125
Iteration 16300: Loss = -10862.5302734375
Iteration 16400: Loss = -10862.5302734375
Iteration 16500: Loss = -10862.5302734375
Iteration 16600: Loss = -10862.529296875
Iteration 16700: Loss = -10862.5283203125
Iteration 16800: Loss = -10862.529296875
1
Iteration 16900: Loss = -10862.52734375
Iteration 17000: Loss = -10862.5283203125
1
Iteration 17100: Loss = -10862.52734375
Iteration 17200: Loss = -10862.5263671875
Iteration 17300: Loss = -10862.5263671875
Iteration 17400: Loss = -10862.5263671875
Iteration 17500: Loss = -10862.52734375
1
Iteration 17600: Loss = -10862.525390625
Iteration 17700: Loss = -10861.708984375
Iteration 17800: Loss = -10861.673828125
Iteration 17900: Loss = -10861.4853515625
Iteration 18000: Loss = -10861.484375
Iteration 18100: Loss = -10861.484375
Iteration 18200: Loss = -10861.4833984375
Iteration 18300: Loss = -10861.482421875
Iteration 18400: Loss = -10861.29296875
Iteration 18500: Loss = -10861.2919921875
Iteration 18600: Loss = -10861.2744140625
Iteration 18700: Loss = -10861.236328125
Iteration 18800: Loss = -10861.2353515625
Iteration 18900: Loss = -10861.234375
Iteration 19000: Loss = -10861.2041015625
Iteration 19100: Loss = -10861.2021484375
Iteration 19200: Loss = -10861.197265625
Iteration 19300: Loss = -10861.197265625
Iteration 19400: Loss = -10861.1962890625
Iteration 19500: Loss = -10861.193359375
Iteration 19600: Loss = -10861.193359375
Iteration 19700: Loss = -10861.193359375
Iteration 19800: Loss = -10861.1943359375
1
Iteration 19900: Loss = -10861.193359375
Iteration 20000: Loss = -10861.1865234375
Iteration 20100: Loss = -10861.185546875
Iteration 20200: Loss = -10861.1875
1
Iteration 20300: Loss = -10861.1875
2
Iteration 20400: Loss = -10861.1845703125
Iteration 20500: Loss = -10861.185546875
1
Iteration 20600: Loss = -10861.185546875
2
Iteration 20700: Loss = -10861.185546875
3
Iteration 20800: Loss = -10861.185546875
4
Iteration 20900: Loss = -10861.185546875
5
Iteration 21000: Loss = -10861.185546875
6
Iteration 21100: Loss = -10861.18359375
Iteration 21200: Loss = -10861.1845703125
1
Iteration 21300: Loss = -10861.185546875
2
Iteration 21400: Loss = -10861.185546875
3
Iteration 21500: Loss = -10861.1845703125
4
Iteration 21600: Loss = -10861.185546875
5
Iteration 21700: Loss = -10861.1865234375
6
Iteration 21800: Loss = -10861.1845703125
7
Iteration 21900: Loss = -10861.185546875
8
Iteration 22000: Loss = -10861.185546875
9
Iteration 22100: Loss = -10861.1845703125
10
Iteration 22200: Loss = -10861.1845703125
11
Iteration 22300: Loss = -10861.18359375
Iteration 22400: Loss = -10861.1845703125
1
Iteration 22500: Loss = -10861.1845703125
2
Iteration 22600: Loss = -10861.1845703125
3
Iteration 22700: Loss = -10861.1845703125
4
Iteration 22800: Loss = -10861.1845703125
5
Iteration 22900: Loss = -10861.1845703125
6
Iteration 23000: Loss = -10861.1845703125
7
Iteration 23100: Loss = -10861.1845703125
8
Iteration 23200: Loss = -10861.1845703125
9
Iteration 23300: Loss = -10861.185546875
10
Iteration 23400: Loss = -10861.1845703125
11
Iteration 23500: Loss = -10861.1845703125
12
Iteration 23600: Loss = -10861.1845703125
13
Iteration 23700: Loss = -10861.1865234375
14
Iteration 23800: Loss = -10861.185546875
15
Stopping early at iteration 23800 due to no improvement.
pi: tensor([[5.2631e-01, 4.7369e-01],
        [1.9024e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1759, 0.8241], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4912, 0.1424],
         [0.0094, 0.1587]],

        [[0.0183, 0.1587],
         [0.9477, 0.1985]],

        [[0.2886, 0.1155],
         [0.0078, 0.0126]],

        [[0.9489, 0.1367],
         [0.9797, 0.9083]],

        [[0.6864, 0.2023],
         [0.0406, 0.0227]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.07167087033623441
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03551074090311267
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: -0.010102533172496984
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.004868348130358715
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.014019411492836234
Global Adjusted Rand Index: 0.04118978792187261
Average Adjusted Rand Index: 0.023193367538009012
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -49105.48046875
Iteration 100: Loss = -30297.044921875
Iteration 200: Loss = -17889.134765625
Iteration 300: Loss = -13969.7548828125
Iteration 400: Loss = -12446.4375
Iteration 500: Loss = -11654.4599609375
Iteration 600: Loss = -11248.4765625
Iteration 700: Loss = -11158.4267578125
Iteration 800: Loss = -11102.1884765625
Iteration 900: Loss = -11069.669921875
Iteration 1000: Loss = -11043.24609375
Iteration 1100: Loss = -11016.49609375
Iteration 1200: Loss = -10993.5322265625
Iteration 1300: Loss = -10973.5244140625
Iteration 1400: Loss = -10961.8037109375
Iteration 1500: Loss = -10953.0615234375
Iteration 1600: Loss = -10943.5234375
Iteration 1700: Loss = -10936.0244140625
Iteration 1800: Loss = -10930.9091796875
Iteration 1900: Loss = -10927.2314453125
Iteration 2000: Loss = -10924.3359375
Iteration 2100: Loss = -10921.9267578125
Iteration 2200: Loss = -10919.8408203125
Iteration 2300: Loss = -10917.9697265625
Iteration 2400: Loss = -10916.197265625
Iteration 2500: Loss = -10914.369140625
Iteration 2600: Loss = -10912.3759765625
Iteration 2700: Loss = -10910.490234375
Iteration 2800: Loss = -10908.828125
Iteration 2900: Loss = -10907.330078125
Iteration 3000: Loss = -10905.9931640625
Iteration 3100: Loss = -10904.7470703125
Iteration 3200: Loss = -10901.708984375
Iteration 3300: Loss = -10898.6015625
Iteration 3400: Loss = -10897.3828125
Iteration 3500: Loss = -10896.5517578125
Iteration 3600: Loss = -10895.9072265625
Iteration 3700: Loss = -10895.3818359375
Iteration 3800: Loss = -10894.9384765625
Iteration 3900: Loss = -10894.55078125
Iteration 4000: Loss = -10894.197265625
Iteration 4100: Loss = -10893.8759765625
Iteration 4200: Loss = -10893.576171875
Iteration 4300: Loss = -10893.2978515625
Iteration 4400: Loss = -10893.0458984375
Iteration 4500: Loss = -10892.8125
Iteration 4600: Loss = -10892.6005859375
Iteration 4700: Loss = -10892.4033203125
Iteration 4800: Loss = -10892.2216796875
Iteration 4900: Loss = -10892.0517578125
Iteration 5000: Loss = -10891.8623046875
Iteration 5100: Loss = -10887.6123046875
Iteration 5200: Loss = -10887.318359375
Iteration 5300: Loss = -10887.12890625
Iteration 5400: Loss = -10882.150390625
Iteration 5500: Loss = -10881.529296875
Iteration 5600: Loss = -10881.2890625
Iteration 5700: Loss = -10881.11328125
Iteration 5800: Loss = -10880.96875
Iteration 5900: Loss = -10880.83203125
Iteration 6000: Loss = -10880.7021484375
Iteration 6100: Loss = -10880.5703125
Iteration 6200: Loss = -10880.4248046875
Iteration 6300: Loss = -10880.2939453125
Iteration 6400: Loss = -10880.189453125
Iteration 6500: Loss = -10880.0908203125
Iteration 6600: Loss = -10879.994140625
Iteration 6700: Loss = -10879.8974609375
Iteration 6800: Loss = -10879.798828125
Iteration 6900: Loss = -10879.693359375
Iteration 7000: Loss = -10874.884765625
Iteration 7100: Loss = -10874.0966796875
Iteration 7200: Loss = -10873.7451171875
Iteration 7300: Loss = -10873.3544921875
Iteration 7400: Loss = -10872.673828125
Iteration 7500: Loss = -10871.791015625
Iteration 7600: Loss = -10870.052734375
Iteration 7700: Loss = -10869.0791015625
Iteration 7800: Loss = -10868.830078125
Iteration 7900: Loss = -10868.7041015625
Iteration 8000: Loss = -10868.5859375
Iteration 8100: Loss = -10868.4521484375
Iteration 8200: Loss = -10868.3447265625
Iteration 8300: Loss = -10868.24609375
Iteration 8400: Loss = -10868.158203125
Iteration 8500: Loss = -10868.0771484375
Iteration 8600: Loss = -10866.9970703125
Iteration 8700: Loss = -10866.8427734375
Iteration 8800: Loss = -10866.75
Iteration 8900: Loss = -10866.6826171875
Iteration 9000: Loss = -10866.634765625
Iteration 9100: Loss = -10866.595703125
Iteration 9200: Loss = -10866.5615234375
Iteration 9300: Loss = -10866.5283203125
Iteration 9400: Loss = -10866.494140625
Iteration 9500: Loss = -10866.4580078125
Iteration 9600: Loss = -10866.4208984375
Iteration 9700: Loss = -10866.376953125
Iteration 9800: Loss = -10866.3564453125
Iteration 9900: Loss = -10866.341796875
Iteration 10000: Loss = -10866.326171875
Iteration 10100: Loss = -10866.3134765625
Iteration 10200: Loss = -10866.30078125
Iteration 10300: Loss = -10866.2890625
Iteration 10400: Loss = -10866.2783203125
Iteration 10500: Loss = -10866.2666015625
Iteration 10600: Loss = -10866.2578125
Iteration 10700: Loss = -10866.248046875
Iteration 10800: Loss = -10866.240234375
Iteration 10900: Loss = -10866.2314453125
Iteration 11000: Loss = -10866.22265625
Iteration 11100: Loss = -10866.2138671875
Iteration 11200: Loss = -10866.2060546875
Iteration 11300: Loss = -10866.19921875
Iteration 11400: Loss = -10866.193359375
Iteration 11500: Loss = -10866.1875
Iteration 11600: Loss = -10866.1806640625
Iteration 11700: Loss = -10866.1591796875
Iteration 11800: Loss = -10866.1396484375
Iteration 11900: Loss = -10866.1298828125
Iteration 12000: Loss = -10866.1162109375
Iteration 12100: Loss = -10866.095703125
Iteration 12200: Loss = -10866.083984375
Iteration 12300: Loss = -10866.0791015625
Iteration 12400: Loss = -10866.0751953125
Iteration 12500: Loss = -10866.0732421875
Iteration 12600: Loss = -10866.06640625
Iteration 12700: Loss = -10866.0634765625
Iteration 12800: Loss = -10866.0615234375
Iteration 12900: Loss = -10866.0595703125
Iteration 13000: Loss = -10866.0576171875
Iteration 13100: Loss = -10866.0556640625
Iteration 13200: Loss = -10866.0537109375
Iteration 13300: Loss = -10866.052734375
Iteration 13400: Loss = -10866.05078125
Iteration 13500: Loss = -10866.048828125
Iteration 13600: Loss = -10866.048828125
Iteration 13700: Loss = -10866.046875
Iteration 13800: Loss = -10866.0458984375
Iteration 13900: Loss = -10866.0439453125
Iteration 14000: Loss = -10866.044921875
1
Iteration 14100: Loss = -10866.04296875
Iteration 14200: Loss = -10866.0419921875
Iteration 14300: Loss = -10866.0390625
Iteration 14400: Loss = -10866.0390625
Iteration 14500: Loss = -10866.0400390625
1
Iteration 14600: Loss = -10866.037109375
Iteration 14700: Loss = -10866.037109375
Iteration 14800: Loss = -10866.0361328125
Iteration 14900: Loss = -10866.0361328125
Iteration 15000: Loss = -10866.0341796875
Iteration 15100: Loss = -10866.03515625
1
Iteration 15200: Loss = -10866.033203125
Iteration 15300: Loss = -10866.0322265625
Iteration 15400: Loss = -10866.033203125
1
Iteration 15500: Loss = -10866.0322265625
Iteration 15600: Loss = -10866.03125
Iteration 15700: Loss = -10866.0302734375
Iteration 15800: Loss = -10866.029296875
Iteration 15900: Loss = -10866.0302734375
1
Iteration 16000: Loss = -10866.029296875
Iteration 16100: Loss = -10866.029296875
Iteration 16200: Loss = -10866.029296875
Iteration 16300: Loss = -10866.02734375
Iteration 16400: Loss = -10866.0283203125
1
Iteration 16500: Loss = -10866.02734375
Iteration 16600: Loss = -10866.02734375
Iteration 16700: Loss = -10866.0283203125
1
Iteration 16800: Loss = -10866.025390625
Iteration 16900: Loss = -10866.02734375
1
Iteration 17000: Loss = -10866.025390625
Iteration 17100: Loss = -10866.025390625
Iteration 17200: Loss = -10866.0234375
Iteration 17300: Loss = -10866.025390625
1
Iteration 17400: Loss = -10866.025390625
2
Iteration 17500: Loss = -10866.0244140625
3
Iteration 17600: Loss = -10866.0244140625
4
Iteration 17700: Loss = -10866.02734375
5
Iteration 17800: Loss = -10866.0234375
Iteration 17900: Loss = -10866.0234375
Iteration 18000: Loss = -10866.0234375
Iteration 18100: Loss = -10866.021484375
Iteration 18200: Loss = -10866.0244140625
1
Iteration 18300: Loss = -10866.0234375
2
Iteration 18400: Loss = -10866.0224609375
3
Iteration 18500: Loss = -10866.0205078125
Iteration 18600: Loss = -10866.021484375
1
Iteration 18700: Loss = -10866.0224609375
2
Iteration 18800: Loss = -10866.0234375
3
Iteration 18900: Loss = -10866.0224609375
4
Iteration 19000: Loss = -10866.0224609375
5
Iteration 19100: Loss = -10866.0205078125
Iteration 19200: Loss = -10866.0234375
1
Iteration 19300: Loss = -10866.0224609375
2
Iteration 19400: Loss = -10866.0224609375
3
Iteration 19500: Loss = -10866.0205078125
Iteration 19600: Loss = -10866.01953125
Iteration 19700: Loss = -10866.0234375
1
Iteration 19800: Loss = -10866.021484375
2
Iteration 19900: Loss = -10866.0205078125
3
Iteration 20000: Loss = -10866.0205078125
4
Iteration 20100: Loss = -10866.021484375
5
Iteration 20200: Loss = -10866.0205078125
6
Iteration 20300: Loss = -10866.0205078125
7
Iteration 20400: Loss = -10866.0205078125
8
Iteration 20500: Loss = -10866.021484375
9
Iteration 20600: Loss = -10866.0205078125
10
Iteration 20700: Loss = -10866.021484375
11
Iteration 20800: Loss = -10866.025390625
12
Iteration 20900: Loss = -10866.0146484375
Iteration 21000: Loss = -10865.8857421875
Iteration 21100: Loss = -10865.703125
Iteration 21200: Loss = -10865.689453125
Iteration 21300: Loss = -10865.666015625
Iteration 21400: Loss = -10865.3291015625
Iteration 21500: Loss = -10865.32421875
Iteration 21600: Loss = -10865.2568359375
Iteration 21700: Loss = -10864.9140625
Iteration 21800: Loss = -10864.9072265625
Iteration 21900: Loss = -10863.76171875
Iteration 22000: Loss = -10863.7333984375
Iteration 22100: Loss = -10863.7275390625
Iteration 22200: Loss = -10863.7158203125
Iteration 22300: Loss = -10863.7158203125
Iteration 22400: Loss = -10863.7158203125
Iteration 22500: Loss = -10863.7138671875
Iteration 22600: Loss = -10863.712890625
Iteration 22700: Loss = -10863.71484375
1
Iteration 22800: Loss = -10863.7119140625
Iteration 22900: Loss = -10863.712890625
1
Iteration 23000: Loss = -10863.712890625
2
Iteration 23100: Loss = -10863.7138671875
3
Iteration 23200: Loss = -10863.7138671875
4
Iteration 23300: Loss = -10863.712890625
5
Iteration 23400: Loss = -10863.712890625
6
Iteration 23500: Loss = -10863.712890625
7
Iteration 23600: Loss = -10863.712890625
8
Iteration 23700: Loss = -10863.712890625
9
Iteration 23800: Loss = -10863.712890625
10
Iteration 23900: Loss = -10863.7119140625
Iteration 24000: Loss = -10863.712890625
1
Iteration 24100: Loss = -10863.712890625
2
Iteration 24200: Loss = -10863.7119140625
Iteration 24300: Loss = -10863.712890625
1
Iteration 24400: Loss = -10863.7119140625
Iteration 24500: Loss = -10863.7119140625
Iteration 24600: Loss = -10863.712890625
1
Iteration 24700: Loss = -10863.7119140625
Iteration 24800: Loss = -10863.712890625
1
Iteration 24900: Loss = -10863.7119140625
Iteration 25000: Loss = -10863.7138671875
1
Iteration 25100: Loss = -10863.712890625
2
Iteration 25200: Loss = -10863.7119140625
Iteration 25300: Loss = -10863.712890625
1
Iteration 25400: Loss = -10863.712890625
2
Iteration 25500: Loss = -10863.712890625
3
Iteration 25600: Loss = -10863.712890625
4
Iteration 25700: Loss = -10863.712890625
5
Iteration 25800: Loss = -10863.7138671875
6
Iteration 25900: Loss = -10863.712890625
7
Iteration 26000: Loss = -10863.712890625
8
Iteration 26100: Loss = -10863.7119140625
Iteration 26200: Loss = -10863.7119140625
Iteration 26300: Loss = -10863.7138671875
1
Iteration 26400: Loss = -10863.712890625
2
Iteration 26500: Loss = -10863.712890625
3
Iteration 26600: Loss = -10863.7138671875
4
Iteration 26700: Loss = -10863.712890625
5
Iteration 26800: Loss = -10863.7119140625
Iteration 26900: Loss = -10863.712890625
1
Iteration 27000: Loss = -10863.712890625
2
Iteration 27100: Loss = -10863.712890625
3
Iteration 27200: Loss = -10863.7138671875
4
Iteration 27300: Loss = -10863.712890625
5
Iteration 27400: Loss = -10863.7119140625
Iteration 27500: Loss = -10863.7119140625
Iteration 27600: Loss = -10863.712890625
1
Iteration 27700: Loss = -10863.7109375
Iteration 27800: Loss = -10863.7138671875
1
Iteration 27900: Loss = -10863.712890625
2
Iteration 28000: Loss = -10863.712890625
3
Iteration 28100: Loss = -10863.7138671875
4
Iteration 28200: Loss = -10863.7119140625
5
Iteration 28300: Loss = -10863.7119140625
6
Iteration 28400: Loss = -10863.7119140625
7
Iteration 28500: Loss = -10863.712890625
8
Iteration 28600: Loss = -10863.7119140625
9
Iteration 28700: Loss = -10863.712890625
10
Iteration 28800: Loss = -10863.712890625
11
Iteration 28900: Loss = -10863.7109375
Iteration 29000: Loss = -10863.712890625
1
Iteration 29100: Loss = -10863.712890625
2
Iteration 29200: Loss = -10863.712890625
3
Iteration 29300: Loss = -10863.7119140625
4
Iteration 29400: Loss = -10863.712890625
5
Iteration 29500: Loss = -10863.7119140625
6
Iteration 29600: Loss = -10863.7119140625
7
Iteration 29700: Loss = -10863.71875
8
Iteration 29800: Loss = -10863.712890625
9
Iteration 29900: Loss = -10863.712890625
10
pi: tensor([[1.0000e+00, 6.5533e-07],
        [6.0390e-01, 3.9610e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8299, 0.1701], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1584, 0.1437],
         [0.7674, 0.5104]],

        [[0.2113, 0.1648],
         [0.0779, 0.0163]],

        [[0.0707, 0.1150],
         [0.0149, 0.0403]],

        [[0.2808, 0.1301],
         [0.6250, 0.0740]],

        [[0.0155, 0.4396],
         [0.3438, 0.2574]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.07167087033623441
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.03551074090311267
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.009987515605493134
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.04436926465009276
Average Adjusted Rand Index: 0.023433825368968042
[0.04118978792187261, 0.04436926465009276] [0.023193367538009012, 0.023433825368968042] [10861.185546875, 10863.7119140625]
-------------------------------------
This iteration is 46
True Objective function: Loss = -10877.2431628145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14428.28515625
Iteration 100: Loss = -12033.8349609375
Iteration 200: Loss = -11211.09375
Iteration 300: Loss = -11034.6337890625
Iteration 400: Loss = -10986.5498046875
Iteration 500: Loss = -10964.0771484375
Iteration 600: Loss = -10953.5302734375
Iteration 700: Loss = -10949.435546875
Iteration 800: Loss = -10946.5556640625
Iteration 900: Loss = -10945.1298828125
Iteration 1000: Loss = -10944.2509765625
Iteration 1100: Loss = -10942.17578125
Iteration 1200: Loss = -10941.6689453125
Iteration 1300: Loss = -10941.328125
Iteration 1400: Loss = -10941.048828125
Iteration 1500: Loss = -10940.8583984375
Iteration 1600: Loss = -10940.7236328125
Iteration 1700: Loss = -10940.619140625
Iteration 1800: Loss = -10940.5400390625
Iteration 1900: Loss = -10940.478515625
Iteration 2000: Loss = -10940.4296875
Iteration 2100: Loss = -10940.3896484375
Iteration 2200: Loss = -10940.35546875
Iteration 2300: Loss = -10940.3271484375
Iteration 2400: Loss = -10940.30078125
Iteration 2500: Loss = -10940.279296875
Iteration 2600: Loss = -10940.2578125
Iteration 2700: Loss = -10940.2421875
Iteration 2800: Loss = -10940.22265625
Iteration 2900: Loss = -10940.203125
Iteration 3000: Loss = -10940.1845703125
Iteration 3100: Loss = -10940.1611328125
Iteration 3200: Loss = -10940.126953125
Iteration 3300: Loss = -10940.0673828125
Iteration 3400: Loss = -10940.03515625
Iteration 3500: Loss = -10939.9853515625
Iteration 3600: Loss = -10939.84375
Iteration 3700: Loss = -10939.71484375
Iteration 3800: Loss = -10939.525390625
Iteration 3900: Loss = -10939.2958984375
Iteration 4000: Loss = -10938.98828125
Iteration 4100: Loss = -10938.3701171875
Iteration 4200: Loss = -10921.392578125
Iteration 4300: Loss = -10867.3642578125
Iteration 4400: Loss = -10840.083984375
Iteration 4500: Loss = -10837.9072265625
Iteration 4600: Loss = -10837.576171875
Iteration 4700: Loss = -10837.3896484375
Iteration 4800: Loss = -10837.30078125
Iteration 4900: Loss = -10837.2265625
Iteration 5000: Loss = -10837.130859375
Iteration 5100: Loss = -10837.076171875
Iteration 5200: Loss = -10834.0517578125
Iteration 5300: Loss = -10833.5966796875
Iteration 5400: Loss = -10833.46875
Iteration 5500: Loss = -10833.1025390625
Iteration 5600: Loss = -10832.607421875
Iteration 5700: Loss = -10832.5634765625
Iteration 5800: Loss = -10832.5458984375
Iteration 5900: Loss = -10832.533203125
Iteration 6000: Loss = -10832.517578125
Iteration 6100: Loss = -10832.4951171875
Iteration 6200: Loss = -10832.4892578125
Iteration 6300: Loss = -10832.4814453125
Iteration 6400: Loss = -10832.4775390625
Iteration 6500: Loss = -10832.4716796875
Iteration 6600: Loss = -10832.46875
Iteration 6700: Loss = -10832.4658203125
Iteration 6800: Loss = -10832.4619140625
Iteration 6900: Loss = -10832.4580078125
Iteration 7000: Loss = -10832.455078125
Iteration 7100: Loss = -10832.4501953125
Iteration 7200: Loss = -10832.421875
Iteration 7300: Loss = -10832.32421875
Iteration 7400: Loss = -10832.3212890625
Iteration 7500: Loss = -10832.3193359375
Iteration 7600: Loss = -10832.3056640625
Iteration 7700: Loss = -10832.1708984375
Iteration 7800: Loss = -10832.16796875
Iteration 7900: Loss = -10832.1552734375
Iteration 8000: Loss = -10832.0966796875
Iteration 8100: Loss = -10832.095703125
Iteration 8200: Loss = -10832.0927734375
Iteration 8300: Loss = -10832.0927734375
Iteration 8400: Loss = -10832.08984375
Iteration 8500: Loss = -10832.0888671875
Iteration 8600: Loss = -10832.0869140625
Iteration 8700: Loss = -10832.0849609375
Iteration 8800: Loss = -10832.0849609375
Iteration 8900: Loss = -10832.083984375
Iteration 9000: Loss = -10832.08203125
Iteration 9100: Loss = -10832.052734375
Iteration 9200: Loss = -10832.037109375
Iteration 9300: Loss = -10832.0166015625
Iteration 9400: Loss = -10832.0146484375
Iteration 9500: Loss = -10832.015625
1
Iteration 9600: Loss = -10832.015625
2
Iteration 9700: Loss = -10832.0146484375
Iteration 9800: Loss = -10832.015625
1
Iteration 9900: Loss = -10832.0146484375
Iteration 10000: Loss = -10832.0146484375
Iteration 10100: Loss = -10832.0146484375
Iteration 10200: Loss = -10832.013671875
Iteration 10300: Loss = -10832.0126953125
Iteration 10400: Loss = -10832.0126953125
Iteration 10500: Loss = -10832.013671875
1
Iteration 10600: Loss = -10832.0126953125
Iteration 10700: Loss = -10832.01171875
Iteration 10800: Loss = -10832.01171875
Iteration 10900: Loss = -10832.01171875
Iteration 11000: Loss = -10832.009765625
Iteration 11100: Loss = -10831.8359375
Iteration 11200: Loss = -10831.830078125
Iteration 11300: Loss = -10831.830078125
Iteration 11400: Loss = -10831.830078125
Iteration 11500: Loss = -10831.8291015625
Iteration 11600: Loss = -10831.830078125
1
Iteration 11700: Loss = -10831.830078125
2
Iteration 11800: Loss = -10831.8291015625
Iteration 11900: Loss = -10831.8291015625
Iteration 12000: Loss = -10831.830078125
1
Iteration 12100: Loss = -10831.828125
Iteration 12200: Loss = -10831.830078125
1
Iteration 12300: Loss = -10831.8291015625
2
Iteration 12400: Loss = -10831.8291015625
3
Iteration 12500: Loss = -10831.828125
Iteration 12600: Loss = -10831.830078125
1
Iteration 12700: Loss = -10831.828125
Iteration 12800: Loss = -10831.828125
Iteration 12900: Loss = -10831.828125
Iteration 13000: Loss = -10831.8271484375
Iteration 13100: Loss = -10831.8271484375
Iteration 13200: Loss = -10831.8271484375
Iteration 13300: Loss = -10831.8271484375
Iteration 13400: Loss = -10831.826171875
Iteration 13500: Loss = -10831.8271484375
1
Iteration 13600: Loss = -10831.826171875
Iteration 13700: Loss = -10831.826171875
Iteration 13800: Loss = -10831.8271484375
1
Iteration 13900: Loss = -10831.826171875
Iteration 14000: Loss = -10831.8251953125
Iteration 14100: Loss = -10831.8251953125
Iteration 14200: Loss = -10831.8271484375
1
Iteration 14300: Loss = -10831.828125
2
Iteration 14400: Loss = -10831.826171875
3
Iteration 14500: Loss = -10831.826171875
4
Iteration 14600: Loss = -10831.826171875
5
Iteration 14700: Loss = -10831.826171875
6
Iteration 14800: Loss = -10831.826171875
7
Iteration 14900: Loss = -10831.826171875
8
Iteration 15000: Loss = -10831.8232421875
Iteration 15100: Loss = -10831.8232421875
Iteration 15200: Loss = -10831.8232421875
Iteration 15300: Loss = -10831.82421875
1
Iteration 15400: Loss = -10831.82421875
2
Iteration 15500: Loss = -10831.82421875
3
Iteration 15600: Loss = -10831.82421875
4
Iteration 15700: Loss = -10831.82421875
5
Iteration 15800: Loss = -10831.8232421875
Iteration 15900: Loss = -10831.82421875
1
Iteration 16000: Loss = -10831.822265625
Iteration 16100: Loss = -10831.8232421875
1
Iteration 16200: Loss = -10831.798828125
Iteration 16300: Loss = -10831.798828125
Iteration 16400: Loss = -10831.80078125
1
Iteration 16500: Loss = -10831.798828125
Iteration 16600: Loss = -10831.798828125
Iteration 16700: Loss = -10831.798828125
Iteration 16800: Loss = -10831.798828125
Iteration 16900: Loss = -10831.798828125
Iteration 17000: Loss = -10831.798828125
Iteration 17100: Loss = -10831.80078125
1
Iteration 17200: Loss = -10831.798828125
Iteration 17300: Loss = -10831.798828125
Iteration 17400: Loss = -10831.798828125
Iteration 17500: Loss = -10831.80078125
1
Iteration 17600: Loss = -10831.796875
Iteration 17700: Loss = -10831.796875
Iteration 17800: Loss = -10831.7958984375
Iteration 17900: Loss = -10831.7958984375
Iteration 18000: Loss = -10831.7958984375
Iteration 18100: Loss = -10831.794921875
Iteration 18200: Loss = -10831.7958984375
1
Iteration 18300: Loss = -10831.7958984375
2
Iteration 18400: Loss = -10831.7958984375
3
Iteration 18500: Loss = -10831.794921875
Iteration 18600: Loss = -10831.7958984375
1
Iteration 18700: Loss = -10831.7958984375
2
Iteration 18800: Loss = -10831.7958984375
3
Iteration 18900: Loss = -10831.7958984375
4
Iteration 19000: Loss = -10831.794921875
Iteration 19100: Loss = -10831.7958984375
1
Iteration 19200: Loss = -10831.796875
2
Iteration 19300: Loss = -10831.7958984375
3
Iteration 19400: Loss = -10831.7958984375
4
Iteration 19500: Loss = -10831.7958984375
5
Iteration 19600: Loss = -10831.7958984375
6
Iteration 19700: Loss = -10831.794921875
Iteration 19800: Loss = -10831.7958984375
1
Iteration 19900: Loss = -10831.7958984375
2
Iteration 20000: Loss = -10831.7958984375
3
Iteration 20100: Loss = -10831.794921875
Iteration 20200: Loss = -10831.796875
1
Iteration 20300: Loss = -10831.794921875
Iteration 20400: Loss = -10831.7958984375
1
Iteration 20500: Loss = -10831.7958984375
2
Iteration 20600: Loss = -10831.7978515625
3
Iteration 20700: Loss = -10831.7958984375
4
Iteration 20800: Loss = -10831.794921875
Iteration 20900: Loss = -10831.7958984375
1
Iteration 21000: Loss = -10831.796875
2
Iteration 21100: Loss = -10831.794921875
Iteration 21200: Loss = -10831.794921875
Iteration 21300: Loss = -10831.7958984375
1
Iteration 21400: Loss = -10831.7958984375
2
Iteration 21500: Loss = -10831.7958984375
3
Iteration 21600: Loss = -10831.7958984375
4
Iteration 21700: Loss = -10831.7958984375
5
Iteration 21800: Loss = -10831.794921875
Iteration 21900: Loss = -10831.794921875
Iteration 22000: Loss = -10831.794921875
Iteration 22100: Loss = -10831.7958984375
1
Iteration 22200: Loss = -10831.794921875
Iteration 22300: Loss = -10831.7958984375
1
Iteration 22400: Loss = -10831.7958984375
2
Iteration 22500: Loss = -10831.7958984375
3
Iteration 22600: Loss = -10831.794921875
Iteration 22700: Loss = -10831.794921875
Iteration 22800: Loss = -10831.794921875
Iteration 22900: Loss = -10831.794921875
Iteration 23000: Loss = -10831.794921875
Iteration 23100: Loss = -10831.796875
1
Iteration 23200: Loss = -10831.794921875
Iteration 23300: Loss = -10831.7958984375
1
Iteration 23400: Loss = -10831.794921875
Iteration 23500: Loss = -10831.794921875
Iteration 23600: Loss = -10831.794921875
Iteration 23700: Loss = -10831.7939453125
Iteration 23800: Loss = -10831.79296875
Iteration 23900: Loss = -10831.79296875
Iteration 24000: Loss = -10831.7939453125
1
Iteration 24100: Loss = -10831.79296875
Iteration 24200: Loss = -10831.79296875
Iteration 24300: Loss = -10831.7939453125
1
Iteration 24400: Loss = -10831.79296875
Iteration 24500: Loss = -10831.79296875
Iteration 24600: Loss = -10831.7939453125
1
Iteration 24700: Loss = -10831.79296875
Iteration 24800: Loss = -10831.79296875
Iteration 24900: Loss = -10831.7939453125
1
Iteration 25000: Loss = -10831.79296875
Iteration 25100: Loss = -10831.79296875
Iteration 25200: Loss = -10831.79296875
Iteration 25300: Loss = -10831.79296875
Iteration 25400: Loss = -10831.7939453125
1
Iteration 25500: Loss = -10831.79296875
Iteration 25600: Loss = -10831.7939453125
1
Iteration 25700: Loss = -10831.79296875
Iteration 25800: Loss = -10831.79296875
Iteration 25900: Loss = -10831.7939453125
1
Iteration 26000: Loss = -10831.79296875
Iteration 26100: Loss = -10831.7939453125
1
Iteration 26200: Loss = -10831.79296875
Iteration 26300: Loss = -10831.7939453125
1
Iteration 26400: Loss = -10831.79296875
Iteration 26500: Loss = -10831.79296875
Iteration 26600: Loss = -10831.79296875
Iteration 26700: Loss = -10831.79296875
Iteration 26800: Loss = -10831.7939453125
1
Iteration 26900: Loss = -10831.79296875
Iteration 27000: Loss = -10831.7939453125
1
Iteration 27100: Loss = -10831.79296875
Iteration 27200: Loss = -10831.7939453125
1
Iteration 27300: Loss = -10831.7939453125
2
Iteration 27400: Loss = -10831.79296875
Iteration 27500: Loss = -10831.7939453125
1
Iteration 27600: Loss = -10831.79296875
Iteration 27700: Loss = -10831.79296875
Iteration 27800: Loss = -10831.7939453125
1
Iteration 27900: Loss = -10831.7919921875
Iteration 28000: Loss = -10831.79296875
1
Iteration 28100: Loss = -10831.7939453125
2
Iteration 28200: Loss = -10831.79296875
3
Iteration 28300: Loss = -10831.79296875
4
Iteration 28400: Loss = -10831.79296875
5
Iteration 28500: Loss = -10831.79296875
6
Iteration 28600: Loss = -10831.7939453125
7
Iteration 28700: Loss = -10831.7939453125
8
Iteration 28800: Loss = -10831.7939453125
9
Iteration 28900: Loss = -10831.79296875
10
Iteration 29000: Loss = -10831.7939453125
11
Iteration 29100: Loss = -10831.79296875
12
Iteration 29200: Loss = -10831.79296875
13
Iteration 29300: Loss = -10831.79296875
14
Iteration 29400: Loss = -10831.7939453125
15
Stopping early at iteration 29400 due to no improvement.
pi: tensor([[0.7626, 0.2374],
        [0.1915, 0.8085]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5758, 0.4242], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1035],
         [0.9754, 0.2427]],

        [[0.0085, 0.1048],
         [0.0737, 0.8192]],

        [[0.7645, 0.1023],
         [0.9714, 0.0138]],

        [[0.0386, 0.0978],
         [0.9483, 0.9220]],

        [[0.1586, 0.1068],
         [0.8653, 0.0283]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448376182574403
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7025982975809663
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7720979591836735
Global Adjusted Rand Index: 0.8096185289456462
Average Adjusted Rand Index: 0.8096833375780367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42375.1796875
Iteration 100: Loss = -25099.421875
Iteration 200: Loss = -14641.7587890625
Iteration 300: Loss = -11831.291015625
Iteration 400: Loss = -11409.58984375
Iteration 500: Loss = -11231.19140625
Iteration 600: Loss = -11139.33984375
Iteration 700: Loss = -11087.09375
Iteration 800: Loss = -11056.6923828125
Iteration 900: Loss = -11025.7373046875
Iteration 1000: Loss = -11011.23828125
Iteration 1100: Loss = -11000.4443359375
Iteration 1200: Loss = -10992.0595703125
Iteration 1300: Loss = -10985.3759765625
Iteration 1400: Loss = -10979.9453125
Iteration 1500: Loss = -10975.4560546875
Iteration 1600: Loss = -10971.69921875
Iteration 1700: Loss = -10968.5107421875
Iteration 1800: Loss = -10965.779296875
Iteration 1900: Loss = -10963.3935546875
Iteration 2000: Loss = -10961.32421875
Iteration 2100: Loss = -10959.5361328125
Iteration 2200: Loss = -10957.9638671875
Iteration 2300: Loss = -10956.5693359375
Iteration 2400: Loss = -10955.3291015625
Iteration 2500: Loss = -10954.2197265625
Iteration 2600: Loss = -10953.2236328125
Iteration 2700: Loss = -10952.3232421875
Iteration 2800: Loss = -10951.5107421875
Iteration 2900: Loss = -10950.771484375
Iteration 3000: Loss = -10950.0986328125
Iteration 3100: Loss = -10949.484375
Iteration 3200: Loss = -10948.9228515625
Iteration 3300: Loss = -10948.40625
Iteration 3400: Loss = -10947.93359375
Iteration 3500: Loss = -10947.498046875
Iteration 3600: Loss = -10947.09375
Iteration 3700: Loss = -10946.72265625
Iteration 3800: Loss = -10946.376953125
Iteration 3900: Loss = -10946.0595703125
Iteration 4000: Loss = -10945.76171875
Iteration 4100: Loss = -10945.486328125
Iteration 4200: Loss = -10945.2294921875
Iteration 4300: Loss = -10944.990234375
Iteration 4400: Loss = -10944.7646484375
Iteration 4500: Loss = -10944.5556640625
Iteration 4600: Loss = -10944.357421875
Iteration 4700: Loss = -10944.16796875
Iteration 4800: Loss = -10943.9921875
Iteration 4900: Loss = -10943.822265625
Iteration 5000: Loss = -10943.66015625
Iteration 5100: Loss = -10943.505859375
Iteration 5200: Loss = -10943.3603515625
Iteration 5300: Loss = -10943.2216796875
Iteration 5400: Loss = -10943.0927734375
Iteration 5500: Loss = -10942.97265625
Iteration 5600: Loss = -10942.8603515625
Iteration 5700: Loss = -10942.755859375
Iteration 5800: Loss = -10942.66015625
Iteration 5900: Loss = -10942.5732421875
Iteration 6000: Loss = -10942.4921875
Iteration 6100: Loss = -10942.416015625
Iteration 6200: Loss = -10942.34765625
Iteration 6300: Loss = -10942.2841796875
Iteration 6400: Loss = -10942.224609375
Iteration 6500: Loss = -10942.1708984375
Iteration 6600: Loss = -10942.12109375
Iteration 6700: Loss = -10942.0732421875
Iteration 6800: Loss = -10942.0283203125
Iteration 6900: Loss = -10941.9873046875
Iteration 7000: Loss = -10941.951171875
Iteration 7100: Loss = -10941.9150390625
Iteration 7200: Loss = -10941.8818359375
Iteration 7300: Loss = -10941.8505859375
Iteration 7400: Loss = -10941.8232421875
Iteration 7500: Loss = -10941.794921875
Iteration 7600: Loss = -10941.76953125
Iteration 7700: Loss = -10941.74609375
Iteration 7800: Loss = -10941.72265625
Iteration 7900: Loss = -10941.701171875
Iteration 8000: Loss = -10941.6796875
Iteration 8100: Loss = -10941.6611328125
Iteration 8200: Loss = -10941.640625
Iteration 8300: Loss = -10941.6259765625
Iteration 8400: Loss = -10941.6083984375
Iteration 8500: Loss = -10941.5947265625
Iteration 8600: Loss = -10941.578125
Iteration 8700: Loss = -10941.5654296875
Iteration 8800: Loss = -10941.5517578125
Iteration 8900: Loss = -10941.5400390625
Iteration 9000: Loss = -10941.529296875
Iteration 9100: Loss = -10941.517578125
Iteration 9200: Loss = -10941.5078125
Iteration 9300: Loss = -10941.498046875
Iteration 9400: Loss = -10941.4892578125
Iteration 9500: Loss = -10941.478515625
Iteration 9600: Loss = -10941.470703125
Iteration 9700: Loss = -10941.462890625
Iteration 9800: Loss = -10941.455078125
Iteration 9900: Loss = -10941.447265625
Iteration 10000: Loss = -10941.4404296875
Iteration 10100: Loss = -10941.43359375
Iteration 10200: Loss = -10941.4287109375
Iteration 10300: Loss = -10941.423828125
Iteration 10400: Loss = -10941.4189453125
Iteration 10500: Loss = -10941.4111328125
Iteration 10600: Loss = -10941.4072265625
Iteration 10700: Loss = -10941.4033203125
Iteration 10800: Loss = -10941.3974609375
Iteration 10900: Loss = -10941.392578125
Iteration 11000: Loss = -10941.390625
Iteration 11100: Loss = -10941.384765625
Iteration 11200: Loss = -10941.380859375
Iteration 11300: Loss = -10941.376953125
Iteration 11400: Loss = -10941.375
Iteration 11500: Loss = -10941.37109375
Iteration 11600: Loss = -10941.3681640625
Iteration 11700: Loss = -10941.3662109375
Iteration 11800: Loss = -10941.3623046875
Iteration 11900: Loss = -10941.361328125
Iteration 12000: Loss = -10941.357421875
Iteration 12100: Loss = -10941.359375
1
Iteration 12200: Loss = -10941.3525390625
Iteration 12300: Loss = -10941.3505859375
Iteration 12400: Loss = -10941.34765625
Iteration 12500: Loss = -10941.3447265625
Iteration 12600: Loss = -10941.3427734375
Iteration 12700: Loss = -10941.3388671875
Iteration 12800: Loss = -10941.3369140625
Iteration 12900: Loss = -10941.3349609375
Iteration 13000: Loss = -10941.333984375
Iteration 13100: Loss = -10941.33203125
Iteration 13200: Loss = -10941.3291015625
Iteration 13300: Loss = -10941.326171875
Iteration 13400: Loss = -10941.3232421875
Iteration 13500: Loss = -10941.3154296875
Iteration 13600: Loss = -10941.306640625
Iteration 13700: Loss = -10941.294921875
Iteration 13800: Loss = -10941.2861328125
Iteration 13900: Loss = -10941.2744140625
Iteration 14000: Loss = -10941.2666015625
Iteration 14100: Loss = -10941.2587890625
Iteration 14200: Loss = -10941.2529296875
Iteration 14300: Loss = -10941.2490234375
Iteration 14400: Loss = -10941.2412109375
Iteration 14500: Loss = -10941.2353515625
Iteration 14600: Loss = -10941.2294921875
Iteration 14700: Loss = -10941.2197265625
Iteration 14800: Loss = -10941.2099609375
Iteration 14900: Loss = -10941.1962890625
Iteration 15000: Loss = -10941.1787109375
Iteration 15100: Loss = -10941.15234375
Iteration 15200: Loss = -10941.1083984375
Iteration 15300: Loss = -10941.025390625
Iteration 15400: Loss = -10940.8330078125
Iteration 15500: Loss = -10940.517578125
Iteration 15600: Loss = -10940.3671875
Iteration 15700: Loss = -10940.3037109375
Iteration 15800: Loss = -10940.2783203125
Iteration 15900: Loss = -10940.2626953125
Iteration 16000: Loss = -10940.2451171875
Iteration 16100: Loss = -10940.2373046875
Iteration 16200: Loss = -10940.2265625
Iteration 16300: Loss = -10940.220703125
Iteration 16400: Loss = -10940.2158203125
Iteration 16500: Loss = -10940.2138671875
Iteration 16600: Loss = -10940.2119140625
Iteration 16700: Loss = -10940.2099609375
Iteration 16800: Loss = -10940.2080078125
Iteration 16900: Loss = -10940.2080078125
Iteration 17000: Loss = -10940.20703125
Iteration 17100: Loss = -10940.20703125
Iteration 17200: Loss = -10940.20703125
Iteration 17300: Loss = -10940.2041015625
Iteration 17400: Loss = -10940.205078125
1
Iteration 17500: Loss = -10940.205078125
2
Iteration 17600: Loss = -10940.2041015625
Iteration 17700: Loss = -10940.205078125
1
Iteration 17800: Loss = -10940.2041015625
Iteration 17900: Loss = -10940.203125
Iteration 18000: Loss = -10940.205078125
1
Iteration 18100: Loss = -10940.2041015625
2
Iteration 18200: Loss = -10940.2041015625
3
Iteration 18300: Loss = -10940.2021484375
Iteration 18400: Loss = -10940.203125
1
Iteration 18500: Loss = -10940.203125
2
Iteration 18600: Loss = -10940.203125
3
Iteration 18700: Loss = -10940.203125
4
Iteration 18800: Loss = -10940.201171875
Iteration 18900: Loss = -10940.205078125
1
Iteration 19000: Loss = -10940.203125
2
Iteration 19100: Loss = -10940.2021484375
3
Iteration 19200: Loss = -10940.203125
4
Iteration 19300: Loss = -10940.2021484375
5
Iteration 19400: Loss = -10940.2021484375
6
Iteration 19500: Loss = -10940.201171875
Iteration 19600: Loss = -10940.203125
1
Iteration 19700: Loss = -10940.201171875
Iteration 19800: Loss = -10940.201171875
Iteration 19900: Loss = -10940.203125
1
Iteration 20000: Loss = -10940.201171875
Iteration 20100: Loss = -10940.2021484375
1
Iteration 20200: Loss = -10940.201171875
Iteration 20300: Loss = -10940.2021484375
1
Iteration 20400: Loss = -10940.201171875
Iteration 20500: Loss = -10940.201171875
Iteration 20600: Loss = -10940.2021484375
1
Iteration 20700: Loss = -10940.203125
2
Iteration 20800: Loss = -10940.2021484375
3
Iteration 20900: Loss = -10940.2021484375
4
Iteration 21000: Loss = -10940.201171875
Iteration 21100: Loss = -10940.2001953125
Iteration 21200: Loss = -10940.203125
1
Iteration 21300: Loss = -10940.201171875
2
Iteration 21400: Loss = -10940.2021484375
3
Iteration 21500: Loss = -10940.2021484375
4
Iteration 21600: Loss = -10940.2001953125
Iteration 21700: Loss = -10940.201171875
1
Iteration 21800: Loss = -10940.201171875
2
Iteration 21900: Loss = -10940.201171875
3
Iteration 22000: Loss = -10940.201171875
4
Iteration 22100: Loss = -10940.2001953125
Iteration 22200: Loss = -10940.2021484375
1
Iteration 22300: Loss = -10940.203125
2
Iteration 22400: Loss = -10940.2001953125
Iteration 22500: Loss = -10940.2021484375
1
Iteration 22600: Loss = -10940.201171875
2
Iteration 22700: Loss = -10940.2021484375
3
Iteration 22800: Loss = -10940.201171875
4
Iteration 22900: Loss = -10940.2021484375
5
Iteration 23000: Loss = -10940.2021484375
6
Iteration 23100: Loss = -10940.2001953125
Iteration 23200: Loss = -10940.201171875
1
Iteration 23300: Loss = -10940.201171875
2
Iteration 23400: Loss = -10940.201171875
3
Iteration 23500: Loss = -10940.2021484375
4
Iteration 23600: Loss = -10940.201171875
5
Iteration 23700: Loss = -10940.201171875
6
Iteration 23800: Loss = -10940.201171875
7
Iteration 23900: Loss = -10940.2021484375
8
Iteration 24000: Loss = -10940.2021484375
9
Iteration 24100: Loss = -10940.201171875
10
Iteration 24200: Loss = -10940.201171875
11
Iteration 24300: Loss = -10940.2021484375
12
Iteration 24400: Loss = -10940.201171875
13
Iteration 24500: Loss = -10940.201171875
14
Iteration 24600: Loss = -10940.2001953125
Iteration 24700: Loss = -10940.201171875
1
Iteration 24800: Loss = -10940.201171875
2
Iteration 24900: Loss = -10940.201171875
3
Iteration 25000: Loss = -10940.201171875
4
Iteration 25100: Loss = -10940.2021484375
5
Iteration 25200: Loss = -10940.201171875
6
Iteration 25300: Loss = -10940.201171875
7
Iteration 25400: Loss = -10940.2021484375
8
Iteration 25500: Loss = -10940.2001953125
Iteration 25600: Loss = -10940.201171875
1
Iteration 25700: Loss = -10940.2021484375
2
Iteration 25800: Loss = -10940.2001953125
Iteration 25900: Loss = -10940.201171875
1
Iteration 26000: Loss = -10940.19921875
Iteration 26100: Loss = -10940.2021484375
1
Iteration 26200: Loss = -10940.2021484375
2
Iteration 26300: Loss = -10940.201171875
3
Iteration 26400: Loss = -10940.2001953125
4
Iteration 26500: Loss = -10940.201171875
5
Iteration 26600: Loss = -10940.2001953125
6
Iteration 26700: Loss = -10940.2021484375
7
Iteration 26800: Loss = -10940.201171875
8
Iteration 26900: Loss = -10940.2021484375
9
Iteration 27000: Loss = -10940.201171875
10
Iteration 27100: Loss = -10940.201171875
11
Iteration 27200: Loss = -10940.201171875
12
Iteration 27300: Loss = -10940.2001953125
13
Iteration 27400: Loss = -10940.2001953125
14
Iteration 27500: Loss = -10940.201171875
15
Stopping early at iteration 27500 due to no improvement.
pi: tensor([[6.8577e-06, 9.9999e-01],
        [3.6650e-02, 9.6335e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.0295e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2390, 0.1680],
         [0.4164, 0.1598]],

        [[0.9799, 0.1891],
         [0.0317, 0.9581]],

        [[0.0113, 0.1192],
         [0.9454, 0.0943]],

        [[0.4388, 0.2198],
         [0.0205, 0.2279]],

        [[0.2004, 0.2111],
         [0.2103, 0.8862]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.8096185289456462, 0.0] [0.8096833375780367, 0.0] [10831.7939453125, 10940.201171875]
-------------------------------------
This iteration is 47
True Objective function: Loss = -10774.101417585178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31790.9140625
Iteration 100: Loss = -20931.013671875
Iteration 200: Loss = -12565.5810546875
Iteration 300: Loss = -11416.8408203125
Iteration 400: Loss = -11222.0791015625
Iteration 500: Loss = -11144.34765625
Iteration 600: Loss = -11099.26953125
Iteration 700: Loss = -11071.7119140625
Iteration 800: Loss = -11045.7587890625
Iteration 900: Loss = -11027.1611328125
Iteration 1000: Loss = -11011.505859375
Iteration 1100: Loss = -11000.8671875
Iteration 1200: Loss = -10993.150390625
Iteration 1300: Loss = -10982.208984375
Iteration 1400: Loss = -10970.025390625
Iteration 1500: Loss = -10960.2900390625
Iteration 1600: Loss = -10947.8779296875
Iteration 1700: Loss = -10940.9638671875
Iteration 1800: Loss = -10933.791015625
Iteration 1900: Loss = -10923.4033203125
Iteration 2000: Loss = -10918.8154296875
Iteration 2100: Loss = -10911.3349609375
Iteration 2200: Loss = -10902.529296875
Iteration 2300: Loss = -10895.7001953125
Iteration 2400: Loss = -10886.861328125
Iteration 2500: Loss = -10880.5498046875
Iteration 2600: Loss = -10876.5703125
Iteration 2700: Loss = -10870.662109375
Iteration 2800: Loss = -10866.1806640625
Iteration 2900: Loss = -10860.6689453125
Iteration 3000: Loss = -10858.7763671875
Iteration 3100: Loss = -10857.1611328125
Iteration 3200: Loss = -10855.61328125
Iteration 3300: Loss = -10854.1142578125
Iteration 3400: Loss = -10852.736328125
Iteration 3500: Loss = -10851.537109375
Iteration 3600: Loss = -10850.521484375
Iteration 3700: Loss = -10849.6416015625
Iteration 3800: Loss = -10848.8603515625
Iteration 3900: Loss = -10848.14453125
Iteration 4000: Loss = -10847.48828125
Iteration 4100: Loss = -10846.8857421875
Iteration 4200: Loss = -10846.248046875
Iteration 4300: Loss = -10845.466796875
Iteration 4400: Loss = -10843.7109375
Iteration 4500: Loss = -10839.75390625
Iteration 4600: Loss = -10838.9375
Iteration 4700: Loss = -10838.4521484375
Iteration 4800: Loss = -10838.111328125
Iteration 4900: Loss = -10837.84765625
Iteration 5000: Loss = -10837.6298828125
Iteration 5100: Loss = -10837.4482421875
Iteration 5200: Loss = -10837.291015625
Iteration 5300: Loss = -10837.1552734375
Iteration 5400: Loss = -10837.0341796875
Iteration 5500: Loss = -10836.92578125
Iteration 5600: Loss = -10836.828125
Iteration 5700: Loss = -10836.7392578125
Iteration 5800: Loss = -10836.66015625
Iteration 5900: Loss = -10836.5869140625
Iteration 6000: Loss = -10836.521484375
Iteration 6100: Loss = -10836.4609375
Iteration 6200: Loss = -10836.404296875
Iteration 6300: Loss = -10836.3525390625
Iteration 6400: Loss = -10836.30078125
Iteration 6500: Loss = -10836.25390625
Iteration 6600: Loss = -10836.2119140625
Iteration 6700: Loss = -10836.1728515625
Iteration 6800: Loss = -10836.13671875
Iteration 6900: Loss = -10836.1015625
Iteration 7000: Loss = -10836.0712890625
Iteration 7100: Loss = -10836.0419921875
Iteration 7200: Loss = -10836.0146484375
Iteration 7300: Loss = -10835.98828125
Iteration 7400: Loss = -10835.96484375
Iteration 7500: Loss = -10835.9423828125
Iteration 7600: Loss = -10835.921875
Iteration 7700: Loss = -10835.9033203125
Iteration 7800: Loss = -10835.884765625
Iteration 7900: Loss = -10835.8671875
Iteration 8000: Loss = -10835.8525390625
Iteration 8100: Loss = -10835.8369140625
Iteration 8200: Loss = -10835.822265625
Iteration 8300: Loss = -10835.8076171875
Iteration 8400: Loss = -10835.794921875
Iteration 8500: Loss = -10835.7802734375
Iteration 8600: Loss = -10835.7685546875
Iteration 8700: Loss = -10835.755859375
Iteration 8800: Loss = -10835.7412109375
Iteration 8900: Loss = -10835.73046875
Iteration 9000: Loss = -10835.7177734375
Iteration 9100: Loss = -10835.7041015625
Iteration 9200: Loss = -10835.693359375
Iteration 9300: Loss = -10835.6806640625
Iteration 9400: Loss = -10835.6669921875
Iteration 9500: Loss = -10835.6533203125
Iteration 9600: Loss = -10835.6396484375
Iteration 9700: Loss = -10835.6259765625
Iteration 9800: Loss = -10835.6083984375
Iteration 9900: Loss = -10835.591796875
Iteration 10000: Loss = -10835.576171875
Iteration 10100: Loss = -10835.5556640625
Iteration 10200: Loss = -10835.537109375
Iteration 10300: Loss = -10835.517578125
Iteration 10400: Loss = -10835.4931640625
Iteration 10500: Loss = -10835.4677734375
Iteration 10600: Loss = -10835.4384765625
Iteration 10700: Loss = -10835.404296875
Iteration 10800: Loss = -10835.365234375
Iteration 10900: Loss = -10835.3251953125
Iteration 11000: Loss = -10835.2802734375
Iteration 11100: Loss = -10835.236328125
Iteration 11200: Loss = -10835.193359375
Iteration 11300: Loss = -10835.1552734375
Iteration 11400: Loss = -10835.123046875
Iteration 11500: Loss = -10835.09375
Iteration 11600: Loss = -10835.068359375
Iteration 11700: Loss = -10835.048828125
Iteration 11800: Loss = -10835.02734375
Iteration 11900: Loss = -10835.013671875
Iteration 12000: Loss = -10835.0009765625
Iteration 12100: Loss = -10834.990234375
Iteration 12200: Loss = -10834.9794921875
Iteration 12300: Loss = -10834.970703125
Iteration 12400: Loss = -10834.962890625
Iteration 12500: Loss = -10834.95703125
Iteration 12600: Loss = -10834.9521484375
Iteration 12700: Loss = -10834.9462890625
Iteration 12800: Loss = -10834.94140625
Iteration 12900: Loss = -10834.9365234375
Iteration 13000: Loss = -10834.93359375
Iteration 13100: Loss = -10834.9306640625
Iteration 13200: Loss = -10834.9287109375
Iteration 13300: Loss = -10834.9248046875
Iteration 13400: Loss = -10834.923828125
Iteration 13500: Loss = -10834.919921875
Iteration 13600: Loss = -10834.91796875
Iteration 13700: Loss = -10834.916015625
Iteration 13800: Loss = -10834.9140625
Iteration 13900: Loss = -10834.912109375
Iteration 14000: Loss = -10834.91015625
Iteration 14100: Loss = -10834.908203125
Iteration 14200: Loss = -10834.90625
Iteration 14300: Loss = -10834.2890625
Iteration 14400: Loss = -10830.048828125
Iteration 14500: Loss = -10829.99609375
Iteration 14600: Loss = -10829.978515625
Iteration 14700: Loss = -10829.9697265625
Iteration 14800: Loss = -10829.9619140625
Iteration 14900: Loss = -10829.9580078125
Iteration 15000: Loss = -10829.9541015625
Iteration 15100: Loss = -10829.951171875
Iteration 15200: Loss = -10829.951171875
Iteration 15300: Loss = -10829.9482421875
Iteration 15400: Loss = -10829.9462890625
Iteration 15500: Loss = -10829.9453125
Iteration 15600: Loss = -10829.9453125
Iteration 15700: Loss = -10829.9443359375
Iteration 15800: Loss = -10829.943359375
Iteration 15900: Loss = -10829.9423828125
Iteration 16000: Loss = -10829.94140625
Iteration 16100: Loss = -10829.94140625
Iteration 16200: Loss = -10829.94140625
Iteration 16300: Loss = -10829.9404296875
Iteration 16400: Loss = -10829.9404296875
Iteration 16500: Loss = -10829.9384765625
Iteration 16600: Loss = -10829.9375
Iteration 16700: Loss = -10829.939453125
1
Iteration 16800: Loss = -10829.939453125
2
Iteration 16900: Loss = -10829.9375
Iteration 17000: Loss = -10829.9365234375
Iteration 17100: Loss = -10829.939453125
1
Iteration 17200: Loss = -10829.9365234375
Iteration 17300: Loss = -10829.9365234375
Iteration 17400: Loss = -10829.9384765625
1
Iteration 17500: Loss = -10829.9365234375
Iteration 17600: Loss = -10829.9345703125
Iteration 17700: Loss = -10829.935546875
1
Iteration 17800: Loss = -10829.935546875
2
Iteration 17900: Loss = -10829.9365234375
3
Iteration 18000: Loss = -10829.935546875
4
Iteration 18100: Loss = -10829.935546875
5
Iteration 18200: Loss = -10829.9345703125
Iteration 18300: Loss = -10829.935546875
1
Iteration 18400: Loss = -10829.9345703125
Iteration 18500: Loss = -10829.93359375
Iteration 18600: Loss = -10829.93359375
Iteration 18700: Loss = -10829.93359375
Iteration 18800: Loss = -10829.9345703125
1
Iteration 18900: Loss = -10829.9345703125
2
Iteration 19000: Loss = -10829.9345703125
3
Iteration 19100: Loss = -10829.9326171875
Iteration 19200: Loss = -10829.9345703125
1
Iteration 19300: Loss = -10829.9326171875
Iteration 19400: Loss = -10829.9326171875
Iteration 19500: Loss = -10829.9345703125
1
Iteration 19600: Loss = -10829.93359375
2
Iteration 19700: Loss = -10829.9326171875
Iteration 19800: Loss = -10829.93359375
1
Iteration 19900: Loss = -10829.9326171875
Iteration 20000: Loss = -10829.9326171875
Iteration 20100: Loss = -10829.93359375
1
Iteration 20200: Loss = -10829.9326171875
Iteration 20300: Loss = -10829.93359375
1
Iteration 20400: Loss = -10829.9326171875
Iteration 20500: Loss = -10829.9345703125
1
Iteration 20600: Loss = -10829.9326171875
Iteration 20700: Loss = -10829.9326171875
Iteration 20800: Loss = -10829.93359375
1
Iteration 20900: Loss = -10829.93359375
2
Iteration 21000: Loss = -10829.93359375
3
Iteration 21100: Loss = -10829.9345703125
4
Iteration 21200: Loss = -10829.9345703125
5
Iteration 21300: Loss = -10829.93359375
6
Iteration 21400: Loss = -10829.9345703125
7
Iteration 21500: Loss = -10829.9326171875
Iteration 21600: Loss = -10829.93359375
1
Iteration 21700: Loss = -10829.9345703125
2
Iteration 21800: Loss = -10829.9326171875
Iteration 21900: Loss = -10829.9326171875
Iteration 22000: Loss = -10829.9326171875
Iteration 22100: Loss = -10829.9326171875
Iteration 22200: Loss = -10829.9326171875
Iteration 22300: Loss = -10829.9326171875
Iteration 22400: Loss = -10829.93359375
1
Iteration 22500: Loss = -10829.93359375
2
Iteration 22600: Loss = -10829.9326171875
Iteration 22700: Loss = -10829.9326171875
Iteration 22800: Loss = -10829.9306640625
Iteration 22900: Loss = -10829.931640625
1
Iteration 23000: Loss = -10829.935546875
2
Iteration 23100: Loss = -10829.9326171875
3
Iteration 23200: Loss = -10829.931640625
4
Iteration 23300: Loss = -10829.9326171875
5
Iteration 23400: Loss = -10829.93359375
6
Iteration 23500: Loss = -10829.931640625
7
Iteration 23600: Loss = -10829.93359375
8
Iteration 23700: Loss = -10829.931640625
9
Iteration 23800: Loss = -10829.9326171875
10
Iteration 23900: Loss = -10829.93359375
11
Iteration 24000: Loss = -10829.931640625
12
Iteration 24100: Loss = -10829.931640625
13
Iteration 24200: Loss = -10829.931640625
14
Iteration 24300: Loss = -10829.931640625
15
Stopping early at iteration 24300 due to no improvement.
pi: tensor([[9.0883e-08, 1.0000e+00],
        [2.0008e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9970, 0.0030], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1626, 0.1625],
         [0.9857, 0.1578]],

        [[0.5727, 0.7469],
         [0.5103, 0.0167]],

        [[0.1136, 0.1555],
         [0.1434, 0.9410]],

        [[0.9466, 0.1556],
         [0.0180, 0.2928]],

        [[0.1916, 0.8056],
         [0.9304, 0.9167]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012812457556082228
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -49297.06640625
Iteration 100: Loss = -27584.01953125
Iteration 200: Loss = -14466.7802734375
Iteration 300: Loss = -12015.267578125
Iteration 400: Loss = -11481.3837890625
Iteration 500: Loss = -11250.919921875
Iteration 600: Loss = -11132.8916015625
Iteration 700: Loss = -11030.109375
Iteration 800: Loss = -10991.787109375
Iteration 900: Loss = -10964.849609375
Iteration 1000: Loss = -10939.333984375
Iteration 1100: Loss = -10923.8173828125
Iteration 1200: Loss = -10912.806640625
Iteration 1300: Loss = -10903.4921875
Iteration 1400: Loss = -10896.732421875
Iteration 1500: Loss = -10891.4609375
Iteration 1600: Loss = -10887.2939453125
Iteration 1700: Loss = -10883.8154296875
Iteration 1800: Loss = -10880.849609375
Iteration 1900: Loss = -10878.2705078125
Iteration 2000: Loss = -10867.9189453125
Iteration 2100: Loss = -10865.453125
Iteration 2200: Loss = -10863.7255859375
Iteration 2300: Loss = -10862.2294921875
Iteration 2400: Loss = -10860.904296875
Iteration 2500: Loss = -10859.7236328125
Iteration 2600: Loss = -10858.666015625
Iteration 2700: Loss = -10857.7099609375
Iteration 2800: Loss = -10856.8447265625
Iteration 2900: Loss = -10856.060546875
Iteration 3000: Loss = -10855.34375
Iteration 3100: Loss = -10854.6904296875
Iteration 3200: Loss = -10854.091796875
Iteration 3300: Loss = -10853.5380859375
Iteration 3400: Loss = -10853.03125
Iteration 3500: Loss = -10852.560546875
Iteration 3600: Loss = -10852.1240234375
Iteration 3700: Loss = -10851.4423828125
Iteration 3800: Loss = -10843.8935546875
Iteration 3900: Loss = -10843.40234375
Iteration 4000: Loss = -10843.0595703125
Iteration 4100: Loss = -10842.7548828125
Iteration 4200: Loss = -10842.48046875
Iteration 4300: Loss = -10842.2255859375
Iteration 4400: Loss = -10841.9892578125
Iteration 4500: Loss = -10841.7734375
Iteration 4600: Loss = -10841.5712890625
Iteration 4700: Loss = -10841.380859375
Iteration 4800: Loss = -10841.205078125
Iteration 4900: Loss = -10841.041015625
Iteration 5000: Loss = -10840.88671875
Iteration 5100: Loss = -10840.7421875
Iteration 5200: Loss = -10840.60546875
Iteration 5300: Loss = -10840.4775390625
Iteration 5400: Loss = -10840.357421875
Iteration 5500: Loss = -10840.24609375
Iteration 5600: Loss = -10840.1376953125
Iteration 5700: Loss = -10840.0361328125
Iteration 5800: Loss = -10839.9423828125
Iteration 5900: Loss = -10839.8486328125
Iteration 6000: Loss = -10839.7626953125
Iteration 6100: Loss = -10839.6787109375
Iteration 6200: Loss = -10839.6015625
Iteration 6300: Loss = -10839.5263671875
Iteration 6400: Loss = -10839.453125
Iteration 6500: Loss = -10839.384765625
Iteration 6600: Loss = -10839.3193359375
Iteration 6700: Loss = -10839.259765625
Iteration 6800: Loss = -10839.2021484375
Iteration 6900: Loss = -10839.146484375
Iteration 7000: Loss = -10839.09375
Iteration 7100: Loss = -10839.0439453125
Iteration 7200: Loss = -10838.9912109375
Iteration 7300: Loss = -10838.943359375
Iteration 7400: Loss = -10838.8955078125
Iteration 7500: Loss = -10838.8505859375
Iteration 7600: Loss = -10838.810546875
Iteration 7700: Loss = -10838.771484375
Iteration 7800: Loss = -10838.7314453125
Iteration 7900: Loss = -10838.6962890625
Iteration 8000: Loss = -10838.66015625
Iteration 8100: Loss = -10838.6298828125
Iteration 8200: Loss = -10838.6005859375
Iteration 8300: Loss = -10838.5703125
Iteration 8400: Loss = -10838.5439453125
Iteration 8500: Loss = -10838.521484375
Iteration 8600: Loss = -10838.4970703125
Iteration 8700: Loss = -10838.478515625
Iteration 8800: Loss = -10838.4580078125
Iteration 8900: Loss = -10838.4404296875
Iteration 9000: Loss = -10838.4228515625
Iteration 9100: Loss = -10838.404296875
Iteration 9200: Loss = -10838.384765625
Iteration 9300: Loss = -10838.0712890625
Iteration 9400: Loss = -10836.197265625
Iteration 9500: Loss = -10836.158203125
Iteration 9600: Loss = -10836.1376953125
Iteration 9700: Loss = -10836.119140625
Iteration 9800: Loss = -10836.0986328125
Iteration 9900: Loss = -10836.0810546875
Iteration 10000: Loss = -10836.06640625
Iteration 10100: Loss = -10836.0517578125
Iteration 10200: Loss = -10836.0400390625
Iteration 10300: Loss = -10836.029296875
Iteration 10400: Loss = -10836.01953125
Iteration 10500: Loss = -10836.0087890625
Iteration 10600: Loss = -10835.9990234375
Iteration 10700: Loss = -10835.9921875
Iteration 10800: Loss = -10835.9833984375
Iteration 10900: Loss = -10835.974609375
Iteration 11000: Loss = -10835.96484375
Iteration 11100: Loss = -10835.9560546875
Iteration 11200: Loss = -10835.9462890625
Iteration 11300: Loss = -10835.8203125
Iteration 11400: Loss = -10835.63671875
Iteration 11500: Loss = -10835.607421875
Iteration 11600: Loss = -10830.7138671875
Iteration 11700: Loss = -10830.5341796875
Iteration 11800: Loss = -10830.4794921875
Iteration 11900: Loss = -10830.4462890625
Iteration 12000: Loss = -10830.423828125
Iteration 12100: Loss = -10830.408203125
Iteration 12200: Loss = -10830.39453125
Iteration 12300: Loss = -10830.3857421875
Iteration 12400: Loss = -10830.376953125
Iteration 12500: Loss = -10830.3681640625
Iteration 12600: Loss = -10830.3623046875
Iteration 12700: Loss = -10830.35546875
Iteration 12800: Loss = -10830.3505859375
Iteration 12900: Loss = -10830.34765625
Iteration 13000: Loss = -10830.341796875
Iteration 13100: Loss = -10830.3388671875
Iteration 13200: Loss = -10830.3349609375
Iteration 13300: Loss = -10830.33203125
Iteration 13400: Loss = -10830.3291015625
Iteration 13500: Loss = -10830.3271484375
Iteration 13600: Loss = -10830.32421875
Iteration 13700: Loss = -10830.322265625
Iteration 13800: Loss = -10830.318359375
Iteration 13900: Loss = -10830.31640625
Iteration 14000: Loss = -10830.3154296875
Iteration 14100: Loss = -10830.3125
Iteration 14200: Loss = -10830.310546875
Iteration 14300: Loss = -10830.30859375
Iteration 14400: Loss = -10830.3076171875
Iteration 14500: Loss = -10830.3056640625
Iteration 14600: Loss = -10830.3056640625
Iteration 14700: Loss = -10830.302734375
Iteration 14800: Loss = -10830.30078125
Iteration 14900: Loss = -10830.30078125
Iteration 15000: Loss = -10830.298828125
Iteration 15100: Loss = -10830.298828125
Iteration 15200: Loss = -10830.296875
Iteration 15300: Loss = -10830.2958984375
Iteration 15400: Loss = -10830.2939453125
Iteration 15500: Loss = -10830.2939453125
Iteration 15600: Loss = -10830.29296875
Iteration 15700: Loss = -10830.291015625
Iteration 15800: Loss = -10830.2939453125
1
Iteration 15900: Loss = -10830.2919921875
2
Iteration 16000: Loss = -10830.291015625
Iteration 16100: Loss = -10830.2890625
Iteration 16200: Loss = -10830.2890625
Iteration 16300: Loss = -10830.2900390625
1
Iteration 16400: Loss = -10830.2890625
Iteration 16500: Loss = -10830.2890625
Iteration 16600: Loss = -10830.287109375
Iteration 16700: Loss = -10830.287109375
Iteration 16800: Loss = -10830.287109375
Iteration 16900: Loss = -10830.287109375
Iteration 17000: Loss = -10830.2861328125
Iteration 17100: Loss = -10830.2861328125
Iteration 17200: Loss = -10830.2841796875
Iteration 17300: Loss = -10830.28515625
1
Iteration 17400: Loss = -10830.28515625
2
Iteration 17500: Loss = -10830.2841796875
Iteration 17600: Loss = -10830.283203125
Iteration 17700: Loss = -10830.2841796875
1
Iteration 17800: Loss = -10830.283203125
Iteration 17900: Loss = -10830.28515625
1
Iteration 18000: Loss = -10830.283203125
Iteration 18100: Loss = -10830.283203125
Iteration 18200: Loss = -10830.2822265625
Iteration 18300: Loss = -10830.2822265625
Iteration 18400: Loss = -10830.283203125
1
Iteration 18500: Loss = -10830.28125
Iteration 18600: Loss = -10830.28125
Iteration 18700: Loss = -10830.28125
Iteration 18800: Loss = -10830.283203125
1
Iteration 18900: Loss = -10830.28125
Iteration 19000: Loss = -10830.2802734375
Iteration 19100: Loss = -10830.2802734375
Iteration 19200: Loss = -10830.2802734375
Iteration 19300: Loss = -10830.2822265625
1
Iteration 19400: Loss = -10830.28125
2
Iteration 19500: Loss = -10830.2802734375
Iteration 19600: Loss = -10830.2802734375
Iteration 19700: Loss = -10830.279296875
Iteration 19800: Loss = -10830.28125
1
Iteration 19900: Loss = -10830.279296875
Iteration 20000: Loss = -10830.279296875
Iteration 20100: Loss = -10830.2802734375
1
Iteration 20200: Loss = -10830.279296875
Iteration 20300: Loss = -10830.2802734375
1
Iteration 20400: Loss = -10830.2802734375
2
Iteration 20500: Loss = -10830.2802734375
3
Iteration 20600: Loss = -10830.2783203125
Iteration 20700: Loss = -10830.2783203125
Iteration 20800: Loss = -10830.2783203125
Iteration 20900: Loss = -10830.2802734375
1
Iteration 21000: Loss = -10830.279296875
2
Iteration 21100: Loss = -10830.279296875
3
Iteration 21200: Loss = -10830.279296875
4
Iteration 21300: Loss = -10830.2802734375
5
Iteration 21400: Loss = -10830.2783203125
Iteration 21500: Loss = -10830.2783203125
Iteration 21600: Loss = -10830.2783203125
Iteration 21700: Loss = -10830.2783203125
Iteration 21800: Loss = -10830.2802734375
1
Iteration 21900: Loss = -10830.2783203125
Iteration 22000: Loss = -10830.2783203125
Iteration 22100: Loss = -10830.2783203125
Iteration 22200: Loss = -10830.279296875
1
Iteration 22300: Loss = -10830.2802734375
2
Iteration 22400: Loss = -10830.2783203125
Iteration 22500: Loss = -10830.2783203125
Iteration 22600: Loss = -10830.2783203125
Iteration 22700: Loss = -10830.2783203125
Iteration 22800: Loss = -10830.2802734375
1
Iteration 22900: Loss = -10830.279296875
2
Iteration 23000: Loss = -10830.27734375
Iteration 23100: Loss = -10830.2783203125
1
Iteration 23200: Loss = -10830.2802734375
2
Iteration 23300: Loss = -10830.279296875
3
Iteration 23400: Loss = -10830.28125
4
Iteration 23500: Loss = -10830.2783203125
5
Iteration 23600: Loss = -10830.2783203125
6
Iteration 23700: Loss = -10830.27734375
Iteration 23800: Loss = -10830.2783203125
1
Iteration 23900: Loss = -10830.279296875
2
Iteration 24000: Loss = -10830.279296875
3
Iteration 24100: Loss = -10830.279296875
4
Iteration 24200: Loss = -10830.2783203125
5
Iteration 24300: Loss = -10830.2802734375
6
Iteration 24400: Loss = -10830.279296875
7
Iteration 24500: Loss = -10830.279296875
8
Iteration 24600: Loss = -10830.279296875
9
Iteration 24700: Loss = -10830.2783203125
10
Iteration 24800: Loss = -10830.27734375
Iteration 24900: Loss = -10830.2783203125
1
Iteration 25000: Loss = -10830.2783203125
2
Iteration 25100: Loss = -10830.27734375
Iteration 25200: Loss = -10829.9208984375
Iteration 25300: Loss = -10828.8125
Iteration 25400: Loss = -10828.607421875
Iteration 25500: Loss = -10828.5888671875
Iteration 25600: Loss = -10828.580078125
Iteration 25700: Loss = -10825.6708984375
Iteration 25800: Loss = -10825.6572265625
Iteration 25900: Loss = -10825.65234375
Iteration 26000: Loss = -10825.6494140625
Iteration 26100: Loss = -10825.646484375
Iteration 26200: Loss = -10825.6474609375
1
Iteration 26300: Loss = -10825.646484375
Iteration 26400: Loss = -10825.64453125
Iteration 26500: Loss = -10825.64453125
Iteration 26600: Loss = -10825.64453125
Iteration 26700: Loss = -10825.64453125
Iteration 26800: Loss = -10825.6435546875
Iteration 26900: Loss = -10825.64453125
1
Iteration 27000: Loss = -10825.6416015625
Iteration 27100: Loss = -10825.6435546875
1
Iteration 27200: Loss = -10825.6435546875
2
Iteration 27300: Loss = -10825.6435546875
3
Iteration 27400: Loss = -10825.6435546875
4
Iteration 27500: Loss = -10825.6435546875
5
Iteration 27600: Loss = -10825.6435546875
6
Iteration 27700: Loss = -10825.6416015625
Iteration 27800: Loss = -10825.6435546875
1
Iteration 27900: Loss = -10825.6435546875
2
Iteration 28000: Loss = -10825.642578125
3
Iteration 28100: Loss = -10825.642578125
4
Iteration 28200: Loss = -10825.642578125
5
Iteration 28300: Loss = -10825.642578125
6
Iteration 28400: Loss = -10825.6416015625
Iteration 28500: Loss = -10825.6416015625
Iteration 28600: Loss = -10825.6416015625
Iteration 28700: Loss = -10825.642578125
1
Iteration 28800: Loss = -10825.6416015625
Iteration 28900: Loss = -10825.6416015625
Iteration 29000: Loss = -10825.6416015625
Iteration 29100: Loss = -10825.6416015625
Iteration 29200: Loss = -10825.6416015625
Iteration 29300: Loss = -10825.6416015625
Iteration 29400: Loss = -10825.642578125
1
Iteration 29500: Loss = -10825.6416015625
Iteration 29600: Loss = -10825.6416015625
Iteration 29700: Loss = -10825.6416015625
Iteration 29800: Loss = -10825.6416015625
Iteration 29900: Loss = -10825.640625
pi: tensor([[9.9991e-01, 8.9384e-05],
        [6.1873e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0240, 0.9760], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3708, 0.2015],
         [0.9808, 0.1582]],

        [[0.7533, 0.1815],
         [0.3308, 0.3759]],

        [[0.0084, 0.1105],
         [0.9837, 0.3660]],

        [[0.0125, 0.2441],
         [0.9913, 0.9542]],

        [[0.8947, 0.1001],
         [0.0437, 0.9895]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: -0.002185903119278217
Average Adjusted Rand Index: -0.0015897406922379454
[-0.0012812457556082228, -0.002185903119278217] [0.0, -0.0015897406922379454] [10829.931640625, 10825.6416015625]
-------------------------------------
This iteration is 48
True Objective function: Loss = -10836.515377466018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27793.42578125
Iteration 100: Loss = -18327.529296875
Iteration 200: Loss = -12731.80859375
Iteration 300: Loss = -11336.75390625
Iteration 400: Loss = -11168.4541015625
Iteration 500: Loss = -11095.2138671875
Iteration 600: Loss = -11062.095703125
Iteration 700: Loss = -11016.2041015625
Iteration 800: Loss = -10984.583984375
Iteration 900: Loss = -10964.4990234375
Iteration 1000: Loss = -10955.158203125
Iteration 1100: Loss = -10947.46484375
Iteration 1200: Loss = -10943.0537109375
Iteration 1300: Loss = -10938.3310546875
Iteration 1400: Loss = -10933.728515625
Iteration 1500: Loss = -10929.6787109375
Iteration 1600: Loss = -10927.18359375
Iteration 1700: Loss = -10925.4794921875
Iteration 1800: Loss = -10924.0458984375
Iteration 1900: Loss = -10922.5576171875
Iteration 2000: Loss = -10921.01953125
Iteration 2100: Loss = -10919.91015625
Iteration 2200: Loss = -10915.6142578125
Iteration 2300: Loss = -10914.51953125
Iteration 2400: Loss = -10913.83984375
Iteration 2500: Loss = -10913.3076171875
Iteration 2600: Loss = -10912.8681640625
Iteration 2700: Loss = -10912.4873046875
Iteration 2800: Loss = -10912.16015625
Iteration 2900: Loss = -10911.869140625
Iteration 3000: Loss = -10911.6083984375
Iteration 3100: Loss = -10911.3779296875
Iteration 3200: Loss = -10911.1669921875
Iteration 3300: Loss = -10910.9765625
Iteration 3400: Loss = -10910.802734375
Iteration 3500: Loss = -10910.64453125
Iteration 3600: Loss = -10910.49609375
Iteration 3700: Loss = -10910.36328125
Iteration 3800: Loss = -10910.23828125
Iteration 3900: Loss = -10910.1181640625
Iteration 4000: Loss = -10910.009765625
Iteration 4100: Loss = -10909.91015625
Iteration 4200: Loss = -10909.80859375
Iteration 4300: Loss = -10909.7197265625
Iteration 4400: Loss = -10909.6396484375
Iteration 4500: Loss = -10909.56640625
Iteration 4600: Loss = -10909.4951171875
Iteration 4700: Loss = -10909.4306640625
Iteration 4800: Loss = -10909.3681640625
Iteration 4900: Loss = -10909.310546875
Iteration 5000: Loss = -10909.2568359375
Iteration 5100: Loss = -10909.2060546875
Iteration 5200: Loss = -10909.1591796875
Iteration 5300: Loss = -10909.1142578125
Iteration 5400: Loss = -10909.0712890625
Iteration 5500: Loss = -10909.03125
Iteration 5600: Loss = -10908.9951171875
Iteration 5700: Loss = -10908.9609375
Iteration 5800: Loss = -10908.927734375
Iteration 5900: Loss = -10908.896484375
Iteration 6000: Loss = -10908.865234375
Iteration 6100: Loss = -10908.837890625
Iteration 6200: Loss = -10908.810546875
Iteration 6300: Loss = -10908.787109375
Iteration 6400: Loss = -10908.76171875
Iteration 6500: Loss = -10908.7392578125
Iteration 6600: Loss = -10908.71875
Iteration 6700: Loss = -10908.7001953125
Iteration 6800: Loss = -10908.6796875
Iteration 6900: Loss = -10908.6640625
Iteration 7000: Loss = -10908.6455078125
Iteration 7100: Loss = -10908.62890625
Iteration 7200: Loss = -10908.61328125
Iteration 7300: Loss = -10908.5986328125
Iteration 7400: Loss = -10908.583984375
Iteration 7500: Loss = -10908.572265625
Iteration 7600: Loss = -10908.55859375
Iteration 7700: Loss = -10908.5458984375
Iteration 7800: Loss = -10908.5341796875
Iteration 7900: Loss = -10908.5244140625
Iteration 8000: Loss = -10908.5126953125
Iteration 8100: Loss = -10908.501953125
Iteration 8200: Loss = -10908.4912109375
Iteration 8300: Loss = -10908.4794921875
Iteration 8400: Loss = -10908.458984375
Iteration 8500: Loss = -10908.3974609375
Iteration 8600: Loss = -10907.6728515625
Iteration 8700: Loss = -10907.146484375
Iteration 8800: Loss = -10907.0498046875
Iteration 8900: Loss = -10907.001953125
Iteration 9000: Loss = -10906.966796875
Iteration 9100: Loss = -10906.9384765625
Iteration 9200: Loss = -10906.91796875
Iteration 9300: Loss = -10906.8994140625
Iteration 9400: Loss = -10906.8837890625
Iteration 9500: Loss = -10906.869140625
Iteration 9600: Loss = -10906.85546875
Iteration 9700: Loss = -10906.845703125
Iteration 9800: Loss = -10906.8359375
Iteration 9900: Loss = -10906.8271484375
Iteration 10000: Loss = -10906.8193359375
Iteration 10100: Loss = -10906.8115234375
Iteration 10200: Loss = -10906.802734375
Iteration 10300: Loss = -10906.796875
Iteration 10400: Loss = -10906.791015625
Iteration 10500: Loss = -10906.78125
Iteration 10600: Loss = -10906.7763671875
Iteration 10700: Loss = -10906.771484375
Iteration 10800: Loss = -10906.763671875
Iteration 10900: Loss = -10906.7587890625
Iteration 11000: Loss = -10906.75390625
Iteration 11100: Loss = -10906.748046875
Iteration 11200: Loss = -10906.744140625
Iteration 11300: Loss = -10906.7392578125
Iteration 11400: Loss = -10906.7353515625
Iteration 11500: Loss = -10906.732421875
Iteration 11600: Loss = -10906.7275390625
Iteration 11700: Loss = -10906.724609375
Iteration 11800: Loss = -10906.71875
Iteration 11900: Loss = -10906.716796875
Iteration 12000: Loss = -10906.71484375
Iteration 12100: Loss = -10906.712890625
Iteration 12200: Loss = -10906.7109375
Iteration 12300: Loss = -10906.7080078125
Iteration 12400: Loss = -10906.7060546875
Iteration 12500: Loss = -10906.703125
Iteration 12600: Loss = -10906.7041015625
1
Iteration 12700: Loss = -10906.701171875
Iteration 12800: Loss = -10906.7001953125
Iteration 12900: Loss = -10906.6982421875
Iteration 13000: Loss = -10906.6982421875
Iteration 13100: Loss = -10906.6962890625
Iteration 13200: Loss = -10906.6962890625
Iteration 13300: Loss = -10906.6953125
Iteration 13400: Loss = -10906.693359375
Iteration 13500: Loss = -10906.693359375
Iteration 13600: Loss = -10906.69140625
Iteration 13700: Loss = -10906.69140625
Iteration 13800: Loss = -10906.6923828125
1
Iteration 13900: Loss = -10906.6904296875
Iteration 14000: Loss = -10906.6904296875
Iteration 14100: Loss = -10906.689453125
Iteration 14200: Loss = -10906.689453125
Iteration 14300: Loss = -10906.689453125
Iteration 14400: Loss = -10906.689453125
Iteration 14500: Loss = -10906.6904296875
1
Iteration 14600: Loss = -10906.6884765625
Iteration 14700: Loss = -10906.6884765625
Iteration 14800: Loss = -10906.6875
Iteration 14900: Loss = -10906.6865234375
Iteration 15000: Loss = -10906.6865234375
Iteration 15100: Loss = -10906.6865234375
Iteration 15200: Loss = -10906.685546875
Iteration 15300: Loss = -10906.6865234375
1
Iteration 15400: Loss = -10906.685546875
Iteration 15500: Loss = -10906.6845703125
Iteration 15600: Loss = -10906.685546875
1
Iteration 15700: Loss = -10906.6845703125
Iteration 15800: Loss = -10906.6845703125
Iteration 15900: Loss = -10906.6845703125
Iteration 16000: Loss = -10906.6845703125
Iteration 16100: Loss = -10906.685546875
1
Iteration 16200: Loss = -10906.6845703125
Iteration 16300: Loss = -10906.6845703125
Iteration 16400: Loss = -10906.6845703125
Iteration 16500: Loss = -10906.681640625
Iteration 16600: Loss = -10906.6474609375
Iteration 16700: Loss = -10906.5302734375
Iteration 16800: Loss = -10906.43359375
Iteration 16900: Loss = -10906.431640625
Iteration 17000: Loss = -10906.4130859375
Iteration 17100: Loss = -10906.412109375
Iteration 17200: Loss = -10906.3955078125
Iteration 17300: Loss = -10906.2919921875
Iteration 17400: Loss = -10906.2880859375
Iteration 17500: Loss = -10906.1572265625
Iteration 17600: Loss = -10906.115234375
Iteration 17700: Loss = -10906.0595703125
Iteration 17800: Loss = -10906.0400390625
Iteration 17900: Loss = -10906.0146484375
Iteration 18000: Loss = -10905.94921875
Iteration 18100: Loss = -10904.76171875
Iteration 18200: Loss = -10904.7373046875
Iteration 18300: Loss = -10904.7001953125
Iteration 18400: Loss = -10904.466796875
Iteration 18500: Loss = -10904.4609375
Iteration 18600: Loss = -10904.4462890625
Iteration 18700: Loss = -10904.2705078125
Iteration 18800: Loss = -10904.2666015625
Iteration 18900: Loss = -10904.2578125
Iteration 19000: Loss = -10904.2431640625
Iteration 19100: Loss = -10904.2392578125
Iteration 19200: Loss = -10904.2353515625
Iteration 19300: Loss = -10904.2236328125
Iteration 19400: Loss = -10904.216796875
Iteration 19500: Loss = -10904.21484375
Iteration 19600: Loss = -10904.2099609375
Iteration 19700: Loss = -10904.2109375
1
Iteration 19800: Loss = -10904.208984375
Iteration 19900: Loss = -10904.208984375
Iteration 20000: Loss = -10904.2080078125
Iteration 20100: Loss = -10904.20703125
Iteration 20200: Loss = -10904.205078125
Iteration 20300: Loss = -10904.205078125
Iteration 20400: Loss = -10904.2041015625
Iteration 20500: Loss = -10904.205078125
1
Iteration 20600: Loss = -10904.2041015625
Iteration 20700: Loss = -10904.2060546875
1
Iteration 20800: Loss = -10904.2041015625
Iteration 20900: Loss = -10904.203125
Iteration 21000: Loss = -10904.203125
Iteration 21100: Loss = -10904.2021484375
Iteration 21200: Loss = -10904.203125
1
Iteration 21300: Loss = -10904.203125
2
Iteration 21400: Loss = -10904.203125
3
Iteration 21500: Loss = -10904.2021484375
Iteration 21600: Loss = -10904.2021484375
Iteration 21700: Loss = -10904.2021484375
Iteration 21800: Loss = -10904.201171875
Iteration 21900: Loss = -10904.2041015625
1
Iteration 22000: Loss = -10904.2001953125
Iteration 22100: Loss = -10904.2001953125
Iteration 22200: Loss = -10904.2001953125
Iteration 22300: Loss = -10904.2021484375
1
Iteration 22400: Loss = -10904.19921875
Iteration 22500: Loss = -10904.2001953125
1
Iteration 22600: Loss = -10904.19921875
Iteration 22700: Loss = -10904.19921875
Iteration 22800: Loss = -10904.1982421875
Iteration 22900: Loss = -10904.19921875
1
Iteration 23000: Loss = -10904.1982421875
Iteration 23100: Loss = -10904.203125
1
Iteration 23200: Loss = -10904.1982421875
Iteration 23300: Loss = -10904.19921875
1
Iteration 23400: Loss = -10904.19921875
2
Iteration 23500: Loss = -10904.2001953125
3
Iteration 23600: Loss = -10904.201171875
4
Iteration 23700: Loss = -10904.1982421875
Iteration 23800: Loss = -10904.1982421875
Iteration 23900: Loss = -10904.19921875
1
Iteration 24000: Loss = -10904.197265625
Iteration 24100: Loss = -10904.19921875
1
Iteration 24200: Loss = -10904.1982421875
2
Iteration 24300: Loss = -10904.1982421875
3
Iteration 24400: Loss = -10904.19921875
4
Iteration 24500: Loss = -10904.197265625
Iteration 24600: Loss = -10904.197265625
Iteration 24700: Loss = -10904.1982421875
1
Iteration 24800: Loss = -10904.1962890625
Iteration 24900: Loss = -10904.1982421875
1
Iteration 25000: Loss = -10904.197265625
2
Iteration 25100: Loss = -10904.197265625
3
Iteration 25200: Loss = -10904.1982421875
4
Iteration 25300: Loss = -10904.1982421875
5
Iteration 25400: Loss = -10904.19921875
6
Iteration 25500: Loss = -10904.197265625
7
Iteration 25600: Loss = -10904.197265625
8
Iteration 25700: Loss = -10904.1982421875
9
Iteration 25800: Loss = -10904.197265625
10
Iteration 25900: Loss = -10904.201171875
11
Iteration 26000: Loss = -10904.19921875
12
Iteration 26100: Loss = -10904.1982421875
13
Iteration 26200: Loss = -10904.19921875
14
Iteration 26300: Loss = -10904.1982421875
15
Stopping early at iteration 26300 due to no improvement.
pi: tensor([[0.9800, 0.0200],
        [0.2486, 0.7514]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 8.9773e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1581, 0.1346],
         [0.9365, 0.4603]],

        [[0.6699, 0.0446],
         [0.0109, 0.5117]],

        [[0.3923, 0.2184],
         [0.0214, 0.0619]],

        [[0.6935, 0.1988],
         [0.4390, 0.3874]],

        [[0.9450, 0.2171],
         [0.7835, 0.0700]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
Global Adjusted Rand Index: 0.00028937871409835203
Average Adjusted Rand Index: -0.001685866750650002
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38984.80859375
Iteration 100: Loss = -20606.447265625
Iteration 200: Loss = -12869.0888671875
Iteration 300: Loss = -11369.375
Iteration 400: Loss = -11134.0908203125
Iteration 500: Loss = -11047.7158203125
Iteration 600: Loss = -11006.0283203125
Iteration 700: Loss = -10976.2744140625
Iteration 800: Loss = -10960.173828125
Iteration 900: Loss = -10948.24609375
Iteration 1000: Loss = -10940.9970703125
Iteration 1100: Loss = -10935.89453125
Iteration 1200: Loss = -10931.9990234375
Iteration 1300: Loss = -10928.9130859375
Iteration 1400: Loss = -10926.41015625
Iteration 1500: Loss = -10924.3349609375
Iteration 1600: Loss = -10922.603515625
Iteration 1700: Loss = -10921.138671875
Iteration 1800: Loss = -10919.8837890625
Iteration 1900: Loss = -10918.798828125
Iteration 2000: Loss = -10917.8564453125
Iteration 2100: Loss = -10917.0302734375
Iteration 2200: Loss = -10916.30078125
Iteration 2300: Loss = -10915.658203125
Iteration 2400: Loss = -10915.0849609375
Iteration 2500: Loss = -10914.5712890625
Iteration 2600: Loss = -10914.1103515625
Iteration 2700: Loss = -10913.6953125
Iteration 2800: Loss = -10913.318359375
Iteration 2900: Loss = -10912.97265625
Iteration 3000: Loss = -10912.6591796875
Iteration 3100: Loss = -10912.373046875
Iteration 3200: Loss = -10912.109375
Iteration 3300: Loss = -10911.865234375
Iteration 3400: Loss = -10911.6435546875
Iteration 3500: Loss = -10911.4365234375
Iteration 3600: Loss = -10911.2431640625
Iteration 3700: Loss = -10911.064453125
Iteration 3800: Loss = -10910.8984375
Iteration 3900: Loss = -10910.7421875
Iteration 4000: Loss = -10910.595703125
Iteration 4100: Loss = -10910.4599609375
Iteration 4200: Loss = -10910.33203125
Iteration 4300: Loss = -10910.2177734375
Iteration 4400: Loss = -10910.1083984375
Iteration 4500: Loss = -10910.005859375
Iteration 4600: Loss = -10909.9111328125
Iteration 4700: Loss = -10909.8212890625
Iteration 4800: Loss = -10909.7373046875
Iteration 4900: Loss = -10909.658203125
Iteration 5000: Loss = -10909.5849609375
Iteration 5100: Loss = -10909.515625
Iteration 5200: Loss = -10909.44921875
Iteration 5300: Loss = -10909.38671875
Iteration 5400: Loss = -10909.3291015625
Iteration 5500: Loss = -10909.275390625
Iteration 5600: Loss = -10909.2216796875
Iteration 5700: Loss = -10909.173828125
Iteration 5800: Loss = -10909.1259765625
Iteration 5900: Loss = -10909.08203125
Iteration 6000: Loss = -10909.0419921875
Iteration 6100: Loss = -10909.0029296875
Iteration 6200: Loss = -10908.96484375
Iteration 6300: Loss = -10908.9326171875
Iteration 6400: Loss = -10908.8984375
Iteration 6500: Loss = -10908.865234375
Iteration 6600: Loss = -10908.8369140625
Iteration 6700: Loss = -10908.80859375
Iteration 6800: Loss = -10908.7822265625
Iteration 6900: Loss = -10908.7587890625
Iteration 7000: Loss = -10908.7333984375
Iteration 7100: Loss = -10908.7109375
Iteration 7200: Loss = -10908.689453125
Iteration 7300: Loss = -10908.669921875
Iteration 7400: Loss = -10908.650390625
Iteration 7500: Loss = -10908.630859375
Iteration 7600: Loss = -10908.6142578125
Iteration 7700: Loss = -10908.595703125
Iteration 7800: Loss = -10908.5810546875
Iteration 7900: Loss = -10908.5673828125
Iteration 8000: Loss = -10908.5517578125
Iteration 8100: Loss = -10908.5390625
Iteration 8200: Loss = -10908.5263671875
Iteration 8300: Loss = -10908.5146484375
Iteration 8400: Loss = -10908.501953125
Iteration 8500: Loss = -10908.490234375
Iteration 8600: Loss = -10908.4814453125
Iteration 8700: Loss = -10908.4697265625
Iteration 8800: Loss = -10908.4619140625
Iteration 8900: Loss = -10908.4521484375
Iteration 9000: Loss = -10908.4423828125
Iteration 9100: Loss = -10908.43359375
Iteration 9200: Loss = -10908.4267578125
Iteration 9300: Loss = -10908.41796875
Iteration 9400: Loss = -10908.4091796875
Iteration 9500: Loss = -10908.4033203125
Iteration 9600: Loss = -10908.3955078125
Iteration 9700: Loss = -10908.3896484375
Iteration 9800: Loss = -10908.3837890625
Iteration 9900: Loss = -10908.37890625
Iteration 10000: Loss = -10908.3740234375
Iteration 10100: Loss = -10908.3671875
Iteration 10200: Loss = -10908.3623046875
Iteration 10300: Loss = -10908.357421875
Iteration 10400: Loss = -10908.3525390625
Iteration 10500: Loss = -10908.34765625
Iteration 10600: Loss = -10908.3388671875
Iteration 10700: Loss = -10907.646484375
Iteration 10800: Loss = -10907.2099609375
Iteration 10900: Loss = -10907.0986328125
Iteration 11000: Loss = -10907.033203125
Iteration 11100: Loss = -10906.9853515625
Iteration 11200: Loss = -10906.94921875
Iteration 11300: Loss = -10906.9208984375
Iteration 11400: Loss = -10906.8984375
Iteration 11500: Loss = -10906.8759765625
Iteration 11600: Loss = -10906.85546875
Iteration 11700: Loss = -10906.8369140625
Iteration 11800: Loss = -10906.818359375
Iteration 11900: Loss = -10906.798828125
Iteration 12000: Loss = -10906.7783203125
Iteration 12100: Loss = -10906.7587890625
Iteration 12200: Loss = -10906.7353515625
Iteration 12300: Loss = -10906.7099609375
Iteration 12400: Loss = -10906.6826171875
Iteration 12500: Loss = -10906.65234375
Iteration 12600: Loss = -10906.611328125
Iteration 12700: Loss = -10906.5595703125
Iteration 12800: Loss = -10906.3984375
Iteration 12900: Loss = -10904.671875
Iteration 13000: Loss = -10904.591796875
Iteration 13100: Loss = -10904.5234375
Iteration 13200: Loss = -10904.4775390625
Iteration 13300: Loss = -10904.435546875
Iteration 13400: Loss = -10904.3916015625
Iteration 13500: Loss = -10904.34765625
Iteration 13600: Loss = -10904.3046875
Iteration 13700: Loss = -10904.267578125
Iteration 13800: Loss = -10904.234375
Iteration 13900: Loss = -10904.2060546875
Iteration 14000: Loss = -10904.1728515625
Iteration 14100: Loss = -10904.1318359375
Iteration 14200: Loss = -10904.1123046875
Iteration 14300: Loss = -10904.095703125
Iteration 14400: Loss = -10904.083984375
Iteration 14500: Loss = -10904.0751953125
Iteration 14600: Loss = -10904.0673828125
Iteration 14700: Loss = -10904.064453125
Iteration 14800: Loss = -10904.0615234375
Iteration 14900: Loss = -10904.0576171875
Iteration 15000: Loss = -10904.0546875
Iteration 15100: Loss = -10904.052734375
Iteration 15200: Loss = -10904.0498046875
Iteration 15300: Loss = -10904.048828125
Iteration 15400: Loss = -10904.0478515625
Iteration 15500: Loss = -10904.0458984375
Iteration 15600: Loss = -10904.046875
1
Iteration 15700: Loss = -10904.0458984375
Iteration 15800: Loss = -10904.044921875
Iteration 15900: Loss = -10904.0458984375
1
Iteration 16000: Loss = -10904.04296875
Iteration 16100: Loss = -10904.04296875
Iteration 16200: Loss = -10904.041015625
Iteration 16300: Loss = -10904.04296875
1
Iteration 16400: Loss = -10904.0419921875
2
Iteration 16500: Loss = -10904.0419921875
3
Iteration 16600: Loss = -10904.041015625
Iteration 16700: Loss = -10904.041015625
Iteration 16800: Loss = -10904.0419921875
1
Iteration 16900: Loss = -10904.041015625
Iteration 17000: Loss = -10904.0400390625
Iteration 17100: Loss = -10904.0400390625
Iteration 17200: Loss = -10904.041015625
1
Iteration 17300: Loss = -10904.041015625
2
Iteration 17400: Loss = -10904.0400390625
Iteration 17500: Loss = -10904.0390625
Iteration 17600: Loss = -10904.0390625
Iteration 17700: Loss = -10904.0380859375
Iteration 17800: Loss = -10904.0400390625
1
Iteration 17900: Loss = -10904.0390625
2
Iteration 18000: Loss = -10904.0390625
3
Iteration 18100: Loss = -10904.0380859375
Iteration 18200: Loss = -10904.0380859375
Iteration 18300: Loss = -10904.0390625
1
Iteration 18400: Loss = -10904.0380859375
Iteration 18500: Loss = -10904.0380859375
Iteration 18600: Loss = -10904.0361328125
Iteration 18700: Loss = -10904.037109375
1
Iteration 18800: Loss = -10904.0390625
2
Iteration 18900: Loss = -10904.037109375
3
Iteration 19000: Loss = -10904.037109375
4
Iteration 19100: Loss = -10904.0380859375
5
Iteration 19200: Loss = -10904.037109375
6
Iteration 19300: Loss = -10904.0380859375
7
Iteration 19400: Loss = -10904.037109375
8
Iteration 19500: Loss = -10904.0361328125
Iteration 19600: Loss = -10904.037109375
1
Iteration 19700: Loss = -10904.037109375
2
Iteration 19800: Loss = -10904.037109375
3
Iteration 19900: Loss = -10904.0361328125
Iteration 20000: Loss = -10904.0380859375
1
Iteration 20100: Loss = -10904.037109375
2
Iteration 20200: Loss = -10904.03515625
Iteration 20300: Loss = -10904.0341796875
Iteration 20400: Loss = -10904.0361328125
1
Iteration 20500: Loss = -10904.03515625
2
Iteration 20600: Loss = -10904.0361328125
3
Iteration 20700: Loss = -10904.0361328125
4
Iteration 20800: Loss = -10904.037109375
5
Iteration 20900: Loss = -10904.0361328125
6
Iteration 21000: Loss = -10904.0361328125
7
Iteration 21100: Loss = -10904.0380859375
8
Iteration 21200: Loss = -10904.037109375
9
Iteration 21300: Loss = -10904.03515625
10
Iteration 21400: Loss = -10904.037109375
11
Iteration 21500: Loss = -10904.0361328125
12
Iteration 21600: Loss = -10904.0361328125
13
Iteration 21700: Loss = -10904.0341796875
Iteration 21800: Loss = -10904.03515625
1
Iteration 21900: Loss = -10904.03515625
2
Iteration 22000: Loss = -10904.037109375
3
Iteration 22100: Loss = -10904.03515625
4
Iteration 22200: Loss = -10904.037109375
5
Iteration 22300: Loss = -10904.03515625
6
Iteration 22400: Loss = -10904.03515625
7
Iteration 22500: Loss = -10904.0361328125
8
Iteration 22600: Loss = -10904.0380859375
9
Iteration 22700: Loss = -10904.03515625
10
Iteration 22800: Loss = -10904.0361328125
11
Iteration 22900: Loss = -10904.03515625
12
Iteration 23000: Loss = -10904.037109375
13
Iteration 23100: Loss = -10904.0361328125
14
Iteration 23200: Loss = -10904.0361328125
15
Stopping early at iteration 23200 due to no improvement.
pi: tensor([[0.9765, 0.0235],
        [0.2511, 0.7489]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 1.4198e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1579, 0.1384],
         [0.0342, 0.4138]],

        [[0.1435, 0.0456],
         [0.0811, 0.9600]],

        [[0.1707, 0.2125],
         [0.2036, 0.4276]],

        [[0.9349, 0.1963],
         [0.0128, 0.0169]],

        [[0.5716, 0.2131],
         [0.9043, 0.9481]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
Global Adjusted Rand Index: 0.00028937871409835203
Average Adjusted Rand Index: -0.001685866750650002
[0.00028937871409835203, 0.00028937871409835203] [-0.001685866750650002, -0.001685866750650002] [10904.1982421875, 10904.0361328125]
-------------------------------------
This iteration is 49
True Objective function: Loss = -10900.915747567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41460.68359375
Iteration 100: Loss = -24854.375
Iteration 200: Loss = -14944.9462890625
Iteration 300: Loss = -12516.3232421875
Iteration 400: Loss = -11735.1396484375
Iteration 500: Loss = -11423.0615234375
Iteration 600: Loss = -11245.173828125
Iteration 700: Loss = -11187.658203125
Iteration 800: Loss = -11147.14453125
Iteration 900: Loss = -11113.4521484375
Iteration 1000: Loss = -11089.0888671875
Iteration 1100: Loss = -11072.1357421875
Iteration 1200: Loss = -11060.1884765625
Iteration 1300: Loss = -11050.4560546875
Iteration 1400: Loss = -11042.244140625
Iteration 1500: Loss = -11035.984375
Iteration 1600: Loss = -11031.0537109375
Iteration 1700: Loss = -11027.04296875
Iteration 1800: Loss = -11023.69921875
Iteration 1900: Loss = -11020.8701171875
Iteration 2000: Loss = -11018.44140625
Iteration 2100: Loss = -11016.3369140625
Iteration 2200: Loss = -11014.498046875
Iteration 2300: Loss = -11012.875
Iteration 2400: Loss = -11011.4375
Iteration 2500: Loss = -11010.158203125
Iteration 2600: Loss = -11009.013671875
Iteration 2700: Loss = -11007.98828125
Iteration 2800: Loss = -11007.05859375
Iteration 2900: Loss = -11006.21875
Iteration 3000: Loss = -11005.45703125
Iteration 3100: Loss = -11004.76171875
Iteration 3200: Loss = -11004.126953125
Iteration 3300: Loss = -11003.5439453125
Iteration 3400: Loss = -11003.0126953125
Iteration 3500: Loss = -11002.5205078125
Iteration 3600: Loss = -11002.068359375
Iteration 3700: Loss = -11001.65234375
Iteration 3800: Loss = -11001.2646484375
Iteration 3900: Loss = -11000.9052734375
Iteration 4000: Loss = -11000.5732421875
Iteration 4100: Loss = -11000.265625
Iteration 4200: Loss = -10999.978515625
Iteration 4300: Loss = -10999.7138671875
Iteration 4400: Loss = -10999.4658203125
Iteration 4500: Loss = -10999.2294921875
Iteration 4600: Loss = -10999.01171875
Iteration 4700: Loss = -10998.8095703125
Iteration 4800: Loss = -10998.62109375
Iteration 4900: Loss = -10998.4423828125
Iteration 5000: Loss = -10998.27734375
Iteration 5100: Loss = -10998.1220703125
Iteration 5200: Loss = -10997.9765625
Iteration 5300: Loss = -10997.83984375
Iteration 5400: Loss = -10997.708984375
Iteration 5500: Loss = -10997.5888671875
Iteration 5600: Loss = -10997.474609375
Iteration 5700: Loss = -10997.3671875
Iteration 5800: Loss = -10997.2666015625
Iteration 5900: Loss = -10997.1708984375
Iteration 6000: Loss = -10997.080078125
Iteration 6100: Loss = -10996.9970703125
Iteration 6200: Loss = -10996.916015625
Iteration 6300: Loss = -10996.83984375
Iteration 6400: Loss = -10996.76953125
Iteration 6500: Loss = -10996.7021484375
Iteration 6600: Loss = -10996.6376953125
Iteration 6700: Loss = -10996.5791015625
Iteration 6800: Loss = -10996.5224609375
Iteration 6900: Loss = -10996.466796875
Iteration 7000: Loss = -10996.4169921875
Iteration 7100: Loss = -10996.369140625
Iteration 7200: Loss = -10996.3232421875
Iteration 7300: Loss = -10996.2802734375
Iteration 7400: Loss = -10996.240234375
Iteration 7500: Loss = -10996.201171875
Iteration 7600: Loss = -10996.1650390625
Iteration 7700: Loss = -10996.1298828125
Iteration 7800: Loss = -10996.0966796875
Iteration 7900: Loss = -10996.0673828125
Iteration 8000: Loss = -10996.0380859375
Iteration 8100: Loss = -10996.009765625
Iteration 8200: Loss = -10995.982421875
Iteration 8300: Loss = -10995.9580078125
Iteration 8400: Loss = -10995.9365234375
Iteration 8500: Loss = -10995.9130859375
Iteration 8600: Loss = -10995.892578125
Iteration 8700: Loss = -10995.873046875
Iteration 8800: Loss = -10995.85546875
Iteration 8900: Loss = -10995.8359375
Iteration 9000: Loss = -10995.8203125
Iteration 9100: Loss = -10995.8056640625
Iteration 9200: Loss = -10995.7890625
Iteration 9300: Loss = -10995.77734375
Iteration 9400: Loss = -10995.7626953125
Iteration 9500: Loss = -10995.75
Iteration 9600: Loss = -10995.7392578125
Iteration 9700: Loss = -10995.7265625
Iteration 9800: Loss = -10995.7158203125
Iteration 9900: Loss = -10995.708984375
Iteration 10000: Loss = -10995.697265625
Iteration 10100: Loss = -10995.689453125
Iteration 10200: Loss = -10995.6796875
Iteration 10300: Loss = -10995.671875
Iteration 10400: Loss = -10995.66796875
Iteration 10500: Loss = -10995.658203125
Iteration 10600: Loss = -10995.6533203125
Iteration 10700: Loss = -10995.6455078125
Iteration 10800: Loss = -10995.6396484375
Iteration 10900: Loss = -10995.6318359375
Iteration 11000: Loss = -10995.62890625
Iteration 11100: Loss = -10995.6220703125
Iteration 11200: Loss = -10995.6171875
Iteration 11300: Loss = -10995.6103515625
Iteration 11400: Loss = -10995.6064453125
Iteration 11500: Loss = -10995.6025390625
Iteration 11600: Loss = -10995.59765625
Iteration 11700: Loss = -10995.59375
Iteration 11800: Loss = -10995.5888671875
Iteration 11900: Loss = -10995.5849609375
Iteration 12000: Loss = -10995.58203125
Iteration 12100: Loss = -10995.578125
Iteration 12200: Loss = -10995.57421875
Iteration 12300: Loss = -10995.5693359375
Iteration 12400: Loss = -10995.564453125
Iteration 12500: Loss = -10995.55859375
Iteration 12600: Loss = -10995.5478515625
Iteration 12700: Loss = -10995.5107421875
Iteration 12800: Loss = -10994.7705078125
Iteration 12900: Loss = -10994.041015625
Iteration 13000: Loss = -10993.8046875
Iteration 13100: Loss = -10993.650390625
Iteration 13200: Loss = -10993.5380859375
Iteration 13300: Loss = -10993.4365234375
Iteration 13400: Loss = -10993.3505859375
Iteration 13500: Loss = -10993.265625
Iteration 13600: Loss = -10993.1806640625
Iteration 13700: Loss = -10993.1630859375
Iteration 13800: Loss = -10993.1494140625
Iteration 13900: Loss = -10993.13671875
Iteration 14000: Loss = -10993.1171875
Iteration 14100: Loss = -10993.1044921875
Iteration 14200: Loss = -10993.0986328125
Iteration 14300: Loss = -10993.087890625
Iteration 14400: Loss = -10993.08203125
Iteration 14500: Loss = -10993.078125
Iteration 14600: Loss = -10993.07421875
Iteration 14700: Loss = -10993.06640625
Iteration 14800: Loss = -10993.0625
Iteration 14900: Loss = -10993.05859375
Iteration 15000: Loss = -10993.0556640625
Iteration 15100: Loss = -10993.0537109375
Iteration 15200: Loss = -10993.048828125
Iteration 15300: Loss = -10993.048828125
Iteration 15400: Loss = -10993.0458984375
Iteration 15500: Loss = -10993.044921875
Iteration 15600: Loss = -10993.04296875
Iteration 15700: Loss = -10993.0400390625
Iteration 15800: Loss = -10993.0390625
Iteration 15900: Loss = -10993.0380859375
Iteration 16000: Loss = -10993.0380859375
Iteration 16100: Loss = -10993.037109375
Iteration 16200: Loss = -10993.0341796875
Iteration 16300: Loss = -10993.033203125
Iteration 16400: Loss = -10993.0341796875
1
Iteration 16500: Loss = -10993.0322265625
Iteration 16600: Loss = -10993.0322265625
Iteration 16700: Loss = -10993.03125
Iteration 16800: Loss = -10993.03125
Iteration 16900: Loss = -10993.0283203125
Iteration 17000: Loss = -10993.02734375
Iteration 17100: Loss = -10993.029296875
1
Iteration 17200: Loss = -10993.0283203125
2
Iteration 17300: Loss = -10993.0263671875
Iteration 17400: Loss = -10993.0263671875
Iteration 17500: Loss = -10993.025390625
Iteration 17600: Loss = -10993.0244140625
Iteration 17700: Loss = -10993.0244140625
Iteration 17800: Loss = -10993.025390625
1
Iteration 17900: Loss = -10993.025390625
2
Iteration 18000: Loss = -10993.0244140625
Iteration 18100: Loss = -10993.0234375
Iteration 18200: Loss = -10993.0224609375
Iteration 18300: Loss = -10993.0244140625
1
Iteration 18400: Loss = -10993.021484375
Iteration 18500: Loss = -10993.0244140625
1
Iteration 18600: Loss = -10993.021484375
Iteration 18700: Loss = -10993.021484375
Iteration 18800: Loss = -10993.0224609375
1
Iteration 18900: Loss = -10993.0224609375
2
Iteration 19000: Loss = -10993.0224609375
3
Iteration 19100: Loss = -10993.021484375
Iteration 19200: Loss = -10993.021484375
Iteration 19300: Loss = -10993.021484375
Iteration 19400: Loss = -10993.0205078125
Iteration 19500: Loss = -10993.021484375
1
Iteration 19600: Loss = -10993.01953125
Iteration 19700: Loss = -10993.0205078125
1
Iteration 19800: Loss = -10993.0205078125
2
Iteration 19900: Loss = -10993.0205078125
3
Iteration 20000: Loss = -10993.0205078125
4
Iteration 20100: Loss = -10993.01953125
Iteration 20200: Loss = -10993.01953125
Iteration 20300: Loss = -10993.021484375
1
Iteration 20400: Loss = -10993.01953125
Iteration 20500: Loss = -10993.01953125
Iteration 20600: Loss = -10993.01953125
Iteration 20700: Loss = -10993.0205078125
1
Iteration 20800: Loss = -10993.01953125
Iteration 20900: Loss = -10993.01953125
Iteration 21000: Loss = -10993.021484375
1
Iteration 21100: Loss = -10993.0205078125
2
Iteration 21200: Loss = -10993.0205078125
3
Iteration 21300: Loss = -10993.0185546875
Iteration 21400: Loss = -10993.01953125
1
Iteration 21500: Loss = -10993.0185546875
Iteration 21600: Loss = -10993.01953125
1
Iteration 21700: Loss = -10993.0185546875
Iteration 21800: Loss = -10993.0185546875
Iteration 21900: Loss = -10993.0185546875
Iteration 22000: Loss = -10993.01953125
1
Iteration 22100: Loss = -10993.0185546875
Iteration 22200: Loss = -10993.0185546875
Iteration 22300: Loss = -10993.017578125
Iteration 22400: Loss = -10993.0146484375
Iteration 22500: Loss = -10993.0126953125
Iteration 22600: Loss = -10993.0146484375
1
Iteration 22700: Loss = -10993.0146484375
2
Iteration 22800: Loss = -10993.0146484375
3
Iteration 22900: Loss = -10993.0146484375
4
Iteration 23000: Loss = -10993.015625
5
Iteration 23100: Loss = -10993.013671875
6
Iteration 23200: Loss = -10993.013671875
7
Iteration 23300: Loss = -10993.0146484375
8
Iteration 23400: Loss = -10993.013671875
9
Iteration 23500: Loss = -10993.0146484375
10
Iteration 23600: Loss = -10993.013671875
11
Iteration 23700: Loss = -10993.013671875
12
Iteration 23800: Loss = -10993.013671875
13
Iteration 23900: Loss = -10993.0146484375
14
Iteration 24000: Loss = -10993.0146484375
15
Stopping early at iteration 24000 due to no improvement.
pi: tensor([[1.0000e+00, 1.8263e-06],
        [8.1071e-01, 1.8929e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9657, 0.0343], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1617, 0.2477],
         [0.4696, 0.9916]],

        [[0.9599, 0.0923],
         [0.0180, 0.7593]],

        [[0.3839, 0.1522],
         [0.0657, 0.9050]],

        [[0.9899, 0.1913],
         [0.8850, 0.3298]],

        [[0.2862, 0.1617],
         [0.2395, 0.9795]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000560772937003748
Average Adjusted Rand Index: 0.00018583368648692847
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38193.75390625
Iteration 100: Loss = -22382.53515625
Iteration 200: Loss = -13749.1669921875
Iteration 300: Loss = -11917.85546875
Iteration 400: Loss = -11453.6708984375
Iteration 500: Loss = -11302.98828125
Iteration 600: Loss = -11227.8857421875
Iteration 700: Loss = -11175.28515625
Iteration 800: Loss = -11139.291015625
Iteration 900: Loss = -11119.09765625
Iteration 1000: Loss = -11089.9306640625
Iteration 1100: Loss = -11073.2412109375
Iteration 1200: Loss = -11061.6044921875
Iteration 1300: Loss = -11042.9267578125
Iteration 1400: Loss = -11037.6611328125
Iteration 1500: Loss = -11033.7001953125
Iteration 1600: Loss = -11030.3232421875
Iteration 1700: Loss = -11027.30078125
Iteration 1800: Loss = -11024.3046875
Iteration 1900: Loss = -11019.703125
Iteration 2000: Loss = -11015.5576171875
Iteration 2100: Loss = -11013.03125
Iteration 2200: Loss = -11011.20703125
Iteration 2300: Loss = -11009.7431640625
Iteration 2400: Loss = -11008.5078125
Iteration 2500: Loss = -11007.435546875
Iteration 2600: Loss = -11006.494140625
Iteration 2700: Loss = -11005.6572265625
Iteration 2800: Loss = -11004.904296875
Iteration 2900: Loss = -11004.2265625
Iteration 3000: Loss = -11003.6123046875
Iteration 3100: Loss = -11003.0546875
Iteration 3200: Loss = -11002.548828125
Iteration 3300: Loss = -11002.0830078125
Iteration 3400: Loss = -11001.65625
Iteration 3500: Loss = -11001.2666015625
Iteration 3600: Loss = -11000.904296875
Iteration 3700: Loss = -11000.572265625
Iteration 3800: Loss = -11000.263671875
Iteration 3900: Loss = -10999.9765625
Iteration 4000: Loss = -10999.7099609375
Iteration 4100: Loss = -10999.46484375
Iteration 4200: Loss = -10999.2333984375
Iteration 4300: Loss = -10999.0205078125
Iteration 4400: Loss = -10998.8193359375
Iteration 4500: Loss = -10998.6337890625
Iteration 4600: Loss = -10998.458984375
Iteration 4700: Loss = -10998.2939453125
Iteration 4800: Loss = -10998.1396484375
Iteration 4900: Loss = -10997.9951171875
Iteration 5000: Loss = -10997.8603515625
Iteration 5100: Loss = -10997.73046875
Iteration 5200: Loss = -10997.6103515625
Iteration 5300: Loss = -10997.498046875
Iteration 5400: Loss = -10997.392578125
Iteration 5500: Loss = -10997.29296875
Iteration 5600: Loss = -10997.19921875
Iteration 5700: Loss = -10997.111328125
Iteration 5800: Loss = -10997.02734375
Iteration 5900: Loss = -10996.94921875
Iteration 6000: Loss = -10996.8740234375
Iteration 6100: Loss = -10996.8037109375
Iteration 6200: Loss = -10996.7373046875
Iteration 6300: Loss = -10996.673828125
Iteration 6400: Loss = -10996.61328125
Iteration 6500: Loss = -10996.5576171875
Iteration 6600: Loss = -10996.505859375
Iteration 6700: Loss = -10996.4560546875
Iteration 6800: Loss = -10996.40625
Iteration 6900: Loss = -10996.361328125
Iteration 7000: Loss = -10996.3193359375
Iteration 7100: Loss = -10996.27734375
Iteration 7200: Loss = -10996.240234375
Iteration 7300: Loss = -10996.2021484375
Iteration 7400: Loss = -10996.16796875
Iteration 7500: Loss = -10996.1337890625
Iteration 7600: Loss = -10996.1025390625
Iteration 7700: Loss = -10996.0732421875
Iteration 7800: Loss = -10996.04296875
Iteration 7900: Loss = -10996.015625
Iteration 8000: Loss = -10995.9892578125
Iteration 8100: Loss = -10995.9619140625
Iteration 8200: Loss = -10995.935546875
Iteration 8300: Loss = -10995.9052734375
Iteration 8400: Loss = -10995.8515625
Iteration 8500: Loss = -10995.546875
Iteration 8600: Loss = -10994.09765625
Iteration 8700: Loss = -10993.8662109375
Iteration 8800: Loss = -10993.7392578125
Iteration 8900: Loss = -10993.6728515625
Iteration 9000: Loss = -10993.62890625
Iteration 9100: Loss = -10993.5927734375
Iteration 9200: Loss = -10993.5625
Iteration 9300: Loss = -10993.537109375
Iteration 9400: Loss = -10993.5126953125
Iteration 9500: Loss = -10993.4912109375
Iteration 9600: Loss = -10993.470703125
Iteration 9700: Loss = -10993.4541015625
Iteration 9800: Loss = -10993.4375
Iteration 9900: Loss = -10993.4208984375
Iteration 10000: Loss = -10993.4072265625
Iteration 10100: Loss = -10993.39453125
Iteration 10200: Loss = -10993.380859375
Iteration 10300: Loss = -10993.3701171875
Iteration 10400: Loss = -10993.357421875
Iteration 10500: Loss = -10993.3466796875
Iteration 10600: Loss = -10993.3359375
Iteration 10700: Loss = -10993.326171875
Iteration 10800: Loss = -10993.31640625
Iteration 10900: Loss = -10993.3095703125
Iteration 11000: Loss = -10993.2998046875
Iteration 11100: Loss = -10993.291015625
Iteration 11200: Loss = -10993.2822265625
Iteration 11300: Loss = -10993.275390625
Iteration 11400: Loss = -10993.2685546875
Iteration 11500: Loss = -10993.2587890625
Iteration 11600: Loss = -10993.2509765625
Iteration 11700: Loss = -10993.244140625
Iteration 11800: Loss = -10993.236328125
Iteration 11900: Loss = -10993.228515625
Iteration 12000: Loss = -10993.2216796875
Iteration 12100: Loss = -10993.2138671875
Iteration 12200: Loss = -10993.208984375
Iteration 12300: Loss = -10993.2021484375
Iteration 12400: Loss = -10993.193359375
Iteration 12500: Loss = -10993.189453125
Iteration 12600: Loss = -10993.1826171875
Iteration 12700: Loss = -10993.173828125
Iteration 12800: Loss = -10993.1669921875
Iteration 12900: Loss = -10993.1611328125
Iteration 13000: Loss = -10993.154296875
Iteration 13100: Loss = -10993.146484375
Iteration 13200: Loss = -10993.1396484375
Iteration 13300: Loss = -10993.130859375
Iteration 13400: Loss = -10993.12109375
Iteration 13500: Loss = -10993.1123046875
Iteration 13600: Loss = -10993.1064453125
Iteration 13700: Loss = -10993.0986328125
Iteration 13800: Loss = -10993.0947265625
Iteration 13900: Loss = -10993.0908203125
Iteration 14000: Loss = -10993.0859375
Iteration 14100: Loss = -10993.0810546875
Iteration 14200: Loss = -10993.0771484375
Iteration 14300: Loss = -10993.072265625
Iteration 14400: Loss = -10993.0693359375
Iteration 14500: Loss = -10993.06640625
Iteration 14600: Loss = -10993.0634765625
Iteration 14700: Loss = -10993.0595703125
Iteration 14800: Loss = -10993.05859375
Iteration 14900: Loss = -10993.0556640625
Iteration 15000: Loss = -10993.052734375
Iteration 15100: Loss = -10993.0498046875
Iteration 15200: Loss = -10993.0478515625
Iteration 15300: Loss = -10993.0439453125
Iteration 15400: Loss = -10993.046875
1
Iteration 15500: Loss = -10993.0419921875
Iteration 15600: Loss = -10993.0390625
Iteration 15700: Loss = -10993.0380859375
Iteration 15800: Loss = -10993.0361328125
Iteration 15900: Loss = -10993.03515625
Iteration 16000: Loss = -10993.03515625
Iteration 16100: Loss = -10993.033203125
Iteration 16200: Loss = -10993.03125
Iteration 16300: Loss = -10993.03125
Iteration 16400: Loss = -10993.029296875
Iteration 16500: Loss = -10993.0302734375
1
Iteration 16600: Loss = -10993.02734375
Iteration 16700: Loss = -10993.0263671875
Iteration 16800: Loss = -10993.0283203125
1
Iteration 16900: Loss = -10993.025390625
Iteration 17000: Loss = -10993.0234375
Iteration 17100: Loss = -10993.0224609375
Iteration 17200: Loss = -10993.01953125
Iteration 17300: Loss = -10993.021484375
1
Iteration 17400: Loss = -10993.01953125
Iteration 17500: Loss = -10993.0185546875
Iteration 17600: Loss = -10993.0185546875
Iteration 17700: Loss = -10993.0205078125
1
Iteration 17800: Loss = -10993.017578125
Iteration 17900: Loss = -10993.0224609375
1
Iteration 18000: Loss = -10993.017578125
Iteration 18100: Loss = -10993.017578125
Iteration 18200: Loss = -10993.0166015625
Iteration 18300: Loss = -10993.0166015625
Iteration 18400: Loss = -10993.0166015625
Iteration 18500: Loss = -10993.015625
Iteration 18600: Loss = -10993.0166015625
1
Iteration 18700: Loss = -10993.0166015625
2
Iteration 18800: Loss = -10993.017578125
3
Iteration 18900: Loss = -10993.0146484375
Iteration 19000: Loss = -10993.015625
1
Iteration 19100: Loss = -10993.015625
2
Iteration 19200: Loss = -10993.015625
3
Iteration 19300: Loss = -10993.015625
4
Iteration 19400: Loss = -10993.015625
5
Iteration 19500: Loss = -10993.0146484375
Iteration 19600: Loss = -10993.0146484375
Iteration 19700: Loss = -10993.0146484375
Iteration 19800: Loss = -10993.0146484375
Iteration 19900: Loss = -10993.0146484375
Iteration 20000: Loss = -10993.015625
1
Iteration 20100: Loss = -10993.0146484375
Iteration 20200: Loss = -10993.0146484375
Iteration 20300: Loss = -10993.0146484375
Iteration 20400: Loss = -10993.0146484375
Iteration 20500: Loss = -10993.015625
1
Iteration 20600: Loss = -10993.0146484375
Iteration 20700: Loss = -10993.0146484375
Iteration 20800: Loss = -10993.0146484375
Iteration 20900: Loss = -10993.013671875
Iteration 21000: Loss = -10993.0146484375
1
Iteration 21100: Loss = -10993.015625
2
Iteration 21200: Loss = -10993.0146484375
3
Iteration 21300: Loss = -10993.0146484375
4
Iteration 21400: Loss = -10993.013671875
Iteration 21500: Loss = -10993.013671875
Iteration 21600: Loss = -10993.013671875
Iteration 21700: Loss = -10993.013671875
Iteration 21800: Loss = -10993.013671875
Iteration 21900: Loss = -10993.0146484375
1
Iteration 22000: Loss = -10993.0146484375
2
Iteration 22100: Loss = -10993.0146484375
3
Iteration 22200: Loss = -10993.01171875
Iteration 22300: Loss = -10993.0146484375
1
Iteration 22400: Loss = -10993.013671875
2
Iteration 22500: Loss = -10993.0146484375
3
Iteration 22600: Loss = -10993.0146484375
4
Iteration 22700: Loss = -10993.0126953125
5
Iteration 22800: Loss = -10993.013671875
6
Iteration 22900: Loss = -10993.017578125
7
Iteration 23000: Loss = -10993.0146484375
8
Iteration 23100: Loss = -10993.013671875
9
Iteration 23200: Loss = -10993.013671875
10
Iteration 23300: Loss = -10993.013671875
11
Iteration 23400: Loss = -10993.0146484375
12
Iteration 23500: Loss = -10993.015625
13
Iteration 23600: Loss = -10993.0126953125
14
Iteration 23700: Loss = -10993.013671875
15
Stopping early at iteration 23700 due to no improvement.
pi: tensor([[1.0000e+00, 3.0124e-06],
        [8.1074e-01, 1.8926e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9657, 0.0343], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1617, 0.2477],
         [0.0193, 0.9913]],

        [[0.3025, 0.0923],
         [0.4647, 0.0068]],

        [[0.8188, 0.1522],
         [0.0257, 0.0306]],

        [[0.0101, 0.1913],
         [0.9709, 0.4297]],

        [[0.0931, 0.1621],
         [0.7698, 0.2844]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000560772937003748
Average Adjusted Rand Index: 0.00018583368648692847
[0.000560772937003748, 0.000560772937003748] [0.00018583368648692847, 0.00018583368648692847] [10993.0146484375, 10993.013671875]
-------------------------------------
This iteration is 50
True Objective function: Loss = -10795.566240811346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40033.9921875
Iteration 100: Loss = -21098.994140625
Iteration 200: Loss = -12525.3212890625
Iteration 300: Loss = -11267.91796875
Iteration 400: Loss = -11081.529296875
Iteration 500: Loss = -11008.388671875
Iteration 600: Loss = -10975.986328125
Iteration 700: Loss = -10956.8642578125
Iteration 800: Loss = -10944.5205078125
Iteration 900: Loss = -10936.0546875
Iteration 1000: Loss = -10929.9375
Iteration 1100: Loss = -10925.4072265625
Iteration 1200: Loss = -10921.9150390625
Iteration 1300: Loss = -10919.14453125
Iteration 1400: Loss = -10916.845703125
Iteration 1500: Loss = -10914.998046875
Iteration 1600: Loss = -10913.4599609375
Iteration 1700: Loss = -10912.15234375
Iteration 1800: Loss = -10911.025390625
Iteration 1900: Loss = -10910.05078125
Iteration 2000: Loss = -10909.197265625
Iteration 2100: Loss = -10908.4423828125
Iteration 2200: Loss = -10907.7763671875
Iteration 2300: Loss = -10907.18359375
Iteration 2400: Loss = -10906.6494140625
Iteration 2500: Loss = -10906.1689453125
Iteration 2600: Loss = -10905.734375
Iteration 2700: Loss = -10905.337890625
Iteration 2800: Loss = -10904.978515625
Iteration 2900: Loss = -10904.6484375
Iteration 3000: Loss = -10904.341796875
Iteration 3100: Loss = -10904.0634765625
Iteration 3200: Loss = -10903.8056640625
Iteration 3300: Loss = -10903.568359375
Iteration 3400: Loss = -10903.3466796875
Iteration 3500: Loss = -10903.142578125
Iteration 3600: Loss = -10902.9521484375
Iteration 3700: Loss = -10902.775390625
Iteration 3800: Loss = -10902.609375
Iteration 3900: Loss = -10902.4560546875
Iteration 4000: Loss = -10902.314453125
Iteration 4100: Loss = -10902.1796875
Iteration 4200: Loss = -10902.056640625
Iteration 4300: Loss = -10901.9404296875
Iteration 4400: Loss = -10901.830078125
Iteration 4500: Loss = -10901.7275390625
Iteration 4600: Loss = -10901.6318359375
Iteration 4700: Loss = -10901.541015625
Iteration 4800: Loss = -10901.4580078125
Iteration 4900: Loss = -10901.376953125
Iteration 5000: Loss = -10901.3017578125
Iteration 5100: Loss = -10901.23046875
Iteration 5200: Loss = -10901.1640625
Iteration 5300: Loss = -10901.1025390625
Iteration 5400: Loss = -10901.04296875
Iteration 5500: Loss = -10900.986328125
Iteration 5600: Loss = -10900.93359375
Iteration 5700: Loss = -10900.8828125
Iteration 5800: Loss = -10900.8359375
Iteration 5900: Loss = -10900.791015625
Iteration 6000: Loss = -10900.751953125
Iteration 6100: Loss = -10900.7109375
Iteration 6200: Loss = -10900.6728515625
Iteration 6300: Loss = -10900.63671875
Iteration 6400: Loss = -10900.6025390625
Iteration 6500: Loss = -10900.5693359375
Iteration 6600: Loss = -10900.5390625
Iteration 6700: Loss = -10900.509765625
Iteration 6800: Loss = -10900.4814453125
Iteration 6900: Loss = -10900.4521484375
Iteration 7000: Loss = -10900.4287109375
Iteration 7100: Loss = -10900.40234375
Iteration 7200: Loss = -10900.3779296875
Iteration 7300: Loss = -10900.3525390625
Iteration 7400: Loss = -10900.3271484375
Iteration 7500: Loss = -10900.3017578125
Iteration 7600: Loss = -10900.2724609375
Iteration 7700: Loss = -10900.2373046875
Iteration 7800: Loss = -10900.19140625
Iteration 7900: Loss = -10900.12109375
Iteration 8000: Loss = -10900.01171875
Iteration 8100: Loss = -10899.833984375
Iteration 8200: Loss = -10899.470703125
Iteration 8300: Loss = -10899.0400390625
Iteration 8400: Loss = -10898.8857421875
Iteration 8500: Loss = -10898.8046875
Iteration 8600: Loss = -10898.7353515625
Iteration 8700: Loss = -10898.6708984375
Iteration 8800: Loss = -10898.609375
Iteration 8900: Loss = -10898.5517578125
Iteration 9000: Loss = -10898.50390625
Iteration 9100: Loss = -10898.4638671875
Iteration 9200: Loss = -10898.4267578125
Iteration 9300: Loss = -10898.3935546875
Iteration 9400: Loss = -10898.359375
Iteration 9500: Loss = -10898.32421875
Iteration 9600: Loss = -10898.2900390625
Iteration 9700: Loss = -10898.255859375
Iteration 9800: Loss = -10898.21875
Iteration 9900: Loss = -10898.1796875
Iteration 10000: Loss = -10898.140625
Iteration 10100: Loss = -10898.0947265625
Iteration 10200: Loss = -10898.0458984375
Iteration 10300: Loss = -10897.9931640625
Iteration 10400: Loss = -10897.931640625
Iteration 10500: Loss = -10897.859375
Iteration 10600: Loss = -10897.759765625
Iteration 10700: Loss = -10897.478515625
Iteration 10800: Loss = -10897.1982421875
Iteration 10900: Loss = -10896.859375
Iteration 11000: Loss = -10895.861328125
Iteration 11100: Loss = -10895.6689453125
Iteration 11200: Loss = -10895.2197265625
Iteration 11300: Loss = -10894.1591796875
Iteration 11400: Loss = -10893.5400390625
Iteration 11500: Loss = -10893.2939453125
Iteration 11600: Loss = -10893.24609375
Iteration 11700: Loss = -10893.1943359375
Iteration 11800: Loss = -10893.123046875
Iteration 11900: Loss = -10893.04296875
Iteration 12000: Loss = -10892.9814453125
Iteration 12100: Loss = -10892.818359375
Iteration 12200: Loss = -10891.103515625
Iteration 12300: Loss = -10888.5771484375
Iteration 12400: Loss = -10886.1103515625
Iteration 12500: Loss = -10884.564453125
Iteration 12600: Loss = -10882.53125
Iteration 12700: Loss = -10879.931640625
Iteration 12800: Loss = -10869.8818359375
Iteration 12900: Loss = -10869.703125
Iteration 13000: Loss = -10869.494140625
Iteration 13100: Loss = -10869.16796875
Iteration 13200: Loss = -10868.958984375
Iteration 13300: Loss = -10868.8701171875
Iteration 13400: Loss = -10868.7724609375
Iteration 13500: Loss = -10868.7275390625
Iteration 13600: Loss = -10868.6953125
Iteration 13700: Loss = -10868.66796875
Iteration 13800: Loss = -10868.6494140625
Iteration 13900: Loss = -10868.63671875
Iteration 14000: Loss = -10868.6240234375
Iteration 14100: Loss = -10868.615234375
Iteration 14200: Loss = -10868.609375
Iteration 14300: Loss = -10868.6064453125
Iteration 14400: Loss = -10868.60546875
Iteration 14500: Loss = -10868.6025390625
Iteration 14600: Loss = -10868.6025390625
Iteration 14700: Loss = -10868.6025390625
Iteration 14800: Loss = -10868.6015625
Iteration 14900: Loss = -10868.6025390625
1
Iteration 15000: Loss = -10868.599609375
Iteration 15100: Loss = -10868.599609375
Iteration 15200: Loss = -10868.5986328125
Iteration 15300: Loss = -10868.59765625
Iteration 15400: Loss = -10868.59765625
Iteration 15500: Loss = -10868.5986328125
1
Iteration 15600: Loss = -10868.59765625
Iteration 15700: Loss = -10868.5966796875
Iteration 15800: Loss = -10868.5966796875
Iteration 15900: Loss = -10868.595703125
Iteration 16000: Loss = -10868.59765625
1
Iteration 16100: Loss = -10868.595703125
Iteration 16200: Loss = -10868.595703125
Iteration 16300: Loss = -10868.595703125
Iteration 16400: Loss = -10868.595703125
Iteration 16500: Loss = -10868.5966796875
1
Iteration 16600: Loss = -10868.5966796875
2
Iteration 16700: Loss = -10868.5947265625
Iteration 16800: Loss = -10868.5966796875
1
Iteration 16900: Loss = -10868.595703125
2
Iteration 17000: Loss = -10868.5947265625
Iteration 17100: Loss = -10868.59375
Iteration 17200: Loss = -10868.595703125
1
Iteration 17300: Loss = -10868.5947265625
2
Iteration 17400: Loss = -10868.5966796875
3
Iteration 17500: Loss = -10868.595703125
4
Iteration 17600: Loss = -10868.59375
Iteration 17700: Loss = -10868.5947265625
1
Iteration 17800: Loss = -10868.5947265625
2
Iteration 17900: Loss = -10868.595703125
3
Iteration 18000: Loss = -10868.595703125
4
Iteration 18100: Loss = -10868.5966796875
5
Iteration 18200: Loss = -10868.595703125
6
Iteration 18300: Loss = -10868.595703125
7
Iteration 18400: Loss = -10868.595703125
8
Iteration 18500: Loss = -10868.59375
Iteration 18600: Loss = -10868.595703125
1
Iteration 18700: Loss = -10868.5947265625
2
Iteration 18800: Loss = -10868.5947265625
3
Iteration 18900: Loss = -10868.59375
Iteration 19000: Loss = -10868.5947265625
1
Iteration 19100: Loss = -10868.5947265625
2
Iteration 19200: Loss = -10868.595703125
3
Iteration 19300: Loss = -10868.59375
Iteration 19400: Loss = -10868.5947265625
1
Iteration 19500: Loss = -10868.595703125
2
Iteration 19600: Loss = -10868.5947265625
3
Iteration 19700: Loss = -10868.595703125
4
Iteration 19800: Loss = -10868.5947265625
5
Iteration 19900: Loss = -10868.595703125
6
Iteration 20000: Loss = -10868.5947265625
7
Iteration 20100: Loss = -10868.595703125
8
Iteration 20200: Loss = -10868.59375
Iteration 20300: Loss = -10868.5947265625
1
Iteration 20400: Loss = -10868.5947265625
2
Iteration 20500: Loss = -10868.5947265625
3
Iteration 20600: Loss = -10868.5947265625
4
Iteration 20700: Loss = -10868.59375
Iteration 20800: Loss = -10868.595703125
1
Iteration 20900: Loss = -10868.59375
Iteration 21000: Loss = -10868.59375
Iteration 21100: Loss = -10868.595703125
1
Iteration 21200: Loss = -10868.59375
Iteration 21300: Loss = -10868.59375
Iteration 21400: Loss = -10868.5927734375
Iteration 21500: Loss = -10868.59375
1
Iteration 21600: Loss = -10868.595703125
2
Iteration 21700: Loss = -10868.59375
3
Iteration 21800: Loss = -10868.59375
4
Iteration 21900: Loss = -10868.5947265625
5
Iteration 22000: Loss = -10868.5947265625
6
Iteration 22100: Loss = -10868.595703125
7
Iteration 22200: Loss = -10868.59375
8
Iteration 22300: Loss = -10868.5947265625
9
Iteration 22400: Loss = -10868.5947265625
10
Iteration 22500: Loss = -10868.595703125
11
Iteration 22600: Loss = -10868.59375
12
Iteration 22700: Loss = -10868.5947265625
13
Iteration 22800: Loss = -10868.5966796875
14
Iteration 22900: Loss = -10868.5947265625
15
Stopping early at iteration 22900 due to no improvement.
pi: tensor([[0.0520, 0.9480],
        [0.0283, 0.9717]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5479, 0.4521], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2592, 0.0962],
         [0.0430, 0.1639]],

        [[0.8404, 0.0861],
         [0.9885, 0.0729]],

        [[0.3821, 0.2008],
         [0.8091, 0.0102]],

        [[0.9766, 0.0831],
         [0.9890, 0.9668]],

        [[0.0655, 0.1046],
         [0.2872, 0.7628]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05260067996529949
Average Adjusted Rand Index: 0.17563864194677348
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48947.375
Iteration 100: Loss = -33565.98828125
Iteration 200: Loss = -20976.01171875
Iteration 300: Loss = -14736.369140625
Iteration 400: Loss = -12315.69921875
Iteration 500: Loss = -11413.1806640625
Iteration 600: Loss = -11135.2646484375
Iteration 700: Loss = -11038.6513671875
Iteration 800: Loss = -10988.6357421875
Iteration 900: Loss = -10969.5908203125
Iteration 1000: Loss = -10961.375
Iteration 1100: Loss = -10952.4326171875
Iteration 1200: Loss = -10942.615234375
Iteration 1300: Loss = -10934.4482421875
Iteration 1400: Loss = -10930.3037109375
Iteration 1500: Loss = -10925.3486328125
Iteration 1600: Loss = -10923.498046875
Iteration 1700: Loss = -10922.23046875
Iteration 1800: Loss = -10919.12109375
Iteration 1900: Loss = -10915.7783203125
Iteration 2000: Loss = -10914.888671875
Iteration 2100: Loss = -10913.103515625
Iteration 2200: Loss = -10910.94921875
Iteration 2300: Loss = -10910.2138671875
Iteration 2400: Loss = -10909.734375
Iteration 2500: Loss = -10909.3544921875
Iteration 2600: Loss = -10909.025390625
Iteration 2700: Loss = -10903.763671875
Iteration 2800: Loss = -10903.287109375
Iteration 2900: Loss = -10902.982421875
Iteration 3000: Loss = -10902.7314453125
Iteration 3100: Loss = -10902.517578125
Iteration 3200: Loss = -10902.33203125
Iteration 3300: Loss = -10902.1650390625
Iteration 3400: Loss = -10902.015625
Iteration 3500: Loss = -10901.880859375
Iteration 3600: Loss = -10901.7587890625
Iteration 3700: Loss = -10901.646484375
Iteration 3800: Loss = -10901.5419921875
Iteration 3900: Loss = -10901.447265625
Iteration 4000: Loss = -10901.357421875
Iteration 4100: Loss = -10901.2724609375
Iteration 4200: Loss = -10901.1953125
Iteration 4300: Loss = -10901.119140625
Iteration 4400: Loss = -10901.0478515625
Iteration 4500: Loss = -10900.978515625
Iteration 4600: Loss = -10900.9091796875
Iteration 4700: Loss = -10900.8330078125
Iteration 4800: Loss = -10900.74609375
Iteration 4900: Loss = -10900.6298828125
Iteration 5000: Loss = -10900.431640625
Iteration 5100: Loss = -10900.046875
Iteration 5200: Loss = -10899.609375
Iteration 5300: Loss = -10899.392578125
Iteration 5400: Loss = -10899.2314453125
Iteration 5500: Loss = -10898.4326171875
Iteration 5600: Loss = -10898.2841796875
Iteration 5700: Loss = -10898.1533203125
Iteration 5800: Loss = -10898.0283203125
Iteration 5900: Loss = -10897.869140625
Iteration 6000: Loss = -10897.60546875
Iteration 6100: Loss = -10897.255859375
Iteration 6200: Loss = -10896.8193359375
Iteration 6300: Loss = -10896.4912109375
Iteration 6400: Loss = -10896.310546875
Iteration 6500: Loss = -10896.2138671875
Iteration 6600: Loss = -10896.158203125
Iteration 6700: Loss = -10896.11328125
Iteration 6800: Loss = -10896.0712890625
Iteration 6900: Loss = -10896.02734375
Iteration 7000: Loss = -10895.9609375
Iteration 7100: Loss = -10895.8955078125
Iteration 7200: Loss = -10895.8720703125
Iteration 7300: Loss = -10895.8505859375
Iteration 7400: Loss = -10895.83203125
Iteration 7500: Loss = -10895.8095703125
Iteration 7600: Loss = -10895.7646484375
Iteration 7700: Loss = -10895.3486328125
Iteration 7800: Loss = -10894.7431640625
Iteration 7900: Loss = -10894.5771484375
Iteration 8000: Loss = -10894.46484375
Iteration 8100: Loss = -10894.375
Iteration 8200: Loss = -10894.30078125
Iteration 8300: Loss = -10894.240234375
Iteration 8400: Loss = -10894.1845703125
Iteration 8500: Loss = -10894.13671875
Iteration 8600: Loss = -10894.087890625
Iteration 8700: Loss = -10894.02734375
Iteration 8800: Loss = -10893.8798828125
Iteration 8900: Loss = -10892.5810546875
Iteration 9000: Loss = -10885.94921875
Iteration 9100: Loss = -10883.77734375
Iteration 9200: Loss = -10882.3681640625
Iteration 9300: Loss = -10862.5458984375
Iteration 9400: Loss = -10847.2177734375
Iteration 9500: Loss = -10845.828125
Iteration 9600: Loss = -10842.0927734375
Iteration 9700: Loss = -10841.90625
Iteration 9800: Loss = -10841.744140625
Iteration 9900: Loss = -10841.01171875
Iteration 10000: Loss = -10840.904296875
Iteration 10100: Loss = -10840.5986328125
Iteration 10200: Loss = -10840.3857421875
Iteration 10300: Loss = -10840.33203125
Iteration 10400: Loss = -10840.2587890625
Iteration 10500: Loss = -10838.603515625
Iteration 10600: Loss = -10838.484375
Iteration 10700: Loss = -10838.4501953125
Iteration 10800: Loss = -10838.4208984375
Iteration 10900: Loss = -10838.400390625
Iteration 11000: Loss = -10838.3798828125
Iteration 11100: Loss = -10838.3603515625
Iteration 11200: Loss = -10838.3359375
Iteration 11300: Loss = -10838.3291015625
Iteration 11400: Loss = -10838.3173828125
Iteration 11500: Loss = -10838.3095703125
Iteration 11600: Loss = -10838.298828125
Iteration 11700: Loss = -10838.275390625
Iteration 11800: Loss = -10838.26953125
Iteration 11900: Loss = -10838.265625
Iteration 12000: Loss = -10838.26171875
Iteration 12100: Loss = -10838.259765625
Iteration 12200: Loss = -10838.2587890625
Iteration 12300: Loss = -10838.255859375
Iteration 12400: Loss = -10838.25390625
Iteration 12500: Loss = -10838.248046875
Iteration 12600: Loss = -10838.23828125
Iteration 12700: Loss = -10838.2373046875
Iteration 12800: Loss = -10838.2353515625
Iteration 12900: Loss = -10838.234375
Iteration 13000: Loss = -10838.2333984375
Iteration 13100: Loss = -10838.232421875
Iteration 13200: Loss = -10838.2314453125
Iteration 13300: Loss = -10838.23046875
Iteration 13400: Loss = -10838.2265625
Iteration 13500: Loss = -10838.2216796875
Iteration 13600: Loss = -10838.2197265625
Iteration 13700: Loss = -10838.2197265625
Iteration 13800: Loss = -10838.2177734375
Iteration 13900: Loss = -10838.2177734375
Iteration 14000: Loss = -10838.2138671875
Iteration 14100: Loss = -10838.2109375
Iteration 14200: Loss = -10838.2080078125
Iteration 14300: Loss = -10838.2060546875
Iteration 14400: Loss = -10838.205078125
Iteration 14500: Loss = -10838.19921875
Iteration 14600: Loss = -10838.1982421875
Iteration 14700: Loss = -10838.1982421875
Iteration 14800: Loss = -10838.1982421875
Iteration 14900: Loss = -10838.1962890625
Iteration 15000: Loss = -10838.197265625
1
Iteration 15100: Loss = -10838.193359375
Iteration 15200: Loss = -10838.1923828125
Iteration 15300: Loss = -10838.193359375
1
Iteration 15400: Loss = -10838.193359375
2
Iteration 15500: Loss = -10838.193359375
3
Iteration 15600: Loss = -10838.1923828125
Iteration 15700: Loss = -10838.19140625
Iteration 15800: Loss = -10838.185546875
Iteration 15900: Loss = -10838.1845703125
Iteration 16000: Loss = -10838.18359375
Iteration 16100: Loss = -10838.18359375
Iteration 16200: Loss = -10838.1787109375
Iteration 16300: Loss = -10838.1787109375
Iteration 16400: Loss = -10838.1796875
1
Iteration 16500: Loss = -10838.1787109375
Iteration 16600: Loss = -10838.177734375
Iteration 16700: Loss = -10838.177734375
Iteration 16800: Loss = -10838.1787109375
1
Iteration 16900: Loss = -10838.177734375
Iteration 17000: Loss = -10838.177734375
Iteration 17100: Loss = -10838.1748046875
Iteration 17200: Loss = -10838.1728515625
Iteration 17300: Loss = -10838.171875
Iteration 17400: Loss = -10838.171875
Iteration 17500: Loss = -10838.1728515625
1
Iteration 17600: Loss = -10838.173828125
2
Iteration 17700: Loss = -10838.1728515625
3
Iteration 17800: Loss = -10838.1708984375
Iteration 17900: Loss = -10838.1728515625
1
Iteration 18000: Loss = -10838.1708984375
Iteration 18100: Loss = -10838.171875
1
Iteration 18200: Loss = -10838.1728515625
2
Iteration 18300: Loss = -10838.1708984375
Iteration 18400: Loss = -10838.171875
1
Iteration 18500: Loss = -10838.171875
2
Iteration 18600: Loss = -10838.1708984375
Iteration 18700: Loss = -10838.1728515625
1
Iteration 18800: Loss = -10838.171875
2
Iteration 18900: Loss = -10838.171875
3
Iteration 19000: Loss = -10838.171875
4
Iteration 19100: Loss = -10838.1728515625
5
Iteration 19200: Loss = -10838.171875
6
Iteration 19300: Loss = -10838.1728515625
7
Iteration 19400: Loss = -10838.1728515625
8
Iteration 19500: Loss = -10838.171875
9
Iteration 19600: Loss = -10838.171875
10
Iteration 19700: Loss = -10838.171875
11
Iteration 19800: Loss = -10838.171875
12
Iteration 19900: Loss = -10838.1728515625
13
Iteration 20000: Loss = -10838.171875
14
Iteration 20100: Loss = -10838.1708984375
Iteration 20200: Loss = -10838.1708984375
Iteration 20300: Loss = -10838.1708984375
Iteration 20400: Loss = -10838.1708984375
Iteration 20500: Loss = -10838.171875
1
Iteration 20600: Loss = -10838.1708984375
Iteration 20700: Loss = -10838.1708984375
Iteration 20800: Loss = -10838.1728515625
1
Iteration 20900: Loss = -10838.171875
2
Iteration 21000: Loss = -10838.171875
3
Iteration 21100: Loss = -10838.1708984375
Iteration 21200: Loss = -10838.1708984375
Iteration 21300: Loss = -10838.1708984375
Iteration 21400: Loss = -10838.1708984375
Iteration 21500: Loss = -10838.1708984375
Iteration 21600: Loss = -10838.1708984375
Iteration 21700: Loss = -10838.171875
1
Iteration 21800: Loss = -10838.1708984375
Iteration 21900: Loss = -10838.171875
1
Iteration 22000: Loss = -10838.169921875
Iteration 22100: Loss = -10838.1708984375
1
Iteration 22200: Loss = -10838.1708984375
2
Iteration 22300: Loss = -10838.171875
3
Iteration 22400: Loss = -10838.1708984375
4
Iteration 22500: Loss = -10838.1708984375
5
Iteration 22600: Loss = -10838.1708984375
6
Iteration 22700: Loss = -10838.1708984375
7
Iteration 22800: Loss = -10838.1708984375
8
Iteration 22900: Loss = -10838.169921875
Iteration 23000: Loss = -10838.1708984375
1
Iteration 23100: Loss = -10838.1708984375
2
Iteration 23200: Loss = -10838.1708984375
3
Iteration 23300: Loss = -10838.1708984375
4
Iteration 23400: Loss = -10838.1708984375
5
Iteration 23500: Loss = -10838.1669921875
Iteration 23600: Loss = -10838.1533203125
Iteration 23700: Loss = -10838.15234375
Iteration 23800: Loss = -10838.15234375
Iteration 23900: Loss = -10838.1533203125
1
Iteration 24000: Loss = -10838.1533203125
2
Iteration 24100: Loss = -10838.154296875
3
Iteration 24200: Loss = -10838.1533203125
4
Iteration 24300: Loss = -10838.1533203125
5
Iteration 24400: Loss = -10838.1533203125
6
Iteration 24500: Loss = -10838.154296875
7
Iteration 24600: Loss = -10838.1513671875
Iteration 24700: Loss = -10838.1533203125
1
Iteration 24800: Loss = -10838.1533203125
2
Iteration 24900: Loss = -10838.15234375
3
Iteration 25000: Loss = -10838.15234375
4
Iteration 25100: Loss = -10838.15234375
5
Iteration 25200: Loss = -10838.1533203125
6
Iteration 25300: Loss = -10838.15234375
7
Iteration 25400: Loss = -10838.15234375
8
Iteration 25500: Loss = -10838.154296875
9
Iteration 25600: Loss = -10838.15234375
10
Iteration 25700: Loss = -10838.15234375
11
Iteration 25800: Loss = -10838.1513671875
Iteration 25900: Loss = -10838.1513671875
Iteration 26000: Loss = -10838.15234375
1
Iteration 26100: Loss = -10838.15234375
2
Iteration 26200: Loss = -10838.15234375
3
Iteration 26300: Loss = -10838.15234375
4
Iteration 26400: Loss = -10838.1513671875
Iteration 26500: Loss = -10838.15234375
1
Iteration 26600: Loss = -10838.15234375
2
Iteration 26700: Loss = -10838.1513671875
Iteration 26800: Loss = -10838.1513671875
Iteration 26900: Loss = -10838.15234375
1
Iteration 27000: Loss = -10838.15234375
2
Iteration 27100: Loss = -10838.15234375
3
Iteration 27200: Loss = -10838.15234375
4
Iteration 27300: Loss = -10838.1513671875
Iteration 27400: Loss = -10838.1513671875
Iteration 27500: Loss = -10838.15234375
1
Iteration 27600: Loss = -10838.15234375
2
Iteration 27700: Loss = -10838.1513671875
Iteration 27800: Loss = -10838.150390625
Iteration 27900: Loss = -10838.1513671875
1
Iteration 28000: Loss = -10838.1513671875
2
Iteration 28100: Loss = -10838.1513671875
3
Iteration 28200: Loss = -10838.1513671875
4
Iteration 28300: Loss = -10838.1513671875
5
Iteration 28400: Loss = -10838.1513671875
6
Iteration 28500: Loss = -10838.1513671875
7
Iteration 28600: Loss = -10838.15234375
8
Iteration 28700: Loss = -10838.1513671875
9
Iteration 28800: Loss = -10838.1513671875
10
Iteration 28900: Loss = -10838.15234375
11
Iteration 29000: Loss = -10838.15234375
12
Iteration 29100: Loss = -10838.1513671875
13
Iteration 29200: Loss = -10838.1533203125
14
Iteration 29300: Loss = -10838.1533203125
15
Stopping early at iteration 29300 due to no improvement.
pi: tensor([[0.5203, 0.4797],
        [0.0087, 0.9913]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5809, 0.4191], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2615, 0.0940],
         [0.1242, 0.1672]],

        [[0.7946, 0.0925],
         [0.8664, 0.7675]],

        [[0.0134, 0.1547],
         [0.9819, 0.0341]],

        [[0.7082, 0.1085],
         [0.9493, 0.0169]],

        [[0.0169, 0.5941],
         [0.9727, 0.8948]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 8
Adjusted Rand Index: 0.7027008103753108
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.08353341175333151
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0049666994303792225
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.217691978285087
Average Adjusted Rand Index: 0.33472266784774685
[0.05260067996529949, 0.217691978285087] [0.17563864194677348, 0.33472266784774685] [10868.5947265625, 10838.1533203125]
-------------------------------------
This iteration is 51
True Objective function: Loss = -10789.181228900363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43112.05859375
Iteration 100: Loss = -25663.96875
Iteration 200: Loss = -15314.64453125
Iteration 300: Loss = -11916.3369140625
Iteration 400: Loss = -11174.685546875
Iteration 500: Loss = -11004.7802734375
Iteration 600: Loss = -10930.2216796875
Iteration 700: Loss = -10902.2333984375
Iteration 800: Loss = -10893.416015625
Iteration 900: Loss = -10887.8212890625
Iteration 1000: Loss = -10878.787109375
Iteration 1100: Loss = -10875.5986328125
Iteration 1200: Loss = -10873.3349609375
Iteration 1300: Loss = -10871.548828125
Iteration 1400: Loss = -10870.0927734375
Iteration 1500: Loss = -10868.8759765625
Iteration 1600: Loss = -10867.84765625
Iteration 1700: Loss = -10866.9658203125
Iteration 1800: Loss = -10866.205078125
Iteration 1900: Loss = -10865.5390625
Iteration 2000: Loss = -10864.953125
Iteration 2100: Loss = -10864.4326171875
Iteration 2200: Loss = -10863.96875
Iteration 2300: Loss = -10863.5546875
Iteration 2400: Loss = -10863.1826171875
Iteration 2500: Loss = -10862.845703125
Iteration 2600: Loss = -10862.5400390625
Iteration 2700: Loss = -10862.2626953125
Iteration 2800: Loss = -10862.009765625
Iteration 2900: Loss = -10861.7783203125
Iteration 3000: Loss = -10861.5673828125
Iteration 3100: Loss = -10861.37109375
Iteration 3200: Loss = -10861.1904296875
Iteration 3300: Loss = -10861.025390625
Iteration 3400: Loss = -10860.87109375
Iteration 3500: Loss = -10860.7294921875
Iteration 3600: Loss = -10860.5966796875
Iteration 3700: Loss = -10860.4755859375
Iteration 3800: Loss = -10860.3623046875
Iteration 3900: Loss = -10860.2548828125
Iteration 4000: Loss = -10860.158203125
Iteration 4100: Loss = -10860.0654296875
Iteration 4200: Loss = -10859.978515625
Iteration 4300: Loss = -10859.8994140625
Iteration 4400: Loss = -10859.82421875
Iteration 4500: Loss = -10859.7529296875
Iteration 4600: Loss = -10859.6865234375
Iteration 4700: Loss = -10859.625
Iteration 4800: Loss = -10859.564453125
Iteration 4900: Loss = -10859.51171875
Iteration 5000: Loss = -10859.458984375
Iteration 5100: Loss = -10859.412109375
Iteration 5200: Loss = -10859.3642578125
Iteration 5300: Loss = -10859.3212890625
Iteration 5400: Loss = -10859.2802734375
Iteration 5500: Loss = -10859.244140625
Iteration 5600: Loss = -10859.20703125
Iteration 5700: Loss = -10859.173828125
Iteration 5800: Loss = -10859.1396484375
Iteration 5900: Loss = -10859.109375
Iteration 6000: Loss = -10859.080078125
Iteration 6100: Loss = -10859.0517578125
Iteration 6200: Loss = -10859.0283203125
Iteration 6300: Loss = -10859.0048828125
Iteration 6400: Loss = -10858.982421875
Iteration 6500: Loss = -10858.958984375
Iteration 6600: Loss = -10858.9384765625
Iteration 6700: Loss = -10858.9189453125
Iteration 6800: Loss = -10858.900390625
Iteration 6900: Loss = -10858.884765625
Iteration 7000: Loss = -10858.8681640625
Iteration 7100: Loss = -10858.8515625
Iteration 7200: Loss = -10858.8369140625
Iteration 7300: Loss = -10858.82421875
Iteration 7400: Loss = -10858.810546875
Iteration 7500: Loss = -10858.7978515625
Iteration 7600: Loss = -10858.78515625
Iteration 7700: Loss = -10858.7744140625
Iteration 7800: Loss = -10858.763671875
Iteration 7900: Loss = -10858.75390625
Iteration 8000: Loss = -10858.7451171875
Iteration 8100: Loss = -10858.7333984375
Iteration 8200: Loss = -10858.7265625
Iteration 8300: Loss = -10858.7177734375
Iteration 8400: Loss = -10858.708984375
Iteration 8500: Loss = -10858.703125
Iteration 8600: Loss = -10858.6953125
Iteration 8700: Loss = -10858.6884765625
Iteration 8800: Loss = -10858.6826171875
Iteration 8900: Loss = -10858.6767578125
Iteration 9000: Loss = -10858.671875
Iteration 9100: Loss = -10858.666015625
Iteration 9200: Loss = -10858.6611328125
Iteration 9300: Loss = -10858.6552734375
Iteration 9400: Loss = -10858.65234375
Iteration 9500: Loss = -10858.6474609375
Iteration 9600: Loss = -10858.6416015625
Iteration 9700: Loss = -10858.638671875
Iteration 9800: Loss = -10858.634765625
Iteration 9900: Loss = -10858.630859375
Iteration 10000: Loss = -10858.62890625
Iteration 10100: Loss = -10858.6259765625
Iteration 10200: Loss = -10858.623046875
Iteration 10300: Loss = -10858.619140625
Iteration 10400: Loss = -10858.615234375
Iteration 10500: Loss = -10858.61328125
Iteration 10600: Loss = -10858.609375
Iteration 10700: Loss = -10858.6083984375
Iteration 10800: Loss = -10858.603515625
Iteration 10900: Loss = -10858.6025390625
Iteration 11000: Loss = -10858.5986328125
Iteration 11100: Loss = -10858.5947265625
Iteration 11200: Loss = -10858.5869140625
Iteration 11300: Loss = -10858.5751953125
Iteration 11400: Loss = -10858.53125
Iteration 11500: Loss = -10858.4599609375
Iteration 11600: Loss = -10858.4189453125
Iteration 11700: Loss = -10858.3955078125
Iteration 11800: Loss = -10858.380859375
Iteration 11900: Loss = -10858.369140625
Iteration 12000: Loss = -10858.3603515625
Iteration 12100: Loss = -10858.3515625
Iteration 12200: Loss = -10858.3466796875
Iteration 12300: Loss = -10858.341796875
Iteration 12400: Loss = -10858.3359375
Iteration 12500: Loss = -10858.3330078125
Iteration 12600: Loss = -10858.328125
Iteration 12700: Loss = -10858.32421875
Iteration 12800: Loss = -10858.31640625
Iteration 12900: Loss = -10858.2978515625
Iteration 13000: Loss = -10858.2177734375
Iteration 13100: Loss = -10858.1572265625
Iteration 13200: Loss = -10858.1279296875
Iteration 13300: Loss = -10858.1083984375
Iteration 13400: Loss = -10858.0927734375
Iteration 13500: Loss = -10858.078125
Iteration 13600: Loss = -10858.0634765625
Iteration 13700: Loss = -10858.0458984375
Iteration 13800: Loss = -10858.03125
Iteration 13900: Loss = -10858.01171875
Iteration 14000: Loss = -10857.9873046875
Iteration 14100: Loss = -10857.962890625
Iteration 14200: Loss = -10857.931640625
Iteration 14300: Loss = -10857.89453125
Iteration 14400: Loss = -10857.845703125
Iteration 14500: Loss = -10857.78125
Iteration 14600: Loss = -10857.697265625
Iteration 14700: Loss = -10857.58203125
Iteration 14800: Loss = -10857.431640625
Iteration 14900: Loss = -10857.2646484375
Iteration 15000: Loss = -10857.12890625
Iteration 15100: Loss = -10857.05859375
Iteration 15200: Loss = -10857.02734375
Iteration 15300: Loss = -10856.998046875
Iteration 15400: Loss = -10856.599609375
Iteration 15500: Loss = -10856.572265625
Iteration 15600: Loss = -10856.5615234375
Iteration 15700: Loss = -10856.556640625
Iteration 15800: Loss = -10856.552734375
Iteration 15900: Loss = -10856.548828125
Iteration 16000: Loss = -10856.544921875
Iteration 16100: Loss = -10856.5439453125
Iteration 16200: Loss = -10856.541015625
Iteration 16300: Loss = -10856.541015625
Iteration 16400: Loss = -10856.537109375
Iteration 16500: Loss = -10856.53515625
Iteration 16600: Loss = -10856.5322265625
Iteration 16700: Loss = -10856.53125
Iteration 16800: Loss = -10856.5302734375
Iteration 16900: Loss = -10856.5322265625
1
Iteration 17000: Loss = -10856.5302734375
Iteration 17100: Loss = -10856.53125
1
Iteration 17200: Loss = -10856.5302734375
Iteration 17300: Loss = -10856.5302734375
Iteration 17400: Loss = -10856.529296875
Iteration 17500: Loss = -10856.529296875
Iteration 17600: Loss = -10856.529296875
Iteration 17700: Loss = -10856.5283203125
Iteration 17800: Loss = -10856.5283203125
Iteration 17900: Loss = -10856.5283203125
Iteration 18000: Loss = -10856.529296875
1
Iteration 18100: Loss = -10856.5302734375
2
Iteration 18200: Loss = -10856.5283203125
Iteration 18300: Loss = -10856.5263671875
Iteration 18400: Loss = -10856.5263671875
Iteration 18500: Loss = -10856.5263671875
Iteration 18600: Loss = -10856.5234375
Iteration 18700: Loss = -10856.5146484375
Iteration 18800: Loss = -10856.494140625
Iteration 18900: Loss = -10856.4296875
Iteration 19000: Loss = -10856.423828125
Iteration 19100: Loss = -10856.423828125
Iteration 19200: Loss = -10856.423828125
Iteration 19300: Loss = -10856.4248046875
1
Iteration 19400: Loss = -10856.4228515625
Iteration 19500: Loss = -10856.4228515625
Iteration 19600: Loss = -10856.423828125
1
Iteration 19700: Loss = -10856.423828125
2
Iteration 19800: Loss = -10856.423828125
3
Iteration 19900: Loss = -10856.4228515625
Iteration 20000: Loss = -10856.423828125
1
Iteration 20100: Loss = -10856.423828125
2
Iteration 20200: Loss = -10856.4248046875
3
Iteration 20300: Loss = -10856.4228515625
Iteration 20400: Loss = -10856.423828125
1
Iteration 20500: Loss = -10856.423828125
2
Iteration 20600: Loss = -10856.423828125
3
Iteration 20700: Loss = -10856.4228515625
Iteration 20800: Loss = -10856.4228515625
Iteration 20900: Loss = -10856.4228515625
Iteration 21000: Loss = -10856.423828125
1
Iteration 21100: Loss = -10856.423828125
2
Iteration 21200: Loss = -10856.4228515625
Iteration 21300: Loss = -10856.423828125
1
Iteration 21400: Loss = -10856.4228515625
Iteration 21500: Loss = -10856.423828125
1
Iteration 21600: Loss = -10856.4228515625
Iteration 21700: Loss = -10856.4228515625
Iteration 21800: Loss = -10856.423828125
1
Iteration 21900: Loss = -10856.423828125
2
Iteration 22000: Loss = -10856.421875
Iteration 22100: Loss = -10856.4248046875
1
Iteration 22200: Loss = -10856.421875
Iteration 22300: Loss = -10856.4228515625
1
Iteration 22400: Loss = -10856.4228515625
2
Iteration 22500: Loss = -10856.423828125
3
Iteration 22600: Loss = -10856.423828125
4
Iteration 22700: Loss = -10856.4267578125
5
Iteration 22800: Loss = -10856.423828125
6
Iteration 22900: Loss = -10856.421875
Iteration 23000: Loss = -10856.423828125
1
Iteration 23100: Loss = -10856.4228515625
2
Iteration 23200: Loss = -10856.421875
Iteration 23300: Loss = -10856.423828125
1
Iteration 23400: Loss = -10856.423828125
2
Iteration 23500: Loss = -10856.4228515625
3
Iteration 23600: Loss = -10856.421875
Iteration 23700: Loss = -10856.4248046875
1
Iteration 23800: Loss = -10856.423828125
2
Iteration 23900: Loss = -10856.4267578125
3
Iteration 24000: Loss = -10856.421875
Iteration 24100: Loss = -10856.423828125
1
Iteration 24200: Loss = -10856.423828125
2
Iteration 24300: Loss = -10856.4248046875
3
Iteration 24400: Loss = -10856.423828125
4
Iteration 24500: Loss = -10856.4248046875
5
Iteration 24600: Loss = -10856.423828125
6
Iteration 24700: Loss = -10856.423828125
7
Iteration 24800: Loss = -10856.4228515625
8
Iteration 24900: Loss = -10856.423828125
9
Iteration 25000: Loss = -10856.423828125
10
Iteration 25100: Loss = -10856.423828125
11
Iteration 25200: Loss = -10856.423828125
12
Iteration 25300: Loss = -10856.423828125
13
Iteration 25400: Loss = -10856.423828125
14
Iteration 25500: Loss = -10856.4228515625
15
Stopping early at iteration 25500 due to no improvement.
pi: tensor([[0.3201, 0.6799],
        [0.0098, 0.9902]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0356, 0.9644], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1241, 0.1521],
         [0.8664, 0.1612]],

        [[0.0141, 0.0719],
         [0.1835, 0.7449]],

        [[0.0752, 0.0883],
         [0.9001, 0.1508]],

        [[0.0091, 0.0733],
         [0.4999, 0.4438]],

        [[0.8510, 0.1848],
         [0.9821, 0.3516]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.009987515605493134
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0014080694472750825
Average Adjusted Rand Index: 0.0029570740920797055
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25403.62109375
Iteration 100: Loss = -15910.197265625
Iteration 200: Loss = -12188.3447265625
Iteration 300: Loss = -11401.005859375
Iteration 400: Loss = -11172.0400390625
Iteration 500: Loss = -11079.6044921875
Iteration 600: Loss = -11040.349609375
Iteration 700: Loss = -11017.7001953125
Iteration 800: Loss = -10999.9150390625
Iteration 900: Loss = -10985.6982421875
Iteration 1000: Loss = -10972.3203125
Iteration 1100: Loss = -10961.2333984375
Iteration 1200: Loss = -10950.1064453125
Iteration 1300: Loss = -10940.7109375
Iteration 1400: Loss = -10933.7099609375
Iteration 1500: Loss = -10925.9599609375
Iteration 1600: Loss = -10921.65625
Iteration 1700: Loss = -10916.5185546875
Iteration 1800: Loss = -10910.3271484375
Iteration 1900: Loss = -10900.1533203125
Iteration 2000: Loss = -10896.7216796875
Iteration 2100: Loss = -10889.6826171875
Iteration 2200: Loss = -10884.548828125
Iteration 2300: Loss = -10882.396484375
Iteration 2400: Loss = -10878.9306640625
Iteration 2500: Loss = -10873.83984375
Iteration 2600: Loss = -10870.1630859375
Iteration 2700: Loss = -10868.880859375
Iteration 2800: Loss = -10868.078125
Iteration 2900: Loss = -10867.4853515625
Iteration 3000: Loss = -10867.01953125
Iteration 3100: Loss = -10866.6298828125
Iteration 3200: Loss = -10866.09765625
Iteration 3300: Loss = -10862.29296875
Iteration 3400: Loss = -10861.892578125
Iteration 3500: Loss = -10861.5712890625
Iteration 3600: Loss = -10861.2939453125
Iteration 3700: Loss = -10861.052734375
Iteration 3800: Loss = -10860.8388671875
Iteration 3900: Loss = -10860.6484375
Iteration 4000: Loss = -10860.48046875
Iteration 4100: Loss = -10860.33203125
Iteration 4200: Loss = -10860.19921875
Iteration 4300: Loss = -10860.0810546875
Iteration 4400: Loss = -10859.9736328125
Iteration 4500: Loss = -10859.8759765625
Iteration 4600: Loss = -10859.7890625
Iteration 4700: Loss = -10859.705078125
Iteration 4800: Loss = -10859.630859375
Iteration 4900: Loss = -10859.5595703125
Iteration 5000: Loss = -10859.4951171875
Iteration 5100: Loss = -10859.43359375
Iteration 5200: Loss = -10859.376953125
Iteration 5300: Loss = -10859.322265625
Iteration 5400: Loss = -10859.2705078125
Iteration 5500: Loss = -10859.21875
Iteration 5600: Loss = -10859.1669921875
Iteration 5700: Loss = -10859.111328125
Iteration 5800: Loss = -10859.052734375
Iteration 5900: Loss = -10858.97265625
Iteration 6000: Loss = -10858.8623046875
Iteration 6100: Loss = -10858.7099609375
Iteration 6200: Loss = -10858.4619140625
Iteration 6300: Loss = -10858.0185546875
Iteration 6400: Loss = -10857.7119140625
Iteration 6500: Loss = -10857.5361328125
Iteration 6600: Loss = -10857.408203125
Iteration 6700: Loss = -10857.3154296875
Iteration 6800: Loss = -10857.2470703125
Iteration 6900: Loss = -10857.1904296875
Iteration 7000: Loss = -10857.142578125
Iteration 7100: Loss = -10857.1025390625
Iteration 7200: Loss = -10857.0654296875
Iteration 7300: Loss = -10857.0361328125
Iteration 7400: Loss = -10857.0087890625
Iteration 7500: Loss = -10856.984375
Iteration 7600: Loss = -10856.9638671875
Iteration 7700: Loss = -10856.9443359375
Iteration 7800: Loss = -10856.92578125
Iteration 7900: Loss = -10856.908203125
Iteration 8000: Loss = -10856.8916015625
Iteration 8100: Loss = -10856.87890625
Iteration 8200: Loss = -10856.8642578125
Iteration 8300: Loss = -10856.8515625
Iteration 8400: Loss = -10856.83984375
Iteration 8500: Loss = -10856.8291015625
Iteration 8600: Loss = -10856.818359375
Iteration 8700: Loss = -10856.8095703125
Iteration 8800: Loss = -10856.7998046875
Iteration 8900: Loss = -10856.79296875
Iteration 9000: Loss = -10856.78515625
Iteration 9100: Loss = -10856.77734375
Iteration 9200: Loss = -10856.771484375
Iteration 9300: Loss = -10856.7646484375
Iteration 9400: Loss = -10856.759765625
Iteration 9500: Loss = -10856.7529296875
Iteration 9600: Loss = -10856.7490234375
Iteration 9700: Loss = -10856.7451171875
Iteration 9800: Loss = -10856.7412109375
Iteration 9900: Loss = -10856.736328125
Iteration 10000: Loss = -10856.732421875
Iteration 10100: Loss = -10856.7275390625
Iteration 10200: Loss = -10856.71875
Iteration 10300: Loss = -10856.7138671875
Iteration 10400: Loss = -10856.7099609375
Iteration 10500: Loss = -10856.70703125
Iteration 10600: Loss = -10856.7021484375
Iteration 10700: Loss = -10856.7001953125
Iteration 10800: Loss = -10856.6982421875
Iteration 10900: Loss = -10856.6962890625
Iteration 11000: Loss = -10856.693359375
Iteration 11100: Loss = -10856.6923828125
Iteration 11200: Loss = -10856.6904296875
Iteration 11300: Loss = -10856.6875
Iteration 11400: Loss = -10856.6875
Iteration 11500: Loss = -10856.6845703125
Iteration 11600: Loss = -10856.68359375
Iteration 11700: Loss = -10856.6826171875
Iteration 11800: Loss = -10856.6796875
Iteration 11900: Loss = -10856.677734375
Iteration 12000: Loss = -10856.677734375
Iteration 12100: Loss = -10856.67578125
Iteration 12200: Loss = -10856.6767578125
1
Iteration 12300: Loss = -10856.673828125
Iteration 12400: Loss = -10856.6728515625
Iteration 12500: Loss = -10856.6728515625
Iteration 12600: Loss = -10856.669921875
Iteration 12700: Loss = -10856.669921875
Iteration 12800: Loss = -10856.669921875
Iteration 12900: Loss = -10856.66796875
Iteration 13000: Loss = -10856.669921875
1
Iteration 13100: Loss = -10856.6669921875
Iteration 13200: Loss = -10856.6669921875
Iteration 13300: Loss = -10856.666015625
Iteration 13400: Loss = -10856.6650390625
Iteration 13500: Loss = -10856.666015625
1
Iteration 13600: Loss = -10856.6650390625
Iteration 13700: Loss = -10856.6630859375
Iteration 13800: Loss = -10856.662109375
Iteration 13900: Loss = -10856.6630859375
1
Iteration 14000: Loss = -10856.6630859375
2
Iteration 14100: Loss = -10856.662109375
Iteration 14200: Loss = -10856.6611328125
Iteration 14300: Loss = -10856.6611328125
Iteration 14400: Loss = -10856.6611328125
Iteration 14500: Loss = -10856.66015625
Iteration 14600: Loss = -10856.6611328125
1
Iteration 14700: Loss = -10856.66015625
Iteration 14800: Loss = -10856.66015625
Iteration 14900: Loss = -10856.6591796875
Iteration 15000: Loss = -10856.6591796875
Iteration 15100: Loss = -10856.6591796875
Iteration 15200: Loss = -10856.658203125
Iteration 15300: Loss = -10856.658203125
Iteration 15400: Loss = -10856.658203125
Iteration 15500: Loss = -10856.658203125
Iteration 15600: Loss = -10856.658203125
Iteration 15700: Loss = -10856.6572265625
Iteration 15800: Loss = -10856.65625
Iteration 15900: Loss = -10856.65625
Iteration 16000: Loss = -10856.6572265625
1
Iteration 16100: Loss = -10856.6572265625
2
Iteration 16200: Loss = -10856.6552734375
Iteration 16300: Loss = -10856.65625
1
Iteration 16400: Loss = -10856.65625
2
Iteration 16500: Loss = -10856.65625
3
Iteration 16600: Loss = -10856.65625
4
Iteration 16700: Loss = -10856.65625
5
Iteration 16800: Loss = -10856.6552734375
Iteration 16900: Loss = -10856.654296875
Iteration 17000: Loss = -10856.654296875
Iteration 17100: Loss = -10856.6552734375
1
Iteration 17200: Loss = -10856.65625
2
Iteration 17300: Loss = -10856.6552734375
3
Iteration 17400: Loss = -10856.654296875
Iteration 17500: Loss = -10856.654296875
Iteration 17600: Loss = -10856.654296875
Iteration 17700: Loss = -10856.654296875
Iteration 17800: Loss = -10856.6552734375
1
Iteration 17900: Loss = -10856.6552734375
2
Iteration 18000: Loss = -10856.6552734375
3
Iteration 18100: Loss = -10856.6552734375
4
Iteration 18200: Loss = -10856.654296875
Iteration 18300: Loss = -10856.6552734375
1
Iteration 18400: Loss = -10856.6552734375
2
Iteration 18500: Loss = -10856.654296875
Iteration 18600: Loss = -10856.6572265625
1
Iteration 18700: Loss = -10856.654296875
Iteration 18800: Loss = -10856.654296875
Iteration 18900: Loss = -10856.6533203125
Iteration 19000: Loss = -10856.654296875
1
Iteration 19100: Loss = -10856.654296875
2
Iteration 19200: Loss = -10856.65234375
Iteration 19300: Loss = -10856.654296875
1
Iteration 19400: Loss = -10856.6533203125
2
Iteration 19500: Loss = -10856.6552734375
3
Iteration 19600: Loss = -10856.654296875
4
Iteration 19700: Loss = -10856.65234375
Iteration 19800: Loss = -10856.654296875
1
Iteration 19900: Loss = -10856.654296875
2
Iteration 20000: Loss = -10856.6533203125
3
Iteration 20100: Loss = -10856.654296875
4
Iteration 20200: Loss = -10856.65234375
Iteration 20300: Loss = -10856.654296875
1
Iteration 20400: Loss = -10856.6533203125
2
Iteration 20500: Loss = -10856.65234375
Iteration 20600: Loss = -10856.65234375
Iteration 20700: Loss = -10856.65234375
Iteration 20800: Loss = -10856.65234375
Iteration 20900: Loss = -10856.65234375
Iteration 21000: Loss = -10856.6533203125
1
Iteration 21100: Loss = -10856.6533203125
2
Iteration 21200: Loss = -10856.65234375
Iteration 21300: Loss = -10856.6533203125
1
Iteration 21400: Loss = -10856.6552734375
2
Iteration 21500: Loss = -10856.654296875
3
Iteration 21600: Loss = -10856.65234375
Iteration 21700: Loss = -10856.6533203125
1
Iteration 21800: Loss = -10856.6533203125
2
Iteration 21900: Loss = -10856.6533203125
3
Iteration 22000: Loss = -10856.65234375
Iteration 22100: Loss = -10856.65234375
Iteration 22200: Loss = -10856.65234375
Iteration 22300: Loss = -10856.6533203125
1
Iteration 22400: Loss = -10856.6533203125
2
Iteration 22500: Loss = -10856.6533203125
3
Iteration 22600: Loss = -10856.65234375
Iteration 22700: Loss = -10856.6552734375
1
Iteration 22800: Loss = -10856.65234375
Iteration 22900: Loss = -10856.65234375
Iteration 23000: Loss = -10856.654296875
1
Iteration 23100: Loss = -10856.6533203125
2
Iteration 23200: Loss = -10856.6533203125
3
Iteration 23300: Loss = -10856.65234375
Iteration 23400: Loss = -10856.654296875
1
Iteration 23500: Loss = -10856.654296875
2
Iteration 23600: Loss = -10856.654296875
3
Iteration 23700: Loss = -10856.654296875
4
Iteration 23800: Loss = -10856.654296875
5
Iteration 23900: Loss = -10856.654296875
6
Iteration 24000: Loss = -10856.65234375
Iteration 24100: Loss = -10856.65234375
Iteration 24200: Loss = -10856.6533203125
1
Iteration 24300: Loss = -10856.6533203125
2
Iteration 24400: Loss = -10856.6533203125
3
Iteration 24500: Loss = -10856.65234375
Iteration 24600: Loss = -10856.6533203125
1
Iteration 24700: Loss = -10856.6533203125
2
Iteration 24800: Loss = -10856.65234375
Iteration 24900: Loss = -10856.65234375
Iteration 25000: Loss = -10856.6533203125
1
Iteration 25100: Loss = -10856.65234375
Iteration 25200: Loss = -10856.6533203125
1
Iteration 25300: Loss = -10856.654296875
2
Iteration 25400: Loss = -10856.6533203125
3
Iteration 25500: Loss = -10856.654296875
4
Iteration 25600: Loss = -10856.6533203125
5
Iteration 25700: Loss = -10856.654296875
6
Iteration 25800: Loss = -10856.654296875
7
Iteration 25900: Loss = -10856.65234375
Iteration 26000: Loss = -10856.65234375
Iteration 26100: Loss = -10856.65234375
Iteration 26200: Loss = -10856.654296875
1
Iteration 26300: Loss = -10856.6533203125
2
Iteration 26400: Loss = -10856.65234375
Iteration 26500: Loss = -10856.65234375
Iteration 26600: Loss = -10856.654296875
1
Iteration 26700: Loss = -10856.654296875
2
Iteration 26800: Loss = -10856.654296875
3
Iteration 26900: Loss = -10856.6533203125
4
Iteration 27000: Loss = -10856.654296875
5
Iteration 27100: Loss = -10856.654296875
6
Iteration 27200: Loss = -10856.6533203125
7
Iteration 27300: Loss = -10856.6533203125
8
Iteration 27400: Loss = -10856.654296875
9
Iteration 27500: Loss = -10856.6552734375
10
Iteration 27600: Loss = -10856.6513671875
Iteration 27700: Loss = -10856.65234375
1
Iteration 27800: Loss = -10856.65234375
2
Iteration 27900: Loss = -10856.6513671875
Iteration 28000: Loss = -10856.65234375
1
Iteration 28100: Loss = -10856.6533203125
2
Iteration 28200: Loss = -10856.6513671875
Iteration 28300: Loss = -10856.6513671875
Iteration 28400: Loss = -10856.65234375
1
Iteration 28500: Loss = -10856.6474609375
Iteration 28600: Loss = -10856.47265625
Iteration 28700: Loss = -10856.4716796875
Iteration 28800: Loss = -10856.4736328125
1
Iteration 28900: Loss = -10856.470703125
Iteration 29000: Loss = -10856.4716796875
1
Iteration 29100: Loss = -10856.4697265625
Iteration 29200: Loss = -10856.4638671875
Iteration 29300: Loss = -10856.46484375
1
Iteration 29400: Loss = -10856.4638671875
Iteration 29500: Loss = -10856.4619140625
Iteration 29600: Loss = -10856.4619140625
Iteration 29700: Loss = -10856.462890625
1
Iteration 29800: Loss = -10856.4638671875
2
Iteration 29900: Loss = -10856.462890625
3
pi: tensor([[1.0000e+00, 7.7389e-07],
        [4.8599e-01, 5.1401e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9623, 0.0377], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1605, 0.1563],
         [0.0227, 0.1343]],

        [[0.0655, 0.0703],
         [0.0417, 0.0306]],

        [[0.0412, 0.0735],
         [0.7797, 0.3198]],

        [[0.5830, 0.1613],
         [0.9645, 0.9754]],

        [[0.2206, 0.1512],
         [0.0342, 0.9801]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.00035341041046094813
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.009987515605493134
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00212981194416214
Average Adjusted Rand Index: 0.00289702026617956
[0.0014080694472750825, 0.00212981194416214] [0.0029570740920797055, 0.00289702026617956] [10856.4228515625, 10856.4638671875]
-------------------------------------
This iteration is 52
True Objective function: Loss = -10918.04078861485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42460.32421875
Iteration 100: Loss = -28811.96875
Iteration 200: Loss = -19888.697265625
Iteration 300: Loss = -14910.728515625
Iteration 400: Loss = -12631.798828125
Iteration 500: Loss = -11675.77734375
Iteration 600: Loss = -11302.4736328125
Iteration 700: Loss = -11157.0556640625
Iteration 800: Loss = -11094.623046875
Iteration 900: Loss = -11069.552734375
Iteration 1000: Loss = -11054.6826171875
Iteration 1100: Loss = -11044.431640625
Iteration 1200: Loss = -11037.0771484375
Iteration 1300: Loss = -11024.9833984375
Iteration 1400: Loss = -11008.1474609375
Iteration 1500: Loss = -10996.14453125
Iteration 1600: Loss = -10991.7060546875
Iteration 1700: Loss = -10988.4052734375
Iteration 1800: Loss = -10982.8564453125
Iteration 1900: Loss = -10978.15625
Iteration 2000: Loss = -10975.2353515625
Iteration 2100: Loss = -10971.5859375
Iteration 2200: Loss = -10968.3466796875
Iteration 2300: Loss = -10964.48046875
Iteration 2400: Loss = -10960.248046875
Iteration 2500: Loss = -10956.92578125
Iteration 2600: Loss = -10953.4443359375
Iteration 2700: Loss = -10952.2197265625
Iteration 2800: Loss = -10951.3271484375
Iteration 2900: Loss = -10950.2177734375
Iteration 3000: Loss = -10948.91015625
Iteration 3100: Loss = -10945.0771484375
Iteration 3200: Loss = -10939.7255859375
Iteration 3300: Loss = -10936.1669921875
Iteration 3400: Loss = -10934.763671875
Iteration 3500: Loss = -10930.890625
Iteration 3600: Loss = -10930.3759765625
Iteration 3700: Loss = -10928.587890625
Iteration 3800: Loss = -10927.255859375
Iteration 3900: Loss = -10926.7646484375
Iteration 4000: Loss = -10926.5478515625
Iteration 4100: Loss = -10926.3681640625
Iteration 4200: Loss = -10926.1953125
Iteration 4300: Loss = -10926.0986328125
Iteration 4400: Loss = -10926.0517578125
Iteration 4500: Loss = -10926.0224609375
Iteration 4600: Loss = -10926.0009765625
Iteration 4700: Loss = -10925.9833984375
Iteration 4800: Loss = -10925.96875
Iteration 4900: Loss = -10925.953125
Iteration 5000: Loss = -10925.935546875
Iteration 5100: Loss = -10925.9248046875
Iteration 5200: Loss = -10925.9150390625
Iteration 5300: Loss = -10925.9052734375
Iteration 5400: Loss = -10925.8896484375
Iteration 5500: Loss = -10925.87890625
Iteration 5600: Loss = -10925.859375
Iteration 5700: Loss = -10925.837890625
Iteration 5800: Loss = -10925.828125
Iteration 5900: Loss = -10925.8212890625
Iteration 6000: Loss = -10925.81640625
Iteration 6100: Loss = -10925.810546875
Iteration 6200: Loss = -10925.8056640625
Iteration 6300: Loss = -10925.80078125
Iteration 6400: Loss = -10925.794921875
Iteration 6500: Loss = -10925.7900390625
Iteration 6600: Loss = -10925.7841796875
Iteration 6700: Loss = -10925.771484375
Iteration 6800: Loss = -10925.435546875
Iteration 6900: Loss = -10925.361328125
Iteration 7000: Loss = -10925.357421875
Iteration 7100: Loss = -10925.353515625
Iteration 7200: Loss = -10925.3505859375
Iteration 7300: Loss = -10925.341796875
Iteration 7400: Loss = -10925.318359375
Iteration 7500: Loss = -10925.3154296875
Iteration 7600: Loss = -10925.3125
Iteration 7700: Loss = -10925.3095703125
Iteration 7800: Loss = -10925.306640625
Iteration 7900: Loss = -10925.3037109375
Iteration 8000: Loss = -10925.2958984375
Iteration 8100: Loss = -10925.255859375
Iteration 8200: Loss = -10925.1806640625
Iteration 8300: Loss = -10925.1611328125
Iteration 8400: Loss = -10925.083984375
Iteration 8500: Loss = -10925.08203125
Iteration 8600: Loss = -10925.0810546875
Iteration 8700: Loss = -10925.0810546875
Iteration 8800: Loss = -10925.080078125
Iteration 8900: Loss = -10925.0791015625
Iteration 9000: Loss = -10925.0771484375
Iteration 9100: Loss = -10925.0751953125
Iteration 9200: Loss = -10923.830078125
Iteration 9300: Loss = -10923.8193359375
Iteration 9400: Loss = -10923.810546875
Iteration 9500: Loss = -10923.599609375
Iteration 9600: Loss = -10923.5986328125
Iteration 9700: Loss = -10923.59765625
Iteration 9800: Loss = -10923.59765625
Iteration 9900: Loss = -10923.59765625
Iteration 10000: Loss = -10923.59765625
Iteration 10100: Loss = -10923.595703125
Iteration 10200: Loss = -10923.595703125
Iteration 10300: Loss = -10923.5927734375
Iteration 10400: Loss = -10923.58984375
Iteration 10500: Loss = -10923.587890625
Iteration 10600: Loss = -10923.587890625
Iteration 10700: Loss = -10923.5869140625
Iteration 10800: Loss = -10923.5849609375
Iteration 10900: Loss = -10923.5849609375
Iteration 11000: Loss = -10923.5859375
1
Iteration 11100: Loss = -10923.5859375
2
Iteration 11200: Loss = -10923.583984375
Iteration 11300: Loss = -10923.5849609375
1
Iteration 11400: Loss = -10923.58203125
Iteration 11500: Loss = -10923.5810546875
Iteration 11600: Loss = -10923.58203125
1
Iteration 11700: Loss = -10923.5810546875
Iteration 11800: Loss = -10923.5810546875
Iteration 11900: Loss = -10923.5810546875
Iteration 12000: Loss = -10923.5810546875
Iteration 12100: Loss = -10923.576171875
Iteration 12200: Loss = -10923.5771484375
1
Iteration 12300: Loss = -10923.576171875
Iteration 12400: Loss = -10923.57421875
Iteration 12500: Loss = -10923.5751953125
1
Iteration 12600: Loss = -10923.560546875
Iteration 12700: Loss = -10923.560546875
Iteration 12800: Loss = -10923.5595703125
Iteration 12900: Loss = -10923.55859375
Iteration 13000: Loss = -10923.560546875
1
Iteration 13100: Loss = -10923.5595703125
2
Iteration 13200: Loss = -10923.5595703125
3
Iteration 13300: Loss = -10923.560546875
4
Iteration 13400: Loss = -10922.9599609375
Iteration 13500: Loss = -10922.9501953125
Iteration 13600: Loss = -10922.9482421875
Iteration 13700: Loss = -10922.9501953125
1
Iteration 13800: Loss = -10922.94921875
2
Iteration 13900: Loss = -10922.9501953125
3
Iteration 14000: Loss = -10922.94921875
4
Iteration 14100: Loss = -10922.94921875
5
Iteration 14200: Loss = -10922.94921875
6
Iteration 14300: Loss = -10922.94921875
7
Iteration 14400: Loss = -10922.9482421875
Iteration 14500: Loss = -10922.9482421875
Iteration 14600: Loss = -10922.9482421875
Iteration 14700: Loss = -10922.94921875
1
Iteration 14800: Loss = -10922.94921875
2
Iteration 14900: Loss = -10922.9482421875
Iteration 15000: Loss = -10922.9482421875
Iteration 15100: Loss = -10922.94921875
1
Iteration 15200: Loss = -10922.947265625
Iteration 15300: Loss = -10922.9482421875
1
Iteration 15400: Loss = -10922.9482421875
2
Iteration 15500: Loss = -10922.947265625
Iteration 15600: Loss = -10922.9482421875
1
Iteration 15700: Loss = -10922.947265625
Iteration 15800: Loss = -10922.947265625
Iteration 15900: Loss = -10922.947265625
Iteration 16000: Loss = -10922.947265625
Iteration 16100: Loss = -10922.9482421875
1
Iteration 16200: Loss = -10922.947265625
Iteration 16300: Loss = -10922.9482421875
1
Iteration 16400: Loss = -10922.9482421875
2
Iteration 16500: Loss = -10922.947265625
Iteration 16600: Loss = -10922.9345703125
Iteration 16700: Loss = -10922.9345703125
Iteration 16800: Loss = -10922.9345703125
Iteration 16900: Loss = -10922.8701171875
Iteration 17000: Loss = -10922.84765625
Iteration 17100: Loss = -10922.8310546875
Iteration 17200: Loss = -10922.8291015625
Iteration 17300: Loss = -10922.8310546875
1
Iteration 17400: Loss = -10922.828125
Iteration 17500: Loss = -10922.8291015625
1
Iteration 17600: Loss = -10922.8291015625
2
Iteration 17700: Loss = -10922.826171875
Iteration 17800: Loss = -10922.8271484375
1
Iteration 17900: Loss = -10922.826171875
Iteration 18000: Loss = -10922.8251953125
Iteration 18100: Loss = -10922.826171875
1
Iteration 18200: Loss = -10922.826171875
2
Iteration 18300: Loss = -10922.82421875
Iteration 18400: Loss = -10922.826171875
1
Iteration 18500: Loss = -10922.822265625
Iteration 18600: Loss = -10922.7958984375
Iteration 18700: Loss = -10922.79296875
Iteration 18800: Loss = -10922.79296875
Iteration 18900: Loss = -10922.79296875
Iteration 19000: Loss = -10922.79296875
Iteration 19100: Loss = -10922.7939453125
1
Iteration 19200: Loss = -10922.7919921875
Iteration 19300: Loss = -10922.7939453125
1
Iteration 19400: Loss = -10922.79296875
2
Iteration 19500: Loss = -10922.7939453125
3
Iteration 19600: Loss = -10922.7939453125
4
Iteration 19700: Loss = -10922.79296875
5
Iteration 19800: Loss = -10922.7939453125
6
Iteration 19900: Loss = -10922.79296875
7
Iteration 20000: Loss = -10922.7216796875
Iteration 20100: Loss = -10922.7216796875
Iteration 20200: Loss = -10922.72265625
1
Iteration 20300: Loss = -10922.7216796875
Iteration 20400: Loss = -10922.7216796875
Iteration 20500: Loss = -10922.720703125
Iteration 20600: Loss = -10922.72265625
1
Iteration 20700: Loss = -10922.720703125
Iteration 20800: Loss = -10922.7216796875
1
Iteration 20900: Loss = -10922.72265625
2
Iteration 21000: Loss = -10922.7216796875
3
Iteration 21100: Loss = -10922.7216796875
4
Iteration 21200: Loss = -10922.72265625
5
Iteration 21300: Loss = -10922.7216796875
6
Iteration 21400: Loss = -10922.7216796875
7
Iteration 21500: Loss = -10922.7216796875
8
Iteration 21600: Loss = -10922.7216796875
9
Iteration 21700: Loss = -10922.72265625
10
Iteration 21800: Loss = -10922.7216796875
11
Iteration 21900: Loss = -10922.72265625
12
Iteration 22000: Loss = -10922.72265625
13
Iteration 22100: Loss = -10922.7216796875
14
Iteration 22200: Loss = -10922.7216796875
15
Stopping early at iteration 22200 due to no improvement.
pi: tensor([[0.5990, 0.4010],
        [0.3440, 0.6560]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5094, 0.4906], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2362, 0.0985],
         [0.8839, 0.2210]],

        [[0.0456, 0.0923],
         [0.9424, 0.3578]],

        [[0.9414, 0.0955],
         [0.0224, 0.7952]],

        [[0.0115, 0.0987],
         [0.9880, 0.2292]],

        [[0.0539, 0.1086],
         [0.9649, 0.9850]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369439308410987
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 90
Adjusted Rand Index: 0.6363978263200962
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721463199647421
Global Adjusted Rand Index: 0.02114671802690587
Average Adjusted Rand Index: 0.760004402818634
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37003.3515625
Iteration 100: Loss = -21521.9765625
Iteration 200: Loss = -13166.748046875
Iteration 300: Loss = -11764.6162109375
Iteration 400: Loss = -11496.5634765625
Iteration 500: Loss = -11357.8876953125
Iteration 600: Loss = -11276.8017578125
Iteration 700: Loss = -11213.78125
Iteration 800: Loss = -11180.9736328125
Iteration 900: Loss = -11160.4248046875
Iteration 1000: Loss = -11143.482421875
Iteration 1100: Loss = -11120.5830078125
Iteration 1200: Loss = -11110.8974609375
Iteration 1300: Loss = -11100.857421875
Iteration 1400: Loss = -11092.51171875
Iteration 1500: Loss = -11086.68359375
Iteration 1600: Loss = -11082.3876953125
Iteration 1700: Loss = -11078.8193359375
Iteration 1800: Loss = -11075.203125
Iteration 1900: Loss = -11067.5546875
Iteration 2000: Loss = -11060.7763671875
Iteration 2100: Loss = -11057.6201171875
Iteration 2200: Loss = -11055.640625
Iteration 2300: Loss = -11054.2021484375
Iteration 2400: Loss = -11053.05859375
Iteration 2500: Loss = -11052.10546875
Iteration 2600: Loss = -11051.279296875
Iteration 2700: Loss = -11050.5537109375
Iteration 2800: Loss = -11049.9033203125
Iteration 2900: Loss = -11049.314453125
Iteration 3000: Loss = -11048.7763671875
Iteration 3100: Loss = -11048.271484375
Iteration 3200: Loss = -11047.7705078125
Iteration 3300: Loss = -11047.15625
Iteration 3400: Loss = -11046.0361328125
Iteration 3500: Loss = -11044.4697265625
Iteration 3600: Loss = -11043.4326171875
Iteration 3700: Loss = -11042.806640625
Iteration 3800: Loss = -11042.2431640625
Iteration 3900: Loss = -11037.6787109375
Iteration 4000: Loss = -11037.3115234375
Iteration 4100: Loss = -11037.0263671875
Iteration 4200: Loss = -11036.77734375
Iteration 4300: Loss = -11036.55078125
Iteration 4400: Loss = -11036.3427734375
Iteration 4500: Loss = -11036.1533203125
Iteration 4600: Loss = -11035.9775390625
Iteration 4700: Loss = -11035.818359375
Iteration 4800: Loss = -11035.6708984375
Iteration 4900: Loss = -11035.53515625
Iteration 5000: Loss = -11035.4091796875
Iteration 5100: Loss = -11035.2900390625
Iteration 5200: Loss = -11035.1767578125
Iteration 5300: Loss = -11035.064453125
Iteration 5400: Loss = -11034.5126953125
Iteration 5500: Loss = -11032.990234375
Iteration 5600: Loss = -11032.826171875
Iteration 5700: Loss = -11032.7177734375
Iteration 5800: Loss = -11032.6318359375
Iteration 5900: Loss = -11032.5576171875
Iteration 6000: Loss = -11032.490234375
Iteration 6100: Loss = -11032.4287109375
Iteration 6200: Loss = -11032.37109375
Iteration 6300: Loss = -11032.3173828125
Iteration 6400: Loss = -11032.267578125
Iteration 6500: Loss = -11032.2197265625
Iteration 6600: Loss = -11032.1728515625
Iteration 6700: Loss = -11032.130859375
Iteration 6800: Loss = -11032.0888671875
Iteration 6900: Loss = -11032.0498046875
Iteration 7000: Loss = -11032.01171875
Iteration 7100: Loss = -11031.97265625
Iteration 7200: Loss = -11031.9326171875
Iteration 7300: Loss = -11031.900390625
Iteration 7400: Loss = -11031.8681640625
Iteration 7500: Loss = -11031.837890625
Iteration 7600: Loss = -11031.8095703125
Iteration 7700: Loss = -11031.78125
Iteration 7800: Loss = -11031.7548828125
Iteration 7900: Loss = -11031.7314453125
Iteration 8000: Loss = -11031.70703125
Iteration 8100: Loss = -11031.6826171875
Iteration 8200: Loss = -11031.6591796875
Iteration 8300: Loss = -11031.63671875
Iteration 8400: Loss = -11031.6142578125
Iteration 8500: Loss = -11031.5908203125
Iteration 8600: Loss = -11031.56640625
Iteration 8700: Loss = -11028.7919921875
Iteration 8800: Loss = -11026.8662109375
Iteration 8900: Loss = -11026.7607421875
Iteration 9000: Loss = -11026.6953125
Iteration 9100: Loss = -11026.638671875
Iteration 9200: Loss = -11026.5673828125
Iteration 9300: Loss = -11026.4150390625
Iteration 9400: Loss = -11025.94140625
Iteration 9500: Loss = -11025.8359375
Iteration 9600: Loss = -11025.7880859375
Iteration 9700: Loss = -11025.7548828125
Iteration 9800: Loss = -11025.7255859375
Iteration 9900: Loss = -11025.705078125
Iteration 10000: Loss = -11025.685546875
Iteration 10100: Loss = -11025.669921875
Iteration 10200: Loss = -11025.6552734375
Iteration 10300: Loss = -11025.642578125
Iteration 10400: Loss = -11025.6318359375
Iteration 10500: Loss = -11025.62109375
Iteration 10600: Loss = -11025.611328125
Iteration 10700: Loss = -11025.603515625
Iteration 10800: Loss = -11025.59375
Iteration 10900: Loss = -11025.5859375
Iteration 11000: Loss = -11025.580078125
Iteration 11100: Loss = -11025.5751953125
Iteration 11200: Loss = -11025.5673828125
Iteration 11300: Loss = -11025.560546875
Iteration 11400: Loss = -11025.5556640625
Iteration 11500: Loss = -11025.5498046875
Iteration 11600: Loss = -11025.548828125
Iteration 11700: Loss = -11025.54296875
Iteration 11800: Loss = -11025.5361328125
Iteration 11900: Loss = -11025.5322265625
Iteration 12000: Loss = -11025.529296875
Iteration 12100: Loss = -11025.5244140625
Iteration 12200: Loss = -11025.5224609375
Iteration 12300: Loss = -11025.517578125
Iteration 12400: Loss = -11025.51171875
Iteration 12500: Loss = -11025.5087890625
Iteration 12600: Loss = -11025.505859375
Iteration 12700: Loss = -11025.50390625
Iteration 12800: Loss = -11025.5009765625
Iteration 12900: Loss = -11025.498046875
Iteration 13000: Loss = -11025.4931640625
Iteration 13100: Loss = -11025.4931640625
Iteration 13200: Loss = -11025.4912109375
Iteration 13300: Loss = -11025.486328125
Iteration 13400: Loss = -11025.484375
Iteration 13500: Loss = -11025.4814453125
Iteration 13600: Loss = -11025.4814453125
Iteration 13700: Loss = -11025.478515625
Iteration 13800: Loss = -11025.4765625
Iteration 13900: Loss = -11025.4755859375
Iteration 14000: Loss = -11025.4736328125
Iteration 14100: Loss = -11025.470703125
Iteration 14200: Loss = -11025.4677734375
Iteration 14300: Loss = -11025.4677734375
Iteration 14400: Loss = -11025.466796875
Iteration 14500: Loss = -11025.4658203125
Iteration 14600: Loss = -11025.4638671875
Iteration 14700: Loss = -11025.4619140625
Iteration 14800: Loss = -11025.4599609375
Iteration 14900: Loss = -11025.4580078125
Iteration 15000: Loss = -11025.4580078125
Iteration 15100: Loss = -11025.4541015625
Iteration 15200: Loss = -11025.451171875
Iteration 15300: Loss = -11025.44921875
Iteration 15400: Loss = -11025.4443359375
Iteration 15500: Loss = -11025.4404296875
Iteration 15600: Loss = -11025.4306640625
Iteration 15700: Loss = -11025.41796875
Iteration 15800: Loss = -11025.384765625
Iteration 15900: Loss = -11025.291015625
Iteration 16000: Loss = -11025.021484375
Iteration 16100: Loss = -11024.8544921875
Iteration 16200: Loss = -11024.791015625
Iteration 16300: Loss = -11024.7646484375
Iteration 16400: Loss = -11024.7431640625
Iteration 16500: Loss = -11024.740234375
Iteration 16600: Loss = -11024.7294921875
Iteration 16700: Loss = -11024.7080078125
Iteration 16800: Loss = -11024.6875
Iteration 16900: Loss = -11024.6845703125
Iteration 17000: Loss = -11024.685546875
1
Iteration 17100: Loss = -11024.68359375
Iteration 17200: Loss = -11024.6767578125
Iteration 17300: Loss = -11024.673828125
Iteration 17400: Loss = -11024.671875
Iteration 17500: Loss = -11024.6669921875
Iteration 17600: Loss = -11024.6669921875
Iteration 17700: Loss = -11024.6630859375
Iteration 17800: Loss = -11024.6640625
1
Iteration 17900: Loss = -11024.662109375
Iteration 18000: Loss = -11024.658203125
Iteration 18100: Loss = -11024.3974609375
Iteration 18200: Loss = -11024.3955078125
Iteration 18300: Loss = -11024.33984375
Iteration 18400: Loss = -11024.2978515625
Iteration 18500: Loss = -11024.29296875
Iteration 18600: Loss = -11024.2919921875
Iteration 18700: Loss = -11024.2900390625
Iteration 18800: Loss = -11024.2880859375
Iteration 18900: Loss = -11024.2880859375
Iteration 19000: Loss = -11024.2890625
1
Iteration 19100: Loss = -11024.2880859375
Iteration 19200: Loss = -11024.2880859375
Iteration 19300: Loss = -11024.287109375
Iteration 19400: Loss = -11024.2880859375
1
Iteration 19500: Loss = -11024.287109375
Iteration 19600: Loss = -11024.2880859375
1
Iteration 19700: Loss = -11024.287109375
Iteration 19800: Loss = -11024.2880859375
1
Iteration 19900: Loss = -11024.287109375
Iteration 20000: Loss = -11024.2861328125
Iteration 20100: Loss = -11024.2861328125
Iteration 20200: Loss = -11024.2861328125
Iteration 20300: Loss = -11024.2880859375
1
Iteration 20400: Loss = -11024.2861328125
Iteration 20500: Loss = -11024.2861328125
Iteration 20600: Loss = -11024.2861328125
Iteration 20700: Loss = -11024.28515625
Iteration 20800: Loss = -11024.2841796875
Iteration 20900: Loss = -11024.2841796875
Iteration 21000: Loss = -11024.287109375
1
Iteration 21100: Loss = -11024.28515625
2
Iteration 21200: Loss = -11024.28515625
3
Iteration 21300: Loss = -11024.28515625
4
Iteration 21400: Loss = -11024.283203125
Iteration 21500: Loss = -11024.28515625
1
Iteration 21600: Loss = -11024.28515625
2
Iteration 21700: Loss = -11024.2841796875
3
Iteration 21800: Loss = -11024.2861328125
4
Iteration 21900: Loss = -11024.28515625
5
Iteration 22000: Loss = -11024.28515625
6
Iteration 22100: Loss = -11024.26171875
Iteration 22200: Loss = -11024.24609375
Iteration 22300: Loss = -11024.24609375
Iteration 22400: Loss = -11024.2353515625
Iteration 22500: Loss = -11024.228515625
Iteration 22600: Loss = -11024.2236328125
Iteration 22700: Loss = -11024.2236328125
Iteration 22800: Loss = -11024.2275390625
1
Iteration 22900: Loss = -11024.224609375
2
Iteration 23000: Loss = -11024.1982421875
Iteration 23100: Loss = -11024.1904296875
Iteration 23200: Loss = -11024.171875
Iteration 23300: Loss = -11024.1650390625
Iteration 23400: Loss = -11024.1650390625
Iteration 23500: Loss = -11024.150390625
Iteration 23600: Loss = -11024.146484375
Iteration 23700: Loss = -11024.140625
Iteration 23800: Loss = -11024.140625
Iteration 23900: Loss = -11024.1357421875
Iteration 24000: Loss = -11024.0830078125
Iteration 24100: Loss = -11024.080078125
Iteration 24200: Loss = -11024.0751953125
Iteration 24300: Loss = -11024.0703125
Iteration 24400: Loss = -11024.06640625
Iteration 24500: Loss = -11024.05078125
Iteration 24600: Loss = -11024.048828125
Iteration 24700: Loss = -11024.033203125
Iteration 24800: Loss = -11024.03125
Iteration 24900: Loss = -11024.0283203125
Iteration 25000: Loss = -11024.017578125
Iteration 25100: Loss = -11024.0126953125
Iteration 25200: Loss = -11024.013671875
1
Iteration 25300: Loss = -11024.0009765625
Iteration 25400: Loss = -11023.998046875
Iteration 25500: Loss = -11023.9970703125
Iteration 25600: Loss = -11023.9951171875
Iteration 25700: Loss = -11023.990234375
Iteration 25800: Loss = -11023.9765625
Iteration 25900: Loss = -11023.9755859375
Iteration 26000: Loss = -11023.9716796875
Iteration 26100: Loss = -11023.9716796875
Iteration 26200: Loss = -11023.966796875
Iteration 26300: Loss = -11023.9677734375
1
Iteration 26400: Loss = -11023.966796875
Iteration 26500: Loss = -11023.9677734375
1
Iteration 26600: Loss = -11023.9677734375
2
Iteration 26700: Loss = -11023.966796875
Iteration 26800: Loss = -11023.9560546875
Iteration 26900: Loss = -11023.9580078125
1
Iteration 27000: Loss = -11023.95703125
2
Iteration 27100: Loss = -11023.9599609375
3
Iteration 27200: Loss = -11023.95703125
4
Iteration 27300: Loss = -11023.9560546875
Iteration 27400: Loss = -11023.955078125
Iteration 27500: Loss = -11023.95703125
1
Iteration 27600: Loss = -11023.9560546875
2
Iteration 27700: Loss = -11023.9560546875
3
Iteration 27800: Loss = -11023.9560546875
4
Iteration 27900: Loss = -11023.9560546875
5
Iteration 28000: Loss = -11023.95703125
6
Iteration 28100: Loss = -11023.9580078125
7
Iteration 28200: Loss = -11023.95703125
8
Iteration 28300: Loss = -11023.9580078125
9
Iteration 28400: Loss = -11023.9560546875
10
Iteration 28500: Loss = -11023.958984375
11
Iteration 28600: Loss = -11023.95703125
12
Iteration 28700: Loss = -11023.9580078125
13
Iteration 28800: Loss = -11023.9560546875
14
Iteration 28900: Loss = -11023.95703125
15
Stopping early at iteration 28900 due to no improvement.
pi: tensor([[9.9137e-01, 8.6298e-03],
        [9.9999e-01, 6.7954e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8024, 0.1976], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1658, 0.1352],
         [0.1985, 0.1153]],

        [[0.8835, 0.1986],
         [0.6640, 0.3243]],

        [[0.9276, 0.0863],
         [0.0776, 0.8850]],

        [[0.8092, 0.1706],
         [0.9849, 0.0090]],

        [[0.4282, 0.2548],
         [0.4369, 0.9046]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002101672322569199
Average Adjusted Rand Index: -0.00030621357158361367
[0.02114671802690587, -0.0002101672322569199] [0.760004402818634, -0.00030621357158361367] [10922.7216796875, 11023.95703125]
-------------------------------------
This iteration is 53
True Objective function: Loss = -10839.362139248846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15915.8623046875
Iteration 100: Loss = -12291.541015625
Iteration 200: Loss = -11222.5009765625
Iteration 300: Loss = -11077.2861328125
Iteration 400: Loss = -11032.9990234375
Iteration 500: Loss = -11011.041015625
Iteration 600: Loss = -10991.4248046875
Iteration 700: Loss = -10976.240234375
Iteration 800: Loss = -10968.48828125
Iteration 900: Loss = -10963.1279296875
Iteration 1000: Loss = -10958.3515625
Iteration 1100: Loss = -10954.982421875
Iteration 1200: Loss = -10952.865234375
Iteration 1300: Loss = -10951.5078125
Iteration 1400: Loss = -10950.5
Iteration 1500: Loss = -10949.5625
Iteration 1600: Loss = -10948.650390625
Iteration 1700: Loss = -10948.087890625
Iteration 1800: Loss = -10947.6728515625
Iteration 1900: Loss = -10947.3330078125
Iteration 2000: Loss = -10947.0498046875
Iteration 2100: Loss = -10946.80859375
Iteration 2200: Loss = -10946.607421875
Iteration 2300: Loss = -10946.435546875
Iteration 2400: Loss = -10946.283203125
Iteration 2500: Loss = -10946.1416015625
Iteration 2600: Loss = -10946.009765625
Iteration 2700: Loss = -10945.9013671875
Iteration 2800: Loss = -10945.8037109375
Iteration 2900: Loss = -10945.7099609375
Iteration 3000: Loss = -10945.625
Iteration 3100: Loss = -10945.5400390625
Iteration 3200: Loss = -10945.4716796875
Iteration 3300: Loss = -10945.4052734375
Iteration 3400: Loss = -10945.3427734375
Iteration 3500: Loss = -10945.283203125
Iteration 3600: Loss = -10945.216796875
Iteration 3700: Loss = -10945.12109375
Iteration 3800: Loss = -10944.861328125
Iteration 3900: Loss = -10943.35546875
Iteration 4000: Loss = -10934.240234375
Iteration 4100: Loss = -10929.0029296875
Iteration 4200: Loss = -10923.6640625
Iteration 4300: Loss = -10922.80078125
Iteration 4400: Loss = -10922.662109375
Iteration 4500: Loss = -10922.5986328125
Iteration 4600: Loss = -10922.5537109375
Iteration 4700: Loss = -10922.5166015625
Iteration 4800: Loss = -10922.486328125
Iteration 4900: Loss = -10922.4599609375
Iteration 5000: Loss = -10922.435546875
Iteration 5100: Loss = -10922.4140625
Iteration 5200: Loss = -10922.392578125
Iteration 5300: Loss = -10922.376953125
Iteration 5400: Loss = -10922.3603515625
Iteration 5500: Loss = -10922.3447265625
Iteration 5600: Loss = -10922.3291015625
Iteration 5700: Loss = -10922.3154296875
Iteration 5800: Loss = -10922.3017578125
Iteration 5900: Loss = -10922.28515625
Iteration 6000: Loss = -10922.2666015625
Iteration 6100: Loss = -10922.2373046875
Iteration 6200: Loss = -10922.134765625
Iteration 6300: Loss = -10921.041015625
Iteration 6400: Loss = -10920.7744140625
Iteration 6500: Loss = -10920.69921875
Iteration 6600: Loss = -10920.654296875
Iteration 6700: Loss = -10920.6259765625
Iteration 6800: Loss = -10920.6025390625
Iteration 6900: Loss = -10920.5791015625
Iteration 7000: Loss = -10920.556640625
Iteration 7100: Loss = -10920.541015625
Iteration 7200: Loss = -10920.5234375
Iteration 7300: Loss = -10920.5078125
Iteration 7400: Loss = -10920.4931640625
Iteration 7500: Loss = -10920.48046875
Iteration 7600: Loss = -10920.46484375
Iteration 7700: Loss = -10920.455078125
Iteration 7800: Loss = -10920.4423828125
Iteration 7900: Loss = -10920.431640625
Iteration 8000: Loss = -10920.4189453125
Iteration 8100: Loss = -10920.41015625
Iteration 8200: Loss = -10920.3994140625
Iteration 8300: Loss = -10920.3896484375
Iteration 8400: Loss = -10920.380859375
Iteration 8500: Loss = -10920.37109375
Iteration 8600: Loss = -10920.361328125
Iteration 8700: Loss = -10920.3525390625
Iteration 8800: Loss = -10920.33984375
Iteration 8900: Loss = -10920.3310546875
Iteration 9000: Loss = -10920.322265625
Iteration 9100: Loss = -10920.314453125
Iteration 9200: Loss = -10920.3076171875
Iteration 9300: Loss = -10920.294921875
Iteration 9400: Loss = -10920.28515625
Iteration 9500: Loss = -10920.2744140625
Iteration 9600: Loss = -10920.259765625
Iteration 9700: Loss = -10920.2431640625
Iteration 9800: Loss = -10920.228515625
Iteration 9900: Loss = -10920.208984375
Iteration 10000: Loss = -10920.1923828125
Iteration 10100: Loss = -10920.1875
Iteration 10200: Loss = -10920.1845703125
Iteration 10300: Loss = -10920.18359375
Iteration 10400: Loss = -10920.18359375
Iteration 10500: Loss = -10920.18359375
Iteration 10600: Loss = -10920.1826171875
Iteration 10700: Loss = -10920.1826171875
Iteration 10800: Loss = -10920.1826171875
Iteration 10900: Loss = -10920.181640625
Iteration 11000: Loss = -10920.1806640625
Iteration 11100: Loss = -10920.1806640625
Iteration 11200: Loss = -10920.1806640625
Iteration 11300: Loss = -10920.1787109375
Iteration 11400: Loss = -10920.1796875
1
Iteration 11500: Loss = -10920.1787109375
Iteration 11600: Loss = -10920.1787109375
Iteration 11700: Loss = -10920.177734375
Iteration 11800: Loss = -10920.1796875
1
Iteration 11900: Loss = -10920.177734375
Iteration 12000: Loss = -10920.1767578125
Iteration 12100: Loss = -10920.17578125
Iteration 12200: Loss = -10920.177734375
1
Iteration 12300: Loss = -10920.1767578125
2
Iteration 12400: Loss = -10920.17578125
Iteration 12500: Loss = -10920.1767578125
1
Iteration 12600: Loss = -10920.17578125
Iteration 12700: Loss = -10920.17578125
Iteration 12800: Loss = -10920.17578125
Iteration 12900: Loss = -10920.17578125
Iteration 13000: Loss = -10920.1748046875
Iteration 13100: Loss = -10920.1748046875
Iteration 13200: Loss = -10920.17578125
1
Iteration 13300: Loss = -10920.173828125
Iteration 13400: Loss = -10920.173828125
Iteration 13500: Loss = -10920.17578125
1
Iteration 13600: Loss = -10920.1748046875
2
Iteration 13700: Loss = -10920.1748046875
3
Iteration 13800: Loss = -10920.1748046875
4
Iteration 13900: Loss = -10920.1748046875
5
Iteration 14000: Loss = -10920.1748046875
6
Iteration 14100: Loss = -10920.173828125
Iteration 14200: Loss = -10920.1728515625
Iteration 14300: Loss = -10920.1748046875
1
Iteration 14400: Loss = -10920.1728515625
Iteration 14500: Loss = -10920.1748046875
1
Iteration 14600: Loss = -10920.173828125
2
Iteration 14700: Loss = -10920.173828125
3
Iteration 14800: Loss = -10920.173828125
4
Iteration 14900: Loss = -10920.173828125
5
Iteration 15000: Loss = -10920.173828125
6
Iteration 15100: Loss = -10920.173828125
7
Iteration 15200: Loss = -10920.173828125
8
Iteration 15300: Loss = -10920.173828125
9
Iteration 15400: Loss = -10920.173828125
10
Iteration 15500: Loss = -10920.173828125
11
Iteration 15600: Loss = -10920.173828125
12
Iteration 15700: Loss = -10920.173828125
13
Iteration 15800: Loss = -10920.1748046875
14
Iteration 15900: Loss = -10920.173828125
15
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[1.0561e-04, 9.9989e-01],
        [1.6731e-02, 9.8327e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4455, 0.5545], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2921, 0.1080],
         [0.0833, 0.1618]],

        [[0.9800, 0.1499],
         [0.0317, 0.1935]],

        [[0.5314, 0.2797],
         [0.9926, 0.5773]],

        [[0.6849, 0.1424],
         [0.3390, 0.8994]],

        [[0.0212, 0.0858],
         [0.8273, 0.0554]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080890789891884
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.030328976418920177
Average Adjusted Rand Index: 0.1603986058062631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21803.41796875
Iteration 100: Loss = -14327.3671875
Iteration 200: Loss = -11641.50390625
Iteration 300: Loss = -11170.1103515625
Iteration 400: Loss = -11061.875
Iteration 500: Loss = -11020.638671875
Iteration 600: Loss = -10997.458984375
Iteration 700: Loss = -10981.880859375
Iteration 800: Loss = -10972.025390625
Iteration 900: Loss = -10965.6318359375
Iteration 1000: Loss = -10961.14453125
Iteration 1100: Loss = -10957.8701171875
Iteration 1200: Loss = -10955.4609375
Iteration 1300: Loss = -10953.587890625
Iteration 1400: Loss = -10952.1083984375
Iteration 1500: Loss = -10950.923828125
Iteration 1600: Loss = -10949.943359375
Iteration 1700: Loss = -10949.115234375
Iteration 1800: Loss = -10948.4150390625
Iteration 1900: Loss = -10947.806640625
Iteration 2000: Loss = -10947.28125
Iteration 2100: Loss = -10946.8291015625
Iteration 2200: Loss = -10946.43359375
Iteration 2300: Loss = -10946.0859375
Iteration 2400: Loss = -10945.7763671875
Iteration 2500: Loss = -10945.49609375
Iteration 2600: Loss = -10945.234375
Iteration 2700: Loss = -10944.9951171875
Iteration 2800: Loss = -10944.796875
Iteration 2900: Loss = -10944.6201171875
Iteration 3000: Loss = -10944.4658203125
Iteration 3100: Loss = -10944.328125
Iteration 3200: Loss = -10944.2041015625
Iteration 3300: Loss = -10944.0947265625
Iteration 3400: Loss = -10944.0009765625
Iteration 3500: Loss = -10943.9169921875
Iteration 3600: Loss = -10943.8388671875
Iteration 3700: Loss = -10943.7685546875
Iteration 3800: Loss = -10943.7041015625
Iteration 3900: Loss = -10943.6435546875
Iteration 4000: Loss = -10943.5869140625
Iteration 4100: Loss = -10943.5341796875
Iteration 4200: Loss = -10943.4833984375
Iteration 4300: Loss = -10943.4375
Iteration 4400: Loss = -10943.3955078125
Iteration 4500: Loss = -10943.353515625
Iteration 4600: Loss = -10943.3154296875
Iteration 4700: Loss = -10943.2783203125
Iteration 4800: Loss = -10943.244140625
Iteration 4900: Loss = -10943.2080078125
Iteration 5000: Loss = -10943.17578125
Iteration 5100: Loss = -10943.1416015625
Iteration 5200: Loss = -10943.111328125
Iteration 5300: Loss = -10943.08203125
Iteration 5400: Loss = -10943.0498046875
Iteration 5500: Loss = -10943.0224609375
Iteration 5600: Loss = -10942.9951171875
Iteration 5700: Loss = -10942.9677734375
Iteration 5800: Loss = -10942.94140625
Iteration 5900: Loss = -10942.9150390625
Iteration 6000: Loss = -10942.8916015625
Iteration 6100: Loss = -10942.8671875
Iteration 6200: Loss = -10942.84375
Iteration 6300: Loss = -10942.8232421875
Iteration 6400: Loss = -10942.7978515625
Iteration 6500: Loss = -10942.77734375
Iteration 6600: Loss = -10942.755859375
Iteration 6700: Loss = -10942.7353515625
Iteration 6800: Loss = -10942.71484375
Iteration 6900: Loss = -10942.6962890625
Iteration 7000: Loss = -10942.677734375
Iteration 7100: Loss = -10942.6591796875
Iteration 7200: Loss = -10942.64453125
Iteration 7300: Loss = -10942.626953125
Iteration 7400: Loss = -10942.61328125
Iteration 7500: Loss = -10942.59765625
Iteration 7600: Loss = -10942.583984375
Iteration 7700: Loss = -10942.572265625
Iteration 7800: Loss = -10942.55859375
Iteration 7900: Loss = -10942.546875
Iteration 8000: Loss = -10942.5361328125
Iteration 8100: Loss = -10942.52734375
Iteration 8200: Loss = -10942.515625
Iteration 8300: Loss = -10942.505859375
Iteration 8400: Loss = -10942.4970703125
Iteration 8500: Loss = -10942.490234375
Iteration 8600: Loss = -10942.4814453125
Iteration 8700: Loss = -10942.474609375
Iteration 8800: Loss = -10942.466796875
Iteration 8900: Loss = -10942.4580078125
Iteration 9000: Loss = -10942.4521484375
Iteration 9100: Loss = -10942.4443359375
Iteration 9200: Loss = -10942.4404296875
Iteration 9300: Loss = -10942.43359375
Iteration 9400: Loss = -10942.4267578125
Iteration 9500: Loss = -10942.4208984375
Iteration 9600: Loss = -10942.4169921875
Iteration 9700: Loss = -10942.4130859375
Iteration 9800: Loss = -10942.41015625
Iteration 9900: Loss = -10942.4052734375
Iteration 10000: Loss = -10942.40234375
Iteration 10100: Loss = -10942.400390625
Iteration 10200: Loss = -10942.3984375
Iteration 10300: Loss = -10942.3955078125
Iteration 10400: Loss = -10942.39453125
Iteration 10500: Loss = -10942.3916015625
Iteration 10600: Loss = -10942.392578125
1
Iteration 10700: Loss = -10942.388671875
Iteration 10800: Loss = -10942.390625
1
Iteration 10900: Loss = -10942.388671875
Iteration 11000: Loss = -10942.3876953125
Iteration 11100: Loss = -10942.388671875
1
Iteration 11200: Loss = -10942.3876953125
Iteration 11300: Loss = -10942.38671875
Iteration 11400: Loss = -10942.384765625
Iteration 11500: Loss = -10942.38671875
1
Iteration 11600: Loss = -10942.384765625
Iteration 11700: Loss = -10942.3857421875
1
Iteration 11800: Loss = -10942.3837890625
Iteration 11900: Loss = -10942.384765625
1
Iteration 12000: Loss = -10942.3837890625
Iteration 12100: Loss = -10942.3828125
Iteration 12200: Loss = -10942.3828125
Iteration 12300: Loss = -10942.3828125
Iteration 12400: Loss = -10942.3818359375
Iteration 12500: Loss = -10942.3828125
1
Iteration 12600: Loss = -10942.380859375
Iteration 12700: Loss = -10942.380859375
Iteration 12800: Loss = -10942.3828125
1
Iteration 12900: Loss = -10942.380859375
Iteration 13000: Loss = -10942.3798828125
Iteration 13100: Loss = -10942.380859375
1
Iteration 13200: Loss = -10942.37890625
Iteration 13300: Loss = -10942.380859375
1
Iteration 13400: Loss = -10942.3798828125
2
Iteration 13500: Loss = -10942.3798828125
3
Iteration 13600: Loss = -10942.3798828125
4
Iteration 13700: Loss = -10942.37890625
Iteration 13800: Loss = -10942.37890625
Iteration 13900: Loss = -10942.37890625
Iteration 14000: Loss = -10942.37890625
Iteration 14100: Loss = -10942.37890625
Iteration 14200: Loss = -10942.3798828125
1
Iteration 14300: Loss = -10942.3779296875
Iteration 14400: Loss = -10942.37890625
1
Iteration 14500: Loss = -10942.3779296875
Iteration 14600: Loss = -10942.376953125
Iteration 14700: Loss = -10942.37890625
1
Iteration 14800: Loss = -10942.37890625
2
Iteration 14900: Loss = -10942.3779296875
3
Iteration 15000: Loss = -10942.3779296875
4
Iteration 15100: Loss = -10942.37890625
5
Iteration 15200: Loss = -10942.3779296875
6
Iteration 15300: Loss = -10942.376953125
Iteration 15400: Loss = -10942.37890625
1
Iteration 15500: Loss = -10942.37890625
2
Iteration 15600: Loss = -10942.3779296875
3
Iteration 15700: Loss = -10942.3759765625
Iteration 15800: Loss = -10942.3779296875
1
Iteration 15900: Loss = -10942.376953125
2
Iteration 16000: Loss = -10942.3759765625
Iteration 16100: Loss = -10942.3779296875
1
Iteration 16200: Loss = -10942.3759765625
Iteration 16300: Loss = -10942.37890625
1
Iteration 16400: Loss = -10942.3759765625
Iteration 16500: Loss = -10942.3759765625
Iteration 16600: Loss = -10942.376953125
1
Iteration 16700: Loss = -10942.376953125
2
Iteration 16800: Loss = -10942.376953125
3
Iteration 16900: Loss = -10942.3759765625
Iteration 17000: Loss = -10942.37890625
1
Iteration 17100: Loss = -10942.3759765625
Iteration 17200: Loss = -10942.376953125
1
Iteration 17300: Loss = -10942.375
Iteration 17400: Loss = -10942.376953125
1
Iteration 17500: Loss = -10942.3759765625
2
Iteration 17600: Loss = -10942.375
Iteration 17700: Loss = -10942.376953125
1
Iteration 17800: Loss = -10942.3759765625
2
Iteration 17900: Loss = -10942.376953125
3
Iteration 18000: Loss = -10942.376953125
4
Iteration 18100: Loss = -10942.3798828125
5
Iteration 18200: Loss = -10942.3759765625
6
Iteration 18300: Loss = -10942.3759765625
7
Iteration 18400: Loss = -10942.376953125
8
Iteration 18500: Loss = -10942.3779296875
9
Iteration 18600: Loss = -10942.3759765625
10
Iteration 18700: Loss = -10942.376953125
11
Iteration 18800: Loss = -10942.3759765625
12
Iteration 18900: Loss = -10942.3759765625
13
Iteration 19000: Loss = -10942.3759765625
14
Iteration 19100: Loss = -10942.375
Iteration 19200: Loss = -10942.3759765625
1
Iteration 19300: Loss = -10942.375
Iteration 19400: Loss = -10942.3759765625
1
Iteration 19500: Loss = -10942.3759765625
2
Iteration 19600: Loss = -10942.3759765625
3
Iteration 19700: Loss = -10942.3759765625
4
Iteration 19800: Loss = -10942.375
Iteration 19900: Loss = -10942.3759765625
1
Iteration 20000: Loss = -10942.3759765625
2
Iteration 20100: Loss = -10942.375
Iteration 20200: Loss = -10942.3759765625
1
Iteration 20300: Loss = -10942.3759765625
2
Iteration 20400: Loss = -10942.375
Iteration 20500: Loss = -10942.3740234375
Iteration 20600: Loss = -10942.380859375
1
Iteration 20700: Loss = -10942.3740234375
Iteration 20800: Loss = -10942.375
1
Iteration 20900: Loss = -10942.3759765625
2
Iteration 21000: Loss = -10942.375
3
Iteration 21100: Loss = -10942.375
4
Iteration 21200: Loss = -10942.376953125
5
Iteration 21300: Loss = -10942.3759765625
6
Iteration 21400: Loss = -10942.376953125
7
Iteration 21500: Loss = -10942.375
8
Iteration 21600: Loss = -10942.3759765625
9
Iteration 21700: Loss = -10942.3759765625
10
Iteration 21800: Loss = -10942.375
11
Iteration 21900: Loss = -10942.376953125
12
Iteration 22000: Loss = -10942.375
13
Iteration 22100: Loss = -10942.3759765625
14
Iteration 22200: Loss = -10942.3759765625
15
Stopping early at iteration 22200 due to no improvement.
pi: tensor([[9.8395e-01, 1.6046e-02],
        [9.9969e-01, 3.0554e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9215, 0.0785], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1593, 0.1954],
         [0.1290, 0.2624]],

        [[0.6762, 0.1697],
         [0.5510, 0.0219]],

        [[0.9850, 0.2787],
         [0.9238, 0.9930]],

        [[0.0646, 0.1522],
         [0.0193, 0.6795]],

        [[0.9539, 0.2020],
         [0.0202, 0.0249]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.1873015856761384e-05
Average Adjusted Rand Index: -0.001219209991574565
[0.030328976418920177, -3.1873015856761384e-05] [0.1603986058062631, -0.001219209991574565] [10920.173828125, 10942.3759765625]
-------------------------------------
This iteration is 54
True Objective function: Loss = -10915.280961418353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40870.8515625
Iteration 100: Loss = -25662.998046875
Iteration 200: Loss = -13698.630859375
Iteration 300: Loss = -11827.42578125
Iteration 400: Loss = -11505.9453125
Iteration 500: Loss = -11378.3115234375
Iteration 600: Loss = -11311.330078125
Iteration 700: Loss = -11271.3583984375
Iteration 800: Loss = -11245.521484375
Iteration 900: Loss = -11226.892578125
Iteration 1000: Loss = -11213.318359375
Iteration 1100: Loss = -11201.30859375
Iteration 1200: Loss = -11192.22265625
Iteration 1300: Loss = -11183.9296875
Iteration 1400: Loss = -11178.0927734375
Iteration 1500: Loss = -11170.984375
Iteration 1600: Loss = -11166.173828125
Iteration 1700: Loss = -11161.0029296875
Iteration 1800: Loss = -11155.76953125
Iteration 1900: Loss = -11151.0302734375
Iteration 2000: Loss = -11145.61328125
Iteration 2100: Loss = -11139.275390625
Iteration 2200: Loss = -11134.439453125
Iteration 2300: Loss = -11128.4462890625
Iteration 2400: Loss = -11123.056640625
Iteration 2500: Loss = -11117.310546875
Iteration 2600: Loss = -11113.1015625
Iteration 2700: Loss = -11107.71484375
Iteration 2800: Loss = -11101.369140625
Iteration 2900: Loss = -11094.58203125
Iteration 3000: Loss = -11088.8447265625
Iteration 3100: Loss = -11085.498046875
Iteration 3200: Loss = -11079.61328125
Iteration 3300: Loss = -11076.146484375
Iteration 3400: Loss = -11074.1611328125
Iteration 3500: Loss = -11070.068359375
Iteration 3600: Loss = -11067.578125
Iteration 3700: Loss = -11066.021484375
Iteration 3800: Loss = -11063.7998046875
Iteration 3900: Loss = -11061.7587890625
Iteration 4000: Loss = -11060.8583984375
Iteration 4100: Loss = -11059.990234375
Iteration 4200: Loss = -11057.720703125
Iteration 4300: Loss = -11057.0107421875
Iteration 4400: Loss = -11056.4716796875
Iteration 4500: Loss = -11054.3701171875
Iteration 4600: Loss = -11053.56640625
Iteration 4700: Loss = -11053.17578125
Iteration 4800: Loss = -11052.8779296875
Iteration 4900: Loss = -11052.6259765625
Iteration 5000: Loss = -11052.40234375
Iteration 5100: Loss = -11049.5419921875
Iteration 5200: Loss = -11049.248046875
Iteration 5300: Loss = -11049.0859375
Iteration 5400: Loss = -11048.9375
Iteration 5500: Loss = -11048.767578125
Iteration 5600: Loss = -11046.7099609375
Iteration 5700: Loss = -11045.9921875
Iteration 5800: Loss = -11045.80078125
Iteration 5900: Loss = -11045.6474609375
Iteration 6000: Loss = -11045.5068359375
Iteration 6100: Loss = -11045.1640625
Iteration 6200: Loss = -11041.943359375
Iteration 6300: Loss = -11041.8359375
Iteration 6400: Loss = -11041.751953125
Iteration 6500: Loss = -11041.677734375
Iteration 6600: Loss = -11041.5546875
Iteration 6700: Loss = -11038.251953125
Iteration 6800: Loss = -11035.1357421875
Iteration 6900: Loss = -11034.5654296875
Iteration 7000: Loss = -11031.7421875
Iteration 7100: Loss = -11030.6767578125
Iteration 7200: Loss = -11030.5537109375
Iteration 7300: Loss = -11030.4765625
Iteration 7400: Loss = -11030.4150390625
Iteration 7500: Loss = -11030.3623046875
Iteration 7600: Loss = -11029.998046875
Iteration 7700: Loss = -11026.365234375
Iteration 7800: Loss = -11026.294921875
Iteration 7900: Loss = -11026.25
Iteration 8000: Loss = -11026.2158203125
Iteration 8100: Loss = -11026.1884765625
Iteration 8200: Loss = -11026.1611328125
Iteration 8300: Loss = -11024.078125
Iteration 8400: Loss = -11021.7373046875
Iteration 8500: Loss = -11021.677734375
Iteration 8600: Loss = -11021.6435546875
Iteration 8700: Loss = -11021.6162109375
Iteration 8800: Loss = -11021.5927734375
Iteration 8900: Loss = -11021.572265625
Iteration 9000: Loss = -11016.978515625
Iteration 9100: Loss = -11016.849609375
Iteration 9200: Loss = -11016.8037109375
Iteration 9300: Loss = -11016.7734375
Iteration 9400: Loss = -11016.7529296875
Iteration 9500: Loss = -11016.7353515625
Iteration 9600: Loss = -11016.7197265625
Iteration 9700: Loss = -11011.5029296875
Iteration 9800: Loss = -11010.828125
Iteration 9900: Loss = -11010.70703125
Iteration 10000: Loss = -11010.6357421875
Iteration 10100: Loss = -11010.587890625
Iteration 10200: Loss = -11010.5517578125
Iteration 10300: Loss = -11010.521484375
Iteration 10400: Loss = -11010.5
Iteration 10500: Loss = -11010.478515625
Iteration 10600: Loss = -11010.4609375
Iteration 10700: Loss = -11010.4462890625
Iteration 10800: Loss = -11010.4326171875
Iteration 10900: Loss = -11010.4189453125
Iteration 11000: Loss = -11010.41015625
Iteration 11100: Loss = -11010.3994140625
Iteration 11200: Loss = -11010.390625
Iteration 11300: Loss = -11010.3837890625
Iteration 11400: Loss = -11010.376953125
Iteration 11500: Loss = -11010.3681640625
Iteration 11600: Loss = -11010.3623046875
Iteration 11700: Loss = -11010.357421875
Iteration 11800: Loss = -11010.3505859375
Iteration 11900: Loss = -11010.345703125
Iteration 12000: Loss = -11010.341796875
Iteration 12100: Loss = -11010.3369140625
Iteration 12200: Loss = -11010.33203125
Iteration 12300: Loss = -11010.328125
Iteration 12400: Loss = -11010.32421875
Iteration 12500: Loss = -11010.322265625
Iteration 12600: Loss = -11010.3173828125
Iteration 12700: Loss = -11010.314453125
Iteration 12800: Loss = -11010.3125
Iteration 12900: Loss = -11010.3095703125
Iteration 13000: Loss = -11010.306640625
Iteration 13100: Loss = -11010.3037109375
Iteration 13200: Loss = -11010.302734375
Iteration 13300: Loss = -11010.2998046875
Iteration 13400: Loss = -11010.298828125
Iteration 13500: Loss = -11010.2958984375
Iteration 13600: Loss = -11010.2958984375
Iteration 13700: Loss = -11010.2939453125
Iteration 13800: Loss = -11010.291015625
Iteration 13900: Loss = -11010.291015625
Iteration 14000: Loss = -11010.2880859375
Iteration 14100: Loss = -11010.2861328125
Iteration 14200: Loss = -11010.28515625
Iteration 14300: Loss = -11010.28515625
Iteration 14400: Loss = -11010.2822265625
Iteration 14500: Loss = -11010.2822265625
Iteration 14600: Loss = -11010.28125
Iteration 14700: Loss = -11010.279296875
Iteration 14800: Loss = -11010.27734375
Iteration 14900: Loss = -11010.27734375
Iteration 15000: Loss = -11010.2783203125
1
Iteration 15100: Loss = -11010.2763671875
Iteration 15200: Loss = -11010.2763671875
Iteration 15300: Loss = -11010.2724609375
Iteration 15400: Loss = -11010.2734375
1
Iteration 15500: Loss = -11010.2734375
2
Iteration 15600: Loss = -11010.2734375
3
Iteration 15700: Loss = -11010.271484375
Iteration 15800: Loss = -11010.2705078125
Iteration 15900: Loss = -11010.2705078125
Iteration 16000: Loss = -11010.2705078125
Iteration 16100: Loss = -11010.2255859375
Iteration 16200: Loss = -11009.6923828125
Iteration 16300: Loss = -11009.5712890625
Iteration 16400: Loss = -11009.5185546875
Iteration 16500: Loss = -11009.4365234375
Iteration 16600: Loss = -11009.40234375
Iteration 16700: Loss = -11009.3720703125
Iteration 16800: Loss = -11009.357421875
Iteration 16900: Loss = -11009.3515625
Iteration 17000: Loss = -11009.3486328125
Iteration 17100: Loss = -11009.3466796875
Iteration 17200: Loss = -11009.34765625
1
Iteration 17300: Loss = -11009.345703125
Iteration 17400: Loss = -11009.3466796875
1
Iteration 17500: Loss = -11009.345703125
Iteration 17600: Loss = -11009.345703125
Iteration 17700: Loss = -11009.34375
Iteration 17800: Loss = -11009.3447265625
1
Iteration 17900: Loss = -11009.34375
Iteration 18000: Loss = -11009.3427734375
Iteration 18100: Loss = -11009.3447265625
1
Iteration 18200: Loss = -11009.3447265625
2
Iteration 18300: Loss = -11009.3447265625
3
Iteration 18400: Loss = -11009.34375
4
Iteration 18500: Loss = -11009.3427734375
Iteration 18600: Loss = -11009.3427734375
Iteration 18700: Loss = -11009.3427734375
Iteration 18800: Loss = -11009.3408203125
Iteration 18900: Loss = -11009.341796875
1
Iteration 19000: Loss = -11009.337890625
Iteration 19100: Loss = -11009.3369140625
Iteration 19200: Loss = -11009.3359375
Iteration 19300: Loss = -11009.3349609375
Iteration 19400: Loss = -11009.3369140625
1
Iteration 19500: Loss = -11009.3349609375
Iteration 19600: Loss = -11009.3369140625
1
Iteration 19700: Loss = -11009.337890625
2
Iteration 19800: Loss = -11009.3359375
3
Iteration 19900: Loss = -11009.3369140625
4
Iteration 20000: Loss = -11009.3349609375
Iteration 20100: Loss = -11009.3359375
1
Iteration 20200: Loss = -11009.3359375
2
Iteration 20300: Loss = -11009.3349609375
Iteration 20400: Loss = -11009.3349609375
Iteration 20500: Loss = -11009.3349609375
Iteration 20600: Loss = -11009.3369140625
1
Iteration 20700: Loss = -11009.3359375
2
Iteration 20800: Loss = -11009.3359375
3
Iteration 20900: Loss = -11009.3369140625
4
Iteration 21000: Loss = -11009.3359375
5
Iteration 21100: Loss = -11009.3349609375
Iteration 21200: Loss = -11009.3349609375
Iteration 21300: Loss = -11009.330078125
Iteration 21400: Loss = -11009.330078125
Iteration 21500: Loss = -11009.3291015625
Iteration 21600: Loss = -11009.3310546875
1
Iteration 21700: Loss = -11009.3291015625
Iteration 21800: Loss = -11009.33203125
1
Iteration 21900: Loss = -11009.3310546875
2
Iteration 22000: Loss = -11009.3310546875
3
Iteration 22100: Loss = -11009.3310546875
4
Iteration 22200: Loss = -11009.3291015625
Iteration 22300: Loss = -11009.3310546875
1
Iteration 22400: Loss = -11009.3310546875
2
Iteration 22500: Loss = -11009.3291015625
Iteration 22600: Loss = -11009.3310546875
1
Iteration 22700: Loss = -11009.3291015625
Iteration 22800: Loss = -11009.3291015625
Iteration 22900: Loss = -11009.3291015625
Iteration 23000: Loss = -11009.3310546875
1
Iteration 23100: Loss = -11009.3310546875
2
Iteration 23200: Loss = -11009.3310546875
3
Iteration 23300: Loss = -11009.3310546875
4
Iteration 23400: Loss = -11009.3310546875
5
Iteration 23500: Loss = -11009.3291015625
Iteration 23600: Loss = -11009.3310546875
1
Iteration 23700: Loss = -11009.328125
Iteration 23800: Loss = -11009.33203125
1
Iteration 23900: Loss = -11009.3291015625
2
Iteration 24000: Loss = -11009.3310546875
3
Iteration 24100: Loss = -11009.330078125
4
Iteration 24200: Loss = -11009.3291015625
5
Iteration 24300: Loss = -11009.3310546875
6
Iteration 24400: Loss = -11009.33203125
7
Iteration 24500: Loss = -11009.3291015625
8
Iteration 24600: Loss = -11009.3291015625
9
Iteration 24700: Loss = -11009.330078125
10
Iteration 24800: Loss = -11009.3310546875
11
Iteration 24900: Loss = -11009.3310546875
12
Iteration 25000: Loss = -11009.25390625
Iteration 25100: Loss = -11009.212890625
Iteration 25200: Loss = -11009.203125
Iteration 25300: Loss = -11009.1943359375
Iteration 25400: Loss = -11009.1923828125
Iteration 25500: Loss = -11009.1689453125
Iteration 25600: Loss = -11009.16796875
Iteration 25700: Loss = -11009.166015625
Iteration 25800: Loss = -11009.166015625
Iteration 25900: Loss = -11009.16796875
1
Iteration 26000: Loss = -11009.1689453125
2
Iteration 26100: Loss = -11009.16796875
3
Iteration 26200: Loss = -11009.15625
Iteration 26300: Loss = -11009.14453125
Iteration 26400: Loss = -11009.1357421875
Iteration 26500: Loss = -11009.134765625
Iteration 26600: Loss = -11009.134765625
Iteration 26700: Loss = -11009.1337890625
Iteration 26800: Loss = -11009.134765625
1
Iteration 26900: Loss = -11009.1337890625
Iteration 27000: Loss = -11009.1337890625
Iteration 27100: Loss = -11009.134765625
1
Iteration 27200: Loss = -11009.1328125
Iteration 27300: Loss = -11009.1337890625
1
Iteration 27400: Loss = -11009.1337890625
2
Iteration 27500: Loss = -11009.1328125
Iteration 27600: Loss = -11009.1318359375
Iteration 27700: Loss = -11009.1337890625
1
Iteration 27800: Loss = -11009.1337890625
2
Iteration 27900: Loss = -11009.1337890625
3
Iteration 28000: Loss = -11009.1337890625
4
Iteration 28100: Loss = -11009.1328125
5
Iteration 28200: Loss = -11009.1318359375
Iteration 28300: Loss = -11009.1318359375
Iteration 28400: Loss = -11009.138671875
1
Iteration 28500: Loss = -11009.1328125
2
Iteration 28600: Loss = -11009.1318359375
Iteration 28700: Loss = -11009.1328125
1
Iteration 28800: Loss = -11009.1318359375
Iteration 28900: Loss = -11009.130859375
Iteration 29000: Loss = -11009.1240234375
Iteration 29100: Loss = -11009.1123046875
Iteration 29200: Loss = -11009.1064453125
Iteration 29300: Loss = -11009.0869140625
Iteration 29400: Loss = -11009.083984375
Iteration 29500: Loss = -11009.0654296875
Iteration 29600: Loss = -11009.0634765625
Iteration 29700: Loss = -11009.0302734375
Iteration 29800: Loss = -11009.0224609375
Iteration 29900: Loss = -11009.0166015625
pi: tensor([[9.9250e-01, 7.4956e-03],
        [1.0000e+00, 4.1006e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.9776e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1639, 0.1846],
         [0.5703, 0.0386]],

        [[0.0169, 0.0588],
         [0.0974, 0.8633]],

        [[0.0361, 0.1855],
         [0.9731, 0.0086]],

        [[0.0244, 0.1418],
         [0.8425, 0.0365]],

        [[0.0105, 0.2385],
         [0.9171, 0.9610]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0003429436742078051
Average Adjusted Rand Index: 0.0001617442499919128
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -48491.9453125
Iteration 100: Loss = -26618.380859375
Iteration 200: Loss = -14735.9501953125
Iteration 300: Loss = -12128.154296875
Iteration 400: Loss = -11626.48046875
Iteration 500: Loss = -11387.4052734375
Iteration 600: Loss = -11271.810546875
Iteration 700: Loss = -11206.3359375
Iteration 800: Loss = -11169.28125
Iteration 900: Loss = -11143.7646484375
Iteration 1000: Loss = -11125.1279296875
Iteration 1100: Loss = -11110.984375
Iteration 1200: Loss = -11089.912109375
Iteration 1300: Loss = -11080.41796875
Iteration 1400: Loss = -11073.51953125
Iteration 1500: Loss = -11067.8583984375
Iteration 1600: Loss = -11063.0771484375
Iteration 1700: Loss = -11053.3291015625
Iteration 1800: Loss = -11049.728515625
Iteration 1900: Loss = -11040.009765625
Iteration 2000: Loss = -11036.93359375
Iteration 2100: Loss = -11034.693359375
Iteration 2200: Loss = -11032.8037109375
Iteration 2300: Loss = -11031.15625
Iteration 2400: Loss = -11029.7138671875
Iteration 2500: Loss = -11028.4345703125
Iteration 2600: Loss = -11027.2939453125
Iteration 2700: Loss = -11026.2724609375
Iteration 2800: Loss = -11025.3515625
Iteration 2900: Loss = -11024.517578125
Iteration 3000: Loss = -11023.7626953125
Iteration 3100: Loss = -11023.0693359375
Iteration 3200: Loss = -11022.251953125
Iteration 3300: Loss = -11018.2587890625
Iteration 3400: Loss = -11017.361328125
Iteration 3500: Loss = -11016.744140625
Iteration 3600: Loss = -11016.2353515625
Iteration 3700: Loss = -11015.7841796875
Iteration 3800: Loss = -11015.37890625
Iteration 3900: Loss = -11015.0126953125
Iteration 4000: Loss = -11014.677734375
Iteration 4100: Loss = -11014.3681640625
Iteration 4200: Loss = -11014.078125
Iteration 4300: Loss = -11013.8232421875
Iteration 4400: Loss = -11013.5927734375
Iteration 4500: Loss = -11013.380859375
Iteration 4600: Loss = -11013.1884765625
Iteration 4700: Loss = -11013.0078125
Iteration 4800: Loss = -11012.841796875
Iteration 4900: Loss = -11012.69140625
Iteration 5000: Loss = -11012.5498046875
Iteration 5100: Loss = -11012.4189453125
Iteration 5200: Loss = -11012.294921875
Iteration 5300: Loss = -11012.1787109375
Iteration 5400: Loss = -11012.064453125
Iteration 5500: Loss = -11011.958984375
Iteration 5600: Loss = -11011.85546875
Iteration 5700: Loss = -11011.75390625
Iteration 5800: Loss = -11011.654296875
Iteration 5900: Loss = -11011.5556640625
Iteration 6000: Loss = -11011.4521484375
Iteration 6100: Loss = -11011.3447265625
Iteration 6200: Loss = -11011.220703125
Iteration 6300: Loss = -11011.083984375
Iteration 6400: Loss = -11010.927734375
Iteration 6500: Loss = -11010.775390625
Iteration 6600: Loss = -11010.6416015625
Iteration 6700: Loss = -11010.5322265625
Iteration 6800: Loss = -11010.4365234375
Iteration 6900: Loss = -11010.353515625
Iteration 7000: Loss = -11010.2783203125
Iteration 7100: Loss = -11010.2060546875
Iteration 7200: Loss = -11010.1396484375
Iteration 7300: Loss = -11010.0771484375
Iteration 7400: Loss = -11010.017578125
Iteration 7500: Loss = -11009.962890625
Iteration 7600: Loss = -11009.91015625
Iteration 7700: Loss = -11009.8623046875
Iteration 7800: Loss = -11009.8154296875
Iteration 7900: Loss = -11009.771484375
Iteration 8000: Loss = -11009.7314453125
Iteration 8100: Loss = -11009.6904296875
Iteration 8200: Loss = -11009.6552734375
Iteration 8300: Loss = -11009.6201171875
Iteration 8400: Loss = -11009.587890625
Iteration 8500: Loss = -11009.5556640625
Iteration 8600: Loss = -11009.525390625
Iteration 8700: Loss = -11009.4990234375
Iteration 8800: Loss = -11009.470703125
Iteration 8900: Loss = -11009.4482421875
Iteration 9000: Loss = -11009.4228515625
Iteration 9100: Loss = -11009.3994140625
Iteration 9200: Loss = -11009.3779296875
Iteration 9300: Loss = -11009.3564453125
Iteration 9400: Loss = -11009.3359375
Iteration 9500: Loss = -11009.31640625
Iteration 9600: Loss = -11009.298828125
Iteration 9700: Loss = -11009.279296875
Iteration 9800: Loss = -11009.263671875
Iteration 9900: Loss = -11009.248046875
Iteration 10000: Loss = -11009.23046875
Iteration 10100: Loss = -11009.216796875
Iteration 10200: Loss = -11009.2021484375
Iteration 10300: Loss = -11009.185546875
Iteration 10400: Loss = -11009.171875
Iteration 10500: Loss = -11009.158203125
Iteration 10600: Loss = -11009.146484375
Iteration 10700: Loss = -11009.130859375
Iteration 10800: Loss = -11009.119140625
Iteration 10900: Loss = -11009.1064453125
Iteration 11000: Loss = -11009.0927734375
Iteration 11100: Loss = -11009.0791015625
Iteration 11200: Loss = -11009.068359375
Iteration 11300: Loss = -11009.0546875
Iteration 11400: Loss = -11009.0439453125
Iteration 11500: Loss = -11009.0283203125
Iteration 11600: Loss = -11009.0166015625
Iteration 11700: Loss = -11009.00390625
Iteration 11800: Loss = -11008.9912109375
Iteration 11900: Loss = -11008.9755859375
Iteration 12000: Loss = -11008.962890625
Iteration 12100: Loss = -11008.94921875
Iteration 12200: Loss = -11008.93359375
Iteration 12300: Loss = -11008.9208984375
Iteration 12400: Loss = -11008.900390625
Iteration 12500: Loss = -11008.8828125
Iteration 12600: Loss = -11008.8603515625
Iteration 12700: Loss = -11008.8408203125
Iteration 12800: Loss = -11008.818359375
Iteration 12900: Loss = -11008.7919921875
Iteration 13000: Loss = -11008.7646484375
Iteration 13100: Loss = -11008.7333984375
Iteration 13200: Loss = -11008.7001953125
Iteration 13300: Loss = -11008.66015625
Iteration 13400: Loss = -11008.619140625
Iteration 13500: Loss = -11008.57421875
Iteration 13600: Loss = -11008.53125
Iteration 13700: Loss = -11008.49609375
Iteration 13800: Loss = -11008.46875
Iteration 13900: Loss = -11008.44140625
Iteration 14000: Loss = -11008.4169921875
Iteration 14100: Loss = -11008.3974609375
Iteration 14200: Loss = -11008.3818359375
Iteration 14300: Loss = -11008.3720703125
Iteration 14400: Loss = -11008.3583984375
Iteration 14500: Loss = -11008.3466796875
Iteration 14600: Loss = -11008.33203125
Iteration 14700: Loss = -11008.3134765625
Iteration 14800: Loss = -11008.2958984375
Iteration 14900: Loss = -11008.283203125
Iteration 15000: Loss = -11008.2705078125
Iteration 15100: Loss = -11008.259765625
Iteration 15200: Loss = -11008.25390625
Iteration 15300: Loss = -11008.2470703125
Iteration 15400: Loss = -11008.2412109375
Iteration 15500: Loss = -11008.236328125
Iteration 15600: Loss = -11008.22265625
Iteration 15700: Loss = -11008.2158203125
Iteration 15800: Loss = -11008.2138671875
Iteration 15900: Loss = -11008.2119140625
Iteration 16000: Loss = -11008.2119140625
Iteration 16100: Loss = -11008.2099609375
Iteration 16200: Loss = -11008.208984375
Iteration 16300: Loss = -11008.20703125
Iteration 16400: Loss = -11008.20703125
Iteration 16500: Loss = -11008.2060546875
Iteration 16600: Loss = -11008.2060546875
Iteration 16700: Loss = -11008.2060546875
Iteration 16800: Loss = -11008.2080078125
1
Iteration 16900: Loss = -11008.2060546875
Iteration 17000: Loss = -11008.2060546875
Iteration 17100: Loss = -11008.205078125
Iteration 17200: Loss = -11008.205078125
Iteration 17300: Loss = -11008.2060546875
1
Iteration 17400: Loss = -11008.2060546875
2
Iteration 17500: Loss = -11008.2041015625
Iteration 17600: Loss = -11008.2041015625
Iteration 17700: Loss = -11008.205078125
1
Iteration 17800: Loss = -11008.2041015625
Iteration 17900: Loss = -11008.2041015625
Iteration 18000: Loss = -11008.2041015625
Iteration 18100: Loss = -11008.2041015625
Iteration 18200: Loss = -11008.2041015625
Iteration 18300: Loss = -11008.2041015625
Iteration 18400: Loss = -11008.2041015625
Iteration 18500: Loss = -11008.2041015625
Iteration 18600: Loss = -11008.205078125
1
Iteration 18700: Loss = -11008.205078125
2
Iteration 18800: Loss = -11008.2041015625
Iteration 18900: Loss = -11008.205078125
1
Iteration 19000: Loss = -11008.2041015625
Iteration 19100: Loss = -11008.2041015625
Iteration 19200: Loss = -11008.2041015625
Iteration 19300: Loss = -11008.205078125
1
Iteration 19400: Loss = -11008.2041015625
Iteration 19500: Loss = -11008.2041015625
Iteration 19600: Loss = -11008.2041015625
Iteration 19700: Loss = -11008.205078125
1
Iteration 19800: Loss = -11008.2041015625
Iteration 19900: Loss = -11008.203125
Iteration 20000: Loss = -11008.2041015625
1
Iteration 20100: Loss = -11008.205078125
2
Iteration 20200: Loss = -11008.205078125
3
Iteration 20300: Loss = -11008.205078125
4
Iteration 20400: Loss = -11008.205078125
5
Iteration 20500: Loss = -11008.2041015625
6
Iteration 20600: Loss = -11008.205078125
7
Iteration 20700: Loss = -11008.203125
Iteration 20800: Loss = -11008.203125
Iteration 20900: Loss = -11008.2080078125
1
Iteration 21000: Loss = -11008.203125
Iteration 21100: Loss = -11008.2041015625
1
Iteration 21200: Loss = -11008.203125
Iteration 21300: Loss = -11008.205078125
1
Iteration 21400: Loss = -11008.2041015625
2
Iteration 21500: Loss = -11008.2041015625
3
Iteration 21600: Loss = -11008.203125
Iteration 21700: Loss = -11008.2041015625
1
Iteration 21800: Loss = -11008.2041015625
2
Iteration 21900: Loss = -11008.2021484375
Iteration 22000: Loss = -11008.203125
1
Iteration 22100: Loss = -11008.2041015625
2
Iteration 22200: Loss = -11008.205078125
3
Iteration 22300: Loss = -11008.203125
4
Iteration 22400: Loss = -11008.2041015625
5
Iteration 22500: Loss = -11008.205078125
6
Iteration 22600: Loss = -11008.2041015625
7
Iteration 22700: Loss = -11008.2041015625
8
Iteration 22800: Loss = -11008.2041015625
9
Iteration 22900: Loss = -11008.203125
10
Iteration 23000: Loss = -11008.19921875
Iteration 23100: Loss = -11005.1806640625
Iteration 23200: Loss = -11005.16015625
Iteration 23300: Loss = -11005.1552734375
Iteration 23400: Loss = -11005.154296875
Iteration 23500: Loss = -11005.1513671875
Iteration 23600: Loss = -11005.150390625
Iteration 23700: Loss = -11005.1513671875
1
Iteration 23800: Loss = -11005.154296875
2
Iteration 23900: Loss = -11005.1494140625
Iteration 24000: Loss = -11005.1513671875
1
Iteration 24100: Loss = -11005.1494140625
Iteration 24200: Loss = -11005.1474609375
Iteration 24300: Loss = -11005.1494140625
1
Iteration 24400: Loss = -11005.150390625
2
Iteration 24500: Loss = -11005.1484375
3
Iteration 24600: Loss = -11005.1494140625
4
Iteration 24700: Loss = -11005.1484375
5
Iteration 24800: Loss = -11005.1494140625
6
Iteration 24900: Loss = -11005.1474609375
Iteration 25000: Loss = -11005.1484375
1
Iteration 25100: Loss = -11005.1474609375
Iteration 25200: Loss = -11005.1494140625
1
Iteration 25300: Loss = -11005.1494140625
2
Iteration 25400: Loss = -11005.1484375
3
Iteration 25500: Loss = -11005.1494140625
4
Iteration 25600: Loss = -11005.1494140625
5
Iteration 25700: Loss = -11005.1474609375
Iteration 25800: Loss = -11005.1494140625
1
Iteration 25900: Loss = -11005.1494140625
2
Iteration 26000: Loss = -11005.1494140625
3
Iteration 26100: Loss = -11005.1494140625
4
Iteration 26200: Loss = -11005.1474609375
Iteration 26300: Loss = -11005.1494140625
1
Iteration 26400: Loss = -11005.1494140625
2
Iteration 26500: Loss = -11005.150390625
3
Iteration 26600: Loss = -11005.1484375
4
Iteration 26700: Loss = -11005.1494140625
5
Iteration 26800: Loss = -11005.1494140625
6
Iteration 26900: Loss = -11005.1474609375
Iteration 27000: Loss = -11005.1484375
1
Iteration 27100: Loss = -11005.1484375
2
Iteration 27200: Loss = -11005.1494140625
3
Iteration 27300: Loss = -11005.1484375
4
Iteration 27400: Loss = -11005.1474609375
Iteration 27500: Loss = -11005.1494140625
1
Iteration 27600: Loss = -11005.1484375
2
Iteration 27700: Loss = -11005.1494140625
3
Iteration 27800: Loss = -11005.1494140625
4
Iteration 27900: Loss = -11005.1484375
5
Iteration 28000: Loss = -11005.1494140625
6
Iteration 28100: Loss = -11005.1484375
7
Iteration 28200: Loss = -11005.1494140625
8
Iteration 28300: Loss = -11005.1484375
9
Iteration 28400: Loss = -11005.1484375
10
Iteration 28500: Loss = -11005.1474609375
Iteration 28600: Loss = -11005.1494140625
1
Iteration 28700: Loss = -11005.1484375
2
Iteration 28800: Loss = -11005.1494140625
3
Iteration 28900: Loss = -11005.1474609375
Iteration 29000: Loss = -11005.1484375
1
Iteration 29100: Loss = -11005.1484375
2
Iteration 29200: Loss = -11005.1484375
3
Iteration 29300: Loss = -11005.1494140625
4
Iteration 29400: Loss = -11005.1474609375
Iteration 29500: Loss = -11005.1474609375
Iteration 29600: Loss = -11005.1494140625
1
Iteration 29700: Loss = -11005.1474609375
Iteration 29800: Loss = -11005.1474609375
Iteration 29900: Loss = -11005.1484375
1
pi: tensor([[9.9999e-01, 8.5635e-06],
        [7.8761e-01, 2.1239e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8799, 0.1201], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1624, 0.1993],
         [0.0633, 0.2568]],

        [[0.9932, 0.0596],
         [0.0237, 0.8078]],

        [[0.4599, 0.1135],
         [0.0672, 0.8737]],

        [[0.8858, 0.1526],
         [0.8883, 0.9904]],

        [[0.2864, 0.1599],
         [0.9921, 0.4667]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -6.771069336677129e-05
Average Adjusted Rand Index: -0.0011311850429373803
[-0.0003429436742078051, -6.771069336677129e-05] [0.0001617442499919128, -0.0011311850429373803] [11008.9892578125, 11005.1484375]
-------------------------------------
This iteration is 55
True Objective function: Loss = -10813.701695665199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43518.2109375
Iteration 100: Loss = -24051.484375
Iteration 200: Loss = -14176.830078125
Iteration 300: Loss = -11991.703125
Iteration 400: Loss = -11487.5419921875
Iteration 500: Loss = -11259.150390625
Iteration 600: Loss = -11132.4208984375
Iteration 700: Loss = -11062.3955078125
Iteration 800: Loss = -11020.6318359375
Iteration 900: Loss = -10987.841796875
Iteration 1000: Loss = -10972.4462890625
Iteration 1100: Loss = -10962.541015625
Iteration 1200: Loss = -10955.0087890625
Iteration 1300: Loss = -10944.9228515625
Iteration 1400: Loss = -10938.134765625
Iteration 1500: Loss = -10934.142578125
Iteration 1600: Loss = -10930.9208984375
Iteration 1700: Loss = -10928.234375
Iteration 1800: Loss = -10925.95703125
Iteration 1900: Loss = -10923.982421875
Iteration 2000: Loss = -10922.0322265625
Iteration 2100: Loss = -10920.548828125
Iteration 2200: Loss = -10919.2470703125
Iteration 2300: Loss = -10918.09375
Iteration 2400: Loss = -10917.0654296875
Iteration 2500: Loss = -10916.14453125
Iteration 2600: Loss = -10915.3154296875
Iteration 2700: Loss = -10914.56640625
Iteration 2800: Loss = -10913.88671875
Iteration 2900: Loss = -10913.26171875
Iteration 3000: Loss = -10912.6435546875
Iteration 3100: Loss = -10910.201171875
Iteration 3200: Loss = -10909.3486328125
Iteration 3300: Loss = -10908.552734375
Iteration 3400: Loss = -10903.99609375
Iteration 3500: Loss = -10903.265625
Iteration 3600: Loss = -10902.7705078125
Iteration 3700: Loss = -10902.3740234375
Iteration 3800: Loss = -10902.0361328125
Iteration 3900: Loss = -10901.7392578125
Iteration 4000: Loss = -10901.4736328125
Iteration 4100: Loss = -10901.232421875
Iteration 4200: Loss = -10901.0107421875
Iteration 4300: Loss = -10900.8056640625
Iteration 4400: Loss = -10899.03125
Iteration 4500: Loss = -10895.12109375
Iteration 4600: Loss = -10894.7783203125
Iteration 4700: Loss = -10894.5263671875
Iteration 4800: Loss = -10894.330078125
Iteration 4900: Loss = -10894.1591796875
Iteration 5000: Loss = -10894.005859375
Iteration 5100: Loss = -10893.8671875
Iteration 5200: Loss = -10893.7392578125
Iteration 5300: Loss = -10893.6201171875
Iteration 5400: Loss = -10893.509765625
Iteration 5500: Loss = -10893.408203125
Iteration 5600: Loss = -10893.3115234375
Iteration 5700: Loss = -10893.2216796875
Iteration 5800: Loss = -10893.134765625
Iteration 5900: Loss = -10893.052734375
Iteration 6000: Loss = -10892.974609375
Iteration 6100: Loss = -10892.8984375
Iteration 6200: Loss = -10892.826171875
Iteration 6300: Loss = -10892.7529296875
Iteration 6400: Loss = -10892.6826171875
Iteration 6500: Loss = -10892.6123046875
Iteration 6600: Loss = -10892.5400390625
Iteration 6700: Loss = -10892.4658203125
Iteration 6800: Loss = -10892.39453125
Iteration 6900: Loss = -10892.326171875
Iteration 7000: Loss = -10892.2626953125
Iteration 7100: Loss = -10892.203125
Iteration 7200: Loss = -10892.146484375
Iteration 7300: Loss = -10892.0966796875
Iteration 7400: Loss = -10892.048828125
Iteration 7500: Loss = -10892.0068359375
Iteration 7600: Loss = -10891.966796875
Iteration 7700: Loss = -10891.9296875
Iteration 7800: Loss = -10891.8955078125
Iteration 7900: Loss = -10891.8642578125
Iteration 8000: Loss = -10891.8359375
Iteration 8100: Loss = -10891.806640625
Iteration 8200: Loss = -10891.78125
Iteration 8300: Loss = -10891.7568359375
Iteration 8400: Loss = -10891.732421875
Iteration 8500: Loss = -10891.712890625
Iteration 8600: Loss = -10891.69140625
Iteration 8700: Loss = -10891.6708984375
Iteration 8800: Loss = -10891.6533203125
Iteration 8900: Loss = -10891.6357421875
Iteration 9000: Loss = -10891.619140625
Iteration 9100: Loss = -10891.6025390625
Iteration 9200: Loss = -10891.5869140625
Iteration 9300: Loss = -10891.5712890625
Iteration 9400: Loss = -10891.556640625
Iteration 9500: Loss = -10891.54296875
Iteration 9600: Loss = -10891.529296875
Iteration 9700: Loss = -10891.5146484375
Iteration 9800: Loss = -10891.501953125
Iteration 9900: Loss = -10891.490234375
Iteration 10000: Loss = -10891.4755859375
Iteration 10100: Loss = -10891.462890625
Iteration 10200: Loss = -10891.4521484375
Iteration 10300: Loss = -10891.4365234375
Iteration 10400: Loss = -10891.4248046875
Iteration 10500: Loss = -10891.412109375
Iteration 10600: Loss = -10891.400390625
Iteration 10700: Loss = -10891.38671875
Iteration 10800: Loss = -10891.3740234375
Iteration 10900: Loss = -10891.353515625
Iteration 11000: Loss = -10891.333984375
Iteration 11100: Loss = -10891.314453125
Iteration 11200: Loss = -10891.2890625
Iteration 11300: Loss = -10891.265625
Iteration 11400: Loss = -10891.2373046875
Iteration 11500: Loss = -10891.1923828125
Iteration 11600: Loss = -10891.1611328125
Iteration 11700: Loss = -10891.126953125
Iteration 11800: Loss = -10891.099609375
Iteration 11900: Loss = -10891.0537109375
Iteration 12000: Loss = -10890.9970703125
Iteration 12100: Loss = -10890.9228515625
Iteration 12200: Loss = -10890.849609375
Iteration 12300: Loss = -10890.76953125
Iteration 12400: Loss = -10890.6357421875
Iteration 12500: Loss = -10890.42578125
Iteration 12600: Loss = -10890.0537109375
Iteration 12700: Loss = -10889.5908203125
Iteration 12800: Loss = -10888.6279296875
Iteration 12900: Loss = -10886.9267578125
Iteration 13000: Loss = -10883.755859375
Iteration 13100: Loss = -10883.5234375
Iteration 13200: Loss = -10883.42578125
Iteration 13300: Loss = -10883.3720703125
Iteration 13400: Loss = -10883.3359375
Iteration 13500: Loss = -10883.306640625
Iteration 13600: Loss = -10883.2021484375
Iteration 13700: Loss = -10883.1240234375
Iteration 13800: Loss = -10882.923828125
Iteration 13900: Loss = -10882.9130859375
Iteration 14000: Loss = -10882.904296875
Iteration 14100: Loss = -10882.892578125
Iteration 14200: Loss = -10882.865234375
Iteration 14300: Loss = -10882.806640625
Iteration 14400: Loss = -10882.7900390625
Iteration 14500: Loss = -10882.7841796875
Iteration 14600: Loss = -10882.7802734375
Iteration 14700: Loss = -10882.7763671875
Iteration 14800: Loss = -10882.771484375
Iteration 14900: Loss = -10882.771484375
Iteration 15000: Loss = -10882.7666015625
Iteration 15100: Loss = -10882.7666015625
Iteration 15200: Loss = -10882.765625
Iteration 15300: Loss = -10882.7587890625
Iteration 15400: Loss = -10882.75390625
Iteration 15500: Loss = -10882.7119140625
Iteration 15600: Loss = -10882.708984375
Iteration 15700: Loss = -10882.70703125
Iteration 15800: Loss = -10882.6943359375
Iteration 15900: Loss = -10882.6875
Iteration 16000: Loss = -10882.6796875
Iteration 16100: Loss = -10882.67578125
Iteration 16200: Loss = -10882.6748046875
Iteration 16300: Loss = -10882.673828125
Iteration 16400: Loss = -10882.67578125
1
Iteration 16500: Loss = -10882.6708984375
Iteration 16600: Loss = -10882.666015625
Iteration 16700: Loss = -10882.615234375
Iteration 16800: Loss = -10882.615234375
Iteration 16900: Loss = -10882.61328125
Iteration 17000: Loss = -10882.61328125
Iteration 17100: Loss = -10882.6123046875
Iteration 17200: Loss = -10882.5576171875
Iteration 17300: Loss = -10882.5546875
Iteration 17400: Loss = -10882.5556640625
1
Iteration 17500: Loss = -10882.5537109375
Iteration 17600: Loss = -10882.5498046875
Iteration 17700: Loss = -10882.546875
Iteration 17800: Loss = -10882.544921875
Iteration 17900: Loss = -10882.544921875
Iteration 18000: Loss = -10882.5419921875
Iteration 18100: Loss = -10882.5439453125
1
Iteration 18200: Loss = -10882.5439453125
2
Iteration 18300: Loss = -10882.5419921875
Iteration 18400: Loss = -10882.5419921875
Iteration 18500: Loss = -10882.541015625
Iteration 18600: Loss = -10882.5400390625
Iteration 18700: Loss = -10882.521484375
Iteration 18800: Loss = -10882.513671875
Iteration 18900: Loss = -10882.5126953125
Iteration 19000: Loss = -10882.513671875
1
Iteration 19100: Loss = -10882.5126953125
Iteration 19200: Loss = -10882.509765625
Iteration 19300: Loss = -10882.5068359375
Iteration 19400: Loss = -10882.490234375
Iteration 19500: Loss = -10882.4892578125
Iteration 19600: Loss = -10882.4892578125
Iteration 19700: Loss = -10882.48828125
Iteration 19800: Loss = -10882.451171875
Iteration 19900: Loss = -10882.451171875
Iteration 20000: Loss = -10882.451171875
Iteration 20100: Loss = -10882.4521484375
1
Iteration 20200: Loss = -10882.451171875
Iteration 20300: Loss = -10882.451171875
Iteration 20400: Loss = -10882.451171875
Iteration 20500: Loss = -10882.451171875
Iteration 20600: Loss = -10882.451171875
Iteration 20700: Loss = -10882.4482421875
Iteration 20800: Loss = -10882.451171875
1
Iteration 20900: Loss = -10882.4501953125
2
Iteration 21000: Loss = -10882.4521484375
3
Iteration 21100: Loss = -10882.4501953125
4
Iteration 21200: Loss = -10882.4501953125
5
Iteration 21300: Loss = -10882.4501953125
6
Iteration 21400: Loss = -10882.4501953125
7
Iteration 21500: Loss = -10882.4501953125
8
Iteration 21600: Loss = -10882.4501953125
9
Iteration 21700: Loss = -10882.447265625
Iteration 21800: Loss = -10882.4453125
Iteration 21900: Loss = -10882.443359375
Iteration 22000: Loss = -10882.4423828125
Iteration 22100: Loss = -10882.4443359375
1
Iteration 22200: Loss = -10882.443359375
2
Iteration 22300: Loss = -10882.396484375
Iteration 22400: Loss = -10882.396484375
Iteration 22500: Loss = -10882.396484375
Iteration 22600: Loss = -10882.396484375
Iteration 22700: Loss = -10882.396484375
Iteration 22800: Loss = -10882.3955078125
Iteration 22900: Loss = -10882.3720703125
Iteration 23000: Loss = -10882.3603515625
Iteration 23100: Loss = -10882.357421875
Iteration 23200: Loss = -10882.3310546875
Iteration 23300: Loss = -10882.3310546875
Iteration 23400: Loss = -10882.330078125
Iteration 23500: Loss = -10882.302734375
Iteration 23600: Loss = -10882.2958984375
Iteration 23700: Loss = -10882.2958984375
Iteration 23800: Loss = -10882.2958984375
Iteration 23900: Loss = -10882.2958984375
Iteration 24000: Loss = -10882.2958984375
Iteration 24100: Loss = -10882.296875
1
Iteration 24200: Loss = -10882.2958984375
Iteration 24300: Loss = -10882.2958984375
Iteration 24400: Loss = -10882.2958984375
Iteration 24500: Loss = -10882.2958984375
Iteration 24600: Loss = -10882.2958984375
Iteration 24700: Loss = -10882.2958984375
Iteration 24800: Loss = -10882.2958984375
Iteration 24900: Loss = -10882.2958984375
Iteration 25000: Loss = -10882.2763671875
Iteration 25100: Loss = -10882.2734375
Iteration 25200: Loss = -10882.2724609375
Iteration 25300: Loss = -10882.265625
Iteration 25400: Loss = -10882.2607421875
Iteration 25500: Loss = -10882.25390625
Iteration 25600: Loss = -10882.25390625
Iteration 25700: Loss = -10882.25390625
Iteration 25800: Loss = -10882.2216796875
Iteration 25900: Loss = -10882.21875
Iteration 26000: Loss = -10882.21875
Iteration 26100: Loss = -10882.212890625
Iteration 26200: Loss = -10882.1201171875
Iteration 26300: Loss = -10882.111328125
Iteration 26400: Loss = -10882.10546875
Iteration 26500: Loss = -10882.1064453125
1
Iteration 26600: Loss = -10882.10546875
Iteration 26700: Loss = -10881.9609375
Iteration 26800: Loss = -10881.95703125
Iteration 26900: Loss = -10881.94921875
Iteration 27000: Loss = -10881.9443359375
Iteration 27100: Loss = -10881.935546875
Iteration 27200: Loss = -10881.9296875
Iteration 27300: Loss = -10881.919921875
Iteration 27400: Loss = -10881.919921875
Iteration 27500: Loss = -10881.900390625
Iteration 27600: Loss = -10881.89453125
Iteration 27700: Loss = -10881.89453125
Iteration 27800: Loss = -10881.8876953125
Iteration 27900: Loss = -10881.8837890625
Iteration 28000: Loss = -10881.8798828125
Iteration 28100: Loss = -10881.87890625
Iteration 28200: Loss = -10881.8779296875
Iteration 28300: Loss = -10881.8779296875
Iteration 28400: Loss = -10881.876953125
Iteration 28500: Loss = -10881.873046875
Iteration 28600: Loss = -10881.8720703125
Iteration 28700: Loss = -10881.8701171875
Iteration 28800: Loss = -10881.8681640625
Iteration 28900: Loss = -10881.865234375
Iteration 29000: Loss = -10881.8671875
1
Iteration 29100: Loss = -10881.865234375
Iteration 29200: Loss = -10881.8486328125
Iteration 29300: Loss = -10881.8466796875
Iteration 29400: Loss = -10881.845703125
Iteration 29500: Loss = -10881.8447265625
Iteration 29600: Loss = -10881.84375
Iteration 29700: Loss = -10881.84375
Iteration 29800: Loss = -10881.841796875
Iteration 29900: Loss = -10881.841796875
pi: tensor([[9.9999e-01, 1.4867e-05],
        [1.0127e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0970, 0.9030], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2564, 0.1609],
         [0.0985, 0.1560]],

        [[0.3124, 0.1315],
         [0.0691, 0.0408]],

        [[0.6789, 0.2329],
         [0.0929, 0.6974]],

        [[0.9164, 0.1746],
         [0.0465, 0.0093]],

        [[0.0374, 0.1739],
         [0.5197, 0.1220]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.012598425196850394
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.030351909206177637
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.022328462449853457
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.01333633701284073
Global Adjusted Rand Index: 0.004688985357976355
Average Adjusted Rand Index: 0.0022904909399588907
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34109.625
Iteration 100: Loss = -19842.10546875
Iteration 200: Loss = -13026.7607421875
Iteration 300: Loss = -11565.4091796875
Iteration 400: Loss = -11252.1650390625
Iteration 500: Loss = -11121.3603515625
Iteration 600: Loss = -11058.7822265625
Iteration 700: Loss = -11011.3212890625
Iteration 800: Loss = -10982.958984375
Iteration 900: Loss = -10967.140625
Iteration 1000: Loss = -10953.8212890625
Iteration 1100: Loss = -10945.7138671875
Iteration 1200: Loss = -10939.4453125
Iteration 1300: Loss = -10934.4814453125
Iteration 1400: Loss = -10930.4462890625
Iteration 1500: Loss = -10927.125
Iteration 1600: Loss = -10924.4228515625
Iteration 1700: Loss = -10918.970703125
Iteration 1800: Loss = -10917.025390625
Iteration 1900: Loss = -10915.4560546875
Iteration 2000: Loss = -10909.9326171875
Iteration 2100: Loss = -10907.998046875
Iteration 2200: Loss = -10902.978515625
Iteration 2300: Loss = -10901.5830078125
Iteration 2400: Loss = -10900.625
Iteration 2500: Loss = -10899.83203125
Iteration 2600: Loss = -10899.1484375
Iteration 2700: Loss = -10898.552734375
Iteration 2800: Loss = -10898.021484375
Iteration 2900: Loss = -10897.55078125
Iteration 3000: Loss = -10897.1279296875
Iteration 3100: Loss = -10896.7451171875
Iteration 3200: Loss = -10896.3974609375
Iteration 3300: Loss = -10896.083984375
Iteration 3400: Loss = -10895.796875
Iteration 3500: Loss = -10895.53125
Iteration 3600: Loss = -10895.291015625
Iteration 3700: Loss = -10895.068359375
Iteration 3800: Loss = -10894.8603515625
Iteration 3900: Loss = -10894.671875
Iteration 4000: Loss = -10894.494140625
Iteration 4100: Loss = -10894.330078125
Iteration 4200: Loss = -10894.1787109375
Iteration 4300: Loss = -10894.0361328125
Iteration 4400: Loss = -10893.904296875
Iteration 4500: Loss = -10893.78125
Iteration 4600: Loss = -10893.6650390625
Iteration 4700: Loss = -10893.556640625
Iteration 4800: Loss = -10893.4580078125
Iteration 4900: Loss = -10893.3623046875
Iteration 5000: Loss = -10893.2744140625
Iteration 5100: Loss = -10893.1904296875
Iteration 5200: Loss = -10893.11328125
Iteration 5300: Loss = -10893.0390625
Iteration 5400: Loss = -10892.970703125
Iteration 5500: Loss = -10892.9052734375
Iteration 5600: Loss = -10892.845703125
Iteration 5700: Loss = -10892.7861328125
Iteration 5800: Loss = -10892.7314453125
Iteration 5900: Loss = -10892.6806640625
Iteration 6000: Loss = -10892.6318359375
Iteration 6100: Loss = -10892.5849609375
Iteration 6200: Loss = -10892.5419921875
Iteration 6300: Loss = -10892.5009765625
Iteration 6400: Loss = -10892.4599609375
Iteration 6500: Loss = -10892.423828125
Iteration 6600: Loss = -10892.3896484375
Iteration 6700: Loss = -10892.35546875
Iteration 6800: Loss = -10892.32421875
Iteration 6900: Loss = -10892.2958984375
Iteration 7000: Loss = -10892.265625
Iteration 7100: Loss = -10892.2373046875
Iteration 7200: Loss = -10892.212890625
Iteration 7300: Loss = -10892.1865234375
Iteration 7400: Loss = -10892.1650390625
Iteration 7500: Loss = -10892.1416015625
Iteration 7600: Loss = -10892.1171875
Iteration 7700: Loss = -10892.0966796875
Iteration 7800: Loss = -10892.0751953125
Iteration 7900: Loss = -10892.0537109375
Iteration 8000: Loss = -10892.03125
Iteration 8100: Loss = -10892.0068359375
Iteration 8200: Loss = -10891.9755859375
Iteration 8300: Loss = -10891.9248046875
Iteration 8400: Loss = -10891.8544921875
Iteration 8500: Loss = -10891.7724609375
Iteration 8600: Loss = -10891.6962890625
Iteration 8700: Loss = -10891.634765625
Iteration 8800: Loss = -10891.5791015625
Iteration 8900: Loss = -10891.52734375
Iteration 9000: Loss = -10891.4736328125
Iteration 9100: Loss = -10891.421875
Iteration 9200: Loss = -10891.369140625
Iteration 9300: Loss = -10891.3173828125
Iteration 9400: Loss = -10891.1865234375
Iteration 9500: Loss = -10890.9091796875
Iteration 9600: Loss = -10890.81640625
Iteration 9700: Loss = -10890.7421875
Iteration 9800: Loss = -10890.6728515625
Iteration 9900: Loss = -10890.607421875
Iteration 10000: Loss = -10890.548828125
Iteration 10100: Loss = -10890.458984375
Iteration 10200: Loss = -10890.2490234375
Iteration 10300: Loss = -10889.9736328125
Iteration 10400: Loss = -10889.755859375
Iteration 10500: Loss = -10889.52734375
Iteration 10600: Loss = -10889.2900390625
Iteration 10700: Loss = -10889.1240234375
Iteration 10800: Loss = -10888.9619140625
Iteration 10900: Loss = -10888.765625
Iteration 11000: Loss = -10888.5078125
Iteration 11100: Loss = -10888.326171875
Iteration 11200: Loss = -10888.20703125
Iteration 11300: Loss = -10888.0302734375
Iteration 11400: Loss = -10887.904296875
Iteration 11500: Loss = -10887.861328125
Iteration 11600: Loss = -10887.7939453125
Iteration 11700: Loss = -10887.68359375
Iteration 11800: Loss = -10887.5732421875
Iteration 11900: Loss = -10887.478515625
Iteration 12000: Loss = -10887.4541015625
Iteration 12100: Loss = -10887.4375
Iteration 12200: Loss = -10887.3994140625
Iteration 12300: Loss = -10887.3544921875
Iteration 12400: Loss = -10887.3173828125
Iteration 12500: Loss = -10887.244140625
Iteration 12600: Loss = -10887.2109375
Iteration 12700: Loss = -10887.1767578125
Iteration 12800: Loss = -10887.146484375
Iteration 12900: Loss = -10887.1025390625
Iteration 13000: Loss = -10887.0712890625
Iteration 13100: Loss = -10887.0478515625
Iteration 13200: Loss = -10887.0224609375
Iteration 13300: Loss = -10887.0009765625
Iteration 13400: Loss = -10886.978515625
Iteration 13500: Loss = -10886.958984375
Iteration 13600: Loss = -10886.94140625
Iteration 13700: Loss = -10886.9208984375
Iteration 13800: Loss = -10886.8994140625
Iteration 13900: Loss = -10886.8779296875
Iteration 14000: Loss = -10886.8447265625
Iteration 14100: Loss = -10886.810546875
Iteration 14200: Loss = -10886.787109375
Iteration 14300: Loss = -10886.771484375
Iteration 14400: Loss = -10886.736328125
Iteration 14500: Loss = -10886.68359375
Iteration 14600: Loss = -10886.662109375
Iteration 14700: Loss = -10886.65625
Iteration 14800: Loss = -10886.6474609375
Iteration 14900: Loss = -10886.642578125
Iteration 15000: Loss = -10886.640625
Iteration 15100: Loss = -10886.638671875
Iteration 15200: Loss = -10886.634765625
Iteration 15300: Loss = -10886.6376953125
1
Iteration 15400: Loss = -10886.634765625
Iteration 15500: Loss = -10886.6318359375
Iteration 15600: Loss = -10886.630859375
Iteration 15700: Loss = -10886.6298828125
Iteration 15800: Loss = -10886.6298828125
Iteration 15900: Loss = -10886.62890625
Iteration 16000: Loss = -10886.62890625
Iteration 16100: Loss = -10886.6298828125
1
Iteration 16200: Loss = -10886.6298828125
2
Iteration 16300: Loss = -10886.6298828125
3
Iteration 16400: Loss = -10886.6298828125
4
Iteration 16500: Loss = -10886.62890625
Iteration 16600: Loss = -10886.62890625
Iteration 16700: Loss = -10886.6298828125
1
Iteration 16800: Loss = -10886.6298828125
2
Iteration 16900: Loss = -10886.6279296875
Iteration 17000: Loss = -10886.62890625
1
Iteration 17100: Loss = -10886.6279296875
Iteration 17200: Loss = -10886.626953125
Iteration 17300: Loss = -10886.6279296875
1
Iteration 17400: Loss = -10886.62890625
2
Iteration 17500: Loss = -10886.6298828125
3
Iteration 17600: Loss = -10886.62890625
4
Iteration 17700: Loss = -10886.6298828125
5
Iteration 17800: Loss = -10886.6279296875
6
Iteration 17900: Loss = -10886.62890625
7
Iteration 18000: Loss = -10886.62890625
8
Iteration 18100: Loss = -10886.62890625
9
Iteration 18200: Loss = -10886.6298828125
10
Iteration 18300: Loss = -10886.6279296875
11
Iteration 18400: Loss = -10886.62890625
12
Iteration 18500: Loss = -10886.6279296875
13
Iteration 18600: Loss = -10886.6298828125
14
Iteration 18700: Loss = -10886.6279296875
15
Stopping early at iteration 18700 due to no improvement.
pi: tensor([[0.1532, 0.8468],
        [0.0360, 0.9640]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0355, 0.9645], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2062, 0.0836],
         [0.9895, 0.1605]],

        [[0.0078, 0.1307],
         [0.7793, 0.8697]],

        [[0.0131, 0.2513],
         [0.8922, 0.9827]],

        [[0.9657, 0.1835],
         [0.5595, 0.4639]],

        [[0.6421, 0.0885],
         [0.9581, 0.0722]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.015837733814333767
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.00035341041046094813
Global Adjusted Rand Index: -0.00018273134585092686
Average Adjusted Rand Index: -0.004120727478473802
[0.004688985357976355, -0.00018273134585092686] [0.0022904909399588907, -0.004120727478473802] [10881.8408203125, 10886.6279296875]
-------------------------------------
This iteration is 56
True Objective function: Loss = -10833.38877324616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22299.119140625
Iteration 100: Loss = -15281.4892578125
Iteration 200: Loss = -11787.603515625
Iteration 300: Loss = -11340.6826171875
Iteration 400: Loss = -11240.8173828125
Iteration 500: Loss = -11195.669921875
Iteration 600: Loss = -11171.9013671875
Iteration 700: Loss = -11152.5537109375
Iteration 800: Loss = -11138.4794921875
Iteration 900: Loss = -11129.4140625
Iteration 1000: Loss = -11118.123046875
Iteration 1100: Loss = -11109.091796875
Iteration 1200: Loss = -11102.701171875
Iteration 1300: Loss = -11097.236328125
Iteration 1400: Loss = -11093.017578125
Iteration 1500: Loss = -11089.3408203125
Iteration 1600: Loss = -11084.453125
Iteration 1700: Loss = -11081.0615234375
Iteration 1800: Loss = -11074.765625
Iteration 1900: Loss = -11069.6103515625
Iteration 2000: Loss = -11066.54296875
Iteration 2100: Loss = -11064.2041015625
Iteration 2200: Loss = -11058.3369140625
Iteration 2300: Loss = -11054.4287109375
Iteration 2400: Loss = -11049.232421875
Iteration 2500: Loss = -11045.9208984375
Iteration 2600: Loss = -11043.947265625
Iteration 2700: Loss = -11042.7138671875
Iteration 2800: Loss = -11041.8310546875
Iteration 2900: Loss = -11040.865234375
Iteration 3000: Loss = -11037.2138671875
Iteration 3100: Loss = -11035.53515625
Iteration 3200: Loss = -11034.611328125
Iteration 3300: Loss = -11034.0498046875
Iteration 3400: Loss = -11033.654296875
Iteration 3500: Loss = -11033.349609375
Iteration 3600: Loss = -11033.1025390625
Iteration 3700: Loss = -11032.89453125
Iteration 3800: Loss = -11032.71484375
Iteration 3900: Loss = -11032.5517578125
Iteration 4000: Loss = -11032.4111328125
Iteration 4100: Loss = -11032.2841796875
Iteration 4200: Loss = -11032.1689453125
Iteration 4300: Loss = -11032.0654296875
Iteration 4400: Loss = -11031.9716796875
Iteration 4500: Loss = -11031.8857421875
Iteration 4600: Loss = -11031.8056640625
Iteration 4700: Loss = -11031.7333984375
Iteration 4800: Loss = -11031.6630859375
Iteration 4900: Loss = -11031.595703125
Iteration 5000: Loss = -11031.5244140625
Iteration 5100: Loss = -11031.443359375
Iteration 5200: Loss = -11031.3544921875
Iteration 5300: Loss = -11031.2734375
Iteration 5400: Loss = -11031.2060546875
Iteration 5500: Loss = -11031.1474609375
Iteration 5600: Loss = -11031.09765625
Iteration 5700: Loss = -11031.0537109375
Iteration 5800: Loss = -11030.994140625
Iteration 5900: Loss = -11026.6376953125
Iteration 6000: Loss = -11026.2275390625
Iteration 6100: Loss = -11026.080078125
Iteration 6200: Loss = -11025.9892578125
Iteration 6300: Loss = -11025.92578125
Iteration 6400: Loss = -11025.876953125
Iteration 6500: Loss = -11025.8349609375
Iteration 6600: Loss = -11025.798828125
Iteration 6700: Loss = -11025.7587890625
Iteration 6800: Loss = -11025.7109375
Iteration 6900: Loss = -11025.6845703125
Iteration 7000: Loss = -11025.6630859375
Iteration 7100: Loss = -11025.64453125
Iteration 7200: Loss = -11025.6279296875
Iteration 7300: Loss = -11025.6083984375
Iteration 7400: Loss = -11025.59375
Iteration 7500: Loss = -11025.5751953125
Iteration 7600: Loss = -11025.5595703125
Iteration 7700: Loss = -11025.5439453125
Iteration 7800: Loss = -11025.5322265625
Iteration 7900: Loss = -11025.5185546875
Iteration 8000: Loss = -11025.48828125
Iteration 8100: Loss = -11025.4423828125
Iteration 8200: Loss = -11025.419921875
Iteration 8300: Loss = -11025.408203125
Iteration 8400: Loss = -11025.3994140625
Iteration 8500: Loss = -11025.3896484375
Iteration 8600: Loss = -11025.3818359375
Iteration 8700: Loss = -11025.375
Iteration 8800: Loss = -11025.369140625
Iteration 8900: Loss = -11025.3623046875
Iteration 9000: Loss = -11025.357421875
Iteration 9100: Loss = -11025.3515625
Iteration 9200: Loss = -11025.34765625
Iteration 9300: Loss = -11025.34375
Iteration 9400: Loss = -11025.1455078125
Iteration 9500: Loss = -11021.025390625
Iteration 9600: Loss = -11020.90234375
Iteration 9700: Loss = -11020.8486328125
Iteration 9800: Loss = -11020.8173828125
Iteration 9900: Loss = -11020.79296875
Iteration 10000: Loss = -11020.7763671875
Iteration 10100: Loss = -11020.7646484375
Iteration 10200: Loss = -11020.7529296875
Iteration 10300: Loss = -11020.7451171875
Iteration 10400: Loss = -11020.7373046875
Iteration 10500: Loss = -11020.7314453125
Iteration 10600: Loss = -11020.7255859375
Iteration 10700: Loss = -11020.7197265625
Iteration 10800: Loss = -11020.7158203125
Iteration 10900: Loss = -11020.7109375
Iteration 11000: Loss = -11020.708984375
Iteration 11100: Loss = -11020.7041015625
Iteration 11200: Loss = -11020.7021484375
Iteration 11300: Loss = -11020.69921875
Iteration 11400: Loss = -11020.697265625
Iteration 11500: Loss = -11020.6953125
Iteration 11600: Loss = -11020.693359375
Iteration 11700: Loss = -11020.689453125
Iteration 11800: Loss = -11020.6884765625
Iteration 11900: Loss = -11020.6865234375
Iteration 12000: Loss = -11020.68359375
Iteration 12100: Loss = -11020.68359375
Iteration 12200: Loss = -11020.681640625
Iteration 12300: Loss = -11020.6796875
Iteration 12400: Loss = -11020.6787109375
Iteration 12500: Loss = -11020.6767578125
Iteration 12600: Loss = -11020.6767578125
Iteration 12700: Loss = -11020.6748046875
Iteration 12800: Loss = -11020.6728515625
Iteration 12900: Loss = -11020.673828125
1
Iteration 13000: Loss = -11020.671875
Iteration 13100: Loss = -11020.6708984375
Iteration 13200: Loss = -11020.6708984375
Iteration 13300: Loss = -11020.669921875
Iteration 13400: Loss = -11020.6689453125
Iteration 13500: Loss = -11020.6689453125
Iteration 13600: Loss = -11020.666015625
Iteration 13700: Loss = -11020.6650390625
Iteration 13800: Loss = -11020.6630859375
Iteration 13900: Loss = -11020.6611328125
Iteration 14000: Loss = -11020.6611328125
Iteration 14100: Loss = -11020.66015625
Iteration 14200: Loss = -11020.6640625
1
Iteration 14300: Loss = -11020.6591796875
Iteration 14400: Loss = -11020.66015625
1
Iteration 14500: Loss = -11020.66015625
2
Iteration 14600: Loss = -11020.6591796875
Iteration 14700: Loss = -11020.6572265625
Iteration 14800: Loss = -11020.658203125
1
Iteration 14900: Loss = -11020.6591796875
2
Iteration 15000: Loss = -11020.658203125
3
Iteration 15100: Loss = -11020.658203125
4
Iteration 15200: Loss = -11020.65625
Iteration 15300: Loss = -11020.6572265625
1
Iteration 15400: Loss = -11020.65625
Iteration 15500: Loss = -11020.6572265625
1
Iteration 15600: Loss = -11020.654296875
Iteration 15700: Loss = -11020.6552734375
1
Iteration 15800: Loss = -11020.65625
2
Iteration 15900: Loss = -11020.654296875
Iteration 16000: Loss = -11020.65625
1
Iteration 16100: Loss = -11020.654296875
Iteration 16200: Loss = -11020.6572265625
1
Iteration 16300: Loss = -11020.6552734375
2
Iteration 16400: Loss = -11020.6552734375
3
Iteration 16500: Loss = -11020.6552734375
4
Iteration 16600: Loss = -11020.654296875
Iteration 16700: Loss = -11020.654296875
Iteration 16800: Loss = -11020.654296875
Iteration 16900: Loss = -11020.654296875
Iteration 17000: Loss = -11020.6533203125
Iteration 17100: Loss = -11020.6552734375
1
Iteration 17200: Loss = -11020.654296875
2
Iteration 17300: Loss = -11020.6533203125
Iteration 17400: Loss = -11020.6552734375
1
Iteration 17500: Loss = -11020.6533203125
Iteration 17600: Loss = -11020.654296875
1
Iteration 17700: Loss = -11020.65234375
Iteration 17800: Loss = -11020.65234375
Iteration 17900: Loss = -11020.6552734375
1
Iteration 18000: Loss = -11020.6533203125
2
Iteration 18100: Loss = -11020.6533203125
3
Iteration 18200: Loss = -11020.6513671875
Iteration 18300: Loss = -11020.65234375
1
Iteration 18400: Loss = -11020.65234375
2
Iteration 18500: Loss = -11020.6533203125
3
Iteration 18600: Loss = -11020.6513671875
Iteration 18700: Loss = -11020.6533203125
1
Iteration 18800: Loss = -11020.654296875
2
Iteration 18900: Loss = -11020.65234375
3
Iteration 19000: Loss = -11020.6533203125
4
Iteration 19100: Loss = -11020.6533203125
5
Iteration 19200: Loss = -11020.65234375
6
Iteration 19300: Loss = -11020.65234375
7
Iteration 19400: Loss = -11020.65234375
8
Iteration 19500: Loss = -11020.6533203125
9
Iteration 19600: Loss = -11020.654296875
10
Iteration 19700: Loss = -11020.65234375
11
Iteration 19800: Loss = -11020.6513671875
Iteration 19900: Loss = -11020.6513671875
Iteration 20000: Loss = -11020.6494140625
Iteration 20100: Loss = -11020.650390625
1
Iteration 20200: Loss = -11020.650390625
2
Iteration 20300: Loss = -11020.650390625
3
Iteration 20400: Loss = -11020.6513671875
4
Iteration 20500: Loss = -11020.6494140625
Iteration 20600: Loss = -11020.650390625
1
Iteration 20700: Loss = -11020.6494140625
Iteration 20800: Loss = -11020.650390625
1
Iteration 20900: Loss = -11020.650390625
2
Iteration 21000: Loss = -11020.6494140625
Iteration 21100: Loss = -11020.6494140625
Iteration 21200: Loss = -11020.6494140625
Iteration 21300: Loss = -11020.6494140625
Iteration 21400: Loss = -11020.6494140625
Iteration 21500: Loss = -11020.650390625
1
Iteration 21600: Loss = -11020.650390625
2
Iteration 21700: Loss = -11020.650390625
3
Iteration 21800: Loss = -11020.6513671875
4
Iteration 21900: Loss = -11020.6484375
Iteration 22000: Loss = -11020.6494140625
1
Iteration 22100: Loss = -11020.650390625
2
Iteration 22200: Loss = -11020.6494140625
3
Iteration 22300: Loss = -11020.650390625
4
Iteration 22400: Loss = -11020.6494140625
5
Iteration 22500: Loss = -11020.6513671875
6
Iteration 22600: Loss = -11020.6513671875
7
Iteration 22700: Loss = -11020.6494140625
8
Iteration 22800: Loss = -11020.6416015625
Iteration 22900: Loss = -11020.634765625
Iteration 23000: Loss = -11020.6337890625
Iteration 23100: Loss = -11020.6357421875
1
Iteration 23200: Loss = -11020.6357421875
2
Iteration 23300: Loss = -11020.6337890625
Iteration 23400: Loss = -11020.6337890625
Iteration 23500: Loss = -11020.6337890625
Iteration 23600: Loss = -11020.634765625
1
Iteration 23700: Loss = -11020.6357421875
2
Iteration 23800: Loss = -11020.634765625
3
Iteration 23900: Loss = -11020.6337890625
Iteration 24000: Loss = -11020.634765625
1
Iteration 24100: Loss = -11020.6357421875
2
Iteration 24200: Loss = -11020.6357421875
3
Iteration 24300: Loss = -11020.634765625
4
Iteration 24400: Loss = -11020.6357421875
5
Iteration 24500: Loss = -11020.6337890625
Iteration 24600: Loss = -11020.6376953125
1
Iteration 24700: Loss = -11020.6357421875
2
Iteration 24800: Loss = -11020.634765625
3
Iteration 24900: Loss = -11020.634765625
4
Iteration 25000: Loss = -11020.634765625
5
Iteration 25100: Loss = -11020.634765625
6
Iteration 25200: Loss = -11020.6357421875
7
Iteration 25300: Loss = -11020.634765625
8
Iteration 25400: Loss = -11020.634765625
9
Iteration 25500: Loss = -11020.6357421875
10
Iteration 25600: Loss = -11020.6396484375
11
Iteration 25700: Loss = -11020.6357421875
12
Iteration 25800: Loss = -11020.6337890625
Iteration 25900: Loss = -11020.6337890625
Iteration 26000: Loss = -11020.634765625
1
Iteration 26100: Loss = -11020.634765625
2
Iteration 26200: Loss = -11020.6337890625
Iteration 26300: Loss = -11020.634765625
1
Iteration 26400: Loss = -11020.63671875
2
Iteration 26500: Loss = -11020.6337890625
Iteration 26600: Loss = -11020.634765625
1
Iteration 26700: Loss = -11020.634765625
2
Iteration 26800: Loss = -11020.634765625
3
Iteration 26900: Loss = -11020.6357421875
4
Iteration 27000: Loss = -11020.6357421875
5
Iteration 27100: Loss = -11020.6357421875
6
Iteration 27200: Loss = -11020.634765625
7
Iteration 27300: Loss = -11020.6357421875
8
Iteration 27400: Loss = -11020.6357421875
9
Iteration 27500: Loss = -11020.6357421875
10
Iteration 27600: Loss = -11020.6357421875
11
Iteration 27700: Loss = -11020.6357421875
12
Iteration 27800: Loss = -11020.6357421875
13
Iteration 27900: Loss = -11020.634765625
14
Iteration 28000: Loss = -11020.634765625
15
Stopping early at iteration 28000 due to no improvement.
pi: tensor([[7.1665e-06, 9.9999e-01],
        [2.2682e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0627, 0.9373], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1059, 0.1065],
         [0.8580, 0.1649]],

        [[0.1372, 0.1783],
         [0.9807, 0.7624]],

        [[0.7918, 0.1541],
         [0.8275, 0.8873]],

        [[0.5962, 0.2517],
         [0.9271, 0.6716]],

        [[0.9633, 0.1504],
         [0.1070, 0.9635]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004929790577405324
Average Adjusted Rand Index: 0.0011834947531591965
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37000.58984375
Iteration 100: Loss = -22698.70703125
Iteration 200: Loss = -14165.2998046875
Iteration 300: Loss = -12229.169921875
Iteration 400: Loss = -11610.9921875
Iteration 500: Loss = -11380.8291015625
Iteration 600: Loss = -11284.6845703125
Iteration 700: Loss = -11220.8974609375
Iteration 800: Loss = -11177.3076171875
Iteration 900: Loss = -11129.78125
Iteration 1000: Loss = -11106.169921875
Iteration 1100: Loss = -11093.171875
Iteration 1200: Loss = -11082.4619140625
Iteration 1300: Loss = -11073.4775390625
Iteration 1400: Loss = -11063.3232421875
Iteration 1500: Loss = -11058.5556640625
Iteration 1600: Loss = -11054.9326171875
Iteration 1700: Loss = -11052.0615234375
Iteration 1800: Loss = -11049.7294921875
Iteration 1900: Loss = -11047.7841796875
Iteration 2000: Loss = -11046.13671875
Iteration 2100: Loss = -11044.73046875
Iteration 2200: Loss = -11043.51953125
Iteration 2300: Loss = -11042.4501953125
Iteration 2400: Loss = -11041.3037109375
Iteration 2500: Loss = -11038.224609375
Iteration 2600: Loss = -11035.9873046875
Iteration 2700: Loss = -11034.5869140625
Iteration 2800: Loss = -11033.5517578125
Iteration 2900: Loss = -11032.748046875
Iteration 3000: Loss = -11032.0986328125
Iteration 3100: Loss = -11031.5400390625
Iteration 3200: Loss = -11031.0693359375
Iteration 3300: Loss = -11030.6484375
Iteration 3400: Loss = -11030.27734375
Iteration 3500: Loss = -11029.9736328125
Iteration 3600: Loss = -11029.703125
Iteration 3700: Loss = -11029.4580078125
Iteration 3800: Loss = -11029.23828125
Iteration 3900: Loss = -11029.037109375
Iteration 4000: Loss = -11028.8564453125
Iteration 4100: Loss = -11028.69140625
Iteration 4200: Loss = -11028.54296875
Iteration 4300: Loss = -11028.40625
Iteration 4400: Loss = -11028.2802734375
Iteration 4500: Loss = -11028.1630859375
Iteration 4600: Loss = -11028.0498046875
Iteration 4700: Loss = -11024.541015625
Iteration 4800: Loss = -11023.9794921875
Iteration 4900: Loss = -11023.751953125
Iteration 5000: Loss = -11023.583984375
Iteration 5100: Loss = -11023.4453125
Iteration 5200: Loss = -11023.3291015625
Iteration 5300: Loss = -11023.2265625
Iteration 5400: Loss = -11023.134765625
Iteration 5500: Loss = -11023.0498046875
Iteration 5600: Loss = -11022.9736328125
Iteration 5700: Loss = -11022.9013671875
Iteration 5800: Loss = -11022.837890625
Iteration 5900: Loss = -11022.7763671875
Iteration 6000: Loss = -11022.71875
Iteration 6100: Loss = -11022.6650390625
Iteration 6200: Loss = -11022.615234375
Iteration 6300: Loss = -11022.568359375
Iteration 6400: Loss = -11022.5234375
Iteration 6500: Loss = -11022.482421875
Iteration 6600: Loss = -11022.4423828125
Iteration 6700: Loss = -11022.4033203125
Iteration 6800: Loss = -11022.3681640625
Iteration 6900: Loss = -11022.3349609375
Iteration 7000: Loss = -11022.3037109375
Iteration 7100: Loss = -11022.2734375
Iteration 7200: Loss = -11022.244140625
Iteration 7300: Loss = -11022.2158203125
Iteration 7400: Loss = -11022.1904296875
Iteration 7500: Loss = -11022.1669921875
Iteration 7600: Loss = -11022.1416015625
Iteration 7700: Loss = -11022.1201171875
Iteration 7800: Loss = -11022.09765625
Iteration 7900: Loss = -11022.076171875
Iteration 8000: Loss = -11022.0556640625
Iteration 8100: Loss = -11022.03515625
Iteration 8200: Loss = -11022.015625
Iteration 8300: Loss = -11021.9951171875
Iteration 8400: Loss = -11021.978515625
Iteration 8500: Loss = -11021.9599609375
Iteration 8600: Loss = -11021.939453125
Iteration 8700: Loss = -11021.919921875
Iteration 8800: Loss = -11021.8994140625
Iteration 8900: Loss = -11021.8798828125
Iteration 9000: Loss = -11021.8603515625
Iteration 9100: Loss = -11021.8408203125
Iteration 9200: Loss = -11021.8232421875
Iteration 9300: Loss = -11021.8076171875
Iteration 9400: Loss = -11021.791015625
Iteration 9500: Loss = -11021.78125
Iteration 9600: Loss = -11021.7666015625
Iteration 9700: Loss = -11021.755859375
Iteration 9800: Loss = -11021.7451171875
Iteration 9900: Loss = -11021.7353515625
Iteration 10000: Loss = -11021.724609375
Iteration 10100: Loss = -11021.716796875
Iteration 10200: Loss = -11021.7060546875
Iteration 10300: Loss = -11021.69921875
Iteration 10400: Loss = -11021.69140625
Iteration 10500: Loss = -11021.6826171875
Iteration 10600: Loss = -11021.67578125
Iteration 10700: Loss = -11021.66796875
Iteration 10800: Loss = -11021.6611328125
Iteration 10900: Loss = -11021.6533203125
Iteration 11000: Loss = -11021.6484375
Iteration 11100: Loss = -11021.6416015625
Iteration 11200: Loss = -11021.634765625
Iteration 11300: Loss = -11021.6279296875
Iteration 11400: Loss = -11021.62109375
Iteration 11500: Loss = -11021.615234375
Iteration 11600: Loss = -11021.6083984375
Iteration 11700: Loss = -11021.6005859375
Iteration 11800: Loss = -11021.5927734375
Iteration 11900: Loss = -11021.5859375
Iteration 12000: Loss = -11021.576171875
Iteration 12100: Loss = -11021.5693359375
Iteration 12200: Loss = -11021.5576171875
Iteration 12300: Loss = -11021.546875
Iteration 12400: Loss = -11021.5341796875
Iteration 12500: Loss = -11021.5205078125
Iteration 12600: Loss = -11021.5029296875
Iteration 12700: Loss = -11021.482421875
Iteration 12800: Loss = -11021.4541015625
Iteration 12900: Loss = -11021.3408203125
Iteration 13000: Loss = -11021.203125
Iteration 13100: Loss = -11021.1669921875
Iteration 13200: Loss = -11021.1328125
Iteration 13300: Loss = -11021.091796875
Iteration 13400: Loss = -11021.0361328125
Iteration 13500: Loss = -11020.958984375
Iteration 13600: Loss = -11020.87109375
Iteration 13700: Loss = -11020.798828125
Iteration 13800: Loss = -11020.75390625
Iteration 13900: Loss = -11020.6982421875
Iteration 14000: Loss = -11020.685546875
Iteration 14100: Loss = -11020.67578125
Iteration 14200: Loss = -11020.6708984375
Iteration 14300: Loss = -11020.6650390625
Iteration 14400: Loss = -11020.6591796875
Iteration 14500: Loss = -11020.6572265625
Iteration 14600: Loss = -11020.6552734375
Iteration 14700: Loss = -11020.654296875
Iteration 14800: Loss = -11020.6533203125
Iteration 14900: Loss = -11020.6552734375
1
Iteration 15000: Loss = -11020.65234375
Iteration 15100: Loss = -11020.6513671875
Iteration 15200: Loss = -11020.6513671875
Iteration 15300: Loss = -11020.6484375
Iteration 15400: Loss = -11020.6494140625
1
Iteration 15500: Loss = -11020.6494140625
2
Iteration 15600: Loss = -11020.6494140625
3
Iteration 15700: Loss = -11020.6494140625
4
Iteration 15800: Loss = -11020.646484375
Iteration 15900: Loss = -11020.646484375
Iteration 16000: Loss = -11020.646484375
Iteration 16100: Loss = -11020.64453125
Iteration 16200: Loss = -11020.6455078125
1
Iteration 16300: Loss = -11020.64453125
Iteration 16400: Loss = -11020.6435546875
Iteration 16500: Loss = -11020.64453125
1
Iteration 16600: Loss = -11020.6435546875
Iteration 16700: Loss = -11020.642578125
Iteration 16800: Loss = -11020.642578125
Iteration 16900: Loss = -11020.642578125
Iteration 17000: Loss = -11020.642578125
Iteration 17100: Loss = -11020.642578125
Iteration 17200: Loss = -11020.642578125
Iteration 17300: Loss = -11020.640625
Iteration 17400: Loss = -11020.640625
Iteration 17500: Loss = -11020.640625
Iteration 17600: Loss = -11020.646484375
1
Iteration 17700: Loss = -11020.6416015625
2
Iteration 17800: Loss = -11020.640625
Iteration 17900: Loss = -11020.6416015625
1
Iteration 18000: Loss = -11020.640625
Iteration 18100: Loss = -11020.6396484375
Iteration 18200: Loss = -11020.6396484375
Iteration 18300: Loss = -11020.6396484375
Iteration 18400: Loss = -11020.6455078125
1
Iteration 18500: Loss = -11020.640625
2
Iteration 18600: Loss = -11020.6396484375
Iteration 18700: Loss = -11020.6396484375
Iteration 18800: Loss = -11020.640625
1
Iteration 18900: Loss = -11020.6396484375
Iteration 19000: Loss = -11020.638671875
Iteration 19100: Loss = -11020.638671875
Iteration 19200: Loss = -11020.6376953125
Iteration 19300: Loss = -11020.63671875
Iteration 19400: Loss = -11020.6357421875
Iteration 19500: Loss = -11020.6357421875
Iteration 19600: Loss = -11020.63671875
1
Iteration 19700: Loss = -11020.634765625
Iteration 19800: Loss = -11020.6357421875
1
Iteration 19900: Loss = -11020.6357421875
2
Iteration 20000: Loss = -11020.634765625
Iteration 20100: Loss = -11020.63671875
1
Iteration 20200: Loss = -11020.63671875
2
Iteration 20300: Loss = -11020.6357421875
3
Iteration 20400: Loss = -11020.63671875
4
Iteration 20500: Loss = -11020.6357421875
5
Iteration 20600: Loss = -11020.634765625
Iteration 20700: Loss = -11020.6357421875
1
Iteration 20800: Loss = -11020.63671875
2
Iteration 20900: Loss = -11020.6337890625
Iteration 21000: Loss = -11020.634765625
1
Iteration 21100: Loss = -11020.634765625
2
Iteration 21200: Loss = -11020.634765625
3
Iteration 21300: Loss = -11020.634765625
4
Iteration 21400: Loss = -11020.6357421875
5
Iteration 21500: Loss = -11020.634765625
6
Iteration 21600: Loss = -11020.634765625
7
Iteration 21700: Loss = -11020.63671875
8
Iteration 21800: Loss = -11020.634765625
9
Iteration 21900: Loss = -11020.6357421875
10
Iteration 22000: Loss = -11020.6337890625
Iteration 22100: Loss = -11020.63671875
1
Iteration 22200: Loss = -11020.634765625
2
Iteration 22300: Loss = -11020.6337890625
Iteration 22400: Loss = -11020.6357421875
1
Iteration 22500: Loss = -11020.6357421875
2
Iteration 22600: Loss = -11020.6357421875
3
Iteration 22700: Loss = -11020.6357421875
4
Iteration 22800: Loss = -11020.6357421875
5
Iteration 22900: Loss = -11020.634765625
6
Iteration 23000: Loss = -11020.634765625
7
Iteration 23100: Loss = -11020.6357421875
8
Iteration 23200: Loss = -11020.63671875
9
Iteration 23300: Loss = -11020.63671875
10
Iteration 23400: Loss = -11020.6357421875
11
Iteration 23500: Loss = -11020.634765625
12
Iteration 23600: Loss = -11020.634765625
13
Iteration 23700: Loss = -11020.634765625
14
Iteration 23800: Loss = -11020.6357421875
15
Stopping early at iteration 23800 due to no improvement.
pi: tensor([[6.0134e-03, 9.9399e-01],
        [5.4492e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0628, 0.9372], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1059, 0.1067],
         [0.0664, 0.1648]],

        [[0.0866, 0.1801],
         [0.8508, 0.0540]],

        [[0.0268, 0.3804],
         [0.3715, 0.0985]],

        [[0.0090, 0.1635],
         [0.5117, 0.5093]],

        [[0.6719, 0.1483],
         [0.9318, 0.7032]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004929790577405324
Average Adjusted Rand Index: 0.0011834947531591965
[0.0004929790577405324, 0.0004929790577405324] [0.0011834947531591965, 0.0011834947531591965] [11020.634765625, 11020.6357421875]
-------------------------------------
This iteration is 57
True Objective function: Loss = -10875.044776960178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19397.880859375
Iteration 100: Loss = -13872.4248046875
Iteration 200: Loss = -11546.3935546875
Iteration 300: Loss = -11243.4921875
Iteration 400: Loss = -11164.1689453125
Iteration 500: Loss = -11127.083984375
Iteration 600: Loss = -11106.8583984375
Iteration 700: Loss = -11095.2177734375
Iteration 800: Loss = -11083.78515625
Iteration 900: Loss = -11069.1884765625
Iteration 1000: Loss = -11059.193359375
Iteration 1100: Loss = -11049.984375
Iteration 1200: Loss = -11042.9638671875
Iteration 1300: Loss = -11035.6630859375
Iteration 1400: Loss = -11031.2353515625
Iteration 1500: Loss = -11027.3193359375
Iteration 1600: Loss = -11020.755859375
Iteration 1700: Loss = -11015.087890625
Iteration 1800: Loss = -11010.2802734375
Iteration 1900: Loss = -11007.7119140625
Iteration 2000: Loss = -11005.853515625
Iteration 2100: Loss = -11004.455078125
Iteration 2200: Loss = -11003.451171875
Iteration 2300: Loss = -11002.6416015625
Iteration 2400: Loss = -11002.013671875
Iteration 2500: Loss = -11000.876953125
Iteration 2600: Loss = -10997.365234375
Iteration 2700: Loss = -10996.5498046875
Iteration 2800: Loss = -10996.103515625
Iteration 2900: Loss = -10995.779296875
Iteration 3000: Loss = -10995.517578125
Iteration 3100: Loss = -10995.263671875
Iteration 3200: Loss = -10993.1982421875
Iteration 3300: Loss = -10992.3310546875
Iteration 3400: Loss = -10992.033203125
Iteration 3500: Loss = -10991.81640625
Iteration 3600: Loss = -10991.638671875
Iteration 3700: Loss = -10991.484375
Iteration 3800: Loss = -10991.3486328125
Iteration 3900: Loss = -10991.2294921875
Iteration 4000: Loss = -10991.123046875
Iteration 4100: Loss = -10991.0244140625
Iteration 4200: Loss = -10990.9384765625
Iteration 4300: Loss = -10990.861328125
Iteration 4400: Loss = -10990.7900390625
Iteration 4500: Loss = -10990.7255859375
Iteration 4600: Loss = -10990.6669921875
Iteration 4700: Loss = -10990.6142578125
Iteration 4800: Loss = -10990.5654296875
Iteration 4900: Loss = -10990.5205078125
Iteration 5000: Loss = -10990.48046875
Iteration 5100: Loss = -10990.4423828125
Iteration 5200: Loss = -10990.4052734375
Iteration 5300: Loss = -10990.365234375
Iteration 5400: Loss = -10988.0146484375
Iteration 5500: Loss = -10987.7587890625
Iteration 5600: Loss = -10987.671875
Iteration 5700: Loss = -10987.61328125
Iteration 5800: Loss = -10987.5693359375
Iteration 5900: Loss = -10987.529296875
Iteration 6000: Loss = -10987.4970703125
Iteration 6100: Loss = -10987.46484375
Iteration 6200: Loss = -10987.4384765625
Iteration 6300: Loss = -10987.412109375
Iteration 6400: Loss = -10987.3896484375
Iteration 6500: Loss = -10987.3671875
Iteration 6600: Loss = -10987.3486328125
Iteration 6700: Loss = -10987.3310546875
Iteration 6800: Loss = -10987.314453125
Iteration 6900: Loss = -10987.30078125
Iteration 7000: Loss = -10987.287109375
Iteration 7100: Loss = -10987.275390625
Iteration 7200: Loss = -10987.2646484375
Iteration 7300: Loss = -10987.2529296875
Iteration 7400: Loss = -10987.244140625
Iteration 7500: Loss = -10987.234375
Iteration 7600: Loss = -10987.2255859375
Iteration 7700: Loss = -10987.2197265625
Iteration 7800: Loss = -10987.2119140625
Iteration 7900: Loss = -10987.2060546875
Iteration 8000: Loss = -10987.2001953125
Iteration 8100: Loss = -10987.193359375
Iteration 8200: Loss = -10987.1875
Iteration 8300: Loss = -10987.1826171875
Iteration 8400: Loss = -10987.1796875
Iteration 8500: Loss = -10987.1748046875
Iteration 8600: Loss = -10987.169921875
Iteration 8700: Loss = -10987.1669921875
Iteration 8800: Loss = -10987.1640625
Iteration 8900: Loss = -10987.16015625
Iteration 9000: Loss = -10987.15625
Iteration 9100: Loss = -10987.1533203125
Iteration 9200: Loss = -10987.1494140625
Iteration 9300: Loss = -10987.1474609375
Iteration 9400: Loss = -10987.1455078125
Iteration 9500: Loss = -10987.142578125
Iteration 9600: Loss = -10987.1396484375
Iteration 9700: Loss = -10987.13671875
Iteration 9800: Loss = -10987.13671875
Iteration 9900: Loss = -10987.134765625
Iteration 10000: Loss = -10987.1318359375
Iteration 10100: Loss = -10987.1298828125
Iteration 10200: Loss = -10987.1279296875
Iteration 10300: Loss = -10987.1279296875
Iteration 10400: Loss = -10987.125
Iteration 10500: Loss = -10987.1240234375
Iteration 10600: Loss = -10987.1240234375
Iteration 10700: Loss = -10987.1220703125
Iteration 10800: Loss = -10987.119140625
Iteration 10900: Loss = -10987.119140625
Iteration 11000: Loss = -10987.119140625
Iteration 11100: Loss = -10987.1171875
Iteration 11200: Loss = -10987.1162109375
Iteration 11300: Loss = -10987.1142578125
Iteration 11400: Loss = -10987.11328125
Iteration 11500: Loss = -10987.11328125
Iteration 11600: Loss = -10987.11328125
Iteration 11700: Loss = -10987.11328125
Iteration 11800: Loss = -10987.111328125
Iteration 11900: Loss = -10987.1123046875
1
Iteration 12000: Loss = -10987.1103515625
Iteration 12100: Loss = -10987.1103515625
Iteration 12200: Loss = -10987.1083984375
Iteration 12300: Loss = -10987.109375
1
Iteration 12400: Loss = -10987.107421875
Iteration 12500: Loss = -10987.1064453125
Iteration 12600: Loss = -10987.1064453125
Iteration 12700: Loss = -10987.1044921875
Iteration 12800: Loss = -10987.1064453125
1
Iteration 12900: Loss = -10987.1044921875
Iteration 13000: Loss = -10987.103515625
Iteration 13100: Loss = -10987.10546875
1
Iteration 13200: Loss = -10987.103515625
Iteration 13300: Loss = -10987.103515625
Iteration 13400: Loss = -10987.1044921875
1
Iteration 13500: Loss = -10987.103515625
Iteration 13600: Loss = -10987.1015625
Iteration 13700: Loss = -10987.1025390625
1
Iteration 13800: Loss = -10987.1015625
Iteration 13900: Loss = -10987.1025390625
1
Iteration 14000: Loss = -10987.1025390625
2
Iteration 14100: Loss = -10987.1025390625
3
Iteration 14200: Loss = -10987.1015625
Iteration 14300: Loss = -10987.1005859375
Iteration 14400: Loss = -10987.1005859375
Iteration 14500: Loss = -10987.099609375
Iteration 14600: Loss = -10987.1005859375
1
Iteration 14700: Loss = -10987.099609375
Iteration 14800: Loss = -10987.099609375
Iteration 14900: Loss = -10987.099609375
Iteration 15000: Loss = -10987.099609375
Iteration 15100: Loss = -10987.099609375
Iteration 15200: Loss = -10987.1005859375
1
Iteration 15300: Loss = -10987.099609375
Iteration 15400: Loss = -10987.1005859375
1
Iteration 15500: Loss = -10987.099609375
Iteration 15600: Loss = -10987.0986328125
Iteration 15700: Loss = -10987.1005859375
1
Iteration 15800: Loss = -10987.0986328125
Iteration 15900: Loss = -10987.0986328125
Iteration 16000: Loss = -10987.0986328125
Iteration 16100: Loss = -10987.0986328125
Iteration 16200: Loss = -10987.0986328125
Iteration 16300: Loss = -10987.0986328125
Iteration 16400: Loss = -10987.1015625
1
Iteration 16500: Loss = -10987.0986328125
Iteration 16600: Loss = -10987.0966796875
Iteration 16700: Loss = -10987.09765625
1
Iteration 16800: Loss = -10987.0986328125
2
Iteration 16900: Loss = -10987.0986328125
3
Iteration 17000: Loss = -10987.09765625
4
Iteration 17100: Loss = -10987.1025390625
5
Iteration 17200: Loss = -10987.0986328125
6
Iteration 17300: Loss = -10987.0986328125
7
Iteration 17400: Loss = -10987.099609375
8
Iteration 17500: Loss = -10987.09765625
9
Iteration 17600: Loss = -10987.0986328125
10
Iteration 17700: Loss = -10987.09765625
11
Iteration 17800: Loss = -10987.09765625
12
Iteration 17900: Loss = -10987.0966796875
Iteration 18000: Loss = -10987.09765625
1
Iteration 18100: Loss = -10987.09765625
2
Iteration 18200: Loss = -10987.09765625
3
Iteration 18300: Loss = -10987.0966796875
Iteration 18400: Loss = -10987.09765625
1
Iteration 18500: Loss = -10987.09765625
2
Iteration 18600: Loss = -10987.09765625
3
Iteration 18700: Loss = -10986.8076171875
Iteration 18800: Loss = -10986.5654296875
Iteration 18900: Loss = -10986.3505859375
Iteration 19000: Loss = -10986.28125
Iteration 19100: Loss = -10986.2783203125
Iteration 19200: Loss = -10986.2763671875
Iteration 19300: Loss = -10986.263671875
Iteration 19400: Loss = -10986.2587890625
Iteration 19500: Loss = -10986.119140625
Iteration 19600: Loss = -10986.072265625
Iteration 19700: Loss = -10986.0380859375
Iteration 19800: Loss = -10985.9814453125
Iteration 19900: Loss = -10985.921875
Iteration 20000: Loss = -10985.9111328125
Iteration 20100: Loss = -10985.8740234375
Iteration 20200: Loss = -10985.806640625
Iteration 20300: Loss = -10985.802734375
Iteration 20400: Loss = -10985.7998046875
Iteration 20500: Loss = -10985.798828125
Iteration 20600: Loss = -10985.79296875
Iteration 20700: Loss = -10985.7880859375
Iteration 20800: Loss = -10985.787109375
Iteration 20900: Loss = -10985.7822265625
Iteration 21000: Loss = -10985.7734375
Iteration 21100: Loss = -10985.771484375
Iteration 21200: Loss = -10985.76953125
Iteration 21300: Loss = -10985.771484375
1
Iteration 21400: Loss = -10985.7685546875
Iteration 21500: Loss = -10985.76953125
1
Iteration 21600: Loss = -10985.767578125
Iteration 21700: Loss = -10985.765625
Iteration 21800: Loss = -10985.763671875
Iteration 21900: Loss = -10985.7626953125
Iteration 22000: Loss = -10985.76171875
Iteration 22100: Loss = -10985.76171875
Iteration 22200: Loss = -10985.76171875
Iteration 22300: Loss = -10985.759765625
Iteration 22400: Loss = -10985.7607421875
1
Iteration 22500: Loss = -10985.759765625
Iteration 22600: Loss = -10985.7607421875
1
Iteration 22700: Loss = -10985.759765625
Iteration 22800: Loss = -10985.7607421875
1
Iteration 22900: Loss = -10985.76171875
2
Iteration 23000: Loss = -10985.759765625
Iteration 23100: Loss = -10985.7607421875
1
Iteration 23200: Loss = -10985.7607421875
2
Iteration 23300: Loss = -10985.7607421875
3
Iteration 23400: Loss = -10985.7607421875
4
Iteration 23500: Loss = -10985.7607421875
5
Iteration 23600: Loss = -10985.7607421875
6
Iteration 23700: Loss = -10985.7607421875
7
Iteration 23800: Loss = -10985.759765625
Iteration 23900: Loss = -10985.7607421875
1
Iteration 24000: Loss = -10985.7607421875
2
Iteration 24100: Loss = -10985.7607421875
3
Iteration 24200: Loss = -10985.759765625
Iteration 24300: Loss = -10985.759765625
Iteration 24400: Loss = -10985.759765625
Iteration 24500: Loss = -10985.7587890625
Iteration 24600: Loss = -10985.7607421875
1
Iteration 24700: Loss = -10985.759765625
2
Iteration 24800: Loss = -10985.759765625
3
Iteration 24900: Loss = -10985.7587890625
Iteration 25000: Loss = -10985.759765625
1
Iteration 25100: Loss = -10985.7578125
Iteration 25200: Loss = -10985.7587890625
1
Iteration 25300: Loss = -10985.759765625
2
Iteration 25400: Loss = -10985.7578125
Iteration 25500: Loss = -10985.7578125
Iteration 25600: Loss = -10985.759765625
1
Iteration 25700: Loss = -10985.7578125
Iteration 25800: Loss = -10985.7587890625
1
Iteration 25900: Loss = -10985.7587890625
2
Iteration 26000: Loss = -10985.7587890625
3
Iteration 26100: Loss = -10985.7587890625
4
Iteration 26200: Loss = -10985.7587890625
5
Iteration 26300: Loss = -10985.7587890625
6
Iteration 26400: Loss = -10985.7587890625
7
Iteration 26500: Loss = -10985.7607421875
8
Iteration 26600: Loss = -10985.759765625
9
Iteration 26700: Loss = -10985.7587890625
10
Iteration 26800: Loss = -10985.7587890625
11
Iteration 26900: Loss = -10985.7587890625
12
Iteration 27000: Loss = -10985.759765625
13
Iteration 27100: Loss = -10985.7587890625
14
Iteration 27200: Loss = -10985.7587890625
15
Stopping early at iteration 27200 due to no improvement.
pi: tensor([[0.0132, 0.9868],
        [0.0185, 0.9815]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 7.7806e-06], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1633, 0.1592],
         [0.0212, 0.1604]],

        [[0.0159, 0.2342],
         [0.1770, 0.8807]],

        [[0.8622, 0.2435],
         [0.9121, 0.4634]],

        [[0.2998, 0.2911],
         [0.9557, 0.9820]],

        [[0.3351, 0.1302],
         [0.9641, 0.1026]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008720131316997135
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32754.90234375
Iteration 100: Loss = -19240.44140625
Iteration 200: Loss = -12564.7802734375
Iteration 300: Loss = -11534.705078125
Iteration 400: Loss = -11341.759765625
Iteration 500: Loss = -11253.0322265625
Iteration 600: Loss = -11189.9892578125
Iteration 700: Loss = -11135.2890625
Iteration 800: Loss = -11100.78125
Iteration 900: Loss = -11079.1787109375
Iteration 1000: Loss = -11063.4951171875
Iteration 1100: Loss = -11049.30078125
Iteration 1200: Loss = -11041.4609375
Iteration 1300: Loss = -11035.3330078125
Iteration 1400: Loss = -11029.8310546875
Iteration 1500: Loss = -11020.4794921875
Iteration 1600: Loss = -11015.2578125
Iteration 1700: Loss = -11011.5390625
Iteration 1800: Loss = -11008.8203125
Iteration 1900: Loss = -11006.7431640625
Iteration 2000: Loss = -11005.1416015625
Iteration 2100: Loss = -11003.8935546875
Iteration 2200: Loss = -11002.888671875
Iteration 2300: Loss = -11002.0703125
Iteration 2400: Loss = -11001.3828125
Iteration 2500: Loss = -11000.796875
Iteration 2600: Loss = -11000.2861328125
Iteration 2700: Loss = -10999.837890625
Iteration 2800: Loss = -10999.4306640625
Iteration 2900: Loss = -10999.0693359375
Iteration 3000: Loss = -10998.7392578125
Iteration 3100: Loss = -10993.8212890625
Iteration 3200: Loss = -10993.291015625
Iteration 3300: Loss = -10992.94921875
Iteration 3400: Loss = -10992.662109375
Iteration 3500: Loss = -10992.41015625
Iteration 3600: Loss = -10992.1845703125
Iteration 3700: Loss = -10991.9775390625
Iteration 3800: Loss = -10991.7890625
Iteration 3900: Loss = -10991.615234375
Iteration 4000: Loss = -10991.45703125
Iteration 4100: Loss = -10991.3095703125
Iteration 4200: Loss = -10991.171875
Iteration 4300: Loss = -10991.044921875
Iteration 4400: Loss = -10990.923828125
Iteration 4500: Loss = -10990.8125
Iteration 4600: Loss = -10990.708984375
Iteration 4700: Loss = -10990.611328125
Iteration 4800: Loss = -10990.517578125
Iteration 4900: Loss = -10990.4326171875
Iteration 5000: Loss = -10990.3505859375
Iteration 5100: Loss = -10990.2705078125
Iteration 5200: Loss = -10990.1875
Iteration 5300: Loss = -10989.990234375
Iteration 5400: Loss = -10989.193359375
Iteration 5500: Loss = -10988.93359375
Iteration 5600: Loss = -10988.7578125
Iteration 5700: Loss = -10988.6123046875
Iteration 5800: Loss = -10988.4833984375
Iteration 5900: Loss = -10988.37109375
Iteration 6000: Loss = -10988.265625
Iteration 6100: Loss = -10988.1689453125
Iteration 6200: Loss = -10988.0771484375
Iteration 6300: Loss = -10987.990234375
Iteration 6400: Loss = -10987.904296875
Iteration 6500: Loss = -10987.8193359375
Iteration 6600: Loss = -10987.734375
Iteration 6700: Loss = -10987.6484375
Iteration 6800: Loss = -10987.56640625
Iteration 6900: Loss = -10987.494140625
Iteration 7000: Loss = -10987.4326171875
Iteration 7100: Loss = -10987.3759765625
Iteration 7200: Loss = -10987.3271484375
Iteration 7300: Loss = -10987.2822265625
Iteration 7400: Loss = -10987.2392578125
Iteration 7500: Loss = -10987.2021484375
Iteration 7600: Loss = -10987.16796875
Iteration 7700: Loss = -10987.1357421875
Iteration 7800: Loss = -10987.103515625
Iteration 7900: Loss = -10987.07421875
Iteration 8000: Loss = -10987.0498046875
Iteration 8100: Loss = -10987.0244140625
Iteration 8200: Loss = -10987.0
Iteration 8300: Loss = -10986.9814453125
Iteration 8400: Loss = -10986.958984375
Iteration 8500: Loss = -10986.9384765625
Iteration 8600: Loss = -10986.9208984375
Iteration 8700: Loss = -10986.9033203125
Iteration 8800: Loss = -10986.884765625
Iteration 8900: Loss = -10986.869140625
Iteration 9000: Loss = -10986.8525390625
Iteration 9100: Loss = -10986.837890625
Iteration 9200: Loss = -10986.8232421875
Iteration 9300: Loss = -10986.806640625
Iteration 9400: Loss = -10986.79296875
Iteration 9500: Loss = -10986.779296875
Iteration 9600: Loss = -10986.765625
Iteration 9700: Loss = -10986.751953125
Iteration 9800: Loss = -10986.7373046875
Iteration 9900: Loss = -10986.7236328125
Iteration 10000: Loss = -10986.7109375
Iteration 10100: Loss = -10986.7001953125
Iteration 10200: Loss = -10986.6845703125
Iteration 10300: Loss = -10986.6748046875
Iteration 10400: Loss = -10986.6630859375
Iteration 10500: Loss = -10986.650390625
Iteration 10600: Loss = -10986.6396484375
Iteration 10700: Loss = -10986.62890625
Iteration 10800: Loss = -10986.6162109375
Iteration 10900: Loss = -10986.6044921875
Iteration 11000: Loss = -10986.59765625
Iteration 11100: Loss = -10986.5859375
Iteration 11200: Loss = -10986.5771484375
Iteration 11300: Loss = -10986.5673828125
Iteration 11400: Loss = -10986.5595703125
Iteration 11500: Loss = -10986.55078125
Iteration 11600: Loss = -10986.5439453125
Iteration 11700: Loss = -10986.537109375
Iteration 11800: Loss = -10986.529296875
Iteration 11900: Loss = -10986.5234375
Iteration 12000: Loss = -10986.517578125
Iteration 12100: Loss = -10986.51171875
Iteration 12200: Loss = -10986.505859375
Iteration 12300: Loss = -10986.5
Iteration 12400: Loss = -10986.4970703125
Iteration 12500: Loss = -10986.4931640625
Iteration 12600: Loss = -10986.4892578125
Iteration 12700: Loss = -10986.4853515625
Iteration 12800: Loss = -10986.482421875
Iteration 12900: Loss = -10986.4814453125
Iteration 13000: Loss = -10986.478515625
Iteration 13100: Loss = -10986.4775390625
Iteration 13200: Loss = -10986.4736328125
Iteration 13300: Loss = -10986.47265625
Iteration 13400: Loss = -10986.4697265625
Iteration 13500: Loss = -10986.4697265625
Iteration 13600: Loss = -10986.46875
Iteration 13700: Loss = -10986.466796875
Iteration 13800: Loss = -10986.4638671875
Iteration 13900: Loss = -10986.46484375
1
Iteration 14000: Loss = -10986.4638671875
Iteration 14100: Loss = -10986.4619140625
Iteration 14200: Loss = -10986.4619140625
Iteration 14300: Loss = -10986.4599609375
Iteration 14400: Loss = -10986.4609375
1
Iteration 14500: Loss = -10986.4599609375
Iteration 14600: Loss = -10986.4599609375
Iteration 14700: Loss = -10986.4609375
1
Iteration 14800: Loss = -10986.45703125
Iteration 14900: Loss = -10986.4580078125
1
Iteration 15000: Loss = -10986.4599609375
2
Iteration 15100: Loss = -10986.45703125
Iteration 15200: Loss = -10986.45703125
Iteration 15300: Loss = -10986.4560546875
Iteration 15400: Loss = -10986.4541015625
Iteration 15500: Loss = -10986.45703125
1
Iteration 15600: Loss = -10986.455078125
2
Iteration 15700: Loss = -10986.4541015625
Iteration 15800: Loss = -10986.453125
Iteration 15900: Loss = -10986.4560546875
1
Iteration 16000: Loss = -10986.453125
Iteration 16100: Loss = -10986.4541015625
1
Iteration 16200: Loss = -10986.453125
Iteration 16300: Loss = -10986.4541015625
1
Iteration 16400: Loss = -10986.453125
Iteration 16500: Loss = -10986.453125
Iteration 16600: Loss = -10986.4560546875
1
Iteration 16700: Loss = -10986.4541015625
2
Iteration 16800: Loss = -10986.4521484375
Iteration 16900: Loss = -10986.453125
1
Iteration 17000: Loss = -10986.4521484375
Iteration 17100: Loss = -10986.451171875
Iteration 17200: Loss = -10986.451171875
Iteration 17300: Loss = -10986.4521484375
1
Iteration 17400: Loss = -10986.4521484375
2
Iteration 17500: Loss = -10986.451171875
Iteration 17600: Loss = -10986.451171875
Iteration 17700: Loss = -10986.451171875
Iteration 17800: Loss = -10986.451171875
Iteration 17900: Loss = -10986.451171875
Iteration 18000: Loss = -10986.451171875
Iteration 18100: Loss = -10986.451171875
Iteration 18200: Loss = -10986.451171875
Iteration 18300: Loss = -10986.44921875
Iteration 18400: Loss = -10986.4501953125
1
Iteration 18500: Loss = -10986.4501953125
2
Iteration 18600: Loss = -10986.4326171875
Iteration 18700: Loss = -10986.431640625
Iteration 18800: Loss = -10986.4326171875
1
Iteration 18900: Loss = -10986.4326171875
2
Iteration 19000: Loss = -10986.4306640625
Iteration 19100: Loss = -10986.431640625
1
Iteration 19200: Loss = -10986.431640625
2
Iteration 19300: Loss = -10986.431640625
3
Iteration 19400: Loss = -10986.431640625
4
Iteration 19500: Loss = -10986.4306640625
Iteration 19600: Loss = -10986.431640625
1
Iteration 19700: Loss = -10986.431640625
2
Iteration 19800: Loss = -10986.431640625
3
Iteration 19900: Loss = -10986.4326171875
4
Iteration 20000: Loss = -10986.431640625
5
Iteration 20100: Loss = -10986.4306640625
Iteration 20200: Loss = -10986.42578125
Iteration 20300: Loss = -10986.4169921875
Iteration 20400: Loss = -10986.416015625
Iteration 20500: Loss = -10986.416015625
Iteration 20600: Loss = -10986.416015625
Iteration 20700: Loss = -10986.416015625
Iteration 20800: Loss = -10986.4150390625
Iteration 20900: Loss = -10986.416015625
1
Iteration 21000: Loss = -10986.4150390625
Iteration 21100: Loss = -10986.416015625
1
Iteration 21200: Loss = -10986.4140625
Iteration 21300: Loss = -10986.4150390625
1
Iteration 21400: Loss = -10986.4150390625
2
Iteration 21500: Loss = -10986.416015625
3
Iteration 21600: Loss = -10986.4150390625
4
Iteration 21700: Loss = -10986.4150390625
5
Iteration 21800: Loss = -10986.416015625
6
Iteration 21900: Loss = -10986.4150390625
7
Iteration 22000: Loss = -10986.4169921875
8
Iteration 22100: Loss = -10986.416015625
9
Iteration 22200: Loss = -10986.4140625
Iteration 22300: Loss = -10986.4140625
Iteration 22400: Loss = -10986.4150390625
1
Iteration 22500: Loss = -10986.4150390625
2
Iteration 22600: Loss = -10986.4150390625
3
Iteration 22700: Loss = -10986.416015625
4
Iteration 22800: Loss = -10986.4140625
Iteration 22900: Loss = -10986.4150390625
1
Iteration 23000: Loss = -10986.4169921875
2
Iteration 23100: Loss = -10986.41796875
3
Iteration 23200: Loss = -10986.4140625
Iteration 23300: Loss = -10986.4169921875
1
Iteration 23400: Loss = -10986.4150390625
2
Iteration 23500: Loss = -10986.4150390625
3
Iteration 23600: Loss = -10986.4150390625
4
Iteration 23700: Loss = -10986.416015625
5
Iteration 23800: Loss = -10986.416015625
6
Iteration 23900: Loss = -10986.416015625
7
Iteration 24000: Loss = -10986.4150390625
8
Iteration 24100: Loss = -10986.416015625
9
Iteration 24200: Loss = -10986.416015625
10
Iteration 24300: Loss = -10986.4169921875
11
Iteration 24400: Loss = -10986.416015625
12
Iteration 24500: Loss = -10986.1845703125
Iteration 24600: Loss = -10986.1767578125
Iteration 24700: Loss = -10986.138671875
Iteration 24800: Loss = -10986.1025390625
Iteration 24900: Loss = -10986.0771484375
Iteration 25000: Loss = -10986.0654296875
Iteration 25100: Loss = -10986.02734375
Iteration 25200: Loss = -10986.0244140625
Iteration 25300: Loss = -10986.0224609375
Iteration 25400: Loss = -10985.943359375
Iteration 25500: Loss = -10985.9423828125
Iteration 25600: Loss = -10985.9404296875
Iteration 25700: Loss = -10985.9423828125
1
Iteration 25800: Loss = -10985.9140625
Iteration 25900: Loss = -10985.904296875
Iteration 26000: Loss = -10985.873046875
Iteration 26100: Loss = -10985.833984375
Iteration 26200: Loss = -10985.8203125
Iteration 26300: Loss = -10985.8193359375
Iteration 26400: Loss = -10985.81640625
Iteration 26500: Loss = -10985.814453125
Iteration 26600: Loss = -10985.8125
Iteration 26700: Loss = -10985.8076171875
Iteration 26800: Loss = -10985.802734375
Iteration 26900: Loss = -10985.8037109375
1
Iteration 27000: Loss = -10985.7998046875
Iteration 27100: Loss = -10985.796875
Iteration 27200: Loss = -10985.7861328125
Iteration 27300: Loss = -10985.7802734375
Iteration 27400: Loss = -10985.775390625
Iteration 27500: Loss = -10985.76171875
Iteration 27600: Loss = -10985.7568359375
Iteration 27700: Loss = -10985.748046875
Iteration 27800: Loss = -10985.6435546875
Iteration 27900: Loss = -10985.5302734375
Iteration 28000: Loss = -10985.048828125
Iteration 28100: Loss = -10984.94921875
Iteration 28200: Loss = -10984.9453125
Iteration 28300: Loss = -10984.9453125
Iteration 28400: Loss = -10984.9443359375
Iteration 28500: Loss = -10984.9443359375
Iteration 28600: Loss = -10984.9423828125
Iteration 28700: Loss = -10984.9443359375
1
Iteration 28800: Loss = -10984.943359375
2
Iteration 28900: Loss = -10984.943359375
3
Iteration 29000: Loss = -10984.943359375
4
Iteration 29100: Loss = -10984.943359375
5
Iteration 29200: Loss = -10984.943359375
6
Iteration 29300: Loss = -10984.9423828125
Iteration 29400: Loss = -10984.9443359375
1
Iteration 29500: Loss = -10984.9443359375
2
Iteration 29600: Loss = -10984.9423828125
Iteration 29700: Loss = -10984.943359375
1
Iteration 29800: Loss = -10984.943359375
2
Iteration 29900: Loss = -10984.943359375
3
pi: tensor([[1.7273e-06, 1.0000e+00],
        [2.2467e-02, 9.7753e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0015, 0.9985], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1308, 0.1708],
         [0.9435, 0.1624]],

        [[0.6089, 0.0919],
         [0.5743, 0.9246]],

        [[0.8474, 0.2334],
         [0.8942, 0.7201]],

        [[0.3418, 0.2896],
         [0.4457, 0.0525]],

        [[0.9047, 0.1132],
         [0.0122, 0.0831]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -1.5600923892690762e-05
Average Adjusted Rand Index: -0.000982071485668608
[-0.0008720131316997135, -1.5600923892690762e-05] [-0.000982071485668608, -0.000982071485668608] [10985.7587890625, 10984.9453125]
-------------------------------------
This iteration is 58
True Objective function: Loss = -10909.957173668823
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34441.69140625
Iteration 100: Loss = -19679.744140625
Iteration 200: Loss = -12844.3408203125
Iteration 300: Loss = -11563.9365234375
Iteration 400: Loss = -11317.0302734375
Iteration 500: Loss = -11223.203125
Iteration 600: Loss = -11157.580078125
Iteration 700: Loss = -11122.04296875
Iteration 800: Loss = -11096.8984375
Iteration 900: Loss = -11084.427734375
Iteration 1000: Loss = -11075.5771484375
Iteration 1100: Loss = -11068.90234375
Iteration 1200: Loss = -11063.7197265625
Iteration 1300: Loss = -11059.611328125
Iteration 1400: Loss = -11056.2978515625
Iteration 1500: Loss = -11053.58203125
Iteration 1600: Loss = -11051.318359375
Iteration 1700: Loss = -11049.40234375
Iteration 1800: Loss = -11047.7529296875
Iteration 1900: Loss = -11046.3525390625
Iteration 2000: Loss = -11045.1435546875
Iteration 2100: Loss = -11044.0849609375
Iteration 2200: Loss = -11043.1513671875
Iteration 2300: Loss = -11042.326171875
Iteration 2400: Loss = -11041.58984375
Iteration 2500: Loss = -11040.931640625
Iteration 2600: Loss = -11040.33984375
Iteration 2700: Loss = -11039.8076171875
Iteration 2800: Loss = -11039.3251953125
Iteration 2900: Loss = -11038.8916015625
Iteration 3000: Loss = -11038.49609375
Iteration 3100: Loss = -11038.134765625
Iteration 3200: Loss = -11037.802734375
Iteration 3300: Loss = -11037.4990234375
Iteration 3400: Loss = -11037.2197265625
Iteration 3500: Loss = -11036.9609375
Iteration 3600: Loss = -11036.72265625
Iteration 3700: Loss = -11036.501953125
Iteration 3800: Loss = -11036.30078125
Iteration 3900: Loss = -11036.1083984375
Iteration 4000: Loss = -11035.9306640625
Iteration 4100: Loss = -11035.767578125
Iteration 4200: Loss = -11035.6162109375
Iteration 4300: Loss = -11035.4736328125
Iteration 4400: Loss = -11035.33984375
Iteration 4500: Loss = -11035.2138671875
Iteration 4600: Loss = -11035.0966796875
Iteration 4700: Loss = -11034.98828125
Iteration 4800: Loss = -11034.88671875
Iteration 4900: Loss = -11034.7900390625
Iteration 5000: Loss = -11034.69921875
Iteration 5100: Loss = -11034.6142578125
Iteration 5200: Loss = -11034.53515625
Iteration 5300: Loss = -11034.458984375
Iteration 5400: Loss = -11034.388671875
Iteration 5500: Loss = -11034.322265625
Iteration 5600: Loss = -11034.2587890625
Iteration 5700: Loss = -11034.2001953125
Iteration 5800: Loss = -11034.1435546875
Iteration 5900: Loss = -11034.08984375
Iteration 6000: Loss = -11034.04296875
Iteration 6100: Loss = -11033.9931640625
Iteration 6200: Loss = -11033.9501953125
Iteration 6300: Loss = -11033.9052734375
Iteration 6400: Loss = -11033.865234375
Iteration 6500: Loss = -11033.828125
Iteration 6600: Loss = -11033.791015625
Iteration 6700: Loss = -11033.7578125
Iteration 6800: Loss = -11033.724609375
Iteration 6900: Loss = -11033.6953125
Iteration 7000: Loss = -11033.6640625
Iteration 7100: Loss = -11033.63671875
Iteration 7200: Loss = -11033.611328125
Iteration 7300: Loss = -11033.5859375
Iteration 7400: Loss = -11033.5625
Iteration 7500: Loss = -11033.5380859375
Iteration 7600: Loss = -11033.517578125
Iteration 7700: Loss = -11033.4951171875
Iteration 7800: Loss = -11033.4755859375
Iteration 7900: Loss = -11033.45703125
Iteration 8000: Loss = -11033.4375
Iteration 8100: Loss = -11033.419921875
Iteration 8200: Loss = -11033.4033203125
Iteration 8300: Loss = -11033.3837890625
Iteration 8400: Loss = -11033.3642578125
Iteration 8500: Loss = -11033.3427734375
Iteration 8600: Loss = -11033.310546875
Iteration 8700: Loss = -11033.244140625
Iteration 8800: Loss = -11033.0390625
Iteration 8900: Loss = -11032.9619140625
Iteration 9000: Loss = -11032.927734375
Iteration 9100: Loss = -11032.9013671875
Iteration 9200: Loss = -11032.876953125
Iteration 9300: Loss = -11032.8564453125
Iteration 9400: Loss = -11032.8408203125
Iteration 9500: Loss = -11032.8232421875
Iteration 9600: Loss = -11032.8076171875
Iteration 9700: Loss = -11032.794921875
Iteration 9800: Loss = -11032.7802734375
Iteration 9900: Loss = -11032.7666015625
Iteration 10000: Loss = -11032.7509765625
Iteration 10100: Loss = -11032.7333984375
Iteration 10200: Loss = -11032.6982421875
Iteration 10300: Loss = -11032.6533203125
Iteration 10400: Loss = -11032.6220703125
Iteration 10500: Loss = -11032.603515625
Iteration 10600: Loss = -11032.5947265625
Iteration 10700: Loss = -11032.5859375
Iteration 10800: Loss = -11032.5751953125
Iteration 10900: Loss = -11032.5634765625
Iteration 11000: Loss = -11032.5478515625
Iteration 11100: Loss = -11032.5224609375
Iteration 11200: Loss = -11032.494140625
Iteration 11300: Loss = -11032.4677734375
Iteration 11400: Loss = -11032.4423828125
Iteration 11500: Loss = -11032.392578125
Iteration 11600: Loss = -11032.287109375
Iteration 11700: Loss = -11032.2216796875
Iteration 11800: Loss = -11032.16796875
Iteration 11900: Loss = -11032.115234375
Iteration 12000: Loss = -11032.0576171875
Iteration 12100: Loss = -11032.0205078125
Iteration 12200: Loss = -11031.9853515625
Iteration 12300: Loss = -11031.947265625
Iteration 12400: Loss = -11031.90234375
Iteration 12500: Loss = -11031.861328125
Iteration 12600: Loss = -11031.830078125
Iteration 12700: Loss = -11031.7998046875
Iteration 12800: Loss = -11031.771484375
Iteration 12900: Loss = -11031.75
Iteration 13000: Loss = -11031.7314453125
Iteration 13100: Loss = -11031.7177734375
Iteration 13200: Loss = -11031.7041015625
Iteration 13300: Loss = -11031.689453125
Iteration 13400: Loss = -11031.6806640625
Iteration 13500: Loss = -11031.6728515625
Iteration 13600: Loss = -11031.6640625
Iteration 13700: Loss = -11031.658203125
Iteration 13800: Loss = -11031.654296875
Iteration 13900: Loss = -11031.6474609375
Iteration 14000: Loss = -11031.6416015625
Iteration 14100: Loss = -11031.6396484375
Iteration 14200: Loss = -11031.6357421875
Iteration 14300: Loss = -11031.6357421875
Iteration 14400: Loss = -11031.6298828125
Iteration 14500: Loss = -11031.62890625
Iteration 14600: Loss = -11031.625
Iteration 14700: Loss = -11031.625
Iteration 14800: Loss = -11031.62109375
Iteration 14900: Loss = -11031.619140625
Iteration 15000: Loss = -11031.619140625
Iteration 15100: Loss = -11031.615234375
Iteration 15200: Loss = -11031.615234375
Iteration 15300: Loss = -11031.6142578125
Iteration 15400: Loss = -11031.61328125
Iteration 15500: Loss = -11031.6123046875
Iteration 15600: Loss = -11031.609375
Iteration 15700: Loss = -11031.609375
Iteration 15800: Loss = -11031.6083984375
Iteration 15900: Loss = -11031.607421875
Iteration 16000: Loss = -11031.6064453125
Iteration 16100: Loss = -11031.6064453125
Iteration 16200: Loss = -11031.60546875
Iteration 16300: Loss = -11031.6044921875
Iteration 16400: Loss = -11031.6064453125
1
Iteration 16500: Loss = -11031.6044921875
Iteration 16600: Loss = -11031.603515625
Iteration 16700: Loss = -11031.6025390625
Iteration 16800: Loss = -11031.6044921875
1
Iteration 16900: Loss = -11031.60546875
2
Iteration 17000: Loss = -11031.603515625
3
Iteration 17100: Loss = -11031.603515625
4
Iteration 17200: Loss = -11031.6044921875
5
Iteration 17300: Loss = -11031.603515625
6
Iteration 17400: Loss = -11031.603515625
7
Iteration 17500: Loss = -11031.6025390625
Iteration 17600: Loss = -11031.6025390625
Iteration 17700: Loss = -11031.603515625
1
Iteration 17800: Loss = -11031.6025390625
Iteration 17900: Loss = -11031.6015625
Iteration 18000: Loss = -11031.6025390625
1
Iteration 18100: Loss = -11031.6025390625
2
Iteration 18200: Loss = -11031.6025390625
3
Iteration 18300: Loss = -11031.6025390625
4
Iteration 18400: Loss = -11031.6015625
Iteration 18500: Loss = -11031.6025390625
1
Iteration 18600: Loss = -11031.6025390625
2
Iteration 18700: Loss = -11031.603515625
3
Iteration 18800: Loss = -11031.6025390625
4
Iteration 18900: Loss = -11031.6025390625
5
Iteration 19000: Loss = -11031.6025390625
6
Iteration 19100: Loss = -11031.6015625
Iteration 19200: Loss = -11031.6015625
Iteration 19300: Loss = -11031.6025390625
1
Iteration 19400: Loss = -11031.6015625
Iteration 19500: Loss = -11031.6015625
Iteration 19600: Loss = -11031.6015625
Iteration 19700: Loss = -11031.6005859375
Iteration 19800: Loss = -11031.6025390625
1
Iteration 19900: Loss = -11031.6015625
2
Iteration 20000: Loss = -11031.6015625
3
Iteration 20100: Loss = -11031.6005859375
Iteration 20200: Loss = -11031.6015625
1
Iteration 20300: Loss = -11031.6015625
2
Iteration 20400: Loss = -11031.6015625
3
Iteration 20500: Loss = -11031.6025390625
4
Iteration 20600: Loss = -11031.6015625
5
Iteration 20700: Loss = -11031.6015625
6
Iteration 20800: Loss = -11031.6015625
7
Iteration 20900: Loss = -11031.6025390625
8
Iteration 21000: Loss = -11031.6005859375
Iteration 21100: Loss = -11031.6015625
1
Iteration 21200: Loss = -11031.6005859375
Iteration 21300: Loss = -11031.6015625
1
Iteration 21400: Loss = -11031.59765625
Iteration 21500: Loss = -11031.5947265625
Iteration 21600: Loss = -11031.5947265625
Iteration 21700: Loss = -11031.5947265625
Iteration 21800: Loss = -11031.5947265625
Iteration 21900: Loss = -11031.5927734375
Iteration 22000: Loss = -11031.5927734375
Iteration 22100: Loss = -11031.5908203125
Iteration 22200: Loss = -11031.591796875
1
Iteration 22300: Loss = -11031.5908203125
Iteration 22400: Loss = -11031.5908203125
Iteration 22500: Loss = -11031.591796875
1
Iteration 22600: Loss = -11031.591796875
2
Iteration 22700: Loss = -11031.5908203125
Iteration 22800: Loss = -11031.58984375
Iteration 22900: Loss = -11031.5927734375
1
Iteration 23000: Loss = -11031.591796875
2
Iteration 23100: Loss = -11031.591796875
3
Iteration 23200: Loss = -11031.5908203125
4
Iteration 23300: Loss = -11031.591796875
5
Iteration 23400: Loss = -11031.5908203125
6
Iteration 23500: Loss = -11031.5908203125
7
Iteration 23600: Loss = -11031.58984375
Iteration 23700: Loss = -11031.591796875
1
Iteration 23800: Loss = -11031.5908203125
2
Iteration 23900: Loss = -11031.5908203125
3
Iteration 24000: Loss = -11031.5908203125
4
Iteration 24100: Loss = -11031.5908203125
5
Iteration 24200: Loss = -11031.591796875
6
Iteration 24300: Loss = -11031.5908203125
7
Iteration 24400: Loss = -11031.5908203125
8
Iteration 24500: Loss = -11031.591796875
9
Iteration 24600: Loss = -11031.5908203125
10
Iteration 24700: Loss = -11031.591796875
11
Iteration 24800: Loss = -11031.5908203125
12
Iteration 24900: Loss = -11031.5947265625
13
Iteration 25000: Loss = -11031.5908203125
14
Iteration 25100: Loss = -11031.5908203125
15
Stopping early at iteration 25100 due to no improvement.
pi: tensor([[9.8618e-01, 1.3820e-02],
        [9.9999e-01, 7.8674e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9707, 0.0293], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1628, 0.2299],
         [0.9457, 0.3229]],

        [[0.6567, 0.2603],
         [0.1165, 0.0844]],

        [[0.2960, 0.1033],
         [0.9470, 0.7321]],

        [[0.9537, 0.0774],
         [0.8922, 0.2043]],

        [[0.7233, 0.2356],
         [0.4400, 0.2365]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000814506368914951
Average Adjusted Rand Index: 0.0017777777777777779
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -44855.5
Iteration 100: Loss = -30877.255859375
Iteration 200: Loss = -17349.962890625
Iteration 300: Loss = -12888.93359375
Iteration 400: Loss = -11931.505859375
Iteration 500: Loss = -11647.091796875
Iteration 600: Loss = -11509.66015625
Iteration 700: Loss = -11431.39453125
Iteration 800: Loss = -11392.7490234375
Iteration 900: Loss = -11365.3671875
Iteration 1000: Loss = -11346.51953125
Iteration 1100: Loss = -11333.0703125
Iteration 1200: Loss = -11322.111328125
Iteration 1300: Loss = -11311.9833984375
Iteration 1400: Loss = -11302.9912109375
Iteration 1500: Loss = -11295.5439453125
Iteration 1600: Loss = -11289.7431640625
Iteration 1700: Loss = -11285.505859375
Iteration 1800: Loss = -11281.31640625
Iteration 1900: Loss = -11277.2880859375
Iteration 2000: Loss = -11272.0185546875
Iteration 2100: Loss = -11268.4091796875
Iteration 2200: Loss = -11265.685546875
Iteration 2300: Loss = -11262.9921875
Iteration 2400: Loss = -11259.1845703125
Iteration 2500: Loss = -11255.2158203125
Iteration 2600: Loss = -11252.30859375
Iteration 2700: Loss = -11249.1748046875
Iteration 2800: Loss = -11246.5283203125
Iteration 2900: Loss = -11244.458984375
Iteration 3000: Loss = -11242.4248046875
Iteration 3100: Loss = -11239.7119140625
Iteration 3200: Loss = -11237.7216796875
Iteration 3300: Loss = -11236.5556640625
Iteration 3400: Loss = -11235.060546875
Iteration 3500: Loss = -11232.9990234375
Iteration 3600: Loss = -11232.3056640625
Iteration 3700: Loss = -11231.591796875
Iteration 3800: Loss = -11230.560546875
Iteration 3900: Loss = -11229.3642578125
Iteration 4000: Loss = -11228.6708984375
Iteration 4100: Loss = -11228.2080078125
Iteration 4200: Loss = -11227.8203125
Iteration 4300: Loss = -11227.455078125
Iteration 4400: Loss = -11226.572265625
Iteration 4500: Loss = -11224.544921875
Iteration 4600: Loss = -11222.166015625
Iteration 4700: Loss = -11221.18359375
Iteration 4800: Loss = -11219.732421875
Iteration 4900: Loss = -11218.779296875
Iteration 5000: Loss = -11217.7490234375
Iteration 5100: Loss = -11217.296875
Iteration 5200: Loss = -11217.06640625
Iteration 5300: Loss = -11216.9150390625
Iteration 5400: Loss = -11216.7802734375
Iteration 5500: Loss = -11216.4990234375
Iteration 5600: Loss = -11216.21484375
Iteration 5700: Loss = -11213.5947265625
Iteration 5800: Loss = -11213.2392578125
Iteration 5900: Loss = -11212.4658203125
Iteration 6000: Loss = -11212.162109375
Iteration 6100: Loss = -11210.7646484375
Iteration 6200: Loss = -11209.8046875
Iteration 6300: Loss = -11208.19921875
Iteration 6400: Loss = -11206.4013671875
Iteration 6500: Loss = -11204.4296875
Iteration 6600: Loss = -11203.2607421875
Iteration 6700: Loss = -11201.888671875
Iteration 6800: Loss = -11199.580078125
Iteration 6900: Loss = -11196.2392578125
Iteration 7000: Loss = -11193.443359375
Iteration 7100: Loss = -11191.36328125
Iteration 7200: Loss = -11189.8369140625
Iteration 7300: Loss = -11184.0107421875
Iteration 7400: Loss = -11176.970703125
Iteration 7500: Loss = -11172.46875
Iteration 7600: Loss = -11166.28125
Iteration 7700: Loss = -11157.9365234375
Iteration 7800: Loss = -11149.6474609375
Iteration 7900: Loss = -11142.19140625
Iteration 8000: Loss = -11139.9658203125
Iteration 8100: Loss = -11134.7978515625
Iteration 8200: Loss = -11127.9716796875
Iteration 8300: Loss = -11124.7734375
Iteration 8400: Loss = -11118.2587890625
Iteration 8500: Loss = -11118.13671875
Iteration 8600: Loss = -11115.24609375
Iteration 8700: Loss = -11110.7373046875
Iteration 8800: Loss = -11110.6630859375
Iteration 8900: Loss = -11110.263671875
Iteration 9000: Loss = -11106.587890625
Iteration 9100: Loss = -11106.544921875
Iteration 9200: Loss = -11106.51953125
Iteration 9300: Loss = -11106.494140625
Iteration 9400: Loss = -11106.427734375
Iteration 9500: Loss = -11102.36328125
Iteration 9600: Loss = -11102.24609375
Iteration 9700: Loss = -11093.677734375
Iteration 9800: Loss = -11087.6884765625
Iteration 9900: Loss = -11082.841796875
Iteration 10000: Loss = -11066.708984375
Iteration 10100: Loss = -11065.7294921875
Iteration 10200: Loss = -11065.615234375
Iteration 10300: Loss = -11065.556640625
Iteration 10400: Loss = -11065.517578125
Iteration 10500: Loss = -11065.4658203125
Iteration 10600: Loss = -11059.6259765625
Iteration 10700: Loss = -11059.517578125
Iteration 10800: Loss = -11059.4814453125
Iteration 10900: Loss = -11059.4609375
Iteration 11000: Loss = -11059.4423828125
Iteration 11100: Loss = -11059.4296875
Iteration 11200: Loss = -11059.41796875
Iteration 11300: Loss = -11059.408203125
Iteration 11400: Loss = -11059.3984375
Iteration 11500: Loss = -11059.390625
Iteration 11600: Loss = -11059.384765625
Iteration 11700: Loss = -11059.37890625
Iteration 11800: Loss = -11059.369140625
Iteration 11900: Loss = -11059.32421875
Iteration 12000: Loss = -11052.548828125
Iteration 12100: Loss = -11044.6494140625
Iteration 12200: Loss = -11044.443359375
Iteration 12300: Loss = -11044.3896484375
Iteration 12400: Loss = -11044.3623046875
Iteration 12500: Loss = -11044.3408203125
Iteration 12600: Loss = -11044.328125
Iteration 12700: Loss = -11044.318359375
Iteration 12800: Loss = -11044.3095703125
Iteration 12900: Loss = -11044.302734375
Iteration 13000: Loss = -11044.2978515625
Iteration 13100: Loss = -11044.2919921875
Iteration 13200: Loss = -11044.287109375
Iteration 13300: Loss = -11044.283203125
Iteration 13400: Loss = -11044.279296875
Iteration 13500: Loss = -11044.2763671875
Iteration 13600: Loss = -11044.2724609375
Iteration 13700: Loss = -11044.271484375
Iteration 13800: Loss = -11044.267578125
Iteration 13900: Loss = -11044.265625
Iteration 14000: Loss = -11044.263671875
Iteration 14100: Loss = -11044.2626953125
Iteration 14200: Loss = -11044.259765625
Iteration 14300: Loss = -11044.259765625
Iteration 14400: Loss = -11044.2568359375
Iteration 14500: Loss = -11044.255859375
Iteration 14600: Loss = -11044.2548828125
Iteration 14700: Loss = -11044.2529296875
Iteration 14800: Loss = -11044.2529296875
Iteration 14900: Loss = -11044.2509765625
Iteration 15000: Loss = -11044.25
Iteration 15100: Loss = -11044.248046875
Iteration 15200: Loss = -11044.1533203125
Iteration 15300: Loss = -11033.3994140625
Iteration 15400: Loss = -11033.2353515625
Iteration 15500: Loss = -11033.1845703125
Iteration 15600: Loss = -11033.15625
Iteration 15700: Loss = -11033.140625
Iteration 15800: Loss = -11033.12890625
Iteration 15900: Loss = -11033.119140625
Iteration 16000: Loss = -11033.1123046875
Iteration 16100: Loss = -11033.107421875
Iteration 16200: Loss = -11033.1015625
Iteration 16300: Loss = -11033.099609375
Iteration 16400: Loss = -11033.0947265625
Iteration 16500: Loss = -11033.091796875
Iteration 16600: Loss = -11033.0888671875
Iteration 16700: Loss = -11033.087890625
Iteration 16800: Loss = -11033.0849609375
Iteration 16900: Loss = -11033.0849609375
Iteration 17000: Loss = -11033.0810546875
Iteration 17100: Loss = -11033.080078125
Iteration 17200: Loss = -11033.0791015625
Iteration 17300: Loss = -11033.0791015625
Iteration 17400: Loss = -11033.078125
Iteration 17500: Loss = -11033.0771484375
Iteration 17600: Loss = -11033.0751953125
Iteration 17700: Loss = -11033.0751953125
Iteration 17800: Loss = -11033.0751953125
Iteration 17900: Loss = -11033.0732421875
Iteration 18000: Loss = -11033.0732421875
Iteration 18100: Loss = -11033.072265625
Iteration 18200: Loss = -11033.0712890625
Iteration 18300: Loss = -11033.072265625
1
Iteration 18400: Loss = -11033.0712890625
Iteration 18500: Loss = -11033.072265625
1
Iteration 18600: Loss = -11033.0693359375
Iteration 18700: Loss = -11033.0703125
1
Iteration 18800: Loss = -11033.068359375
Iteration 18900: Loss = -11033.0693359375
1
Iteration 19000: Loss = -11033.0693359375
2
Iteration 19100: Loss = -11033.0673828125
Iteration 19200: Loss = -11033.0673828125
Iteration 19300: Loss = -11033.0673828125
Iteration 19400: Loss = -11033.06640625
Iteration 19500: Loss = -11033.06640625
Iteration 19600: Loss = -11033.06640625
Iteration 19700: Loss = -11033.06640625
Iteration 19800: Loss = -11033.0673828125
1
Iteration 19900: Loss = -11033.0654296875
Iteration 20000: Loss = -11033.0634765625
Iteration 20100: Loss = -11033.064453125
1
Iteration 20200: Loss = -11033.064453125
2
Iteration 20300: Loss = -11033.064453125
3
Iteration 20400: Loss = -11033.064453125
4
Iteration 20500: Loss = -11033.0654296875
5
Iteration 20600: Loss = -11033.064453125
6
Iteration 20700: Loss = -11033.064453125
7
Iteration 20800: Loss = -11033.0634765625
Iteration 20900: Loss = -11033.064453125
1
Iteration 21000: Loss = -11033.0634765625
Iteration 21100: Loss = -11033.0625
Iteration 21200: Loss = -11033.0615234375
Iteration 21300: Loss = -11033.0625
1
Iteration 21400: Loss = -11033.0634765625
2
Iteration 21500: Loss = -11033.0634765625
3
Iteration 21600: Loss = -11033.0625
4
Iteration 21700: Loss = -11033.0625
5
Iteration 21800: Loss = -11033.0625
6
Iteration 21900: Loss = -11033.0625
7
Iteration 22000: Loss = -11033.064453125
8
Iteration 22100: Loss = -11033.0625
9
Iteration 22200: Loss = -11033.0625
10
Iteration 22300: Loss = -11033.064453125
11
Iteration 22400: Loss = -11033.0625
12
Iteration 22500: Loss = -11033.064453125
13
Iteration 22600: Loss = -11033.0625
14
Iteration 22700: Loss = -11033.0625
15
Stopping early at iteration 22700 due to no improvement.
pi: tensor([[7.2176e-07, 1.0000e+00],
        [1.0000e+00, 2.9488e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 2.5258e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1644, 0.2366],
         [0.9326, 0.1627]],

        [[0.0100, 0.1623],
         [0.4249, 0.9739]],

        [[0.0236, 0.1524],
         [0.6306, 0.2958]],

        [[0.0671, 0.5783],
         [0.9743, 0.9137]],

        [[0.0096, 0.6410],
         [0.8948, 0.9915]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0002570766699997728
Average Adjusted Rand Index: 0.0
[0.000814506368914951, -0.0002570766699997728] [0.0017777777777777779, 0.0] [11031.5908203125, 11033.0625]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11007.789061252
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42975.7890625
Iteration 100: Loss = -26710.1640625
Iteration 200: Loss = -14889.7783203125
Iteration 300: Loss = -12258.1953125
Iteration 400: Loss = -11754.40234375
Iteration 500: Loss = -11574.927734375
Iteration 600: Loss = -11477.62109375
Iteration 700: Loss = -11415.4375
Iteration 800: Loss = -11369.6142578125
Iteration 900: Loss = -11331.166015625
Iteration 1000: Loss = -11309.0810546875
Iteration 1100: Loss = -11292.9501953125
Iteration 1200: Loss = -11276.2763671875
Iteration 1300: Loss = -11265.205078125
Iteration 1400: Loss = -11255.6298828125
Iteration 1500: Loss = -11247.943359375
Iteration 1600: Loss = -11237.3837890625
Iteration 1700: Loss = -11232.82421875
Iteration 1800: Loss = -11229.236328125
Iteration 1900: Loss = -11226.2490234375
Iteration 2000: Loss = -11223.7021484375
Iteration 2100: Loss = -11221.501953125
Iteration 2200: Loss = -11219.5751953125
Iteration 2300: Loss = -11217.8759765625
Iteration 2400: Loss = -11216.365234375
Iteration 2500: Loss = -11215.0087890625
Iteration 2600: Loss = -11213.7822265625
Iteration 2700: Loss = -11212.66015625
Iteration 2800: Loss = -11211.55078125
Iteration 2900: Loss = -11209.8486328125
Iteration 3000: Loss = -11206.0458984375
Iteration 3100: Loss = -11205.09375
Iteration 3200: Loss = -11204.3486328125
Iteration 3300: Loss = -11203.68359375
Iteration 3400: Loss = -11203.0791015625
Iteration 3500: Loss = -11202.5244140625
Iteration 3600: Loss = -11202.009765625
Iteration 3700: Loss = -11201.5341796875
Iteration 3800: Loss = -11201.09375
Iteration 3900: Loss = -11200.681640625
Iteration 4000: Loss = -11200.2919921875
Iteration 4100: Loss = -11198.4453125
Iteration 4200: Loss = -11196.4755859375
Iteration 4300: Loss = -11196.0029296875
Iteration 4400: Loss = -11195.626953125
Iteration 4500: Loss = -11195.2958984375
Iteration 4600: Loss = -11194.9892578125
Iteration 4700: Loss = -11194.7060546875
Iteration 4800: Loss = -11194.4384765625
Iteration 4900: Loss = -11194.185546875
Iteration 5000: Loss = -11193.947265625
Iteration 5100: Loss = -11193.13671875
Iteration 5200: Loss = -11189.9453125
Iteration 5300: Loss = -11189.7431640625
Iteration 5400: Loss = -11189.58203125
Iteration 5500: Loss = -11189.43359375
Iteration 5600: Loss = -11189.2919921875
Iteration 5700: Loss = -11189.1611328125
Iteration 5800: Loss = -11189.0361328125
Iteration 5900: Loss = -11188.8935546875
Iteration 6000: Loss = -11188.7685546875
Iteration 6100: Loss = -11188.6591796875
Iteration 6200: Loss = -11188.55859375
Iteration 6300: Loss = -11188.453125
Iteration 6400: Loss = -11183.8603515625
Iteration 6500: Loss = -11183.70703125
Iteration 6600: Loss = -11183.6171875
Iteration 6700: Loss = -11183.5400390625
Iteration 6800: Loss = -11183.4716796875
Iteration 6900: Loss = -11183.40625
Iteration 7000: Loss = -11183.345703125
Iteration 7100: Loss = -11183.2861328125
Iteration 7200: Loss = -11183.2294921875
Iteration 7300: Loss = -11183.17578125
Iteration 7400: Loss = -11183.1259765625
Iteration 7500: Loss = -11183.0771484375
Iteration 7600: Loss = -11183.0341796875
Iteration 7700: Loss = -11182.990234375
Iteration 7800: Loss = -11182.94921875
Iteration 7900: Loss = -11182.908203125
Iteration 8000: Loss = -11182.8701171875
Iteration 8100: Loss = -11182.83203125
Iteration 8200: Loss = -11182.79296875
Iteration 8300: Loss = -11182.7578125
Iteration 8400: Loss = -11182.720703125
Iteration 8500: Loss = -11182.6826171875
Iteration 8600: Loss = -11182.6494140625
Iteration 8700: Loss = -11182.6162109375
Iteration 8800: Loss = -11182.5859375
Iteration 8900: Loss = -11182.55859375
Iteration 9000: Loss = -11182.5302734375
Iteration 9100: Loss = -11182.5029296875
Iteration 9200: Loss = -11182.4775390625
Iteration 9300: Loss = -11182.451171875
Iteration 9400: Loss = -11182.4228515625
Iteration 9500: Loss = -11182.3857421875
Iteration 9600: Loss = -11182.353515625
Iteration 9700: Loss = -11182.330078125
Iteration 9800: Loss = -11182.30859375
Iteration 9900: Loss = -11182.2900390625
Iteration 10000: Loss = -11182.2724609375
Iteration 10100: Loss = -11182.2568359375
Iteration 10200: Loss = -11182.2392578125
Iteration 10300: Loss = -11182.2265625
Iteration 10400: Loss = -11182.2138671875
Iteration 10500: Loss = -11182.201171875
Iteration 10600: Loss = -11182.1884765625
Iteration 10700: Loss = -11182.1796875
Iteration 10800: Loss = -11178.2138671875
Iteration 10900: Loss = -11172.84765625
Iteration 11000: Loss = -11172.65234375
Iteration 11100: Loss = -11172.5673828125
Iteration 11200: Loss = -11172.5205078125
Iteration 11300: Loss = -11172.4873046875
Iteration 11400: Loss = -11172.4638671875
Iteration 11500: Loss = -11172.4462890625
Iteration 11600: Loss = -11172.431640625
Iteration 11700: Loss = -11172.4169921875
Iteration 11800: Loss = -11172.4072265625
Iteration 11900: Loss = -11172.3984375
Iteration 12000: Loss = -11172.390625
Iteration 12100: Loss = -11172.3837890625
Iteration 12200: Loss = -11172.3759765625
Iteration 12300: Loss = -11172.3720703125
Iteration 12400: Loss = -11172.365234375
Iteration 12500: Loss = -11172.361328125
Iteration 12600: Loss = -11172.35546875
Iteration 12700: Loss = -11172.3525390625
Iteration 12800: Loss = -11172.3486328125
Iteration 12900: Loss = -11172.345703125
Iteration 13000: Loss = -11172.3408203125
Iteration 13100: Loss = -11172.337890625
Iteration 13200: Loss = -11172.3359375
Iteration 13300: Loss = -11172.33203125
Iteration 13400: Loss = -11172.3291015625
Iteration 13500: Loss = -11172.3271484375
Iteration 13600: Loss = -11172.3251953125
Iteration 13700: Loss = -11172.3232421875
Iteration 13800: Loss = -11172.322265625
Iteration 13900: Loss = -11172.3193359375
Iteration 14000: Loss = -11172.31640625
Iteration 14100: Loss = -11172.3154296875
Iteration 14200: Loss = -11172.31640625
1
Iteration 14300: Loss = -11172.3125
Iteration 14400: Loss = -11172.3115234375
Iteration 14500: Loss = -11172.3125
1
Iteration 14600: Loss = -11172.3095703125
Iteration 14700: Loss = -11172.30859375
Iteration 14800: Loss = -11172.306640625
Iteration 14900: Loss = -11172.3046875
Iteration 15000: Loss = -11172.3056640625
1
Iteration 15100: Loss = -11172.3037109375
Iteration 15200: Loss = -11172.3046875
1
Iteration 15300: Loss = -11172.3037109375
Iteration 15400: Loss = -11172.3017578125
Iteration 15500: Loss = -11172.2998046875
Iteration 15600: Loss = -11172.298828125
Iteration 15700: Loss = -11172.30078125
1
Iteration 15800: Loss = -11172.2978515625
Iteration 15900: Loss = -11172.2978515625
Iteration 16000: Loss = -11172.2978515625
Iteration 16100: Loss = -11172.298828125
1
Iteration 16200: Loss = -11172.2978515625
Iteration 16300: Loss = -11172.296875
Iteration 16400: Loss = -11172.2958984375
Iteration 16500: Loss = -11172.2958984375
Iteration 16600: Loss = -11172.294921875
Iteration 16700: Loss = -11172.294921875
Iteration 16800: Loss = -11172.294921875
Iteration 16900: Loss = -11172.2939453125
Iteration 17000: Loss = -11172.29296875
Iteration 17100: Loss = -11172.2900390625
Iteration 17200: Loss = -11169.0009765625
Iteration 17300: Loss = -11168.4873046875
Iteration 17400: Loss = -11168.310546875
Iteration 17500: Loss = -11168.2119140625
Iteration 17600: Loss = -11168.15625
Iteration 17700: Loss = -11168.1201171875
Iteration 17800: Loss = -11168.0830078125
Iteration 17900: Loss = -11167.859375
Iteration 18000: Loss = -11167.8271484375
Iteration 18100: Loss = -11167.8154296875
Iteration 18200: Loss = -11167.5087890625
Iteration 18300: Loss = -11167.4736328125
Iteration 18400: Loss = -11167.4677734375
Iteration 18500: Loss = -11167.466796875
Iteration 18600: Loss = -11167.4541015625
Iteration 18700: Loss = -11167.427734375
Iteration 18800: Loss = -11167.423828125
Iteration 18900: Loss = -11167.375
Iteration 19000: Loss = -11167.3720703125
Iteration 19100: Loss = -11167.03125
Iteration 19200: Loss = -11166.892578125
Iteration 19300: Loss = -11166.83203125
Iteration 19400: Loss = -11166.7802734375
Iteration 19500: Loss = -11166.6923828125
Iteration 19600: Loss = -11166.1826171875
Iteration 19700: Loss = -11165.9873046875
Iteration 19800: Loss = -11165.8349609375
Iteration 19900: Loss = -11165.7529296875
Iteration 20000: Loss = -11165.689453125
Iteration 20100: Loss = -11165.650390625
Iteration 20200: Loss = -11165.6298828125
Iteration 20300: Loss = -11165.5166015625
Iteration 20400: Loss = -11165.474609375
Iteration 20500: Loss = -11165.455078125
Iteration 20600: Loss = -11165.3974609375
Iteration 20700: Loss = -11165.322265625
Iteration 20800: Loss = -11165.2021484375
Iteration 20900: Loss = -11164.9912109375
Iteration 21000: Loss = -11164.9208984375
Iteration 21100: Loss = -11164.8662109375
Iteration 21200: Loss = -11164.8544921875
Iteration 21300: Loss = -11164.8408203125
Iteration 21400: Loss = -11164.8291015625
Iteration 21500: Loss = -11164.7802734375
Iteration 21600: Loss = -11164.7763671875
Iteration 21700: Loss = -11164.7744140625
Iteration 21800: Loss = -11164.7724609375
Iteration 21900: Loss = -11164.7705078125
Iteration 22000: Loss = -11164.7392578125
Iteration 22100: Loss = -11164.7314453125
Iteration 22200: Loss = -11164.7294921875
Iteration 22300: Loss = -11164.7294921875
Iteration 22400: Loss = -11164.7294921875
Iteration 22500: Loss = -11164.7294921875
Iteration 22600: Loss = -11164.728515625
Iteration 22700: Loss = -11164.728515625
Iteration 22800: Loss = -11164.7294921875
1
Iteration 22900: Loss = -11164.728515625
Iteration 23000: Loss = -11164.73046875
1
Iteration 23100: Loss = -11164.728515625
Iteration 23200: Loss = -11164.7275390625
Iteration 23300: Loss = -11164.7275390625
Iteration 23400: Loss = -11164.728515625
1
Iteration 23500: Loss = -11164.7275390625
Iteration 23600: Loss = -11164.7294921875
1
Iteration 23700: Loss = -11164.7294921875
2
Iteration 23800: Loss = -11164.7275390625
Iteration 23900: Loss = -11164.728515625
1
Iteration 24000: Loss = -11164.7275390625
Iteration 24100: Loss = -11164.7275390625
Iteration 24200: Loss = -11164.7294921875
1
Iteration 24300: Loss = -11164.728515625
2
Iteration 24400: Loss = -11164.7294921875
3
Iteration 24500: Loss = -11164.7294921875
4
Iteration 24600: Loss = -11164.728515625
5
Iteration 24700: Loss = -11164.728515625
6
Iteration 24800: Loss = -11164.728515625
7
Iteration 24900: Loss = -11164.7294921875
8
Iteration 25000: Loss = -11164.7275390625
Iteration 25100: Loss = -11164.7294921875
1
Iteration 25200: Loss = -11164.7275390625
Iteration 25300: Loss = -11164.7265625
Iteration 25400: Loss = -11164.728515625
1
Iteration 25500: Loss = -11164.73046875
2
Iteration 25600: Loss = -11164.7294921875
3
Iteration 25700: Loss = -11164.728515625
4
Iteration 25800: Loss = -11164.728515625
5
Iteration 25900: Loss = -11164.7275390625
6
Iteration 26000: Loss = -11164.7294921875
7
Iteration 26100: Loss = -11164.7275390625
8
Iteration 26200: Loss = -11164.7275390625
9
Iteration 26300: Loss = -11164.728515625
10
Iteration 26400: Loss = -11164.728515625
11
Iteration 26500: Loss = -11164.7275390625
12
Iteration 26600: Loss = -11164.7275390625
13
Iteration 26700: Loss = -11164.7275390625
14
Iteration 26800: Loss = -11164.728515625
15
Stopping early at iteration 26800 due to no improvement.
pi: tensor([[0.2561, 0.7439],
        [0.0347, 0.9653]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.8696e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4011, 0.1602],
         [0.0481, 0.1650]],

        [[0.0384, 0.1635],
         [0.8578, 0.5950]],

        [[0.8330, 0.2132],
         [0.4523, 0.1239]],

        [[0.1184, 0.2560],
         [0.0098, 0.0881]],

        [[0.6548, 0.0883],
         [0.9922, 0.1177]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.025916162480371957
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.02523388575915466
Global Adjusted Rand Index: -0.0003729734804485935
Average Adjusted Rand Index: -0.00013645534424345935
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18498.283203125
Iteration 100: Loss = -13226.8359375
Iteration 200: Loss = -11562.5859375
Iteration 300: Loss = -11331.8623046875
Iteration 400: Loss = -11279.6015625
Iteration 500: Loss = -11254.091796875
Iteration 600: Loss = -11236.1767578125
Iteration 700: Loss = -11224.619140625
Iteration 800: Loss = -11216.3056640625
Iteration 900: Loss = -11212.125
Iteration 1000: Loss = -11207.4169921875
Iteration 1100: Loss = -11204.08203125
Iteration 1200: Loss = -11197.6357421875
Iteration 1300: Loss = -11192.185546875
Iteration 1400: Loss = -11188.06640625
Iteration 1500: Loss = -11185.4970703125
Iteration 1600: Loss = -11184.3466796875
Iteration 1700: Loss = -11180.435546875
Iteration 1800: Loss = -11177.5
Iteration 1900: Loss = -11176.732421875
Iteration 2000: Loss = -11173.318359375
Iteration 2100: Loss = -11171.69921875
Iteration 2200: Loss = -11170.931640625
Iteration 2300: Loss = -11170.39453125
Iteration 2400: Loss = -11169.9814453125
Iteration 2500: Loss = -11169.646484375
Iteration 2600: Loss = -11169.353515625
Iteration 2700: Loss = -11169.1123046875
Iteration 2800: Loss = -11168.9091796875
Iteration 2900: Loss = -11168.73046875
Iteration 3000: Loss = -11168.5732421875
Iteration 3100: Loss = -11168.4365234375
Iteration 3200: Loss = -11168.3125
Iteration 3300: Loss = -11168.2001953125
Iteration 3400: Loss = -11168.0966796875
Iteration 3500: Loss = -11168.001953125
Iteration 3600: Loss = -11167.912109375
Iteration 3700: Loss = -11167.82421875
Iteration 3800: Loss = -11167.7333984375
Iteration 3900: Loss = -11167.634765625
Iteration 4000: Loss = -11167.5400390625
Iteration 4100: Loss = -11167.458984375
Iteration 4200: Loss = -11167.388671875
Iteration 4300: Loss = -11167.3291015625
Iteration 4400: Loss = -11167.275390625
Iteration 4500: Loss = -11167.2265625
Iteration 4600: Loss = -11167.1796875
Iteration 4700: Loss = -11167.1337890625
Iteration 4800: Loss = -11167.087890625
Iteration 4900: Loss = -11167.041015625
Iteration 5000: Loss = -11166.98828125
Iteration 5100: Loss = -11166.92578125
Iteration 5200: Loss = -11166.837890625
Iteration 5300: Loss = -11166.7041015625
Iteration 5400: Loss = -11166.5478515625
Iteration 5500: Loss = -11166.435546875
Iteration 5600: Loss = -11166.345703125
Iteration 5700: Loss = -11166.287109375
Iteration 5800: Loss = -11166.2373046875
Iteration 5900: Loss = -11166.1953125
Iteration 6000: Loss = -11166.166015625
Iteration 6100: Loss = -11166.1376953125
Iteration 6200: Loss = -11166.083984375
Iteration 6300: Loss = -11166.0625
Iteration 6400: Loss = -11166.0419921875
Iteration 6500: Loss = -11166.0205078125
Iteration 6600: Loss = -11165.9873046875
Iteration 6700: Loss = -11165.9560546875
Iteration 6800: Loss = -11165.9365234375
Iteration 6900: Loss = -11165.91796875
Iteration 7000: Loss = -11165.904296875
Iteration 7100: Loss = -11165.8916015625
Iteration 7200: Loss = -11165.8837890625
Iteration 7300: Loss = -11165.875
Iteration 7400: Loss = -11165.8671875
Iteration 7500: Loss = -11165.8603515625
Iteration 7600: Loss = -11165.85546875
Iteration 7700: Loss = -11165.8486328125
Iteration 7800: Loss = -11165.8427734375
Iteration 7900: Loss = -11165.8388671875
Iteration 8000: Loss = -11165.8330078125
Iteration 8100: Loss = -11165.828125
Iteration 8200: Loss = -11165.8251953125
Iteration 8300: Loss = -11165.8212890625
Iteration 8400: Loss = -11165.81640625
Iteration 8500: Loss = -11165.8134765625
Iteration 8600: Loss = -11165.8076171875
Iteration 8700: Loss = -11165.798828125
Iteration 8800: Loss = -11165.7861328125
Iteration 8900: Loss = -11165.77734375
Iteration 9000: Loss = -11165.771484375
Iteration 9100: Loss = -11165.7705078125
Iteration 9200: Loss = -11165.7666015625
Iteration 9300: Loss = -11165.7646484375
Iteration 9400: Loss = -11165.7626953125
Iteration 9500: Loss = -11165.7607421875
Iteration 9600: Loss = -11165.7587890625
Iteration 9700: Loss = -11165.7548828125
Iteration 9800: Loss = -11165.7548828125
Iteration 9900: Loss = -11165.7548828125
Iteration 10000: Loss = -11165.751953125
Iteration 10100: Loss = -11165.75
Iteration 10200: Loss = -11165.75
Iteration 10300: Loss = -11165.748046875
Iteration 10400: Loss = -11165.74609375
Iteration 10500: Loss = -11165.7470703125
1
Iteration 10600: Loss = -11165.744140625
Iteration 10700: Loss = -11165.744140625
Iteration 10800: Loss = -11165.7431640625
Iteration 10900: Loss = -11165.7421875
Iteration 11000: Loss = -11165.7412109375
Iteration 11100: Loss = -11165.7412109375
Iteration 11200: Loss = -11165.7392578125
Iteration 11300: Loss = -11165.7392578125
Iteration 11400: Loss = -11165.7373046875
Iteration 11500: Loss = -11165.7373046875
Iteration 11600: Loss = -11165.736328125
Iteration 11700: Loss = -11165.7353515625
Iteration 11800: Loss = -11165.7333984375
Iteration 11900: Loss = -11165.7353515625
1
Iteration 12000: Loss = -11165.734375
2
Iteration 12100: Loss = -11165.7314453125
Iteration 12200: Loss = -11165.7314453125
Iteration 12300: Loss = -11165.728515625
Iteration 12400: Loss = -11165.7236328125
Iteration 12500: Loss = -11165.697265625
Iteration 12600: Loss = -11165.3115234375
Iteration 12700: Loss = -11164.7021484375
Iteration 12800: Loss = -11164.4921875
Iteration 12900: Loss = -11164.41015625
Iteration 13000: Loss = -11164.0498046875
Iteration 13100: Loss = -11164.001953125
Iteration 13200: Loss = -11163.96484375
Iteration 13300: Loss = -11163.89453125
Iteration 13400: Loss = -11163.853515625
Iteration 13500: Loss = -11163.8388671875
Iteration 13600: Loss = -11163.791015625
Iteration 13700: Loss = -11163.7802734375
Iteration 13800: Loss = -11163.7607421875
Iteration 13900: Loss = -11163.736328125
Iteration 14000: Loss = -11163.7333984375
Iteration 14100: Loss = -11163.728515625
Iteration 14200: Loss = -11163.7236328125
Iteration 14300: Loss = -11163.720703125
Iteration 14400: Loss = -11163.7197265625
Iteration 14500: Loss = -11163.716796875
Iteration 14600: Loss = -11163.716796875
Iteration 14700: Loss = -11163.7158203125
Iteration 14800: Loss = -11163.71484375
Iteration 14900: Loss = -11163.7158203125
1
Iteration 15000: Loss = -11163.7158203125
2
Iteration 15100: Loss = -11163.7158203125
3
Iteration 15200: Loss = -11163.71484375
Iteration 15300: Loss = -11163.71484375
Iteration 15400: Loss = -11163.71484375
Iteration 15500: Loss = -11163.71484375
Iteration 15600: Loss = -11163.7158203125
1
Iteration 15700: Loss = -11163.716796875
2
Iteration 15800: Loss = -11163.71484375
Iteration 15900: Loss = -11163.71484375
Iteration 16000: Loss = -11163.71484375
Iteration 16100: Loss = -11163.71484375
Iteration 16200: Loss = -11163.71484375
Iteration 16300: Loss = -11163.71484375
Iteration 16400: Loss = -11163.7138671875
Iteration 16500: Loss = -11163.712890625
Iteration 16600: Loss = -11163.71484375
1
Iteration 16700: Loss = -11163.7119140625
Iteration 16800: Loss = -11163.712890625
1
Iteration 16900: Loss = -11163.7158203125
2
Iteration 17000: Loss = -11163.7138671875
3
Iteration 17100: Loss = -11163.7138671875
4
Iteration 17200: Loss = -11163.712890625
5
Iteration 17300: Loss = -11163.7119140625
Iteration 17400: Loss = -11163.7119140625
Iteration 17500: Loss = -11163.7109375
Iteration 17600: Loss = -11163.7109375
Iteration 17700: Loss = -11163.712890625
1
Iteration 17800: Loss = -11163.7119140625
2
Iteration 17900: Loss = -11163.712890625
3
Iteration 18000: Loss = -11163.712890625
4
Iteration 18100: Loss = -11163.712890625
5
Iteration 18200: Loss = -11163.712890625
6
Iteration 18300: Loss = -11163.7119140625
7
Iteration 18400: Loss = -11163.712890625
8
Iteration 18500: Loss = -11163.712890625
9
Iteration 18600: Loss = -11163.7109375
Iteration 18700: Loss = -11163.7138671875
1
Iteration 18800: Loss = -11163.712890625
2
Iteration 18900: Loss = -11163.7119140625
3
Iteration 19000: Loss = -11163.712890625
4
Iteration 19100: Loss = -11163.7109375
Iteration 19200: Loss = -11163.7138671875
1
Iteration 19300: Loss = -11163.7109375
Iteration 19400: Loss = -11163.7119140625
1
Iteration 19500: Loss = -11163.712890625
2
Iteration 19600: Loss = -11163.712890625
3
Iteration 19700: Loss = -11163.7119140625
4
Iteration 19800: Loss = -11163.7109375
Iteration 19900: Loss = -11163.7158203125
1
Iteration 20000: Loss = -11163.7119140625
2
Iteration 20100: Loss = -11163.712890625
3
Iteration 20200: Loss = -11163.712890625
4
Iteration 20300: Loss = -11163.71484375
5
Iteration 20400: Loss = -11163.712890625
6
Iteration 20500: Loss = -11163.7119140625
7
Iteration 20600: Loss = -11163.7109375
Iteration 20700: Loss = -11163.712890625
1
Iteration 20800: Loss = -11163.7119140625
2
Iteration 20900: Loss = -11163.712890625
3
Iteration 21000: Loss = -11163.7119140625
4
Iteration 21100: Loss = -11163.7119140625
5
Iteration 21200: Loss = -11163.7119140625
6
Iteration 21300: Loss = -11163.7109375
Iteration 21400: Loss = -11163.7119140625
1
Iteration 21500: Loss = -11163.712890625
2
Iteration 21600: Loss = -11163.7119140625
3
Iteration 21700: Loss = -11163.7109375
Iteration 21800: Loss = -11163.712890625
1
Iteration 21900: Loss = -11163.712890625
2
Iteration 22000: Loss = -11163.7119140625
3
Iteration 22100: Loss = -11163.712890625
4
Iteration 22200: Loss = -11163.712890625
5
Iteration 22300: Loss = -11163.7119140625
6
Iteration 22400: Loss = -11163.712890625
7
Iteration 22500: Loss = -11163.712890625
8
Iteration 22600: Loss = -11163.7119140625
9
Iteration 22700: Loss = -11163.7119140625
10
Iteration 22800: Loss = -11163.7119140625
11
Iteration 22900: Loss = -11163.712890625
12
Iteration 23000: Loss = -11163.7119140625
13
Iteration 23100: Loss = -11163.712890625
14
Iteration 23200: Loss = -11163.712890625
15
Stopping early at iteration 23200 due to no improvement.
pi: tensor([[4.4105e-01, 5.5895e-01],
        [3.7630e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9997e-01, 3.3679e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1595, 0.1655],
         [0.9884, 0.1662]],

        [[0.0744, 0.1605],
         [0.9827, 0.8972]],

        [[0.0102, 0.1655],
         [0.1809, 0.9371]],

        [[0.9907, 0.2505],
         [0.0167, 0.1980]],

        [[0.0152, 0.2066],
         [0.9742, 0.2939]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.019559057699512924
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0006921612735767433
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: -0.01075483988213986
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
Global Adjusted Rand Index: 0.0012007486725887514
Average Adjusted Rand Index: 0.0007965337318413182
[-0.0003729734804485935, 0.0012007486725887514] [-0.00013645534424345935, 0.0007965337318413182] [11164.728515625, 11163.712890625]
-------------------------------------
This iteration is 60
True Objective function: Loss = -10697.644427382842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28842.2578125
Iteration 100: Loss = -16268.4560546875
Iteration 200: Loss = -11816.9658203125
Iteration 300: Loss = -11080.560546875
Iteration 400: Loss = -10949.6259765625
Iteration 500: Loss = -10895.666015625
Iteration 600: Loss = -10862.1904296875
Iteration 700: Loss = -10841.640625
Iteration 800: Loss = -10826.9140625
Iteration 900: Loss = -10820.7490234375
Iteration 1000: Loss = -10816.4755859375
Iteration 1100: Loss = -10813.294921875
Iteration 1200: Loss = -10810.8330078125
Iteration 1300: Loss = -10808.87890625
Iteration 1400: Loss = -10807.29296875
Iteration 1500: Loss = -10805.9794921875
Iteration 1600: Loss = -10804.8779296875
Iteration 1700: Loss = -10803.9130859375
Iteration 1800: Loss = -10803.1083984375
Iteration 1900: Loss = -10802.408203125
Iteration 2000: Loss = -10801.791015625
Iteration 2100: Loss = -10801.248046875
Iteration 2200: Loss = -10800.765625
Iteration 2300: Loss = -10800.330078125
Iteration 2400: Loss = -10799.939453125
Iteration 2500: Loss = -10799.5888671875
Iteration 2600: Loss = -10799.2685546875
Iteration 2700: Loss = -10798.98046875
Iteration 2800: Loss = -10798.71484375
Iteration 2900: Loss = -10798.47265625
Iteration 3000: Loss = -10798.2509765625
Iteration 3100: Loss = -10798.048828125
Iteration 3200: Loss = -10797.859375
Iteration 3300: Loss = -10797.685546875
Iteration 3400: Loss = -10797.52734375
Iteration 3500: Loss = -10797.376953125
Iteration 3600: Loss = -10797.23828125
Iteration 3700: Loss = -10797.111328125
Iteration 3800: Loss = -10796.9912109375
Iteration 3900: Loss = -10796.880859375
Iteration 4000: Loss = -10796.77734375
Iteration 4100: Loss = -10796.6826171875
Iteration 4200: Loss = -10796.591796875
Iteration 4300: Loss = -10796.5048828125
Iteration 4400: Loss = -10796.4248046875
Iteration 4500: Loss = -10796.3515625
Iteration 4600: Loss = -10796.283203125
Iteration 4700: Loss = -10796.216796875
Iteration 4800: Loss = -10796.15625
Iteration 4900: Loss = -10796.099609375
Iteration 5000: Loss = -10796.046875
Iteration 5100: Loss = -10795.99609375
Iteration 5200: Loss = -10795.9501953125
Iteration 5300: Loss = -10795.9052734375
Iteration 5400: Loss = -10795.86328125
Iteration 5500: Loss = -10795.8232421875
Iteration 5600: Loss = -10795.787109375
Iteration 5700: Loss = -10795.75
Iteration 5800: Loss = -10795.7158203125
Iteration 5900: Loss = -10795.6865234375
Iteration 6000: Loss = -10795.65625
Iteration 6100: Loss = -10795.626953125
Iteration 6200: Loss = -10795.6005859375
Iteration 6300: Loss = -10795.5751953125
Iteration 6400: Loss = -10795.552734375
Iteration 6500: Loss = -10795.5283203125
Iteration 6600: Loss = -10795.5078125
Iteration 6700: Loss = -10795.486328125
Iteration 6800: Loss = -10795.466796875
Iteration 6900: Loss = -10795.4482421875
Iteration 7000: Loss = -10795.4296875
Iteration 7100: Loss = -10795.412109375
Iteration 7200: Loss = -10795.3974609375
Iteration 7300: Loss = -10795.3828125
Iteration 7400: Loss = -10795.3642578125
Iteration 7500: Loss = -10795.3515625
Iteration 7600: Loss = -10795.3369140625
Iteration 7700: Loss = -10795.322265625
Iteration 7800: Loss = -10795.3115234375
Iteration 7900: Loss = -10795.2958984375
Iteration 8000: Loss = -10795.2841796875
Iteration 8100: Loss = -10795.2734375
Iteration 8200: Loss = -10795.2607421875
Iteration 8300: Loss = -10795.2509765625
Iteration 8400: Loss = -10795.23828125
Iteration 8500: Loss = -10795.2275390625
Iteration 8600: Loss = -10795.21875
Iteration 8700: Loss = -10795.208984375
Iteration 8800: Loss = -10795.19921875
Iteration 8900: Loss = -10795.189453125
Iteration 9000: Loss = -10795.181640625
Iteration 9100: Loss = -10795.173828125
Iteration 9200: Loss = -10795.1650390625
Iteration 9300: Loss = -10795.15625
Iteration 9400: Loss = -10795.1494140625
Iteration 9500: Loss = -10795.142578125
Iteration 9600: Loss = -10795.1337890625
Iteration 9700: Loss = -10795.12890625
Iteration 9800: Loss = -10795.119140625
Iteration 9900: Loss = -10795.1142578125
Iteration 10000: Loss = -10795.107421875
Iteration 10100: Loss = -10795.1015625
Iteration 10200: Loss = -10795.0947265625
Iteration 10300: Loss = -10795.087890625
Iteration 10400: Loss = -10795.0830078125
Iteration 10500: Loss = -10795.0771484375
Iteration 10600: Loss = -10795.0703125
Iteration 10700: Loss = -10795.0654296875
Iteration 10800: Loss = -10795.060546875
Iteration 10900: Loss = -10795.0546875
Iteration 11000: Loss = -10795.0498046875
Iteration 11100: Loss = -10795.0419921875
Iteration 11200: Loss = -10795.037109375
Iteration 11300: Loss = -10795.03125
Iteration 11400: Loss = -10795.025390625
Iteration 11500: Loss = -10795.0185546875
Iteration 11600: Loss = -10795.0107421875
Iteration 11700: Loss = -10795.001953125
Iteration 11800: Loss = -10794.9931640625
Iteration 11900: Loss = -10794.9833984375
Iteration 12000: Loss = -10794.97265625
Iteration 12100: Loss = -10794.9580078125
Iteration 12200: Loss = -10794.9423828125
Iteration 12300: Loss = -10794.9228515625
Iteration 12400: Loss = -10794.9013671875
Iteration 12500: Loss = -10794.8671875
Iteration 12600: Loss = -10794.82421875
Iteration 12700: Loss = -10794.7646484375
Iteration 12800: Loss = -10794.6728515625
Iteration 12900: Loss = -10794.51953125
Iteration 13000: Loss = -10794.26953125
Iteration 13100: Loss = -10793.9306640625
Iteration 13200: Loss = -10793.623046875
Iteration 13300: Loss = -10793.3720703125
Iteration 13400: Loss = -10793.166015625
Iteration 13500: Loss = -10792.986328125
Iteration 13600: Loss = -10792.791015625
Iteration 13700: Loss = -10792.59375
Iteration 13800: Loss = -10792.541015625
Iteration 13900: Loss = -10792.5126953125
Iteration 14000: Loss = -10792.4921875
Iteration 14100: Loss = -10792.47265625
Iteration 14200: Loss = -10792.447265625
Iteration 14300: Loss = -10792.4326171875
Iteration 14400: Loss = -10792.4267578125
Iteration 14500: Loss = -10792.419921875
Iteration 14600: Loss = -10792.4189453125
Iteration 14700: Loss = -10792.4150390625
Iteration 14800: Loss = -10792.41015625
Iteration 14900: Loss = -10792.408203125
Iteration 15000: Loss = -10792.40625
Iteration 15100: Loss = -10792.4052734375
Iteration 15200: Loss = -10792.4033203125
Iteration 15300: Loss = -10792.4052734375
1
Iteration 15400: Loss = -10792.40234375
Iteration 15500: Loss = -10792.4013671875
Iteration 15600: Loss = -10792.400390625
Iteration 15700: Loss = -10792.3994140625
Iteration 15800: Loss = -10792.3984375
Iteration 15900: Loss = -10792.3984375
Iteration 16000: Loss = -10792.3984375
Iteration 16100: Loss = -10792.396484375
Iteration 16200: Loss = -10792.3984375
1
Iteration 16300: Loss = -10792.3984375
2
Iteration 16400: Loss = -10792.396484375
Iteration 16500: Loss = -10792.396484375
Iteration 16600: Loss = -10792.3974609375
1
Iteration 16700: Loss = -10792.3955078125
Iteration 16800: Loss = -10792.3955078125
Iteration 16900: Loss = -10792.396484375
1
Iteration 17000: Loss = -10792.396484375
2
Iteration 17100: Loss = -10792.396484375
3
Iteration 17200: Loss = -10792.39453125
Iteration 17300: Loss = -10792.39453125
Iteration 17400: Loss = -10792.3955078125
1
Iteration 17500: Loss = -10792.3935546875
Iteration 17600: Loss = -10792.3955078125
1
Iteration 17700: Loss = -10792.39453125
2
Iteration 17800: Loss = -10792.39453125
3
Iteration 17900: Loss = -10792.3955078125
4
Iteration 18000: Loss = -10792.39453125
5
Iteration 18100: Loss = -10792.39453125
6
Iteration 18200: Loss = -10792.3955078125
7
Iteration 18300: Loss = -10792.3955078125
8
Iteration 18400: Loss = -10792.3955078125
9
Iteration 18500: Loss = -10792.392578125
Iteration 18600: Loss = -10792.3935546875
1
Iteration 18700: Loss = -10792.3935546875
2
Iteration 18800: Loss = -10792.3935546875
3
Iteration 18900: Loss = -10792.3935546875
4
Iteration 19000: Loss = -10792.3935546875
5
Iteration 19100: Loss = -10792.392578125
Iteration 19200: Loss = -10792.39453125
1
Iteration 19300: Loss = -10792.39453125
2
Iteration 19400: Loss = -10792.39453125
3
Iteration 19500: Loss = -10792.3935546875
4
Iteration 19600: Loss = -10792.39453125
5
Iteration 19700: Loss = -10792.39453125
6
Iteration 19800: Loss = -10792.392578125
Iteration 19900: Loss = -10792.392578125
Iteration 20000: Loss = -10792.396484375
1
Iteration 20100: Loss = -10792.39453125
2
Iteration 20200: Loss = -10792.39453125
3
Iteration 20300: Loss = -10792.3935546875
4
Iteration 20400: Loss = -10792.3935546875
5
Iteration 20500: Loss = -10792.392578125
Iteration 20600: Loss = -10792.3935546875
1
Iteration 20700: Loss = -10792.3935546875
2
Iteration 20800: Loss = -10792.3935546875
3
Iteration 20900: Loss = -10792.39453125
4
Iteration 21000: Loss = -10792.392578125
Iteration 21100: Loss = -10792.3935546875
1
Iteration 21200: Loss = -10792.3935546875
2
Iteration 21300: Loss = -10792.39453125
3
Iteration 21400: Loss = -10792.392578125
Iteration 21500: Loss = -10792.3935546875
1
Iteration 21600: Loss = -10792.392578125
Iteration 21700: Loss = -10792.39453125
1
Iteration 21800: Loss = -10792.3935546875
2
Iteration 21900: Loss = -10792.39453125
3
Iteration 22000: Loss = -10792.3935546875
4
Iteration 22100: Loss = -10792.392578125
Iteration 22200: Loss = -10792.3935546875
1
Iteration 22300: Loss = -10792.392578125
Iteration 22400: Loss = -10792.3935546875
1
Iteration 22500: Loss = -10792.3935546875
2
Iteration 22600: Loss = -10792.3935546875
3
Iteration 22700: Loss = -10792.39453125
4
Iteration 22800: Loss = -10792.392578125
Iteration 22900: Loss = -10792.3935546875
1
Iteration 23000: Loss = -10792.39453125
2
Iteration 23100: Loss = -10792.3935546875
3
Iteration 23200: Loss = -10792.392578125
Iteration 23300: Loss = -10792.3935546875
1
Iteration 23400: Loss = -10792.392578125
Iteration 23500: Loss = -10792.392578125
Iteration 23600: Loss = -10792.3935546875
1
Iteration 23700: Loss = -10792.392578125
Iteration 23800: Loss = -10792.3935546875
1
Iteration 23900: Loss = -10792.3935546875
2
Iteration 24000: Loss = -10792.392578125
Iteration 24100: Loss = -10792.39453125
1
Iteration 24200: Loss = -10792.392578125
Iteration 24300: Loss = -10792.392578125
Iteration 24400: Loss = -10792.392578125
Iteration 24500: Loss = -10792.392578125
Iteration 24600: Loss = -10792.39453125
1
Iteration 24700: Loss = -10792.39453125
2
Iteration 24800: Loss = -10792.3935546875
3
Iteration 24900: Loss = -10792.392578125
Iteration 25000: Loss = -10792.392578125
Iteration 25100: Loss = -10792.3935546875
1
Iteration 25200: Loss = -10792.392578125
Iteration 25300: Loss = -10792.392578125
Iteration 25400: Loss = -10792.3935546875
1
Iteration 25500: Loss = -10792.39453125
2
Iteration 25600: Loss = -10792.392578125
Iteration 25700: Loss = -10792.3935546875
1
Iteration 25800: Loss = -10792.3935546875
2
Iteration 25900: Loss = -10792.3935546875
3
Iteration 26000: Loss = -10792.3935546875
4
Iteration 26100: Loss = -10792.3935546875
5
Iteration 26200: Loss = -10792.392578125
Iteration 26300: Loss = -10792.392578125
Iteration 26400: Loss = -10792.392578125
Iteration 26500: Loss = -10792.39453125
1
Iteration 26600: Loss = -10792.39453125
2
Iteration 26700: Loss = -10792.39453125
3
Iteration 26800: Loss = -10792.3935546875
4
Iteration 26900: Loss = -10792.39453125
5
Iteration 27000: Loss = -10792.3935546875
6
Iteration 27100: Loss = -10792.3935546875
7
Iteration 27200: Loss = -10792.392578125
Iteration 27300: Loss = -10792.3935546875
1
Iteration 27400: Loss = -10792.3935546875
2
Iteration 27500: Loss = -10792.392578125
Iteration 27600: Loss = -10792.3935546875
1
Iteration 27700: Loss = -10792.3935546875
2
Iteration 27800: Loss = -10792.39453125
3
Iteration 27900: Loss = -10792.3935546875
4
Iteration 28000: Loss = -10792.3935546875
5
Iteration 28100: Loss = -10792.392578125
Iteration 28200: Loss = -10792.3935546875
1
Iteration 28300: Loss = -10792.3935546875
2
Iteration 28400: Loss = -10792.3935546875
3
Iteration 28500: Loss = -10792.3935546875
4
Iteration 28600: Loss = -10792.3935546875
5
Iteration 28700: Loss = -10792.3935546875
6
Iteration 28800: Loss = -10792.392578125
Iteration 28900: Loss = -10792.3935546875
1
Iteration 29000: Loss = -10792.3935546875
2
Iteration 29100: Loss = -10792.392578125
Iteration 29200: Loss = -10792.3935546875
1
Iteration 29300: Loss = -10792.3935546875
2
Iteration 29400: Loss = -10792.3935546875
3
Iteration 29500: Loss = -10792.39453125
4
Iteration 29600: Loss = -10792.392578125
Iteration 29700: Loss = -10792.392578125
Iteration 29800: Loss = -10792.39453125
1
Iteration 29900: Loss = -10792.39453125
2
pi: tensor([[8.7677e-01, 1.2323e-01],
        [9.9999e-01, 1.1829e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9928, 0.0072], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1622, 0.2008],
         [0.0427, 0.1291]],

        [[0.0709, 0.1568],
         [0.1591, 0.8394]],

        [[0.8069, 0.1389],
         [0.7346, 0.4861]],

        [[0.8760, 0.1019],
         [0.7412, 0.9900]],

        [[0.9921, 0.1484],
         [0.0626, 0.3499]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.046788024672784714
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0035360056838992164
Average Adjusted Rand Index: 0.009357604934556943
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40582.41015625
Iteration 100: Loss = -22203.59375
Iteration 200: Loss = -13107.587890625
Iteration 300: Loss = -11447.306640625
Iteration 400: Loss = -11176.98046875
Iteration 500: Loss = -11049.9501953125
Iteration 600: Loss = -10981.9765625
Iteration 700: Loss = -10926.125
Iteration 800: Loss = -10887.94921875
Iteration 900: Loss = -10865.892578125
Iteration 1000: Loss = -10853.2470703125
Iteration 1100: Loss = -10844.3583984375
Iteration 1200: Loss = -10837.2470703125
Iteration 1300: Loss = -10831.408203125
Iteration 1400: Loss = -10827.177734375
Iteration 1500: Loss = -10823.994140625
Iteration 1600: Loss = -10821.4365234375
Iteration 1700: Loss = -10819.2958984375
Iteration 1800: Loss = -10817.4306640625
Iteration 1900: Loss = -10815.869140625
Iteration 2000: Loss = -10814.4990234375
Iteration 2100: Loss = -10813.3046875
Iteration 2200: Loss = -10812.244140625
Iteration 2300: Loss = -10811.2998046875
Iteration 2400: Loss = -10810.451171875
Iteration 2500: Loss = -10809.6904296875
Iteration 2600: Loss = -10809.00390625
Iteration 2700: Loss = -10808.37890625
Iteration 2800: Loss = -10807.8125
Iteration 2900: Loss = -10807.2919921875
Iteration 3000: Loss = -10806.8173828125
Iteration 3100: Loss = -10806.3818359375
Iteration 3200: Loss = -10805.9814453125
Iteration 3300: Loss = -10805.611328125
Iteration 3400: Loss = -10805.2685546875
Iteration 3500: Loss = -10804.953125
Iteration 3600: Loss = -10804.6572265625
Iteration 3700: Loss = -10804.3798828125
Iteration 3800: Loss = -10804.115234375
Iteration 3900: Loss = -10802.8828125
Iteration 4000: Loss = -10799.228515625
Iteration 4100: Loss = -10798.7548828125
Iteration 4200: Loss = -10798.4521484375
Iteration 4300: Loss = -10798.2119140625
Iteration 4400: Loss = -10798.005859375
Iteration 4500: Loss = -10797.82421875
Iteration 4600: Loss = -10797.66015625
Iteration 4700: Loss = -10797.5107421875
Iteration 4800: Loss = -10797.3759765625
Iteration 4900: Loss = -10797.248046875
Iteration 5000: Loss = -10797.1328125
Iteration 5100: Loss = -10797.0224609375
Iteration 5200: Loss = -10796.9208984375
Iteration 5300: Loss = -10796.826171875
Iteration 5400: Loss = -10796.7373046875
Iteration 5500: Loss = -10796.654296875
Iteration 5600: Loss = -10796.5751953125
Iteration 5700: Loss = -10796.5009765625
Iteration 5800: Loss = -10796.431640625
Iteration 5900: Loss = -10796.3671875
Iteration 6000: Loss = -10796.306640625
Iteration 6100: Loss = -10796.2470703125
Iteration 6200: Loss = -10796.1923828125
Iteration 6300: Loss = -10796.1416015625
Iteration 6400: Loss = -10796.091796875
Iteration 6500: Loss = -10796.0458984375
Iteration 6600: Loss = -10796.0
Iteration 6700: Loss = -10795.958984375
Iteration 6800: Loss = -10795.9189453125
Iteration 6900: Loss = -10795.8828125
Iteration 7000: Loss = -10795.84765625
Iteration 7100: Loss = -10795.8125
Iteration 7200: Loss = -10795.7802734375
Iteration 7300: Loss = -10795.7490234375
Iteration 7400: Loss = -10795.720703125
Iteration 7500: Loss = -10795.6943359375
Iteration 7600: Loss = -10795.669921875
Iteration 7700: Loss = -10795.64453125
Iteration 7800: Loss = -10795.62109375
Iteration 7900: Loss = -10795.599609375
Iteration 8000: Loss = -10795.578125
Iteration 8100: Loss = -10795.556640625
Iteration 8200: Loss = -10795.5390625
Iteration 8300: Loss = -10795.5185546875
Iteration 8400: Loss = -10795.5029296875
Iteration 8500: Loss = -10795.486328125
Iteration 8600: Loss = -10795.4716796875
Iteration 8700: Loss = -10795.45703125
Iteration 8800: Loss = -10795.44140625
Iteration 8900: Loss = -10795.4296875
Iteration 9000: Loss = -10795.4150390625
Iteration 9100: Loss = -10795.404296875
Iteration 9200: Loss = -10795.3896484375
Iteration 9300: Loss = -10795.3798828125
Iteration 9400: Loss = -10795.3681640625
Iteration 9500: Loss = -10795.3583984375
Iteration 9600: Loss = -10795.349609375
Iteration 9700: Loss = -10795.3388671875
Iteration 9800: Loss = -10795.3291015625
Iteration 9900: Loss = -10795.3193359375
Iteration 10000: Loss = -10795.30859375
Iteration 10100: Loss = -10795.3017578125
Iteration 10200: Loss = -10795.2939453125
Iteration 10300: Loss = -10795.2880859375
Iteration 10400: Loss = -10795.279296875
Iteration 10500: Loss = -10795.271484375
Iteration 10600: Loss = -10795.267578125
Iteration 10700: Loss = -10795.2607421875
Iteration 10800: Loss = -10795.2529296875
Iteration 10900: Loss = -10795.2490234375
Iteration 11000: Loss = -10795.244140625
Iteration 11100: Loss = -10795.2392578125
Iteration 11200: Loss = -10795.234375
Iteration 11300: Loss = -10795.2294921875
Iteration 11400: Loss = -10795.224609375
Iteration 11500: Loss = -10795.22265625
Iteration 11600: Loss = -10795.216796875
Iteration 11700: Loss = -10795.216796875
Iteration 11800: Loss = -10795.2138671875
Iteration 11900: Loss = -10795.208984375
Iteration 12000: Loss = -10795.2080078125
Iteration 12100: Loss = -10795.2060546875
Iteration 12200: Loss = -10795.203125
Iteration 12300: Loss = -10795.2021484375
Iteration 12400: Loss = -10795.1982421875
Iteration 12500: Loss = -10795.19921875
1
Iteration 12600: Loss = -10795.197265625
Iteration 12700: Loss = -10795.1943359375
Iteration 12800: Loss = -10795.1943359375
Iteration 12900: Loss = -10795.1923828125
Iteration 13000: Loss = -10795.19140625
Iteration 13100: Loss = -10795.19140625
Iteration 13200: Loss = -10795.1875
Iteration 13300: Loss = -10795.1875
Iteration 13400: Loss = -10795.185546875
Iteration 13500: Loss = -10795.1865234375
1
Iteration 13600: Loss = -10795.1845703125
Iteration 13700: Loss = -10795.181640625
Iteration 13800: Loss = -10795.181640625
Iteration 13900: Loss = -10795.1806640625
Iteration 14000: Loss = -10795.1796875
Iteration 14100: Loss = -10795.1796875
Iteration 14200: Loss = -10795.1796875
Iteration 14300: Loss = -10795.1787109375
Iteration 14400: Loss = -10795.1787109375
Iteration 14500: Loss = -10795.177734375
Iteration 14600: Loss = -10795.1767578125
Iteration 14700: Loss = -10795.1767578125
Iteration 14800: Loss = -10795.1767578125
Iteration 14900: Loss = -10795.17578125
Iteration 15000: Loss = -10795.1748046875
Iteration 15100: Loss = -10795.173828125
Iteration 15200: Loss = -10795.1728515625
Iteration 15300: Loss = -10795.1728515625
Iteration 15400: Loss = -10795.1728515625
Iteration 15500: Loss = -10795.1708984375
Iteration 15600: Loss = -10795.1708984375
Iteration 15700: Loss = -10795.169921875
Iteration 15800: Loss = -10795.169921875
Iteration 15900: Loss = -10795.1669921875
Iteration 16000: Loss = -10795.1689453125
1
Iteration 16100: Loss = -10795.166015625
Iteration 16200: Loss = -10795.166015625
Iteration 16300: Loss = -10795.1650390625
Iteration 16400: Loss = -10795.162109375
Iteration 16500: Loss = -10795.162109375
Iteration 16600: Loss = -10795.1611328125
Iteration 16700: Loss = -10795.158203125
Iteration 16800: Loss = -10795.1552734375
Iteration 16900: Loss = -10795.1533203125
Iteration 17000: Loss = -10795.1484375
Iteration 17100: Loss = -10795.14453125
Iteration 17200: Loss = -10795.1337890625
Iteration 17300: Loss = -10795.1162109375
Iteration 17400: Loss = -10795.07421875
Iteration 17500: Loss = -10794.9736328125
Iteration 17600: Loss = -10794.46875
Iteration 17700: Loss = -10793.6748046875
Iteration 17800: Loss = -10793.2724609375
Iteration 17900: Loss = -10792.875
Iteration 18000: Loss = -10792.4833984375
Iteration 18100: Loss = -10792.443359375
Iteration 18200: Loss = -10792.416015625
Iteration 18300: Loss = -10792.404296875
Iteration 18400: Loss = -10792.40234375
Iteration 18500: Loss = -10792.3994140625
Iteration 18600: Loss = -10792.3994140625
Iteration 18700: Loss = -10792.3974609375
Iteration 18800: Loss = -10792.3984375
1
Iteration 18900: Loss = -10792.396484375
Iteration 19000: Loss = -10792.39453125
Iteration 19100: Loss = -10792.3955078125
1
Iteration 19200: Loss = -10792.3955078125
2
Iteration 19300: Loss = -10792.3935546875
Iteration 19400: Loss = -10792.3955078125
1
Iteration 19500: Loss = -10792.3955078125
2
Iteration 19600: Loss = -10792.3955078125
3
Iteration 19700: Loss = -10792.39453125
4
Iteration 19800: Loss = -10792.39453125
5
Iteration 19900: Loss = -10792.39453125
6
Iteration 20000: Loss = -10792.39453125
7
Iteration 20100: Loss = -10792.3955078125
8
Iteration 20200: Loss = -10792.39453125
9
Iteration 20300: Loss = -10792.3935546875
Iteration 20400: Loss = -10792.3955078125
1
Iteration 20500: Loss = -10792.39453125
2
Iteration 20600: Loss = -10792.3935546875
Iteration 20700: Loss = -10792.3935546875
Iteration 20800: Loss = -10792.3935546875
Iteration 20900: Loss = -10792.3935546875
Iteration 21000: Loss = -10792.3935546875
Iteration 21100: Loss = -10792.3935546875
Iteration 21200: Loss = -10792.3935546875
Iteration 21300: Loss = -10792.3955078125
1
Iteration 21400: Loss = -10792.3935546875
Iteration 21500: Loss = -10792.3935546875
Iteration 21600: Loss = -10792.3935546875
Iteration 21700: Loss = -10792.39453125
1
Iteration 21800: Loss = -10792.39453125
2
Iteration 21900: Loss = -10792.3935546875
Iteration 22000: Loss = -10792.3935546875
Iteration 22100: Loss = -10792.3935546875
Iteration 22200: Loss = -10792.39453125
1
Iteration 22300: Loss = -10792.3935546875
Iteration 22400: Loss = -10792.39453125
1
Iteration 22500: Loss = -10792.39453125
2
Iteration 22600: Loss = -10792.39453125
3
Iteration 22700: Loss = -10792.392578125
Iteration 22800: Loss = -10792.3935546875
1
Iteration 22900: Loss = -10792.392578125
Iteration 23000: Loss = -10792.392578125
Iteration 23100: Loss = -10792.3935546875
1
Iteration 23200: Loss = -10792.3935546875
2
Iteration 23300: Loss = -10792.392578125
Iteration 23400: Loss = -10792.392578125
Iteration 23500: Loss = -10792.3935546875
1
Iteration 23600: Loss = -10792.392578125
Iteration 23700: Loss = -10792.3935546875
1
Iteration 23800: Loss = -10792.392578125
Iteration 23900: Loss = -10792.3935546875
1
Iteration 24000: Loss = -10792.3935546875
2
Iteration 24100: Loss = -10792.392578125
Iteration 24200: Loss = -10792.392578125
Iteration 24300: Loss = -10792.3935546875
1
Iteration 24400: Loss = -10792.3935546875
2
Iteration 24500: Loss = -10792.392578125
Iteration 24600: Loss = -10792.3935546875
1
Iteration 24700: Loss = -10792.392578125
Iteration 24800: Loss = -10792.3935546875
1
Iteration 24900: Loss = -10792.392578125
Iteration 25000: Loss = -10792.3935546875
1
Iteration 25100: Loss = -10792.3935546875
2
Iteration 25200: Loss = -10792.3916015625
Iteration 25300: Loss = -10792.3935546875
1
Iteration 25400: Loss = -10792.392578125
2
Iteration 25500: Loss = -10792.392578125
3
Iteration 25600: Loss = -10792.392578125
4
Iteration 25700: Loss = -10792.3935546875
5
Iteration 25800: Loss = -10792.396484375
6
Iteration 25900: Loss = -10792.392578125
7
Iteration 26000: Loss = -10792.3935546875
8
Iteration 26100: Loss = -10792.39453125
9
Iteration 26200: Loss = -10792.3935546875
10
Iteration 26300: Loss = -10792.3935546875
11
Iteration 26400: Loss = -10792.3935546875
12
Iteration 26500: Loss = -10792.392578125
13
Iteration 26600: Loss = -10792.3935546875
14
Iteration 26700: Loss = -10792.3935546875
15
Stopping early at iteration 26700 due to no improvement.
pi: tensor([[3.1015e-05, 9.9997e-01],
        [1.2305e-01, 8.7695e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0072, 0.9928], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1291, 0.2009],
         [0.7316, 0.1621]],

        [[0.9931, 0.1568],
         [0.0686, 0.9834]],

        [[0.0367, 0.1389],
         [0.3332, 0.9302]],

        [[0.0151, 0.1018],
         [0.0253, 0.0073]],

        [[0.0300, 0.1484],
         [0.9846, 0.6968]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.046788024672784714
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0035360056838992164
Average Adjusted Rand Index: 0.009357604934556943
[0.0035360056838992164, 0.0035360056838992164] [0.009357604934556943, 0.009357604934556943] [10792.3935546875, 10792.3935546875]
-------------------------------------
This iteration is 61
True Objective function: Loss = -10730.421627466018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31390.00390625
Iteration 100: Loss = -22463.484375
Iteration 200: Loss = -13964.19140625
Iteration 300: Loss = -11550.3525390625
Iteration 400: Loss = -11194.9267578125
Iteration 500: Loss = -11081.640625
Iteration 600: Loss = -11019.748046875
Iteration 700: Loss = -10980.8681640625
Iteration 800: Loss = -10954.1328125
Iteration 900: Loss = -10935.498046875
Iteration 1000: Loss = -10921.53125
Iteration 1100: Loss = -10910.3310546875
Iteration 1200: Loss = -10900.5908203125
Iteration 1300: Loss = -10889.470703125
Iteration 1400: Loss = -10882.126953125
Iteration 1500: Loss = -10874.537109375
Iteration 1600: Loss = -10866.41015625
Iteration 1700: Loss = -10860.330078125
Iteration 1800: Loss = -10855.6953125
Iteration 1900: Loss = -10851.759765625
Iteration 2000: Loss = -10849.3876953125
Iteration 2100: Loss = -10846.8271484375
Iteration 2200: Loss = -10844.5009765625
Iteration 2300: Loss = -10841.7958984375
Iteration 2400: Loss = -10837.0068359375
Iteration 2500: Loss = -10834.158203125
Iteration 2600: Loss = -10830.046875
Iteration 2700: Loss = -10826.732421875
Iteration 2800: Loss = -10825.396484375
Iteration 2900: Loss = -10824.59375
Iteration 3000: Loss = -10823.7060546875
Iteration 3100: Loss = -10821.091796875
Iteration 3200: Loss = -10819.8193359375
Iteration 3300: Loss = -10818.5166015625
Iteration 3400: Loss = -10817.52734375
Iteration 3500: Loss = -10816.4130859375
Iteration 3600: Loss = -10814.6171875
Iteration 3700: Loss = -10813.521484375
Iteration 3800: Loss = -10813.1943359375
Iteration 3900: Loss = -10812.9130859375
Iteration 4000: Loss = -10812.6455078125
Iteration 4100: Loss = -10812.107421875
Iteration 4200: Loss = -10809.171875
Iteration 4300: Loss = -10808.91796875
Iteration 4400: Loss = -10808.72265625
Iteration 4500: Loss = -10808.54296875
Iteration 4600: Loss = -10807.103515625
Iteration 4700: Loss = -10806.80859375
Iteration 4800: Loss = -10805.65625
Iteration 4900: Loss = -10805.4873046875
Iteration 5000: Loss = -10805.3681640625
Iteration 5100: Loss = -10805.259765625
Iteration 5200: Loss = -10805.1572265625
Iteration 5300: Loss = -10805.0625
Iteration 5400: Loss = -10804.9677734375
Iteration 5500: Loss = -10804.6171875
Iteration 5600: Loss = -10802.5146484375
Iteration 5700: Loss = -10802.40234375
Iteration 5800: Loss = -10802.3271484375
Iteration 5900: Loss = -10802.2568359375
Iteration 6000: Loss = -10802.1953125
Iteration 6100: Loss = -10802.130859375
Iteration 6200: Loss = -10802.0458984375
Iteration 6300: Loss = -10801.9951171875
Iteration 6400: Loss = -10801.9453125
Iteration 6500: Loss = -10801.8935546875
Iteration 6600: Loss = -10801.6015625
Iteration 6700: Loss = -10801.560546875
Iteration 6800: Loss = -10801.5224609375
Iteration 6900: Loss = -10801.486328125
Iteration 7000: Loss = -10801.30078125
Iteration 7100: Loss = -10799.1591796875
Iteration 7200: Loss = -10798.5859375
Iteration 7300: Loss = -10798.5009765625
Iteration 7400: Loss = -10798.4482421875
Iteration 7500: Loss = -10798.4091796875
Iteration 7600: Loss = -10798.3779296875
Iteration 7700: Loss = -10798.3505859375
Iteration 7800: Loss = -10798.3232421875
Iteration 7900: Loss = -10798.2998046875
Iteration 8000: Loss = -10798.2822265625
Iteration 8100: Loss = -10798.26171875
Iteration 8200: Loss = -10798.244140625
Iteration 8300: Loss = -10798.2275390625
Iteration 8400: Loss = -10798.2109375
Iteration 8500: Loss = -10798.1962890625
Iteration 8600: Loss = -10798.1826171875
Iteration 8700: Loss = -10798.169921875
Iteration 8800: Loss = -10798.1572265625
Iteration 8900: Loss = -10798.1455078125
Iteration 9000: Loss = -10798.134765625
Iteration 9100: Loss = -10798.1240234375
Iteration 9200: Loss = -10798.11328125
Iteration 9300: Loss = -10798.1044921875
Iteration 9400: Loss = -10798.095703125
Iteration 9500: Loss = -10798.0888671875
Iteration 9600: Loss = -10798.080078125
Iteration 9700: Loss = -10798.0732421875
Iteration 9800: Loss = -10798.064453125
Iteration 9900: Loss = -10798.05859375
Iteration 10000: Loss = -10798.05078125
Iteration 10100: Loss = -10798.044921875
Iteration 10200: Loss = -10798.037109375
Iteration 10300: Loss = -10798.0322265625
Iteration 10400: Loss = -10798.025390625
Iteration 10500: Loss = -10798.021484375
Iteration 10600: Loss = -10798.0166015625
Iteration 10700: Loss = -10798.009765625
Iteration 10800: Loss = -10798.0048828125
Iteration 10900: Loss = -10798.001953125
Iteration 11000: Loss = -10797.99609375
Iteration 11100: Loss = -10797.994140625
Iteration 11200: Loss = -10797.9892578125
Iteration 11300: Loss = -10797.9853515625
Iteration 11400: Loss = -10797.982421875
Iteration 11500: Loss = -10797.98046875
Iteration 11600: Loss = -10797.9755859375
Iteration 11700: Loss = -10797.9716796875
Iteration 11800: Loss = -10797.9716796875
Iteration 11900: Loss = -10797.9677734375
Iteration 12000: Loss = -10797.96484375
Iteration 12100: Loss = -10797.962890625
Iteration 12200: Loss = -10797.9609375
Iteration 12300: Loss = -10797.958984375
Iteration 12400: Loss = -10797.955078125
Iteration 12500: Loss = -10797.9560546875
1
Iteration 12600: Loss = -10797.953125
Iteration 12700: Loss = -10797.951171875
Iteration 12800: Loss = -10797.94921875
Iteration 12900: Loss = -10797.9482421875
Iteration 13000: Loss = -10797.9462890625
Iteration 13100: Loss = -10797.9453125
Iteration 13200: Loss = -10797.9453125
Iteration 13300: Loss = -10797.9423828125
Iteration 13400: Loss = -10797.94140625
Iteration 13500: Loss = -10797.9404296875
Iteration 13600: Loss = -10797.9404296875
Iteration 13700: Loss = -10797.9384765625
Iteration 13800: Loss = -10797.9365234375
Iteration 13900: Loss = -10797.9365234375
Iteration 14000: Loss = -10797.9345703125
Iteration 14100: Loss = -10797.9345703125
Iteration 14200: Loss = -10797.93359375
Iteration 14300: Loss = -10797.93359375
Iteration 14400: Loss = -10797.9326171875
Iteration 14500: Loss = -10797.7451171875
Iteration 14600: Loss = -10797.6083984375
Iteration 14700: Loss = -10797.5966796875
Iteration 14800: Loss = -10797.591796875
Iteration 14900: Loss = -10797.58984375
Iteration 15000: Loss = -10797.587890625
Iteration 15100: Loss = -10797.58984375
1
Iteration 15200: Loss = -10797.5869140625
Iteration 15300: Loss = -10797.5869140625
Iteration 15400: Loss = -10797.5859375
Iteration 15500: Loss = -10797.5849609375
Iteration 15600: Loss = -10797.5830078125
Iteration 15700: Loss = -10797.5849609375
1
Iteration 15800: Loss = -10797.583984375
2
Iteration 15900: Loss = -10797.58203125
Iteration 16000: Loss = -10797.5810546875
Iteration 16100: Loss = -10797.5810546875
Iteration 16200: Loss = -10797.58203125
1
Iteration 16300: Loss = -10797.5810546875
Iteration 16400: Loss = -10797.58203125
1
Iteration 16500: Loss = -10797.5810546875
Iteration 16600: Loss = -10797.58203125
1
Iteration 16700: Loss = -10797.5810546875
Iteration 16800: Loss = -10797.580078125
Iteration 16900: Loss = -10797.583984375
1
Iteration 17000: Loss = -10797.580078125
Iteration 17100: Loss = -10797.580078125
Iteration 17200: Loss = -10797.5791015625
Iteration 17300: Loss = -10797.578125
Iteration 17400: Loss = -10797.578125
Iteration 17500: Loss = -10797.578125
Iteration 17600: Loss = -10797.5791015625
1
Iteration 17700: Loss = -10797.578125
Iteration 17800: Loss = -10797.5791015625
1
Iteration 17900: Loss = -10797.578125
Iteration 18000: Loss = -10797.578125
Iteration 18100: Loss = -10797.5673828125
Iteration 18200: Loss = -10797.56640625
Iteration 18300: Loss = -10797.5654296875
Iteration 18400: Loss = -10797.5673828125
1
Iteration 18500: Loss = -10797.56640625
2
Iteration 18600: Loss = -10797.564453125
Iteration 18700: Loss = -10797.564453125
Iteration 18800: Loss = -10797.5654296875
1
Iteration 18900: Loss = -10797.5654296875
2
Iteration 19000: Loss = -10797.564453125
Iteration 19100: Loss = -10797.5654296875
1
Iteration 19200: Loss = -10797.5654296875
2
Iteration 19300: Loss = -10797.5654296875
3
Iteration 19400: Loss = -10797.5654296875
4
Iteration 19500: Loss = -10797.564453125
Iteration 19600: Loss = -10797.5654296875
1
Iteration 19700: Loss = -10797.5654296875
2
Iteration 19800: Loss = -10797.5654296875
3
Iteration 19900: Loss = -10797.5634765625
Iteration 20000: Loss = -10797.5634765625
Iteration 20100: Loss = -10797.5634765625
Iteration 20200: Loss = -10797.564453125
1
Iteration 20300: Loss = -10797.564453125
2
Iteration 20400: Loss = -10797.564453125
3
Iteration 20500: Loss = -10797.564453125
4
Iteration 20600: Loss = -10797.564453125
5
Iteration 20700: Loss = -10797.5654296875
6
Iteration 20800: Loss = -10797.5634765625
Iteration 20900: Loss = -10797.564453125
1
Iteration 21000: Loss = -10797.5634765625
Iteration 21100: Loss = -10797.56640625
1
Iteration 21200: Loss = -10797.564453125
2
Iteration 21300: Loss = -10797.564453125
3
Iteration 21400: Loss = -10797.56640625
4
Iteration 21500: Loss = -10797.564453125
5
Iteration 21600: Loss = -10797.564453125
6
Iteration 21700: Loss = -10797.5634765625
Iteration 21800: Loss = -10797.56640625
1
Iteration 21900: Loss = -10797.5634765625
Iteration 22000: Loss = -10797.5634765625
Iteration 22100: Loss = -10797.5625
Iteration 22200: Loss = -10797.564453125
1
Iteration 22300: Loss = -10797.564453125
2
Iteration 22400: Loss = -10797.5634765625
3
Iteration 22500: Loss = -10797.5654296875
4
Iteration 22600: Loss = -10797.5634765625
5
Iteration 22700: Loss = -10797.5625
Iteration 22800: Loss = -10797.564453125
1
Iteration 22900: Loss = -10797.564453125
2
Iteration 23000: Loss = -10797.564453125
3
Iteration 23100: Loss = -10797.5625
Iteration 23200: Loss = -10797.564453125
1
Iteration 23300: Loss = -10797.5654296875
2
Iteration 23400: Loss = -10797.5625
Iteration 23500: Loss = -10797.5634765625
1
Iteration 23600: Loss = -10797.564453125
2
Iteration 23700: Loss = -10797.564453125
3
Iteration 23800: Loss = -10797.564453125
4
Iteration 23900: Loss = -10797.5634765625
5
Iteration 24000: Loss = -10797.5634765625
6
Iteration 24100: Loss = -10797.5634765625
7
Iteration 24200: Loss = -10797.564453125
8
Iteration 24300: Loss = -10797.5625
Iteration 24400: Loss = -10797.5625
Iteration 24500: Loss = -10797.5634765625
1
Iteration 24600: Loss = -10797.564453125
2
Iteration 24700: Loss = -10797.564453125
3
Iteration 24800: Loss = -10797.5634765625
4
Iteration 24900: Loss = -10797.5634765625
5
Iteration 25000: Loss = -10797.5634765625
6
Iteration 25100: Loss = -10797.564453125
7
Iteration 25200: Loss = -10797.564453125
8
Iteration 25300: Loss = -10797.560546875
Iteration 25400: Loss = -10797.5537109375
Iteration 25500: Loss = -10797.552734375
Iteration 25600: Loss = -10797.5537109375
1
Iteration 25700: Loss = -10797.5546875
2
Iteration 25800: Loss = -10797.5498046875
Iteration 25900: Loss = -10797.533203125
Iteration 26000: Loss = -10797.5224609375
Iteration 26100: Loss = -10797.5205078125
Iteration 26200: Loss = -10797.521484375
1
Iteration 26300: Loss = -10797.4765625
Iteration 26400: Loss = -10797.4599609375
Iteration 26500: Loss = -10796.951171875
Iteration 26600: Loss = -10796.9482421875
Iteration 26700: Loss = -10796.94140625
Iteration 26800: Loss = -10796.9296875
Iteration 26900: Loss = -10796.9287109375
Iteration 27000: Loss = -10796.9267578125
Iteration 27100: Loss = -10796.92578125
Iteration 27200: Loss = -10796.92578125
Iteration 27300: Loss = -10796.9248046875
Iteration 27400: Loss = -10796.9248046875
Iteration 27500: Loss = -10796.9248046875
Iteration 27600: Loss = -10796.9208984375
Iteration 27700: Loss = -10796.9208984375
Iteration 27800: Loss = -10796.9111328125
Iteration 27900: Loss = -10796.892578125
Iteration 28000: Loss = -10796.8896484375
Iteration 28100: Loss = -10796.8896484375
Iteration 28200: Loss = -10796.888671875
Iteration 28300: Loss = -10796.8876953125
Iteration 28400: Loss = -10796.88671875
Iteration 28500: Loss = -10796.8876953125
1
Iteration 28600: Loss = -10796.88671875
Iteration 28700: Loss = -10796.88671875
Iteration 28800: Loss = -10796.88671875
Iteration 28900: Loss = -10796.88671875
Iteration 29000: Loss = -10796.88671875
Iteration 29100: Loss = -10796.88671875
Iteration 29200: Loss = -10796.888671875
1
Iteration 29300: Loss = -10796.8876953125
2
Iteration 29400: Loss = -10796.888671875
3
Iteration 29500: Loss = -10796.88671875
Iteration 29600: Loss = -10796.8876953125
1
Iteration 29700: Loss = -10796.8876953125
2
Iteration 29800: Loss = -10796.888671875
3
Iteration 29900: Loss = -10796.88671875
pi: tensor([[1.0000e+00, 5.8369e-07],
        [7.3283e-01, 2.6717e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7834e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.1729],
         [0.8850, 0.1617]],

        [[0.9312, 0.1615],
         [0.8068, 0.3879]],

        [[0.0365, 0.1143],
         [0.1773, 0.9229]],

        [[0.3546, 0.0926],
         [0.8961, 0.3317]],

        [[0.3803, 0.0885],
         [0.0165, 0.0784]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: -0.004058868311844209
Average Adjusted Rand Index: -0.00414891536848411
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20022.330078125
Iteration 100: Loss = -13753.7607421875
Iteration 200: Loss = -11410.46484375
Iteration 300: Loss = -11000.18359375
Iteration 400: Loss = -10924.369140625
Iteration 500: Loss = -10888.1220703125
Iteration 600: Loss = -10869.1484375
Iteration 700: Loss = -10854.83203125
Iteration 800: Loss = -10843.6201171875
Iteration 900: Loss = -10830.0546875
Iteration 1000: Loss = -10821.609375
Iteration 1100: Loss = -10815.76953125
Iteration 1200: Loss = -10811.8662109375
Iteration 1300: Loss = -10809.2158203125
Iteration 1400: Loss = -10807.3125
Iteration 1500: Loss = -10805.865234375
Iteration 1600: Loss = -10804.728515625
Iteration 1700: Loss = -10803.83203125
Iteration 1800: Loss = -10803.1201171875
Iteration 1900: Loss = -10802.5458984375
Iteration 2000: Loss = -10802.0654296875
Iteration 2100: Loss = -10801.6376953125
Iteration 2200: Loss = -10801.298828125
Iteration 2300: Loss = -10801.01171875
Iteration 2400: Loss = -10800.7607421875
Iteration 2500: Loss = -10800.5400390625
Iteration 2600: Loss = -10800.3466796875
Iteration 2700: Loss = -10800.1728515625
Iteration 2800: Loss = -10800.015625
Iteration 2900: Loss = -10799.875
Iteration 3000: Loss = -10799.7470703125
Iteration 3100: Loss = -10799.6328125
Iteration 3200: Loss = -10799.52734375
Iteration 3300: Loss = -10799.4326171875
Iteration 3400: Loss = -10799.3525390625
Iteration 3500: Loss = -10799.2763671875
Iteration 3600: Loss = -10799.2041015625
Iteration 3700: Loss = -10799.1328125
Iteration 3800: Loss = -10799.0625
Iteration 3900: Loss = -10798.9951171875
Iteration 4000: Loss = -10798.935546875
Iteration 4100: Loss = -10798.8916015625
Iteration 4200: Loss = -10798.8408203125
Iteration 4300: Loss = -10798.748046875
Iteration 4400: Loss = -10798.7138671875
Iteration 4500: Loss = -10798.6826171875
Iteration 4600: Loss = -10798.6513671875
Iteration 4700: Loss = -10798.6220703125
Iteration 4800: Loss = -10798.5966796875
Iteration 4900: Loss = -10798.572265625
Iteration 5000: Loss = -10798.5498046875
Iteration 5100: Loss = -10798.5302734375
Iteration 5200: Loss = -10798.51171875
Iteration 5300: Loss = -10798.494140625
Iteration 5400: Loss = -10798.4755859375
Iteration 5500: Loss = -10798.4609375
Iteration 5600: Loss = -10798.447265625
Iteration 5700: Loss = -10798.43359375
Iteration 5800: Loss = -10798.421875
Iteration 5900: Loss = -10798.408203125
Iteration 6000: Loss = -10798.3974609375
Iteration 6100: Loss = -10798.3876953125
Iteration 6200: Loss = -10798.376953125
Iteration 6300: Loss = -10798.3681640625
Iteration 6400: Loss = -10798.359375
Iteration 6500: Loss = -10798.349609375
Iteration 6600: Loss = -10798.3408203125
Iteration 6700: Loss = -10798.3349609375
Iteration 6800: Loss = -10798.3271484375
Iteration 6900: Loss = -10798.318359375
Iteration 7000: Loss = -10798.3125
Iteration 7100: Loss = -10798.3037109375
Iteration 7200: Loss = -10798.279296875
Iteration 7300: Loss = -10798.2705078125
Iteration 7400: Loss = -10798.2626953125
Iteration 7500: Loss = -10798.2578125
Iteration 7600: Loss = -10798.2529296875
Iteration 7700: Loss = -10798.2470703125
Iteration 7800: Loss = -10798.2431640625
Iteration 7900: Loss = -10798.240234375
Iteration 8000: Loss = -10798.2353515625
Iteration 8100: Loss = -10798.2314453125
Iteration 8200: Loss = -10798.228515625
Iteration 8300: Loss = -10798.2255859375
Iteration 8400: Loss = -10798.2216796875
Iteration 8500: Loss = -10798.21875
Iteration 8600: Loss = -10798.216796875
Iteration 8700: Loss = -10798.21484375
Iteration 8800: Loss = -10798.2119140625
Iteration 8900: Loss = -10798.208984375
Iteration 9000: Loss = -10798.2080078125
Iteration 9100: Loss = -10798.205078125
Iteration 9200: Loss = -10798.197265625
Iteration 9300: Loss = -10798.119140625
Iteration 9400: Loss = -10798.107421875
Iteration 9500: Loss = -10798.1015625
Iteration 9600: Loss = -10798.0966796875
Iteration 9700: Loss = -10798.091796875
Iteration 9800: Loss = -10798.091796875
Iteration 9900: Loss = -10798.0888671875
Iteration 10000: Loss = -10798.0859375
Iteration 10100: Loss = -10798.0849609375
Iteration 10200: Loss = -10798.0830078125
Iteration 10300: Loss = -10798.08203125
Iteration 10400: Loss = -10798.0810546875
Iteration 10500: Loss = -10798.080078125
Iteration 10600: Loss = -10798.0791015625
Iteration 10700: Loss = -10798.0791015625
Iteration 10800: Loss = -10798.0771484375
Iteration 10900: Loss = -10798.076171875
Iteration 11000: Loss = -10798.076171875
Iteration 11100: Loss = -10798.07421875
Iteration 11200: Loss = -10798.07421875
Iteration 11300: Loss = -10798.0732421875
Iteration 11400: Loss = -10798.072265625
Iteration 11500: Loss = -10798.0732421875
1
Iteration 11600: Loss = -10798.0732421875
2
Iteration 11700: Loss = -10798.0703125
Iteration 11800: Loss = -10798.0712890625
1
Iteration 11900: Loss = -10798.0703125
Iteration 12000: Loss = -10798.0703125
Iteration 12100: Loss = -10798.068359375
Iteration 12200: Loss = -10798.0673828125
Iteration 12300: Loss = -10798.068359375
1
Iteration 12400: Loss = -10798.06640625
Iteration 12500: Loss = -10798.0654296875
Iteration 12600: Loss = -10798.068359375
1
Iteration 12700: Loss = -10798.06640625
2
Iteration 12800: Loss = -10798.0673828125
3
Iteration 12900: Loss = -10798.0673828125
4
Iteration 13000: Loss = -10798.0654296875
Iteration 13100: Loss = -10798.0654296875
Iteration 13200: Loss = -10798.064453125
Iteration 13300: Loss = -10798.064453125
Iteration 13400: Loss = -10798.064453125
Iteration 13500: Loss = -10798.0634765625
Iteration 13600: Loss = -10798.0625
Iteration 13700: Loss = -10798.0634765625
1
Iteration 13800: Loss = -10798.0634765625
2
Iteration 13900: Loss = -10798.0634765625
3
Iteration 14000: Loss = -10798.0625
Iteration 14100: Loss = -10798.0625
Iteration 14200: Loss = -10798.0625
Iteration 14300: Loss = -10798.0615234375
Iteration 14400: Loss = -10798.0615234375
Iteration 14500: Loss = -10798.0576171875
Iteration 14600: Loss = -10798.056640625
Iteration 14700: Loss = -10798.052734375
Iteration 14800: Loss = -10798.0517578125
Iteration 14900: Loss = -10798.0498046875
Iteration 15000: Loss = -10798.048828125
Iteration 15100: Loss = -10798.044921875
Iteration 15200: Loss = -10798.0419921875
Iteration 15300: Loss = -10798.037109375
Iteration 15400: Loss = -10798.0341796875
Iteration 15500: Loss = -10798.0322265625
Iteration 15600: Loss = -10798.0263671875
Iteration 15700: Loss = -10798.021484375
Iteration 15800: Loss = -10798.015625
Iteration 15900: Loss = -10798.0048828125
Iteration 16000: Loss = -10797.9619140625
Iteration 16100: Loss = -10797.8544921875
Iteration 16200: Loss = -10797.681640625
Iteration 16300: Loss = -10797.525390625
Iteration 16400: Loss = -10794.7998046875
Iteration 16500: Loss = -10794.7060546875
Iteration 16600: Loss = -10794.6953125
Iteration 16700: Loss = -10794.69140625
Iteration 16800: Loss = -10794.689453125
Iteration 16900: Loss = -10794.6875
Iteration 17000: Loss = -10794.685546875
Iteration 17100: Loss = -10794.685546875
Iteration 17200: Loss = -10794.6845703125
Iteration 17300: Loss = -10794.68359375
Iteration 17400: Loss = -10794.6826171875
Iteration 17500: Loss = -10794.6826171875
Iteration 17600: Loss = -10794.6826171875
Iteration 17700: Loss = -10794.681640625
Iteration 17800: Loss = -10794.681640625
Iteration 17900: Loss = -10794.681640625
Iteration 18000: Loss = -10794.681640625
Iteration 18100: Loss = -10794.6806640625
Iteration 18200: Loss = -10794.6796875
Iteration 18300: Loss = -10794.681640625
1
Iteration 18400: Loss = -10794.6806640625
2
Iteration 18500: Loss = -10794.6806640625
3
Iteration 18600: Loss = -10794.6796875
Iteration 18700: Loss = -10794.6806640625
1
Iteration 18800: Loss = -10794.6591796875
Iteration 18900: Loss = -10794.6591796875
Iteration 19000: Loss = -10794.6591796875
Iteration 19100: Loss = -10794.6591796875
Iteration 19200: Loss = -10794.66015625
1
Iteration 19300: Loss = -10794.6591796875
Iteration 19400: Loss = -10794.658203125
Iteration 19500: Loss = -10794.66015625
1
Iteration 19600: Loss = -10794.66015625
2
Iteration 19700: Loss = -10794.658203125
Iteration 19800: Loss = -10794.66015625
1
Iteration 19900: Loss = -10794.658203125
Iteration 20000: Loss = -10794.658203125
Iteration 20100: Loss = -10794.6591796875
1
Iteration 20200: Loss = -10794.6591796875
2
Iteration 20300: Loss = -10794.658203125
Iteration 20400: Loss = -10794.6591796875
1
Iteration 20500: Loss = -10794.658203125
Iteration 20600: Loss = -10794.658203125
Iteration 20700: Loss = -10794.6572265625
Iteration 20800: Loss = -10794.658203125
1
Iteration 20900: Loss = -10794.658203125
2
Iteration 21000: Loss = -10794.6591796875
3
Iteration 21100: Loss = -10794.6591796875
4
Iteration 21200: Loss = -10794.658203125
5
Iteration 21300: Loss = -10794.6591796875
6
Iteration 21400: Loss = -10794.66015625
7
Iteration 21500: Loss = -10794.6591796875
8
Iteration 21600: Loss = -10794.6572265625
Iteration 21700: Loss = -10794.658203125
1
Iteration 21800: Loss = -10794.6591796875
2
Iteration 21900: Loss = -10794.6591796875
3
Iteration 22000: Loss = -10794.66015625
4
Iteration 22100: Loss = -10794.6591796875
5
Iteration 22200: Loss = -10794.66015625
6
Iteration 22300: Loss = -10794.587890625
Iteration 22400: Loss = -10794.5791015625
Iteration 22500: Loss = -10794.5791015625
Iteration 22600: Loss = -10794.578125
Iteration 22700: Loss = -10794.578125
Iteration 22800: Loss = -10794.580078125
1
Iteration 22900: Loss = -10794.578125
Iteration 23000: Loss = -10794.5791015625
1
Iteration 23100: Loss = -10794.580078125
2
Iteration 23200: Loss = -10794.5791015625
3
Iteration 23300: Loss = -10794.5791015625
4
Iteration 23400: Loss = -10794.5791015625
5
Iteration 23500: Loss = -10794.578125
Iteration 23600: Loss = -10794.578125
Iteration 23700: Loss = -10794.5791015625
1
Iteration 23800: Loss = -10794.5791015625
2
Iteration 23900: Loss = -10794.5791015625
3
Iteration 24000: Loss = -10794.5791015625
4
Iteration 24100: Loss = -10794.580078125
5
Iteration 24200: Loss = -10794.578125
Iteration 24300: Loss = -10794.5791015625
1
Iteration 24400: Loss = -10794.5791015625
2
Iteration 24500: Loss = -10794.58203125
3
Iteration 24600: Loss = -10794.5791015625
4
Iteration 24700: Loss = -10794.580078125
5
Iteration 24800: Loss = -10794.5771484375
Iteration 24900: Loss = -10794.576171875
Iteration 25000: Loss = -10794.576171875
Iteration 25100: Loss = -10794.5771484375
1
Iteration 25200: Loss = -10794.5751953125
Iteration 25300: Loss = -10794.576171875
1
Iteration 25400: Loss = -10794.576171875
2
Iteration 25500: Loss = -10794.576171875
3
Iteration 25600: Loss = -10794.5751953125
Iteration 25700: Loss = -10794.5751953125
Iteration 25800: Loss = -10794.568359375
Iteration 25900: Loss = -10794.5693359375
1
Iteration 26000: Loss = -10794.5693359375
2
Iteration 26100: Loss = -10794.5673828125
Iteration 26200: Loss = -10794.568359375
1
Iteration 26300: Loss = -10794.568359375
2
Iteration 26400: Loss = -10794.5693359375
3
Iteration 26500: Loss = -10794.5703125
4
Iteration 26600: Loss = -10794.5703125
5
Iteration 26700: Loss = -10794.568359375
6
Iteration 26800: Loss = -10794.568359375
7
Iteration 26900: Loss = -10794.5693359375
8
Iteration 27000: Loss = -10794.568359375
9
Iteration 27100: Loss = -10794.5693359375
10
Iteration 27200: Loss = -10794.568359375
11
Iteration 27300: Loss = -10794.5693359375
12
Iteration 27400: Loss = -10794.568359375
13
Iteration 27500: Loss = -10794.5693359375
14
Iteration 27600: Loss = -10794.5693359375
15
Stopping early at iteration 27600 due to no improvement.
pi: tensor([[1.0000e+00, 3.9758e-06],
        [4.3465e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0675, 0.9325], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0297, 0.1720],
         [0.9860, 0.1578]],

        [[0.0174, 0.2218],
         [0.0128, 0.9708]],

        [[0.0921, 0.1220],
         [0.9016, 0.0076]],

        [[0.1445, 0.1769],
         [0.6419, 0.9879]],

        [[0.9861, 0.1227],
         [0.9918, 0.6636]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.010860348555184451
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.020655664058706252
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.003702958125914224
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.003485103334991517
Global Adjusted Rand Index: -0.0052127299647356205
Average Adjusted Rand Index: -0.00707404620823541
[-0.004058868311844209, -0.0052127299647356205] [-0.00414891536848411, -0.00707404620823541] [10796.8876953125, 10794.5693359375]
-------------------------------------
This iteration is 62
True Objective function: Loss = -10861.594663743004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26796.3515625
Iteration 100: Loss = -17031.78515625
Iteration 200: Loss = -12227.740234375
Iteration 300: Loss = -11494.748046875
Iteration 400: Loss = -11320.7099609375
Iteration 500: Loss = -11239.322265625
Iteration 600: Loss = -11194.48828125
Iteration 700: Loss = -11162.609375
Iteration 800: Loss = -11140.2216796875
Iteration 900: Loss = -11123.970703125
Iteration 1000: Loss = -11112.6181640625
Iteration 1100: Loss = -11103.7294921875
Iteration 1200: Loss = -11095.265625
Iteration 1300: Loss = -11087.802734375
Iteration 1400: Loss = -11079.830078125
Iteration 1500: Loss = -11075.046875
Iteration 1600: Loss = -11069.4140625
Iteration 1700: Loss = -11062.38671875
Iteration 1800: Loss = -11058.53125
Iteration 1900: Loss = -11053.1455078125
Iteration 2000: Loss = -11047.697265625
Iteration 2100: Loss = -11041.3564453125
Iteration 2200: Loss = -11032.2763671875
Iteration 2300: Loss = -11024.79296875
Iteration 2400: Loss = -11016.2998046875
Iteration 2500: Loss = -11009.2783203125
Iteration 2600: Loss = -11001.9345703125
Iteration 2700: Loss = -10998.14453125
Iteration 2800: Loss = -10995.3349609375
Iteration 2900: Loss = -10993.0869140625
Iteration 3000: Loss = -10991.248046875
Iteration 3100: Loss = -10988.259765625
Iteration 3200: Loss = -10983.5087890625
Iteration 3300: Loss = -10978.37109375
Iteration 3400: Loss = -10977.185546875
Iteration 3500: Loss = -10976.3857421875
Iteration 3600: Loss = -10975.73046875
Iteration 3700: Loss = -10975.12109375
Iteration 3800: Loss = -10974.1572265625
Iteration 3900: Loss = -10972.0556640625
Iteration 4000: Loss = -10969.412109375
Iteration 4100: Loss = -10967.8134765625
Iteration 4200: Loss = -10966.833984375
Iteration 4300: Loss = -10966.0693359375
Iteration 4400: Loss = -10963.5966796875
Iteration 4500: Loss = -10962.7431640625
Iteration 4600: Loss = -10962.34765625
Iteration 4700: Loss = -10962.0693359375
Iteration 4800: Loss = -10961.849609375
Iteration 4900: Loss = -10961.6669921875
Iteration 5000: Loss = -10961.5087890625
Iteration 5100: Loss = -10961.3740234375
Iteration 5200: Loss = -10961.2529296875
Iteration 5300: Loss = -10961.1474609375
Iteration 5400: Loss = -10961.05078125
Iteration 5500: Loss = -10960.9638671875
Iteration 5600: Loss = -10960.884765625
Iteration 5700: Loss = -10960.8125
Iteration 5800: Loss = -10960.74609375
Iteration 5900: Loss = -10960.685546875
Iteration 6000: Loss = -10960.630859375
Iteration 6100: Loss = -10960.578125
Iteration 6200: Loss = -10960.5302734375
Iteration 6300: Loss = -10960.486328125
Iteration 6400: Loss = -10960.4453125
Iteration 6500: Loss = -10960.4072265625
Iteration 6600: Loss = -10960.37109375
Iteration 6700: Loss = -10960.3388671875
Iteration 6800: Loss = -10960.306640625
Iteration 6900: Loss = -10960.2783203125
Iteration 7000: Loss = -10960.2509765625
Iteration 7100: Loss = -10960.224609375
Iteration 7200: Loss = -10960.19921875
Iteration 7300: Loss = -10960.173828125
Iteration 7400: Loss = -10960.1455078125
Iteration 7500: Loss = -10960.08203125
Iteration 7600: Loss = -10959.8359375
Iteration 7700: Loss = -10959.810546875
Iteration 7800: Loss = -10959.7919921875
Iteration 7900: Loss = -10959.7763671875
Iteration 8000: Loss = -10959.759765625
Iteration 8100: Loss = -10959.744140625
Iteration 8200: Loss = -10959.7294921875
Iteration 8300: Loss = -10959.71484375
Iteration 8400: Loss = -10959.701171875
Iteration 8500: Loss = -10959.6884765625
Iteration 8600: Loss = -10959.6748046875
Iteration 8700: Loss = -10959.6650390625
Iteration 8800: Loss = -10959.6533203125
Iteration 8900: Loss = -10959.6435546875
Iteration 9000: Loss = -10959.6337890625
Iteration 9100: Loss = -10959.6259765625
Iteration 9200: Loss = -10959.6162109375
Iteration 9300: Loss = -10959.6083984375
Iteration 9400: Loss = -10959.5986328125
Iteration 9500: Loss = -10959.5908203125
Iteration 9600: Loss = -10959.5810546875
Iteration 9700: Loss = -10959.5712890625
Iteration 9800: Loss = -10959.556640625
Iteration 9900: Loss = -10959.5390625
Iteration 10000: Loss = -10959.5244140625
Iteration 10100: Loss = -10959.5107421875
Iteration 10200: Loss = -10959.501953125
Iteration 10300: Loss = -10959.49609375
Iteration 10400: Loss = -10959.4912109375
Iteration 10500: Loss = -10959.4873046875
Iteration 10600: Loss = -10959.482421875
Iteration 10700: Loss = -10959.478515625
Iteration 10800: Loss = -10959.4755859375
Iteration 10900: Loss = -10959.47265625
Iteration 11000: Loss = -10959.46875
Iteration 11100: Loss = -10959.4658203125
Iteration 11200: Loss = -10959.462890625
Iteration 11300: Loss = -10959.4609375
Iteration 11400: Loss = -10959.458984375
Iteration 11500: Loss = -10959.4560546875
Iteration 11600: Loss = -10959.4541015625
Iteration 11700: Loss = -10959.451171875
Iteration 11800: Loss = -10959.451171875
Iteration 11900: Loss = -10959.447265625
Iteration 12000: Loss = -10959.4462890625
Iteration 12100: Loss = -10959.4443359375
Iteration 12200: Loss = -10959.4423828125
Iteration 12300: Loss = -10959.44140625
Iteration 12400: Loss = -10959.44140625
Iteration 12500: Loss = -10959.4384765625
Iteration 12600: Loss = -10959.4365234375
Iteration 12700: Loss = -10959.42578125
Iteration 12800: Loss = -10959.4091796875
Iteration 12900: Loss = -10959.40234375
Iteration 13000: Loss = -10959.3720703125
Iteration 13100: Loss = -10959.3642578125
Iteration 13200: Loss = -10959.361328125
Iteration 13300: Loss = -10959.361328125
Iteration 13400: Loss = -10959.3603515625
Iteration 13500: Loss = -10959.359375
Iteration 13600: Loss = -10959.3583984375
Iteration 13700: Loss = -10959.357421875
Iteration 13800: Loss = -10959.3564453125
Iteration 13900: Loss = -10959.35546875
Iteration 14000: Loss = -10959.35546875
Iteration 14100: Loss = -10959.3544921875
Iteration 14200: Loss = -10959.353515625
Iteration 14300: Loss = -10959.3525390625
Iteration 14400: Loss = -10959.3525390625
Iteration 14500: Loss = -10959.3525390625
Iteration 14600: Loss = -10959.3525390625
Iteration 14700: Loss = -10959.3515625
Iteration 14800: Loss = -10959.3515625
Iteration 14900: Loss = -10959.349609375
Iteration 15000: Loss = -10959.3505859375
1
Iteration 15100: Loss = -10959.3505859375
2
Iteration 15200: Loss = -10959.349609375
Iteration 15300: Loss = -10959.3486328125
Iteration 15400: Loss = -10959.3466796875
Iteration 15500: Loss = -10959.34765625
1
Iteration 15600: Loss = -10959.345703125
Iteration 15700: Loss = -10959.3447265625
Iteration 15800: Loss = -10959.345703125
1
Iteration 15900: Loss = -10959.34375
Iteration 16000: Loss = -10959.34375
Iteration 16100: Loss = -10959.3447265625
1
Iteration 16200: Loss = -10959.34375
Iteration 16300: Loss = -10959.34375
Iteration 16400: Loss = -10959.30859375
Iteration 16500: Loss = -10959.3076171875
Iteration 16600: Loss = -10959.30859375
1
Iteration 16700: Loss = -10959.30859375
2
Iteration 16800: Loss = -10959.30859375
3
Iteration 16900: Loss = -10959.30859375
4
Iteration 17000: Loss = -10959.2958984375
Iteration 17100: Loss = -10959.294921875
Iteration 17200: Loss = -10959.294921875
Iteration 17300: Loss = -10959.2939453125
Iteration 17400: Loss = -10959.294921875
1
Iteration 17500: Loss = -10959.2939453125
Iteration 17600: Loss = -10959.2919921875
Iteration 17700: Loss = -10959.294921875
1
Iteration 17800: Loss = -10959.2958984375
2
Iteration 17900: Loss = -10959.28515625
Iteration 18000: Loss = -10959.2841796875
Iteration 18100: Loss = -10959.283203125
Iteration 18200: Loss = -10959.28515625
1
Iteration 18300: Loss = -10959.2841796875
2
Iteration 18400: Loss = -10959.2841796875
3
Iteration 18500: Loss = -10959.283203125
Iteration 18600: Loss = -10959.283203125
Iteration 18700: Loss = -10959.2841796875
1
Iteration 18800: Loss = -10959.2822265625
Iteration 18900: Loss = -10959.27734375
Iteration 19000: Loss = -10959.2802734375
1
Iteration 19100: Loss = -10959.2783203125
2
Iteration 19200: Loss = -10959.27734375
Iteration 19300: Loss = -10959.27734375
Iteration 19400: Loss = -10959.27734375
Iteration 19500: Loss = -10959.2783203125
1
Iteration 19600: Loss = -10959.2763671875
Iteration 19700: Loss = -10959.2783203125
1
Iteration 19800: Loss = -10959.275390625
Iteration 19900: Loss = -10959.2763671875
1
Iteration 20000: Loss = -10959.263671875
Iteration 20100: Loss = -10959.2626953125
Iteration 20200: Loss = -10959.2646484375
1
Iteration 20300: Loss = -10959.2626953125
Iteration 20400: Loss = -10959.263671875
1
Iteration 20500: Loss = -10959.263671875
2
Iteration 20600: Loss = -10959.2626953125
Iteration 20700: Loss = -10959.265625
1
Iteration 20800: Loss = -10959.2626953125
Iteration 20900: Loss = -10959.263671875
1
Iteration 21000: Loss = -10959.2626953125
Iteration 21100: Loss = -10959.2490234375
Iteration 21200: Loss = -10959.248046875
Iteration 21300: Loss = -10959.248046875
Iteration 21400: Loss = -10959.2490234375
1
Iteration 21500: Loss = -10959.2490234375
2
Iteration 21600: Loss = -10959.2490234375
3
Iteration 21700: Loss = -10959.2490234375
4
Iteration 21800: Loss = -10959.248046875
Iteration 21900: Loss = -10959.248046875
Iteration 22000: Loss = -10959.2490234375
1
Iteration 22100: Loss = -10959.2490234375
2
Iteration 22200: Loss = -10959.2490234375
3
Iteration 22300: Loss = -10959.248046875
Iteration 22400: Loss = -10959.2470703125
Iteration 22500: Loss = -10959.2470703125
Iteration 22600: Loss = -10959.2490234375
1
Iteration 22700: Loss = -10959.248046875
2
Iteration 22800: Loss = -10959.2490234375
3
Iteration 22900: Loss = -10959.248046875
4
Iteration 23000: Loss = -10959.2490234375
5
Iteration 23100: Loss = -10959.248046875
6
Iteration 23200: Loss = -10959.248046875
7
Iteration 23300: Loss = -10959.248046875
8
Iteration 23400: Loss = -10959.248046875
9
Iteration 23500: Loss = -10959.25
10
Iteration 23600: Loss = -10959.248046875
11
Iteration 23700: Loss = -10959.2490234375
12
Iteration 23800: Loss = -10959.2470703125
Iteration 23900: Loss = -10959.248046875
1
Iteration 24000: Loss = -10959.248046875
2
Iteration 24100: Loss = -10959.2470703125
Iteration 24200: Loss = -10959.2470703125
Iteration 24300: Loss = -10959.2490234375
1
Iteration 24400: Loss = -10959.2470703125
Iteration 24500: Loss = -10959.2470703125
Iteration 24600: Loss = -10959.248046875
1
Iteration 24700: Loss = -10959.2490234375
2
Iteration 24800: Loss = -10959.248046875
3
Iteration 24900: Loss = -10959.2490234375
4
Iteration 25000: Loss = -10959.2490234375
5
Iteration 25100: Loss = -10959.248046875
6
Iteration 25200: Loss = -10959.248046875
7
Iteration 25300: Loss = -10959.2470703125
Iteration 25400: Loss = -10959.248046875
1
Iteration 25500: Loss = -10959.248046875
2
Iteration 25600: Loss = -10959.248046875
3
Iteration 25700: Loss = -10959.2490234375
4
Iteration 25800: Loss = -10959.248046875
5
Iteration 25900: Loss = -10959.248046875
6
Iteration 26000: Loss = -10959.248046875
7
Iteration 26100: Loss = -10959.2470703125
Iteration 26200: Loss = -10959.2490234375
1
Iteration 26300: Loss = -10959.248046875
2
Iteration 26400: Loss = -10959.2294921875
Iteration 26500: Loss = -10957.9619140625
Iteration 26600: Loss = -10957.9072265625
Iteration 26700: Loss = -10957.6455078125
Iteration 26800: Loss = -10957.6435546875
Iteration 26900: Loss = -10957.572265625
Iteration 27000: Loss = -10957.501953125
Iteration 27100: Loss = -10957.4970703125
Iteration 27200: Loss = -10957.49609375
Iteration 27300: Loss = -10957.4951171875
Iteration 27400: Loss = -10957.48828125
Iteration 27500: Loss = -10957.4765625
Iteration 27600: Loss = -10957.4765625
Iteration 27700: Loss = -10957.4208984375
Iteration 27800: Loss = -10957.4189453125
Iteration 27900: Loss = -10957.41796875
Iteration 28000: Loss = -10957.416015625
Iteration 28100: Loss = -10957.416015625
Iteration 28200: Loss = -10957.41796875
1
Iteration 28300: Loss = -10957.416015625
Iteration 28400: Loss = -10957.4150390625
Iteration 28500: Loss = -10957.41015625
Iteration 28600: Loss = -10957.412109375
1
Iteration 28700: Loss = -10957.41015625
Iteration 28800: Loss = -10957.4111328125
1
Iteration 28900: Loss = -10957.4111328125
2
Iteration 29000: Loss = -10957.412109375
3
Iteration 29100: Loss = -10957.41015625
Iteration 29200: Loss = -10957.400390625
Iteration 29300: Loss = -10957.3056640625
Iteration 29400: Loss = -10952.0908203125
Iteration 29500: Loss = -10945.6337890625
Iteration 29600: Loss = -10916.51953125
Iteration 29700: Loss = -10859.83203125
Iteration 29800: Loss = -10839.47265625
Iteration 29900: Loss = -10834.8515625
pi: tensor([[0.7576, 0.2424],
        [0.1865, 0.8135]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5565, 0.4435], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2416, 0.0931],
         [0.2144, 0.2104]],

        [[0.0197, 0.0996],
         [0.0965, 0.8078]],

        [[0.9389, 0.1068],
         [0.0673, 0.5341]],

        [[0.9928, 0.0996],
         [0.8875, 0.9909]],

        [[0.6426, 0.0884],
         [0.3216, 0.8608]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8445229681978799
Global Adjusted Rand Index: 0.8985007858906296
Average Adjusted Rand Index: 0.8980024982567766
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25878.205078125
Iteration 100: Loss = -15422.3251953125
Iteration 200: Loss = -11836.8291015625
Iteration 300: Loss = -11217.185546875
Iteration 400: Loss = -11094.1201171875
Iteration 500: Loss = -11051.5517578125
Iteration 600: Loss = -11026.81640625
Iteration 700: Loss = -11015.2529296875
Iteration 800: Loss = -11007.7431640625
Iteration 900: Loss = -11002.470703125
Iteration 1000: Loss = -10998.6748046875
Iteration 1100: Loss = -10995.86328125
Iteration 1200: Loss = -10993.73046875
Iteration 1300: Loss = -10992.080078125
Iteration 1400: Loss = -10990.794921875
Iteration 1500: Loss = -10989.7607421875
Iteration 1600: Loss = -10988.904296875
Iteration 1700: Loss = -10988.185546875
Iteration 1800: Loss = -10987.5751953125
Iteration 1900: Loss = -10987.0517578125
Iteration 2000: Loss = -10986.591796875
Iteration 2100: Loss = -10986.189453125
Iteration 2200: Loss = -10985.8310546875
Iteration 2300: Loss = -10985.513671875
Iteration 2400: Loss = -10985.2265625
Iteration 2500: Loss = -10984.9697265625
Iteration 2600: Loss = -10984.7392578125
Iteration 2700: Loss = -10984.5283203125
Iteration 2800: Loss = -10984.3388671875
Iteration 2900: Loss = -10984.1611328125
Iteration 3000: Loss = -10984.0
Iteration 3100: Loss = -10983.845703125
Iteration 3200: Loss = -10983.701171875
Iteration 3300: Loss = -10983.560546875
Iteration 3400: Loss = -10983.421875
Iteration 3500: Loss = -10983.2783203125
Iteration 3600: Loss = -10983.1259765625
Iteration 3700: Loss = -10982.966796875
Iteration 3800: Loss = -10982.80078125
Iteration 3900: Loss = -10982.6416015625
Iteration 4000: Loss = -10982.5009765625
Iteration 4100: Loss = -10982.3740234375
Iteration 4200: Loss = -10982.2626953125
Iteration 4300: Loss = -10982.1640625
Iteration 4400: Loss = -10982.0751953125
Iteration 4500: Loss = -10981.9912109375
Iteration 4600: Loss = -10981.9150390625
Iteration 4700: Loss = -10981.845703125
Iteration 4800: Loss = -10981.7802734375
Iteration 4900: Loss = -10981.720703125
Iteration 5000: Loss = -10981.662109375
Iteration 5100: Loss = -10981.6083984375
Iteration 5200: Loss = -10981.556640625
Iteration 5300: Loss = -10981.5078125
Iteration 5400: Loss = -10981.4609375
Iteration 5500: Loss = -10981.4169921875
Iteration 5600: Loss = -10981.375
Iteration 5700: Loss = -10981.333984375
Iteration 5800: Loss = -10981.2939453125
Iteration 5900: Loss = -10981.2578125
Iteration 6000: Loss = -10981.220703125
Iteration 6100: Loss = -10981.1865234375
Iteration 6200: Loss = -10981.1533203125
Iteration 6300: Loss = -10981.1201171875
Iteration 6400: Loss = -10981.0869140625
Iteration 6500: Loss = -10981.0576171875
Iteration 6600: Loss = -10981.025390625
Iteration 6700: Loss = -10980.9951171875
Iteration 6800: Loss = -10980.9638671875
Iteration 6900: Loss = -10980.9365234375
Iteration 7000: Loss = -10980.9072265625
Iteration 7100: Loss = -10980.87890625
Iteration 7200: Loss = -10980.8505859375
Iteration 7300: Loss = -10980.8212890625
Iteration 7400: Loss = -10980.7919921875
Iteration 7500: Loss = -10980.7626953125
Iteration 7600: Loss = -10980.732421875
Iteration 7700: Loss = -10980.7041015625
Iteration 7800: Loss = -10980.669921875
Iteration 7900: Loss = -10980.638671875
Iteration 8000: Loss = -10980.60546875
Iteration 8100: Loss = -10980.5693359375
Iteration 8200: Loss = -10980.533203125
Iteration 8300: Loss = -10980.4931640625
Iteration 8400: Loss = -10980.4521484375
Iteration 8500: Loss = -10980.4072265625
Iteration 8600: Loss = -10980.359375
Iteration 8700: Loss = -10980.3095703125
Iteration 8800: Loss = -10980.25390625
Iteration 8900: Loss = -10980.1923828125
Iteration 9000: Loss = -10980.1279296875
Iteration 9100: Loss = -10980.0576171875
Iteration 9200: Loss = -10979.98046875
Iteration 9300: Loss = -10979.8955078125
Iteration 9400: Loss = -10979.8056640625
Iteration 9500: Loss = -10979.7119140625
Iteration 9600: Loss = -10979.61328125
Iteration 9700: Loss = -10979.515625
Iteration 9800: Loss = -10979.4208984375
Iteration 9900: Loss = -10979.3330078125
Iteration 10000: Loss = -10979.2548828125
Iteration 10100: Loss = -10979.185546875
Iteration 10200: Loss = -10979.125
Iteration 10300: Loss = -10979.0771484375
Iteration 10400: Loss = -10979.0380859375
Iteration 10500: Loss = -10979.0068359375
Iteration 10600: Loss = -10978.982421875
Iteration 10700: Loss = -10978.95703125
Iteration 10800: Loss = -10978.93359375
Iteration 10900: Loss = -10978.9150390625
Iteration 11000: Loss = -10978.8974609375
Iteration 11100: Loss = -10978.8818359375
Iteration 11200: Loss = -10978.869140625
Iteration 11300: Loss = -10978.857421875
Iteration 11400: Loss = -10978.8486328125
Iteration 11500: Loss = -10978.83984375
Iteration 11600: Loss = -10978.8330078125
Iteration 11700: Loss = -10978.8291015625
Iteration 11800: Loss = -10978.8251953125
Iteration 11900: Loss = -10978.822265625
Iteration 12000: Loss = -10978.8173828125
Iteration 12100: Loss = -10978.818359375
1
Iteration 12200: Loss = -10978.814453125
Iteration 12300: Loss = -10978.814453125
Iteration 12400: Loss = -10978.81640625
1
Iteration 12500: Loss = -10978.8115234375
Iteration 12600: Loss = -10978.8115234375
Iteration 12700: Loss = -10978.8095703125
Iteration 12800: Loss = -10978.8095703125
Iteration 12900: Loss = -10978.8076171875
Iteration 13000: Loss = -10978.8076171875
Iteration 13100: Loss = -10978.8076171875
Iteration 13200: Loss = -10978.8076171875
Iteration 13300: Loss = -10978.806640625
Iteration 13400: Loss = -10978.806640625
Iteration 13500: Loss = -10978.8056640625
Iteration 13600: Loss = -10978.806640625
1
Iteration 13700: Loss = -10978.8076171875
2
Iteration 13800: Loss = -10978.8046875
Iteration 13900: Loss = -10978.8037109375
Iteration 14000: Loss = -10978.8046875
1
Iteration 14100: Loss = -10978.8037109375
Iteration 14200: Loss = -10978.802734375
Iteration 14300: Loss = -10978.8046875
1
Iteration 14400: Loss = -10978.8037109375
2
Iteration 14500: Loss = -10978.802734375
Iteration 14600: Loss = -10978.8037109375
1
Iteration 14700: Loss = -10978.802734375
Iteration 14800: Loss = -10978.8037109375
1
Iteration 14900: Loss = -10978.8037109375
2
Iteration 15000: Loss = -10978.8037109375
3
Iteration 15100: Loss = -10978.802734375
Iteration 15200: Loss = -10978.802734375
Iteration 15300: Loss = -10978.8037109375
1
Iteration 15400: Loss = -10978.802734375
Iteration 15500: Loss = -10978.80078125
Iteration 15600: Loss = -10978.80078125
Iteration 15700: Loss = -10978.7998046875
Iteration 15800: Loss = -10978.802734375
1
Iteration 15900: Loss = -10978.80078125
2
Iteration 16000: Loss = -10978.8017578125
3
Iteration 16100: Loss = -10978.80078125
4
Iteration 16200: Loss = -10978.8017578125
5
Iteration 16300: Loss = -10978.7998046875
Iteration 16400: Loss = -10978.80078125
1
Iteration 16500: Loss = -10978.7998046875
Iteration 16600: Loss = -10978.8017578125
1
Iteration 16700: Loss = -10978.80078125
2
Iteration 16800: Loss = -10978.80078125
3
Iteration 16900: Loss = -10978.80078125
4
Iteration 17000: Loss = -10978.80078125
5
Iteration 17100: Loss = -10978.80078125
6
Iteration 17200: Loss = -10978.7998046875
Iteration 17300: Loss = -10978.7998046875
Iteration 17400: Loss = -10978.7998046875
Iteration 17500: Loss = -10978.80078125
1
Iteration 17600: Loss = -10978.7998046875
Iteration 17700: Loss = -10978.7998046875
Iteration 17800: Loss = -10978.80078125
1
Iteration 17900: Loss = -10978.80078125
2
Iteration 18000: Loss = -10978.798828125
Iteration 18100: Loss = -10978.7998046875
1
Iteration 18200: Loss = -10978.7998046875
2
Iteration 18300: Loss = -10978.7998046875
3
Iteration 18400: Loss = -10978.80078125
4
Iteration 18500: Loss = -10978.80078125
5
Iteration 18600: Loss = -10978.7998046875
6
Iteration 18700: Loss = -10978.798828125
Iteration 18800: Loss = -10978.7998046875
1
Iteration 18900: Loss = -10978.798828125
Iteration 19000: Loss = -10978.798828125
Iteration 19100: Loss = -10978.80078125
1
Iteration 19200: Loss = -10978.798828125
Iteration 19300: Loss = -10978.7998046875
1
Iteration 19400: Loss = -10978.798828125
Iteration 19500: Loss = -10978.798828125
Iteration 19600: Loss = -10978.798828125
Iteration 19700: Loss = -10978.7998046875
1
Iteration 19800: Loss = -10978.798828125
Iteration 19900: Loss = -10978.798828125
Iteration 20000: Loss = -10978.798828125
Iteration 20100: Loss = -10978.7998046875
1
Iteration 20200: Loss = -10978.798828125
Iteration 20300: Loss = -10978.7998046875
1
Iteration 20400: Loss = -10978.798828125
Iteration 20500: Loss = -10978.7978515625
Iteration 20600: Loss = -10978.7998046875
1
Iteration 20700: Loss = -10978.7998046875
2
Iteration 20800: Loss = -10978.80078125
3
Iteration 20900: Loss = -10978.7998046875
4
Iteration 21000: Loss = -10978.7998046875
5
Iteration 21100: Loss = -10978.80078125
6
Iteration 21200: Loss = -10978.7998046875
7
Iteration 21300: Loss = -10978.7998046875
8
Iteration 21400: Loss = -10978.7998046875
9
Iteration 21500: Loss = -10978.798828125
10
Iteration 21600: Loss = -10978.7998046875
11
Iteration 21700: Loss = -10978.798828125
12
Iteration 21800: Loss = -10978.7978515625
Iteration 21900: Loss = -10978.798828125
1
Iteration 22000: Loss = -10978.7998046875
2
Iteration 22100: Loss = -10978.798828125
3
Iteration 22200: Loss = -10978.798828125
4
Iteration 22300: Loss = -10978.798828125
5
Iteration 22400: Loss = -10978.7978515625
Iteration 22500: Loss = -10978.798828125
1
Iteration 22600: Loss = -10978.7998046875
2
Iteration 22700: Loss = -10978.7998046875
3
Iteration 22800: Loss = -10978.798828125
4
Iteration 22900: Loss = -10978.798828125
5
Iteration 23000: Loss = -10978.7998046875
6
Iteration 23100: Loss = -10978.7998046875
7
Iteration 23200: Loss = -10978.7978515625
Iteration 23300: Loss = -10978.7998046875
1
Iteration 23400: Loss = -10978.798828125
2
Iteration 23500: Loss = -10978.798828125
3
Iteration 23600: Loss = -10978.7998046875
4
Iteration 23700: Loss = -10978.7998046875
5
Iteration 23800: Loss = -10978.798828125
6
Iteration 23900: Loss = -10978.7998046875
7
Iteration 24000: Loss = -10978.798828125
8
Iteration 24100: Loss = -10978.7978515625
Iteration 24200: Loss = -10978.80078125
1
Iteration 24300: Loss = -10978.7998046875
2
Iteration 24400: Loss = -10978.7998046875
3
Iteration 24500: Loss = -10978.798828125
4
Iteration 24600: Loss = -10978.798828125
5
Iteration 24700: Loss = -10978.7998046875
6
Iteration 24800: Loss = -10978.7978515625
Iteration 24900: Loss = -10978.7998046875
1
Iteration 25000: Loss = -10978.80078125
2
Iteration 25100: Loss = -10978.798828125
3
Iteration 25200: Loss = -10978.7998046875
4
Iteration 25300: Loss = -10978.798828125
5
Iteration 25400: Loss = -10978.798828125
6
Iteration 25500: Loss = -10978.798828125
7
Iteration 25600: Loss = -10978.798828125
8
Iteration 25700: Loss = -10978.7998046875
9
Iteration 25800: Loss = -10978.7978515625
Iteration 25900: Loss = -10978.798828125
1
Iteration 26000: Loss = -10978.7998046875
2
Iteration 26100: Loss = -10978.798828125
3
Iteration 26200: Loss = -10978.798828125
4
Iteration 26300: Loss = -10978.798828125
5
Iteration 26400: Loss = -10978.798828125
6
Iteration 26500: Loss = -10978.7998046875
7
Iteration 26600: Loss = -10978.798828125
8
Iteration 26700: Loss = -10978.798828125
9
Iteration 26800: Loss = -10978.798828125
10
Iteration 26900: Loss = -10978.7998046875
11
Iteration 27000: Loss = -10978.798828125
12
Iteration 27100: Loss = -10978.798828125
13
Iteration 27200: Loss = -10978.798828125
14
Iteration 27300: Loss = -10978.7998046875
15
Stopping early at iteration 27300 due to no improvement.
pi: tensor([[1.6186e-05, 9.9998e-01],
        [8.0043e-02, 9.1996e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0148, 0.9852], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1239],
         [0.9838, 0.1612]],

        [[0.0270, 0.2040],
         [0.9386, 0.9613]],

        [[0.0132, 0.2012],
         [0.0357, 0.6650]],

        [[0.4248, 0.1631],
         [0.9852, 0.6965]],

        [[0.2289, 0.1165],
         [0.0356, 0.9819]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002304549185215683
Average Adjusted Rand Index: -0.0013316687473990845
[0.8985007858906296, 0.0002304549185215683] [0.8980024982567766, -0.0013316687473990845] [10830.9873046875, 10978.7998046875]
-------------------------------------
This iteration is 63
True Objective function: Loss = -10922.026078605853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31629.291015625
Iteration 100: Loss = -21212.4765625
Iteration 200: Loss = -13459.8447265625
Iteration 300: Loss = -11643.728515625
Iteration 400: Loss = -11418.3330078125
Iteration 500: Loss = -11327.234375
Iteration 600: Loss = -11275.12109375
Iteration 700: Loss = -11237.85546875
Iteration 800: Loss = -11212.1650390625
Iteration 900: Loss = -11193.4365234375
Iteration 1000: Loss = -11178.365234375
Iteration 1100: Loss = -11166.2421875
Iteration 1200: Loss = -11154.1689453125
Iteration 1300: Loss = -11143.498046875
Iteration 1400: Loss = -11132.9013671875
Iteration 1500: Loss = -11120.990234375
Iteration 1600: Loss = -11114.130859375
Iteration 1700: Loss = -11108.9521484375
Iteration 1800: Loss = -11102.3310546875
Iteration 1900: Loss = -11094.3076171875
Iteration 2000: Loss = -11083.78125
Iteration 2100: Loss = -11076.505859375
Iteration 2200: Loss = -11065.3525390625
Iteration 2300: Loss = -11053.5068359375
Iteration 2400: Loss = -11043.5830078125
Iteration 2500: Loss = -11036.73046875
Iteration 2600: Loss = -11032.2685546875
Iteration 2700: Loss = -11025.5556640625
Iteration 2800: Loss = -11017.40234375
Iteration 2900: Loss = -11014.125
Iteration 3000: Loss = -11012.2568359375
Iteration 3100: Loss = -11006.8544921875
Iteration 3200: Loss = -10997.2451171875
Iteration 3300: Loss = -10995.59765625
Iteration 3400: Loss = -10994.587890625
Iteration 3500: Loss = -10993.8125
Iteration 3600: Loss = -10993.1728515625
Iteration 3700: Loss = -10992.6064453125
Iteration 3800: Loss = -10991.9892578125
Iteration 3900: Loss = -10988.873046875
Iteration 4000: Loss = -10986.5869140625
Iteration 4100: Loss = -10986.05078125
Iteration 4200: Loss = -10985.693359375
Iteration 4300: Loss = -10985.404296875
Iteration 4400: Loss = -10985.15234375
Iteration 4500: Loss = -10984.9296875
Iteration 4600: Loss = -10984.728515625
Iteration 4700: Loss = -10984.5458984375
Iteration 4800: Loss = -10984.380859375
Iteration 4900: Loss = -10984.2294921875
Iteration 5000: Loss = -10984.0908203125
Iteration 5100: Loss = -10983.9638671875
Iteration 5200: Loss = -10983.8447265625
Iteration 5300: Loss = -10983.7353515625
Iteration 5400: Loss = -10983.6318359375
Iteration 5500: Loss = -10983.53515625
Iteration 5600: Loss = -10983.44140625
Iteration 5700: Loss = -10983.2919921875
Iteration 5800: Loss = -10974.3115234375
Iteration 5900: Loss = -10973.6943359375
Iteration 6000: Loss = -10973.4716796875
Iteration 6100: Loss = -10973.3203125
Iteration 6200: Loss = -10973.2001953125
Iteration 6300: Loss = -10973.09765625
Iteration 6400: Loss = -10973.0087890625
Iteration 6500: Loss = -10972.9287109375
Iteration 6600: Loss = -10972.85546875
Iteration 6700: Loss = -10972.791015625
Iteration 6800: Loss = -10972.7294921875
Iteration 6900: Loss = -10972.673828125
Iteration 7000: Loss = -10972.6220703125
Iteration 7100: Loss = -10972.572265625
Iteration 7200: Loss = -10972.5283203125
Iteration 7300: Loss = -10972.48828125
Iteration 7400: Loss = -10972.44921875
Iteration 7500: Loss = -10972.4140625
Iteration 7600: Loss = -10972.3798828125
Iteration 7700: Loss = -10972.34765625
Iteration 7800: Loss = -10972.318359375
Iteration 7900: Loss = -10972.2900390625
Iteration 8000: Loss = -10972.2666015625
Iteration 8100: Loss = -10972.2412109375
Iteration 8200: Loss = -10972.216796875
Iteration 8300: Loss = -10972.1962890625
Iteration 8400: Loss = -10972.173828125
Iteration 8500: Loss = -10972.15625
Iteration 8600: Loss = -10972.138671875
Iteration 8700: Loss = -10972.1220703125
Iteration 8800: Loss = -10972.1044921875
Iteration 8900: Loss = -10972.08984375
Iteration 9000: Loss = -10972.076171875
Iteration 9100: Loss = -10972.0634765625
Iteration 9200: Loss = -10972.05078125
Iteration 9300: Loss = -10972.0380859375
Iteration 9400: Loss = -10972.0263671875
Iteration 9500: Loss = -10972.015625
Iteration 9600: Loss = -10972.0068359375
Iteration 9700: Loss = -10971.9970703125
Iteration 9800: Loss = -10971.98828125
Iteration 9900: Loss = -10971.9794921875
Iteration 10000: Loss = -10971.9697265625
Iteration 10100: Loss = -10971.9619140625
Iteration 10200: Loss = -10971.9560546875
Iteration 10300: Loss = -10971.9482421875
Iteration 10400: Loss = -10971.9423828125
Iteration 10500: Loss = -10971.9375
Iteration 10600: Loss = -10971.9306640625
Iteration 10700: Loss = -10971.9248046875
Iteration 10800: Loss = -10971.919921875
Iteration 10900: Loss = -10971.9140625
Iteration 11000: Loss = -10971.91015625
Iteration 11100: Loss = -10971.9052734375
Iteration 11200: Loss = -10971.8994140625
Iteration 11300: Loss = -10971.8974609375
Iteration 11400: Loss = -10971.8935546875
Iteration 11500: Loss = -10971.8896484375
Iteration 11600: Loss = -10971.8857421875
Iteration 11700: Loss = -10971.8837890625
Iteration 11800: Loss = -10971.8798828125
Iteration 11900: Loss = -10971.876953125
Iteration 12000: Loss = -10971.8740234375
Iteration 12100: Loss = -10971.87109375
Iteration 12200: Loss = -10971.8681640625
Iteration 12300: Loss = -10971.8671875
Iteration 12400: Loss = -10971.86328125
Iteration 12500: Loss = -10971.8603515625
Iteration 12600: Loss = -10971.859375
Iteration 12700: Loss = -10971.8564453125
Iteration 12800: Loss = -10971.8544921875
Iteration 12900: Loss = -10971.8544921875
Iteration 13000: Loss = -10971.8525390625
Iteration 13100: Loss = -10971.8505859375
Iteration 13200: Loss = -10971.8486328125
Iteration 13300: Loss = -10971.8466796875
Iteration 13400: Loss = -10971.845703125
Iteration 13500: Loss = -10971.84375
Iteration 13600: Loss = -10971.841796875
Iteration 13700: Loss = -10971.8408203125
Iteration 13800: Loss = -10971.8408203125
Iteration 13900: Loss = -10971.8388671875
Iteration 14000: Loss = -10971.8388671875
Iteration 14100: Loss = -10971.8369140625
Iteration 14200: Loss = -10971.8369140625
Iteration 14300: Loss = -10971.8359375
Iteration 14400: Loss = -10971.8359375
Iteration 14500: Loss = -10971.8330078125
Iteration 14600: Loss = -10971.8330078125
Iteration 14700: Loss = -10971.8330078125
Iteration 14800: Loss = -10971.83203125
Iteration 14900: Loss = -10971.8310546875
Iteration 15000: Loss = -10971.830078125
Iteration 15100: Loss = -10971.8291015625
Iteration 15200: Loss = -10971.8291015625
Iteration 15300: Loss = -10971.828125
Iteration 15400: Loss = -10971.828125
Iteration 15500: Loss = -10971.828125
Iteration 15600: Loss = -10971.828125
Iteration 15700: Loss = -10971.8271484375
Iteration 15800: Loss = -10971.8251953125
Iteration 15900: Loss = -10971.8251953125
Iteration 16000: Loss = -10971.8251953125
Iteration 16100: Loss = -10971.8251953125
Iteration 16200: Loss = -10971.8251953125
Iteration 16300: Loss = -10971.82421875
Iteration 16400: Loss = -10971.82421875
Iteration 16500: Loss = -10971.8251953125
1
Iteration 16600: Loss = -10971.8251953125
2
Iteration 16700: Loss = -10971.82421875
Iteration 16800: Loss = -10971.8232421875
Iteration 16900: Loss = -10971.822265625
Iteration 17000: Loss = -10971.8212890625
Iteration 17100: Loss = -10971.8212890625
Iteration 17200: Loss = -10971.8212890625
Iteration 17300: Loss = -10971.8232421875
1
Iteration 17400: Loss = -10971.8212890625
Iteration 17500: Loss = -10971.8212890625
Iteration 17600: Loss = -10971.822265625
1
Iteration 17700: Loss = -10971.822265625
2
Iteration 17800: Loss = -10971.8203125
Iteration 17900: Loss = -10971.8212890625
1
Iteration 18000: Loss = -10971.8203125
Iteration 18100: Loss = -10971.8193359375
Iteration 18200: Loss = -10971.8212890625
1
Iteration 18300: Loss = -10971.8203125
2
Iteration 18400: Loss = -10971.8203125
3
Iteration 18500: Loss = -10971.8203125
4
Iteration 18600: Loss = -10971.8203125
5
Iteration 18700: Loss = -10971.8203125
6
Iteration 18800: Loss = -10971.818359375
Iteration 18900: Loss = -10971.818359375
Iteration 19000: Loss = -10971.8203125
1
Iteration 19100: Loss = -10971.818359375
Iteration 19200: Loss = -10971.818359375
Iteration 19300: Loss = -10971.8203125
1
Iteration 19400: Loss = -10971.8203125
2
Iteration 19500: Loss = -10971.8193359375
3
Iteration 19600: Loss = -10971.818359375
Iteration 19700: Loss = -10971.8203125
1
Iteration 19800: Loss = -10971.8203125
2
Iteration 19900: Loss = -10971.8203125
3
Iteration 20000: Loss = -10971.8203125
4
Iteration 20100: Loss = -10971.818359375
Iteration 20200: Loss = -10971.8203125
1
Iteration 20300: Loss = -10971.8193359375
2
Iteration 20400: Loss = -10971.8173828125
Iteration 20500: Loss = -10971.8193359375
1
Iteration 20600: Loss = -10971.818359375
2
Iteration 20700: Loss = -10971.8203125
3
Iteration 20800: Loss = -10971.818359375
4
Iteration 20900: Loss = -10971.8193359375
5
Iteration 21000: Loss = -10971.818359375
6
Iteration 21100: Loss = -10971.818359375
7
Iteration 21200: Loss = -10971.818359375
8
Iteration 21300: Loss = -10971.818359375
9
Iteration 21400: Loss = -10971.8193359375
10
Iteration 21500: Loss = -10971.818359375
11
Iteration 21600: Loss = -10971.8173828125
Iteration 21700: Loss = -10971.8173828125
Iteration 21800: Loss = -10971.81640625
Iteration 21900: Loss = -10971.8173828125
1
Iteration 22000: Loss = -10971.8173828125
2
Iteration 22100: Loss = -10971.818359375
3
Iteration 22200: Loss = -10971.8173828125
4
Iteration 22300: Loss = -10971.8173828125
5
Iteration 22400: Loss = -10971.818359375
6
Iteration 22500: Loss = -10971.818359375
7
Iteration 22600: Loss = -10971.8173828125
8
Iteration 22700: Loss = -10971.8173828125
9
Iteration 22800: Loss = -10971.818359375
10
Iteration 22900: Loss = -10971.818359375
11
Iteration 23000: Loss = -10971.8193359375
12
Iteration 23100: Loss = -10971.8173828125
13
Iteration 23200: Loss = -10971.818359375
14
Iteration 23300: Loss = -10971.818359375
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[1.2156e-06, 1.0000e+00],
        [1.0000e+00, 5.2671e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.3630e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1587, 0.1699],
         [0.2200, 0.1646]],

        [[0.2381, 0.1535],
         [0.8975, 0.0128]],

        [[0.3119, 0.8876],
         [0.6147, 0.8314]],

        [[0.9883, 0.2938],
         [0.3079, 0.4113]],

        [[0.5927, 0.1806],
         [0.4039, 0.0417]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015763047444543158
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45886.09375
Iteration 100: Loss = -27078.146484375
Iteration 200: Loss = -15138.05859375
Iteration 300: Loss = -12610.1318359375
Iteration 400: Loss = -11941.4091796875
Iteration 500: Loss = -11623.93359375
Iteration 600: Loss = -11426.1953125
Iteration 700: Loss = -11314.76171875
Iteration 800: Loss = -11242.349609375
Iteration 900: Loss = -11192.41796875
Iteration 1000: Loss = -11159.818359375
Iteration 1100: Loss = -11126.1640625
Iteration 1200: Loss = -11108.4072265625
Iteration 1300: Loss = -11095.9404296875
Iteration 1400: Loss = -11084.390625
Iteration 1500: Loss = -11074.8408203125
Iteration 1600: Loss = -11068.8994140625
Iteration 1700: Loss = -11062.275390625
Iteration 1800: Loss = -11057.3994140625
Iteration 1900: Loss = -11053.2509765625
Iteration 2000: Loss = -11049.8076171875
Iteration 2100: Loss = -11047.05859375
Iteration 2200: Loss = -11041.587890625
Iteration 2300: Loss = -11036.59765625
Iteration 2400: Loss = -11034.2646484375
Iteration 2500: Loss = -11031.1826171875
Iteration 2600: Loss = -11027.96484375
Iteration 2700: Loss = -11024.3984375
Iteration 2800: Loss = -11018.8310546875
Iteration 2900: Loss = -11015.6201171875
Iteration 3000: Loss = -11013.7626953125
Iteration 3100: Loss = -11012.6708984375
Iteration 3200: Loss = -11007.7412109375
Iteration 3300: Loss = -11006.2060546875
Iteration 3400: Loss = -11003.5625
Iteration 3500: Loss = -11002.7978515625
Iteration 3600: Loss = -11002.283203125
Iteration 3700: Loss = -11001.8408203125
Iteration 3800: Loss = -11001.44921875
Iteration 3900: Loss = -11000.98828125
Iteration 4000: Loss = -10997.2412109375
Iteration 4100: Loss = -10996.77734375
Iteration 4200: Loss = -10996.45703125
Iteration 4300: Loss = -10996.18359375
Iteration 4400: Loss = -10995.8828125
Iteration 4500: Loss = -10991.5302734375
Iteration 4600: Loss = -10991.091796875
Iteration 4700: Loss = -10988.990234375
Iteration 4800: Loss = -10985.75
Iteration 4900: Loss = -10985.3330078125
Iteration 5000: Loss = -10985.0693359375
Iteration 5100: Loss = -10984.8623046875
Iteration 5200: Loss = -10984.6904296875
Iteration 5300: Loss = -10984.5380859375
Iteration 5400: Loss = -10984.4013671875
Iteration 5500: Loss = -10984.28125
Iteration 5600: Loss = -10984.169921875
Iteration 5700: Loss = -10984.0673828125
Iteration 5800: Loss = -10983.9755859375
Iteration 5900: Loss = -10983.890625
Iteration 6000: Loss = -10983.8095703125
Iteration 6100: Loss = -10983.7333984375
Iteration 6200: Loss = -10983.6640625
Iteration 6300: Loss = -10983.599609375
Iteration 6400: Loss = -10983.533203125
Iteration 6500: Loss = -10983.4716796875
Iteration 6600: Loss = -10983.4140625
Iteration 6700: Loss = -10983.3623046875
Iteration 6800: Loss = -10983.3115234375
Iteration 6900: Loss = -10983.263671875
Iteration 7000: Loss = -10983.2177734375
Iteration 7100: Loss = -10983.1728515625
Iteration 7200: Loss = -10983.087890625
Iteration 7300: Loss = -10978.1796875
Iteration 7400: Loss = -10977.9990234375
Iteration 7500: Loss = -10973.7138671875
Iteration 7600: Loss = -10973.1845703125
Iteration 7700: Loss = -10973.0078125
Iteration 7800: Loss = -10972.87890625
Iteration 7900: Loss = -10972.7685546875
Iteration 8000: Loss = -10972.677734375
Iteration 8100: Loss = -10972.6005859375
Iteration 8200: Loss = -10972.529296875
Iteration 8300: Loss = -10972.4609375
Iteration 8400: Loss = -10972.3837890625
Iteration 8500: Loss = -10972.2890625
Iteration 8600: Loss = -10972.232421875
Iteration 8700: Loss = -10972.189453125
Iteration 8800: Loss = -10972.1474609375
Iteration 8900: Loss = -10972.1123046875
Iteration 9000: Loss = -10972.078125
Iteration 9100: Loss = -10972.046875
Iteration 9200: Loss = -10972.0166015625
Iteration 9300: Loss = -10971.986328125
Iteration 9400: Loss = -10971.951171875
Iteration 9500: Loss = -10971.916015625
Iteration 9600: Loss = -10971.890625
Iteration 9700: Loss = -10971.8642578125
Iteration 9800: Loss = -10971.841796875
Iteration 9900: Loss = -10971.8212890625
Iteration 10000: Loss = -10971.8046875
Iteration 10100: Loss = -10971.7900390625
Iteration 10200: Loss = -10971.7763671875
Iteration 10300: Loss = -10971.7626953125
Iteration 10400: Loss = -10971.7431640625
Iteration 10500: Loss = -10971.4443359375
Iteration 10600: Loss = -10971.4130859375
Iteration 10700: Loss = -10971.4013671875
Iteration 10800: Loss = -10971.3896484375
Iteration 10900: Loss = -10971.3818359375
Iteration 11000: Loss = -10971.373046875
Iteration 11100: Loss = -10971.365234375
Iteration 11200: Loss = -10971.35546875
Iteration 11300: Loss = -10971.349609375
Iteration 11400: Loss = -10971.341796875
Iteration 11500: Loss = -10971.333984375
Iteration 11600: Loss = -10971.328125
Iteration 11700: Loss = -10971.322265625
Iteration 11800: Loss = -10971.31640625
Iteration 11900: Loss = -10971.3115234375
Iteration 12000: Loss = -10971.3037109375
Iteration 12100: Loss = -10971.2998046875
Iteration 12200: Loss = -10971.291015625
Iteration 12300: Loss = -10971.287109375
Iteration 12400: Loss = -10971.2763671875
Iteration 12500: Loss = -10971.265625
Iteration 12600: Loss = -10971.228515625
Iteration 12700: Loss = -10971.16015625
Iteration 12800: Loss = -10971.1513671875
Iteration 12900: Loss = -10971.1474609375
Iteration 13000: Loss = -10971.146484375
Iteration 13100: Loss = -10971.1416015625
Iteration 13200: Loss = -10971.1396484375
Iteration 13300: Loss = -10971.1357421875
Iteration 13400: Loss = -10971.1357421875
Iteration 13500: Loss = -10971.1328125
Iteration 13600: Loss = -10971.130859375
Iteration 13700: Loss = -10971.1298828125
Iteration 13800: Loss = -10971.1279296875
Iteration 13900: Loss = -10971.1259765625
Iteration 14000: Loss = -10971.1220703125
Iteration 14100: Loss = -10971.12109375
Iteration 14200: Loss = -10971.115234375
Iteration 14300: Loss = -10971.1083984375
Iteration 14400: Loss = -10971.1044921875
Iteration 14500: Loss = -10971.1044921875
Iteration 14600: Loss = -10971.103515625
Iteration 14700: Loss = -10971.099609375
Iteration 14800: Loss = -10971.091796875
Iteration 14900: Loss = -10971.06640625
Iteration 15000: Loss = -10971.06640625
Iteration 15100: Loss = -10971.0634765625
Iteration 15200: Loss = -10971.0625
Iteration 15300: Loss = -10971.060546875
Iteration 15400: Loss = -10971.060546875
Iteration 15500: Loss = -10971.05859375
Iteration 15600: Loss = -10971.0576171875
Iteration 15700: Loss = -10971.0576171875
Iteration 15800: Loss = -10971.056640625
Iteration 15900: Loss = -10971.0546875
Iteration 16000: Loss = -10971.0537109375
Iteration 16100: Loss = -10971.0537109375
Iteration 16200: Loss = -10971.0537109375
Iteration 16300: Loss = -10971.052734375
Iteration 16400: Loss = -10971.0517578125
Iteration 16500: Loss = -10971.05078125
Iteration 16600: Loss = -10971.05078125
Iteration 16700: Loss = -10971.05078125
Iteration 16800: Loss = -10971.05078125
Iteration 16900: Loss = -10971.0517578125
1
Iteration 17000: Loss = -10971.0498046875
Iteration 17100: Loss = -10971.0478515625
Iteration 17200: Loss = -10971.0478515625
Iteration 17300: Loss = -10971.048828125
1
Iteration 17400: Loss = -10971.048828125
2
Iteration 17500: Loss = -10971.0478515625
Iteration 17600: Loss = -10971.046875
Iteration 17700: Loss = -10971.0458984375
Iteration 17800: Loss = -10971.046875
1
Iteration 17900: Loss = -10971.0458984375
Iteration 18000: Loss = -10971.0458984375
Iteration 18100: Loss = -10971.0458984375
Iteration 18200: Loss = -10971.044921875
Iteration 18300: Loss = -10971.044921875
Iteration 18400: Loss = -10971.0458984375
1
Iteration 18500: Loss = -10971.0244140625
Iteration 18600: Loss = -10971.0224609375
Iteration 18700: Loss = -10971.0234375
1
Iteration 18800: Loss = -10971.0234375
2
Iteration 18900: Loss = -10971.0224609375
Iteration 19000: Loss = -10971.0224609375
Iteration 19100: Loss = -10971.0224609375
Iteration 19200: Loss = -10971.021484375
Iteration 19300: Loss = -10971.021484375
Iteration 19400: Loss = -10971.021484375
Iteration 19500: Loss = -10971.021484375
Iteration 19600: Loss = -10971.0224609375
1
Iteration 19700: Loss = -10971.0205078125
Iteration 19800: Loss = -10971.021484375
1
Iteration 19900: Loss = -10971.0205078125
Iteration 20000: Loss = -10971.0205078125
Iteration 20100: Loss = -10971.021484375
1
Iteration 20200: Loss = -10971.01953125
Iteration 20300: Loss = -10971.0224609375
1
Iteration 20400: Loss = -10971.0205078125
2
Iteration 20500: Loss = -10971.0205078125
3
Iteration 20600: Loss = -10971.0205078125
4
Iteration 20700: Loss = -10971.0205078125
5
Iteration 20800: Loss = -10971.0224609375
6
Iteration 20900: Loss = -10971.0205078125
7
Iteration 21000: Loss = -10971.021484375
8
Iteration 21100: Loss = -10971.0205078125
9
Iteration 21200: Loss = -10971.0205078125
10
Iteration 21300: Loss = -10971.0205078125
11
Iteration 21400: Loss = -10971.021484375
12
Iteration 21500: Loss = -10971.01953125
Iteration 21600: Loss = -10971.01953125
Iteration 21700: Loss = -10971.0205078125
1
Iteration 21800: Loss = -10971.0205078125
2
Iteration 21900: Loss = -10971.0205078125
3
Iteration 22000: Loss = -10971.0205078125
4
Iteration 22100: Loss = -10971.0205078125
5
Iteration 22200: Loss = -10971.01953125
Iteration 22300: Loss = -10971.0205078125
1
Iteration 22400: Loss = -10971.0205078125
2
Iteration 22500: Loss = -10971.0205078125
3
Iteration 22600: Loss = -10971.01953125
Iteration 22700: Loss = -10971.0205078125
1
Iteration 22800: Loss = -10971.021484375
2
Iteration 22900: Loss = -10971.01953125
Iteration 23000: Loss = -10971.021484375
1
Iteration 23100: Loss = -10971.021484375
2
Iteration 23200: Loss = -10971.0185546875
Iteration 23300: Loss = -10971.01953125
1
Iteration 23400: Loss = -10971.0205078125
2
Iteration 23500: Loss = -10971.0205078125
3
Iteration 23600: Loss = -10971.0205078125
4
Iteration 23700: Loss = -10971.0205078125
5
Iteration 23800: Loss = -10971.0205078125
6
Iteration 23900: Loss = -10971.0205078125
7
Iteration 24000: Loss = -10971.0205078125
8
Iteration 24100: Loss = -10971.01953125
9
Iteration 24200: Loss = -10971.021484375
10
Iteration 24300: Loss = -10971.01953125
11
Iteration 24400: Loss = -10971.021484375
12
Iteration 24500: Loss = -10971.0185546875
Iteration 24600: Loss = -10971.017578125
Iteration 24700: Loss = -10971.0185546875
1
Iteration 24800: Loss = -10971.01953125
2
Iteration 24900: Loss = -10971.0166015625
Iteration 25000: Loss = -10971.01953125
1
Iteration 25100: Loss = -10971.017578125
2
Iteration 25200: Loss = -10971.0166015625
Iteration 25300: Loss = -10971.01171875
Iteration 25400: Loss = -10971.0078125
Iteration 25500: Loss = -10971.005859375
Iteration 25600: Loss = -10971.0009765625
Iteration 25700: Loss = -10970.9931640625
Iteration 25800: Loss = -10970.982421875
Iteration 25900: Loss = -10970.958984375
Iteration 26000: Loss = -10970.9404296875
Iteration 26100: Loss = -10970.927734375
Iteration 26200: Loss = -10970.9140625
Iteration 26300: Loss = -10970.91015625
Iteration 26400: Loss = -10970.9052734375
Iteration 26500: Loss = -10970.2216796875
Iteration 26600: Loss = -10969.119140625
Iteration 26700: Loss = -10969.1162109375
Iteration 26800: Loss = -10969.1162109375
Iteration 26900: Loss = -10969.115234375
Iteration 27000: Loss = -10969.115234375
Iteration 27100: Loss = -10969.1162109375
1
Iteration 27200: Loss = -10969.115234375
Iteration 27300: Loss = -10969.115234375
Iteration 27400: Loss = -10969.1142578125
Iteration 27500: Loss = -10969.1162109375
1
Iteration 27600: Loss = -10969.115234375
2
Iteration 27700: Loss = -10969.115234375
3
Iteration 27800: Loss = -10969.115234375
4
Iteration 27900: Loss = -10969.115234375
5
Iteration 28000: Loss = -10969.1142578125
Iteration 28100: Loss = -10969.115234375
1
Iteration 28200: Loss = -10969.1162109375
2
Iteration 28300: Loss = -10969.1171875
3
Iteration 28400: Loss = -10969.1142578125
Iteration 28500: Loss = -10969.115234375
1
Iteration 28600: Loss = -10969.1142578125
Iteration 28700: Loss = -10969.115234375
1
Iteration 28800: Loss = -10969.1162109375
2
Iteration 28900: Loss = -10969.1142578125
Iteration 29000: Loss = -10969.115234375
1
Iteration 29100: Loss = -10969.1162109375
2
Iteration 29200: Loss = -10969.1162109375
3
Iteration 29300: Loss = -10969.1162109375
4
Iteration 29400: Loss = -10969.1162109375
5
Iteration 29500: Loss = -10969.1142578125
Iteration 29600: Loss = -10969.1162109375
1
Iteration 29700: Loss = -10969.1142578125
Iteration 29800: Loss = -10969.115234375
1
Iteration 29900: Loss = -10969.115234375
2
pi: tensor([[3.9559e-02, 9.6044e-01],
        [8.9323e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9178, 0.0822], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1816, 0.0972],
         [0.9890, 0.1615]],

        [[0.2368, 0.1086],
         [0.2270, 0.2249]],

        [[0.0678, 0.1582],
         [0.9893, 0.0405]],

        [[0.1189, 0.1745],
         [0.9186, 0.0541]],

        [[0.0463, 0.2388],
         [0.2147, 0.0127]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.04480290613445197
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.009954447973679092
Average Adjusted Rand Index: 0.008960581226890394
[-0.0015763047444543158, 0.009954447973679092] [0.0, 0.008960581226890394] [10971.818359375, 10969.115234375]
-------------------------------------
This iteration is 64
True Objective function: Loss = -10829.127846344172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39388.91015625
Iteration 100: Loss = -22794.8515625
Iteration 200: Loss = -13519.947265625
Iteration 300: Loss = -11919.9482421875
Iteration 400: Loss = -11536.8076171875
Iteration 500: Loss = -11375.6494140625
Iteration 600: Loss = -11287.28125
Iteration 700: Loss = -11225.3408203125
Iteration 800: Loss = -11181.9814453125
Iteration 900: Loss = -11138.7041015625
Iteration 1000: Loss = -11110.029296875
Iteration 1100: Loss = -11082.2578125
Iteration 1200: Loss = -11060.302734375
Iteration 1300: Loss = -11039.75390625
Iteration 1400: Loss = -11022.7734375
Iteration 1500: Loss = -11011.8974609375
Iteration 1600: Loss = -11003.142578125
Iteration 1700: Loss = -10994.7158203125
Iteration 1800: Loss = -10988.2236328125
Iteration 1900: Loss = -10983.0009765625
Iteration 2000: Loss = -10978.400390625
Iteration 2100: Loss = -10974.3291015625
Iteration 2200: Loss = -10971.3515625
Iteration 2300: Loss = -10969.0927734375
Iteration 2400: Loss = -10966.8154296875
Iteration 2500: Loss = -10964.5341796875
Iteration 2600: Loss = -10960.7587890625
Iteration 2700: Loss = -10958.9248046875
Iteration 2800: Loss = -10957.8330078125
Iteration 2900: Loss = -10956.876953125
Iteration 3000: Loss = -10955.830078125
Iteration 3100: Loss = -10954.1103515625
Iteration 3200: Loss = -10952.5341796875
Iteration 3300: Loss = -10951.703125
Iteration 3400: Loss = -10951.0244140625
Iteration 3500: Loss = -10949.2509765625
Iteration 3600: Loss = -10948.681640625
Iteration 3700: Loss = -10948.2568359375
Iteration 3800: Loss = -10947.892578125
Iteration 3900: Loss = -10947.5712890625
Iteration 4000: Loss = -10947.275390625
Iteration 4100: Loss = -10946.99609375
Iteration 4200: Loss = -10945.2783203125
Iteration 4300: Loss = -10939.2607421875
Iteration 4400: Loss = -10938.8486328125
Iteration 4500: Loss = -10938.572265625
Iteration 4600: Loss = -10938.34375
Iteration 4700: Loss = -10938.1455078125
Iteration 4800: Loss = -10937.9697265625
Iteration 4900: Loss = -10937.810546875
Iteration 5000: Loss = -10937.6640625
Iteration 5100: Loss = -10937.5302734375
Iteration 5200: Loss = -10937.4072265625
Iteration 5300: Loss = -10937.29296875
Iteration 5400: Loss = -10937.173828125
Iteration 5500: Loss = -10933.0400390625
Iteration 5600: Loss = -10932.71875
Iteration 5700: Loss = -10932.5625
Iteration 5800: Loss = -10932.4443359375
Iteration 5900: Loss = -10932.345703125
Iteration 6000: Loss = -10932.2587890625
Iteration 6100: Loss = -10932.1826171875
Iteration 6200: Loss = -10932.111328125
Iteration 6300: Loss = -10932.044921875
Iteration 6400: Loss = -10931.984375
Iteration 6500: Loss = -10931.9287109375
Iteration 6600: Loss = -10931.875
Iteration 6700: Loss = -10931.826171875
Iteration 6800: Loss = -10931.7783203125
Iteration 6900: Loss = -10931.734375
Iteration 7000: Loss = -10931.693359375
Iteration 7100: Loss = -10931.654296875
Iteration 7200: Loss = -10931.6142578125
Iteration 7300: Loss = -10931.580078125
Iteration 7400: Loss = -10931.5478515625
Iteration 7500: Loss = -10931.5146484375
Iteration 7600: Loss = -10931.4853515625
Iteration 7700: Loss = -10931.45703125
Iteration 7800: Loss = -10931.4287109375
Iteration 7900: Loss = -10931.40234375
Iteration 8000: Loss = -10931.3798828125
Iteration 8100: Loss = -10931.3544921875
Iteration 8200: Loss = -10931.3330078125
Iteration 8300: Loss = -10931.3115234375
Iteration 8400: Loss = -10931.291015625
Iteration 8500: Loss = -10931.2705078125
Iteration 8600: Loss = -10931.25390625
Iteration 8700: Loss = -10931.236328125
Iteration 8800: Loss = -10931.216796875
Iteration 8900: Loss = -10931.201171875
Iteration 9000: Loss = -10931.1826171875
Iteration 9100: Loss = -10931.166015625
Iteration 9200: Loss = -10926.267578125
Iteration 9300: Loss = -10925.92578125
Iteration 9400: Loss = -10925.869140625
Iteration 9500: Loss = -10925.8349609375
Iteration 9600: Loss = -10925.810546875
Iteration 9700: Loss = -10925.791015625
Iteration 9800: Loss = -10925.7744140625
Iteration 9900: Loss = -10925.7548828125
Iteration 10000: Loss = -10925.736328125
Iteration 10100: Loss = -10925.712890625
Iteration 10200: Loss = -10925.689453125
Iteration 10300: Loss = -10925.669921875
Iteration 10400: Loss = -10925.654296875
Iteration 10500: Loss = -10925.630859375
Iteration 10600: Loss = -10925.6083984375
Iteration 10700: Loss = -10925.5986328125
Iteration 10800: Loss = -10925.5908203125
Iteration 10900: Loss = -10925.5859375
Iteration 11000: Loss = -10925.576171875
Iteration 11100: Loss = -10920.158203125
Iteration 11200: Loss = -10919.904296875
Iteration 11300: Loss = -10919.8076171875
Iteration 11400: Loss = -10919.75
Iteration 11500: Loss = -10919.7119140625
Iteration 11600: Loss = -10919.68359375
Iteration 11700: Loss = -10919.662109375
Iteration 11800: Loss = -10919.6435546875
Iteration 11900: Loss = -10919.626953125
Iteration 12000: Loss = -10919.6162109375
Iteration 12100: Loss = -10919.60546875
Iteration 12200: Loss = -10919.595703125
Iteration 12300: Loss = -10919.5888671875
Iteration 12400: Loss = -10919.5810546875
Iteration 12500: Loss = -10919.57421875
Iteration 12600: Loss = -10919.5703125
Iteration 12700: Loss = -10919.5634765625
Iteration 12800: Loss = -10919.55859375
Iteration 12900: Loss = -10919.5556640625
Iteration 13000: Loss = -10919.55078125
Iteration 13100: Loss = -10919.546875
Iteration 13200: Loss = -10919.544921875
Iteration 13300: Loss = -10919.5390625
Iteration 13400: Loss = -10919.5341796875
Iteration 13500: Loss = -10919.3037109375
Iteration 13600: Loss = -10919.2578125
Iteration 13700: Loss = -10919.251953125
Iteration 13800: Loss = -10919.248046875
Iteration 13900: Loss = -10919.2451171875
Iteration 14000: Loss = -10919.236328125
Iteration 14100: Loss = -10919.19921875
Iteration 14200: Loss = -10919.197265625
Iteration 14300: Loss = -10919.1953125
Iteration 14400: Loss = -10919.193359375
Iteration 14500: Loss = -10919.19140625
Iteration 14600: Loss = -10919.189453125
Iteration 14700: Loss = -10919.189453125
Iteration 14800: Loss = -10919.189453125
Iteration 14900: Loss = -10919.1875
Iteration 15000: Loss = -10919.1865234375
Iteration 15100: Loss = -10919.185546875
Iteration 15200: Loss = -10919.1845703125
Iteration 15300: Loss = -10919.181640625
Iteration 15400: Loss = -10919.1826171875
1
Iteration 15500: Loss = -10919.1806640625
Iteration 15600: Loss = -10919.1806640625
Iteration 15700: Loss = -10919.1806640625
Iteration 15800: Loss = -10919.1796875
Iteration 15900: Loss = -10919.1806640625
1
Iteration 16000: Loss = -10919.1953125
2
Iteration 16100: Loss = -10919.177734375
Iteration 16200: Loss = -10919.17578125
Iteration 16300: Loss = -10919.177734375
1
Iteration 16400: Loss = -10919.17578125
Iteration 16500: Loss = -10919.17578125
Iteration 16600: Loss = -10919.173828125
Iteration 16700: Loss = -10919.173828125
Iteration 16800: Loss = -10919.173828125
Iteration 16900: Loss = -10919.1728515625
Iteration 17000: Loss = -10919.171875
Iteration 17100: Loss = -10919.1728515625
1
Iteration 17200: Loss = -10919.1728515625
2
Iteration 17300: Loss = -10919.173828125
3
Iteration 17400: Loss = -10919.1728515625
4
Iteration 17500: Loss = -10919.1728515625
5
Iteration 17600: Loss = -10919.171875
Iteration 17700: Loss = -10919.1708984375
Iteration 17800: Loss = -10919.169921875
Iteration 17900: Loss = -10919.169921875
Iteration 18000: Loss = -10919.169921875
Iteration 18100: Loss = -10919.169921875
Iteration 18200: Loss = -10919.1708984375
1
Iteration 18300: Loss = -10919.169921875
Iteration 18400: Loss = -10919.169921875
Iteration 18500: Loss = -10919.169921875
Iteration 18600: Loss = -10919.1708984375
1
Iteration 18700: Loss = -10919.1689453125
Iteration 18800: Loss = -10919.1689453125
Iteration 18900: Loss = -10919.1689453125
Iteration 19000: Loss = -10919.1708984375
1
Iteration 19100: Loss = -10919.16796875
Iteration 19200: Loss = -10919.1689453125
1
Iteration 19300: Loss = -10919.16796875
Iteration 19400: Loss = -10919.1689453125
1
Iteration 19500: Loss = -10919.1611328125
Iteration 19600: Loss = -10919.162109375
1
Iteration 19700: Loss = -10919.1611328125
Iteration 19800: Loss = -10919.1611328125
Iteration 19900: Loss = -10919.162109375
1
Iteration 20000: Loss = -10919.1611328125
Iteration 20100: Loss = -10919.162109375
1
Iteration 20200: Loss = -10919.1630859375
2
Iteration 20300: Loss = -10919.16015625
Iteration 20400: Loss = -10919.1611328125
1
Iteration 20500: Loss = -10919.16015625
Iteration 20600: Loss = -10919.162109375
1
Iteration 20700: Loss = -10919.16015625
Iteration 20800: Loss = -10919.162109375
1
Iteration 20900: Loss = -10919.1611328125
2
Iteration 21000: Loss = -10919.1630859375
3
Iteration 21100: Loss = -10919.16015625
Iteration 21200: Loss = -10919.16015625
Iteration 21300: Loss = -10919.1611328125
1
Iteration 21400: Loss = -10919.1611328125
2
Iteration 21500: Loss = -10919.162109375
3
Iteration 21600: Loss = -10919.1611328125
4
Iteration 21700: Loss = -10919.16015625
Iteration 21800: Loss = -10919.1611328125
1
Iteration 21900: Loss = -10919.158203125
Iteration 22000: Loss = -10919.1611328125
1
Iteration 22100: Loss = -10919.162109375
2
Iteration 22200: Loss = -10919.1611328125
3
Iteration 22300: Loss = -10919.162109375
4
Iteration 22400: Loss = -10919.1611328125
5
Iteration 22500: Loss = -10919.1591796875
6
Iteration 22600: Loss = -10919.16015625
7
Iteration 22700: Loss = -10919.1640625
8
Iteration 22800: Loss = -10919.1611328125
9
Iteration 22900: Loss = -10919.1591796875
10
Iteration 23000: Loss = -10919.1611328125
11
Iteration 23100: Loss = -10919.162109375
12
Iteration 23200: Loss = -10919.1640625
13
Iteration 23300: Loss = -10919.1611328125
14
Iteration 23400: Loss = -10919.162109375
15
Stopping early at iteration 23400 due to no improvement.
pi: tensor([[1.0000e+00, 4.3062e-06],
        [9.9990e-01, 1.0429e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9169, 0.0831], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.1987],
         [0.0375, 0.2469]],

        [[0.8408, 0.1581],
         [0.0836, 0.0254]],

        [[0.0532, 0.6478],
         [0.0653, 0.9232]],

        [[0.9796, 0.1641],
         [0.9376, 0.0930]],

        [[0.9685, 0.2789],
         [0.9031, 0.9244]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22143.50390625
Iteration 100: Loss = -15911.20703125
Iteration 200: Loss = -11801.78515625
Iteration 300: Loss = -11303.166015625
Iteration 400: Loss = -11176.099609375
Iteration 500: Loss = -11121.8154296875
Iteration 600: Loss = -11089.9013671875
Iteration 700: Loss = -11072.7861328125
Iteration 800: Loss = -11056.0615234375
Iteration 900: Loss = -11043.541015625
Iteration 1000: Loss = -11033.96875
Iteration 1100: Loss = -11022.90234375
Iteration 1200: Loss = -11013.1650390625
Iteration 1300: Loss = -11004.21484375
Iteration 1400: Loss = -10996.1572265625
Iteration 1500: Loss = -10990.9208984375
Iteration 1600: Loss = -10985.23046875
Iteration 1700: Loss = -10979.935546875
Iteration 1800: Loss = -10973.9521484375
Iteration 1900: Loss = -10971.0302734375
Iteration 2000: Loss = -10967.2099609375
Iteration 2100: Loss = -10963.3857421875
Iteration 2200: Loss = -10957.564453125
Iteration 2300: Loss = -10953.6787109375
Iteration 2400: Loss = -10950.8251953125
Iteration 2500: Loss = -10946.744140625
Iteration 2600: Loss = -10939.2822265625
Iteration 2700: Loss = -10935.7392578125
Iteration 2800: Loss = -10932.2392578125
Iteration 2900: Loss = -10930.1025390625
Iteration 3000: Loss = -10928.849609375
Iteration 3100: Loss = -10927.9794921875
Iteration 3200: Loss = -10927.318359375
Iteration 3300: Loss = -10926.7919921875
Iteration 3400: Loss = -10926.3583984375
Iteration 3500: Loss = -10925.9931640625
Iteration 3600: Loss = -10925.6787109375
Iteration 3700: Loss = -10925.40625
Iteration 3800: Loss = -10925.1650390625
Iteration 3900: Loss = -10924.9521484375
Iteration 4000: Loss = -10924.76171875
Iteration 4100: Loss = -10924.5908203125
Iteration 4200: Loss = -10924.4326171875
Iteration 4300: Loss = -10924.287109375
Iteration 4400: Loss = -10924.1435546875
Iteration 4500: Loss = -10923.9306640625
Iteration 4600: Loss = -10923.2685546875
Iteration 4700: Loss = -10923.07421875
Iteration 4800: Loss = -10922.9443359375
Iteration 4900: Loss = -10922.8388671875
Iteration 5000: Loss = -10922.75
Iteration 5100: Loss = -10922.6708984375
Iteration 5200: Loss = -10922.599609375
Iteration 5300: Loss = -10922.5361328125
Iteration 5400: Loss = -10922.4794921875
Iteration 5500: Loss = -10922.4248046875
Iteration 5600: Loss = -10922.3740234375
Iteration 5700: Loss = -10922.328125
Iteration 5800: Loss = -10922.283203125
Iteration 5900: Loss = -10922.2412109375
Iteration 6000: Loss = -10922.2041015625
Iteration 6100: Loss = -10922.1689453125
Iteration 6200: Loss = -10922.13671875
Iteration 6300: Loss = -10922.10546875
Iteration 6400: Loss = -10922.07421875
Iteration 6500: Loss = -10922.046875
Iteration 6600: Loss = -10922.01953125
Iteration 6700: Loss = -10921.99609375
Iteration 6800: Loss = -10921.9716796875
Iteration 6900: Loss = -10921.9453125
Iteration 7000: Loss = -10921.923828125
Iteration 7100: Loss = -10921.9052734375
Iteration 7200: Loss = -10921.884765625
Iteration 7300: Loss = -10921.8681640625
Iteration 7400: Loss = -10921.8505859375
Iteration 7500: Loss = -10921.8349609375
Iteration 7600: Loss = -10921.8203125
Iteration 7700: Loss = -10921.8056640625
Iteration 7800: Loss = -10921.79296875
Iteration 7900: Loss = -10921.779296875
Iteration 8000: Loss = -10921.7685546875
Iteration 8100: Loss = -10921.7568359375
Iteration 8200: Loss = -10921.7470703125
Iteration 8300: Loss = -10921.736328125
Iteration 8400: Loss = -10921.7255859375
Iteration 8500: Loss = -10921.716796875
Iteration 8600: Loss = -10921.7099609375
Iteration 8700: Loss = -10921.701171875
Iteration 8800: Loss = -10921.6923828125
Iteration 8900: Loss = -10921.685546875
Iteration 9000: Loss = -10921.6796875
Iteration 9100: Loss = -10921.6728515625
Iteration 9200: Loss = -10921.666015625
Iteration 9300: Loss = -10921.6591796875
Iteration 9400: Loss = -10921.654296875
Iteration 9500: Loss = -10921.650390625
Iteration 9600: Loss = -10921.6435546875
Iteration 9700: Loss = -10921.640625
Iteration 9800: Loss = -10921.6357421875
Iteration 9900: Loss = -10921.6318359375
Iteration 10000: Loss = -10921.626953125
Iteration 10100: Loss = -10921.6240234375
Iteration 10200: Loss = -10921.6201171875
Iteration 10300: Loss = -10921.6171875
Iteration 10400: Loss = -10921.61328125
Iteration 10500: Loss = -10921.6103515625
Iteration 10600: Loss = -10921.6064453125
Iteration 10700: Loss = -10921.603515625
Iteration 10800: Loss = -10921.6005859375
Iteration 10900: Loss = -10921.5986328125
Iteration 11000: Loss = -10921.595703125
Iteration 11100: Loss = -10921.5927734375
Iteration 11200: Loss = -10921.591796875
Iteration 11300: Loss = -10921.5888671875
Iteration 11400: Loss = -10921.587890625
Iteration 11500: Loss = -10921.5849609375
Iteration 11600: Loss = -10921.5830078125
Iteration 11700: Loss = -10921.5830078125
Iteration 11800: Loss = -10921.5810546875
Iteration 11900: Loss = -10921.5791015625
Iteration 12000: Loss = -10921.5771484375
Iteration 12100: Loss = -10921.5751953125
Iteration 12200: Loss = -10921.57421875
Iteration 12300: Loss = -10921.572265625
Iteration 12400: Loss = -10921.5712890625
Iteration 12500: Loss = -10921.5703125
Iteration 12600: Loss = -10921.5693359375
Iteration 12700: Loss = -10921.568359375
Iteration 12800: Loss = -10921.568359375
Iteration 12900: Loss = -10921.56640625
Iteration 13000: Loss = -10921.5654296875
Iteration 13100: Loss = -10921.5654296875
Iteration 13200: Loss = -10921.5654296875
Iteration 13300: Loss = -10921.5625
Iteration 13400: Loss = -10921.5615234375
Iteration 13500: Loss = -10921.564453125
1
Iteration 13600: Loss = -10921.560546875
Iteration 13700: Loss = -10921.5615234375
1
Iteration 13800: Loss = -10921.5595703125
Iteration 13900: Loss = -10921.560546875
1
Iteration 14000: Loss = -10921.5595703125
Iteration 14100: Loss = -10921.5576171875
Iteration 14200: Loss = -10921.55859375
1
Iteration 14300: Loss = -10921.55859375
2
Iteration 14400: Loss = -10921.5556640625
Iteration 14500: Loss = -10921.5556640625
Iteration 14600: Loss = -10921.556640625
1
Iteration 14700: Loss = -10921.5546875
Iteration 14800: Loss = -10921.5537109375
Iteration 14900: Loss = -10921.552734375
Iteration 15000: Loss = -10921.5517578125
Iteration 15100: Loss = -10921.44921875
Iteration 15200: Loss = -10919.9306640625
Iteration 15300: Loss = -10919.6103515625
Iteration 15400: Loss = -10919.2880859375
Iteration 15500: Loss = -10919.23046875
Iteration 15600: Loss = -10918.5732421875
Iteration 15700: Loss = -10918.431640625
Iteration 15800: Loss = -10918.328125
Iteration 15900: Loss = -10918.15625
Iteration 16000: Loss = -10918.154296875
Iteration 16100: Loss = -10918.1552734375
1
Iteration 16200: Loss = -10918.1533203125
Iteration 16300: Loss = -10918.154296875
1
Iteration 16400: Loss = -10918.154296875
2
Iteration 16500: Loss = -10918.1533203125
Iteration 16600: Loss = -10918.1513671875
Iteration 16700: Loss = -10918.15234375
1
Iteration 16800: Loss = -10918.150390625
Iteration 16900: Loss = -10918.150390625
Iteration 17000: Loss = -10918.1494140625
Iteration 17100: Loss = -10918.1494140625
Iteration 17200: Loss = -10918.1494140625
Iteration 17300: Loss = -10918.1494140625
Iteration 17400: Loss = -10918.1494140625
Iteration 17500: Loss = -10918.1494140625
Iteration 17600: Loss = -10918.150390625
1
Iteration 17700: Loss = -10918.1474609375
Iteration 17800: Loss = -10918.1474609375
Iteration 17900: Loss = -10918.1484375
1
Iteration 18000: Loss = -10918.1494140625
2
Iteration 18100: Loss = -10918.1484375
3
Iteration 18200: Loss = -10918.1484375
4
Iteration 18300: Loss = -10918.1494140625
5
Iteration 18400: Loss = -10918.1474609375
Iteration 18500: Loss = -10918.1484375
1
Iteration 18600: Loss = -10918.1484375
2
Iteration 18700: Loss = -10918.1484375
3
Iteration 18800: Loss = -10918.1474609375
Iteration 18900: Loss = -10918.1474609375
Iteration 19000: Loss = -10918.1494140625
1
Iteration 19100: Loss = -10918.1474609375
Iteration 19200: Loss = -10918.1484375
1
Iteration 19300: Loss = -10918.1484375
2
Iteration 19400: Loss = -10918.1474609375
Iteration 19500: Loss = -10918.1484375
1
Iteration 19600: Loss = -10918.1474609375
Iteration 19700: Loss = -10918.1484375
1
Iteration 19800: Loss = -10918.1123046875
Iteration 19900: Loss = -10918.076171875
Iteration 20000: Loss = -10918.0751953125
Iteration 20100: Loss = -10918.0751953125
Iteration 20200: Loss = -10918.076171875
1
Iteration 20300: Loss = -10918.076171875
2
Iteration 20400: Loss = -10918.07421875
Iteration 20500: Loss = -10918.07421875
Iteration 20600: Loss = -10918.076171875
1
Iteration 20700: Loss = -10918.080078125
2
Iteration 20800: Loss = -10918.076171875
3
Iteration 20900: Loss = -10918.07421875
Iteration 21000: Loss = -10918.076171875
1
Iteration 21100: Loss = -10918.0751953125
2
Iteration 21200: Loss = -10918.07421875
Iteration 21300: Loss = -10918.0751953125
1
Iteration 21400: Loss = -10918.0673828125
Iteration 21500: Loss = -10918.06640625
Iteration 21600: Loss = -10918.0673828125
1
Iteration 21700: Loss = -10918.0517578125
Iteration 21800: Loss = -10918.0498046875
Iteration 21900: Loss = -10918.0517578125
1
Iteration 22000: Loss = -10918.05078125
2
Iteration 22100: Loss = -10918.0498046875
Iteration 22200: Loss = -10918.05078125
1
Iteration 22300: Loss = -10918.0498046875
Iteration 22400: Loss = -10918.0498046875
Iteration 22500: Loss = -10918.05078125
1
Iteration 22600: Loss = -10918.029296875
Iteration 22700: Loss = -10918.0302734375
1
Iteration 22800: Loss = -10918.0283203125
Iteration 22900: Loss = -10918.02734375
Iteration 23000: Loss = -10918.029296875
1
Iteration 23100: Loss = -10918.009765625
Iteration 23200: Loss = -10918.009765625
Iteration 23300: Loss = -10917.9990234375
Iteration 23400: Loss = -10917.9990234375
Iteration 23500: Loss = -10917.9990234375
Iteration 23600: Loss = -10917.998046875
Iteration 23700: Loss = -10917.9970703125
Iteration 23800: Loss = -10917.998046875
1
Iteration 23900: Loss = -10917.998046875
2
Iteration 24000: Loss = -10917.9970703125
Iteration 24100: Loss = -10918.0
1
Iteration 24200: Loss = -10917.98046875
Iteration 24300: Loss = -10917.9814453125
1
Iteration 24400: Loss = -10917.98046875
Iteration 24500: Loss = -10917.9814453125
1
Iteration 24600: Loss = -10917.98046875
Iteration 24700: Loss = -10917.9794921875
Iteration 24800: Loss = -10917.98046875
1
Iteration 24900: Loss = -10917.98046875
2
Iteration 25000: Loss = -10917.98046875
3
Iteration 25100: Loss = -10917.9814453125
4
Iteration 25200: Loss = -10917.9814453125
5
Iteration 25300: Loss = -10917.9814453125
6
Iteration 25400: Loss = -10917.98046875
7
Iteration 25500: Loss = -10917.9794921875
Iteration 25600: Loss = -10917.982421875
1
Iteration 25700: Loss = -10917.9794921875
Iteration 25800: Loss = -10917.9814453125
1
Iteration 25900: Loss = -10917.9794921875
Iteration 26000: Loss = -10917.9814453125
1
Iteration 26100: Loss = -10917.9794921875
Iteration 26200: Loss = -10917.9794921875
Iteration 26300: Loss = -10917.9208984375
Iteration 26400: Loss = -10917.9189453125
Iteration 26500: Loss = -10917.919921875
1
Iteration 26600: Loss = -10917.9189453125
Iteration 26700: Loss = -10917.9189453125
Iteration 26800: Loss = -10917.9189453125
Iteration 26900: Loss = -10917.91796875
Iteration 27000: Loss = -10917.9189453125
1
Iteration 27100: Loss = -10917.9189453125
2
Iteration 27200: Loss = -10917.919921875
3
Iteration 27300: Loss = -10917.9228515625
4
Iteration 27400: Loss = -10917.9189453125
5
Iteration 27500: Loss = -10917.9189453125
6
Iteration 27600: Loss = -10917.916015625
Iteration 27700: Loss = -10917.916015625
Iteration 27800: Loss = -10917.9150390625
Iteration 27900: Loss = -10917.9169921875
1
Iteration 28000: Loss = -10917.916015625
2
Iteration 28100: Loss = -10917.916015625
3
Iteration 28200: Loss = -10917.916015625
4
Iteration 28300: Loss = -10917.916015625
5
Iteration 28400: Loss = -10917.916015625
6
Iteration 28500: Loss = -10917.916015625
7
Iteration 28600: Loss = -10917.9169921875
8
Iteration 28700: Loss = -10917.9150390625
Iteration 28800: Loss = -10917.916015625
1
Iteration 28900: Loss = -10917.916015625
2
Iteration 29000: Loss = -10917.916015625
3
Iteration 29100: Loss = -10917.9150390625
Iteration 29200: Loss = -10917.916015625
1
Iteration 29300: Loss = -10917.9150390625
Iteration 29400: Loss = -10917.9150390625
Iteration 29500: Loss = -10917.9150390625
Iteration 29600: Loss = -10917.916015625
1
Iteration 29700: Loss = -10917.9150390625
Iteration 29800: Loss = -10917.9150390625
Iteration 29900: Loss = -10917.9150390625
pi: tensor([[8.2985e-02, 9.1702e-01],
        [4.7401e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9601, 0.0399], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1605, 0.2070],
         [0.0313, 0.1624]],

        [[0.9910, 0.1163],
         [0.0493, 0.1633]],

        [[0.9908, 0.0618],
         [0.3726, 0.1279]],

        [[0.9609, 0.1650],
         [0.2240, 0.0128]],

        [[0.0478, 0.2732],
         [0.0852, 0.4667]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016505798130715438
Average Adjusted Rand Index: -0.00023365334617957725
[0.0, -0.0016505798130715438] [0.0, -0.00023365334617957725] [10919.162109375, 10917.9150390625]
-------------------------------------
This iteration is 65
True Objective function: Loss = -10830.940100058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29638.79296875
Iteration 100: Loss = -17501.263671875
Iteration 200: Loss = -12398.3955078125
Iteration 300: Loss = -11585.4765625
Iteration 400: Loss = -11369.390625
Iteration 500: Loss = -11273.0009765625
Iteration 600: Loss = -11224.412109375
Iteration 700: Loss = -11188.599609375
Iteration 800: Loss = -11151.70703125
Iteration 900: Loss = -11120.982421875
Iteration 1000: Loss = -11100.54296875
Iteration 1100: Loss = -11084.2099609375
Iteration 1200: Loss = -11071.4365234375
Iteration 1300: Loss = -11058.9267578125
Iteration 1400: Loss = -11049.990234375
Iteration 1500: Loss = -11041.1123046875
Iteration 1600: Loss = -11034.8017578125
Iteration 1700: Loss = -11026.396484375
Iteration 1800: Loss = -11021.998046875
Iteration 1900: Loss = -11017.521484375
Iteration 2000: Loss = -11012.7802734375
Iteration 2100: Loss = -11008.1865234375
Iteration 2200: Loss = -11004.4052734375
Iteration 2300: Loss = -11000.671875
Iteration 2400: Loss = -10997.9609375
Iteration 2500: Loss = -10996.072265625
Iteration 2600: Loss = -10994.6201171875
Iteration 2700: Loss = -10993.4814453125
Iteration 2800: Loss = -10992.546875
Iteration 2900: Loss = -10991.7548828125
Iteration 3000: Loss = -10991.0830078125
Iteration 3100: Loss = -10990.5166015625
Iteration 3200: Loss = -10990.0078125
Iteration 3300: Loss = -10989.263671875
Iteration 3400: Loss = -10985.037109375
Iteration 3500: Loss = -10982.513671875
Iteration 3600: Loss = -10981.568359375
Iteration 3700: Loss = -10980.9345703125
Iteration 3800: Loss = -10980.41796875
Iteration 3900: Loss = -10979.962890625
Iteration 4000: Loss = -10979.5322265625
Iteration 4100: Loss = -10979.09375
Iteration 4200: Loss = -10978.6025390625
Iteration 4300: Loss = -10977.9453125
Iteration 4400: Loss = -10976.9375
Iteration 4500: Loss = -10975.9697265625
Iteration 4600: Loss = -10975.1025390625
Iteration 4700: Loss = -10973.4912109375
Iteration 4800: Loss = -10971.806640625
Iteration 4900: Loss = -10971.2734375
Iteration 5000: Loss = -10970.9169921875
Iteration 5100: Loss = -10970.6796875
Iteration 5200: Loss = -10970.4560546875
Iteration 5300: Loss = -10970.1552734375
Iteration 5400: Loss = -10969.8310546875
Iteration 5500: Loss = -10969.615234375
Iteration 5600: Loss = -10969.4736328125
Iteration 5700: Loss = -10969.37109375
Iteration 5800: Loss = -10969.2841796875
Iteration 5900: Loss = -10969.2001953125
Iteration 6000: Loss = -10969.08984375
Iteration 6100: Loss = -10968.7490234375
Iteration 6200: Loss = -10967.8505859375
Iteration 6300: Loss = -10967.6669921875
Iteration 6400: Loss = -10967.55859375
Iteration 6500: Loss = -10965.966796875
Iteration 6600: Loss = -10964.21484375
Iteration 6700: Loss = -10963.9912109375
Iteration 6800: Loss = -10963.5322265625
Iteration 6900: Loss = -10963.125
Iteration 7000: Loss = -10962.9931640625
Iteration 7100: Loss = -10962.93359375
Iteration 7200: Loss = -10962.892578125
Iteration 7300: Loss = -10962.859375
Iteration 7400: Loss = -10962.828125
Iteration 7500: Loss = -10962.8017578125
Iteration 7600: Loss = -10962.7783203125
Iteration 7700: Loss = -10962.755859375
Iteration 7800: Loss = -10962.7353515625
Iteration 7900: Loss = -10962.71875
Iteration 8000: Loss = -10962.701171875
Iteration 8100: Loss = -10962.685546875
Iteration 8200: Loss = -10962.669921875
Iteration 8300: Loss = -10962.658203125
Iteration 8400: Loss = -10962.6435546875
Iteration 8500: Loss = -10962.630859375
Iteration 8600: Loss = -10962.619140625
Iteration 8700: Loss = -10962.607421875
Iteration 8800: Loss = -10962.59765625
Iteration 8900: Loss = -10962.5869140625
Iteration 9000: Loss = -10962.5791015625
Iteration 9100: Loss = -10962.568359375
Iteration 9200: Loss = -10962.560546875
Iteration 9300: Loss = -10962.552734375
Iteration 9400: Loss = -10962.544921875
Iteration 9500: Loss = -10962.5390625
Iteration 9600: Loss = -10962.53125
Iteration 9700: Loss = -10962.5234375
Iteration 9800: Loss = -10962.5185546875
Iteration 9900: Loss = -10962.5126953125
Iteration 10000: Loss = -10962.5078125
Iteration 10100: Loss = -10962.5029296875
Iteration 10200: Loss = -10962.498046875
Iteration 10300: Loss = -10962.4931640625
Iteration 10400: Loss = -10962.48828125
Iteration 10500: Loss = -10962.482421875
Iteration 10600: Loss = -10962.4794921875
Iteration 10700: Loss = -10962.4765625
Iteration 10800: Loss = -10962.47265625
Iteration 10900: Loss = -10962.4697265625
Iteration 11000: Loss = -10962.4658203125
Iteration 11100: Loss = -10962.462890625
Iteration 11200: Loss = -10962.4599609375
Iteration 11300: Loss = -10962.45703125
Iteration 11400: Loss = -10962.4541015625
Iteration 11500: Loss = -10962.451171875
Iteration 11600: Loss = -10962.44921875
Iteration 11700: Loss = -10962.447265625
Iteration 11800: Loss = -10962.4453125
Iteration 11900: Loss = -10962.4443359375
Iteration 12000: Loss = -10962.44140625
Iteration 12100: Loss = -10962.4384765625
Iteration 12200: Loss = -10962.4365234375
Iteration 12300: Loss = -10962.435546875
Iteration 12400: Loss = -10962.43359375
Iteration 12500: Loss = -10962.43359375
Iteration 12600: Loss = -10962.4296875
Iteration 12700: Loss = -10962.4287109375
Iteration 12800: Loss = -10962.4287109375
Iteration 12900: Loss = -10962.4267578125
Iteration 13000: Loss = -10962.4248046875
Iteration 13100: Loss = -10962.4228515625
Iteration 13200: Loss = -10962.4228515625
Iteration 13300: Loss = -10962.421875
Iteration 13400: Loss = -10962.419921875
Iteration 13500: Loss = -10962.4208984375
1
Iteration 13600: Loss = -10962.4189453125
Iteration 13700: Loss = -10962.41796875
Iteration 13800: Loss = -10962.4169921875
Iteration 13900: Loss = -10962.4169921875
Iteration 14000: Loss = -10962.416015625
Iteration 14100: Loss = -10962.4150390625
Iteration 14200: Loss = -10962.4140625
Iteration 14300: Loss = -10962.4150390625
1
Iteration 14400: Loss = -10962.4130859375
Iteration 14500: Loss = -10962.4130859375
Iteration 14600: Loss = -10962.4130859375
Iteration 14700: Loss = -10962.412109375
Iteration 14800: Loss = -10962.41015625
Iteration 14900: Loss = -10962.41015625
Iteration 15000: Loss = -10962.4091796875
Iteration 15100: Loss = -10962.408203125
Iteration 15200: Loss = -10962.40234375
Iteration 15300: Loss = -10962.4013671875
Iteration 15400: Loss = -10962.4013671875
Iteration 15500: Loss = -10962.3994140625
Iteration 15600: Loss = -10962.3994140625
Iteration 15700: Loss = -10962.3984375
Iteration 15800: Loss = -10962.3994140625
1
Iteration 15900: Loss = -10962.400390625
2
Iteration 16000: Loss = -10962.3984375
Iteration 16100: Loss = -10962.3984375
Iteration 16200: Loss = -10962.3994140625
1
Iteration 16300: Loss = -10962.3984375
Iteration 16400: Loss = -10962.3984375
Iteration 16500: Loss = -10962.3974609375
Iteration 16600: Loss = -10962.3974609375
Iteration 16700: Loss = -10962.396484375
Iteration 16800: Loss = -10962.3984375
1
Iteration 16900: Loss = -10962.396484375
Iteration 17000: Loss = -10962.3974609375
1
Iteration 17100: Loss = -10962.3974609375
2
Iteration 17200: Loss = -10962.396484375
Iteration 17300: Loss = -10962.3974609375
1
Iteration 17400: Loss = -10962.396484375
Iteration 17500: Loss = -10962.3955078125
Iteration 17600: Loss = -10962.39453125
Iteration 17700: Loss = -10962.396484375
1
Iteration 17800: Loss = -10962.39453125
Iteration 17900: Loss = -10962.39453125
Iteration 18000: Loss = -10962.396484375
1
Iteration 18100: Loss = -10962.39453125
Iteration 18200: Loss = -10962.3955078125
1
Iteration 18300: Loss = -10962.3955078125
2
Iteration 18400: Loss = -10962.3955078125
3
Iteration 18500: Loss = -10962.3955078125
4
Iteration 18600: Loss = -10962.396484375
5
Iteration 18700: Loss = -10962.3955078125
6
Iteration 18800: Loss = -10962.3935546875
Iteration 18900: Loss = -10962.3955078125
1
Iteration 19000: Loss = -10962.39453125
2
Iteration 19100: Loss = -10962.39453125
3
Iteration 19200: Loss = -10962.39453125
4
Iteration 19300: Loss = -10962.39453125
5
Iteration 19400: Loss = -10962.39453125
6
Iteration 19500: Loss = -10962.39453125
7
Iteration 19600: Loss = -10962.39453125
8
Iteration 19700: Loss = -10962.39453125
9
Iteration 19800: Loss = -10962.3935546875
Iteration 19900: Loss = -10962.3935546875
Iteration 20000: Loss = -10962.3935546875
Iteration 20100: Loss = -10962.3935546875
Iteration 20200: Loss = -10962.39453125
1
Iteration 20300: Loss = -10962.3935546875
Iteration 20400: Loss = -10962.3955078125
1
Iteration 20500: Loss = -10962.39453125
2
Iteration 20600: Loss = -10962.3935546875
Iteration 20700: Loss = -10962.39453125
1
Iteration 20800: Loss = -10962.39453125
2
Iteration 20900: Loss = -10962.39453125
3
Iteration 21000: Loss = -10962.3935546875
Iteration 21100: Loss = -10962.39453125
1
Iteration 21200: Loss = -10962.39453125
2
Iteration 21300: Loss = -10962.39453125
3
Iteration 21400: Loss = -10962.39453125
4
Iteration 21500: Loss = -10962.3935546875
Iteration 21600: Loss = -10962.39453125
1
Iteration 21700: Loss = -10962.39453125
2
Iteration 21800: Loss = -10962.39453125
3
Iteration 21900: Loss = -10962.3935546875
Iteration 22000: Loss = -10962.392578125
Iteration 22100: Loss = -10962.39453125
1
Iteration 22200: Loss = -10962.3935546875
2
Iteration 22300: Loss = -10962.3935546875
3
Iteration 22400: Loss = -10962.39453125
4
Iteration 22500: Loss = -10962.39453125
5
Iteration 22600: Loss = -10962.39453125
6
Iteration 22700: Loss = -10962.3935546875
7
Iteration 22800: Loss = -10962.3935546875
8
Iteration 22900: Loss = -10962.392578125
Iteration 23000: Loss = -10962.3935546875
1
Iteration 23100: Loss = -10962.3935546875
2
Iteration 23200: Loss = -10962.3955078125
3
Iteration 23300: Loss = -10962.3935546875
4
Iteration 23400: Loss = -10962.39453125
5
Iteration 23500: Loss = -10962.3935546875
6
Iteration 23600: Loss = -10962.3935546875
7
Iteration 23700: Loss = -10962.39453125
8
Iteration 23800: Loss = -10962.392578125
Iteration 23900: Loss = -10962.3935546875
1
Iteration 24000: Loss = -10962.3935546875
2
Iteration 24100: Loss = -10962.392578125
Iteration 24200: Loss = -10962.392578125
Iteration 24300: Loss = -10962.392578125
Iteration 24400: Loss = -10962.39453125
1
Iteration 24500: Loss = -10962.392578125
Iteration 24600: Loss = -10962.392578125
Iteration 24700: Loss = -10962.39453125
1
Iteration 24800: Loss = -10962.392578125
Iteration 24900: Loss = -10962.3935546875
1
Iteration 25000: Loss = -10962.39453125
2
Iteration 25100: Loss = -10962.39453125
3
Iteration 25200: Loss = -10962.39453125
4
Iteration 25300: Loss = -10962.39453125
5
Iteration 25400: Loss = -10962.3935546875
6
Iteration 25500: Loss = -10962.3935546875
7
Iteration 25600: Loss = -10962.3955078125
8
Iteration 25700: Loss = -10962.3935546875
9
Iteration 25800: Loss = -10962.3935546875
10
Iteration 25900: Loss = -10962.39453125
11
Iteration 26000: Loss = -10962.3935546875
12
Iteration 26100: Loss = -10962.3935546875
13
Iteration 26200: Loss = -10962.3935546875
14
Iteration 26300: Loss = -10962.392578125
Iteration 26400: Loss = -10962.39453125
1
Iteration 26500: Loss = -10962.39453125
2
Iteration 26600: Loss = -10962.392578125
Iteration 26700: Loss = -10962.3935546875
1
Iteration 26800: Loss = -10962.39453125
2
Iteration 26900: Loss = -10962.39453125
3
Iteration 27000: Loss = -10962.3935546875
4
Iteration 27100: Loss = -10962.39453125
5
Iteration 27200: Loss = -10962.3935546875
6
Iteration 27300: Loss = -10962.396484375
7
Iteration 27400: Loss = -10962.3935546875
8
Iteration 27500: Loss = -10962.3955078125
9
Iteration 27600: Loss = -10962.392578125
Iteration 27700: Loss = -10962.3935546875
1
Iteration 27800: Loss = -10962.39453125
2
Iteration 27900: Loss = -10962.3935546875
3
Iteration 28000: Loss = -10962.3935546875
4
Iteration 28100: Loss = -10962.3935546875
5
Iteration 28200: Loss = -10962.3935546875
6
Iteration 28300: Loss = -10962.392578125
Iteration 28400: Loss = -10962.3916015625
Iteration 28500: Loss = -10962.392578125
1
Iteration 28600: Loss = -10962.3935546875
2
Iteration 28700: Loss = -10962.39453125
3
Iteration 28800: Loss = -10962.39453125
4
Iteration 28900: Loss = -10962.3935546875
5
Iteration 29000: Loss = -10962.3935546875
6
Iteration 29100: Loss = -10962.3955078125
7
Iteration 29200: Loss = -10962.39453125
8
Iteration 29300: Loss = -10962.3935546875
9
Iteration 29400: Loss = -10962.3955078125
10
Iteration 29500: Loss = -10962.3935546875
11
Iteration 29600: Loss = -10962.39453125
12
Iteration 29700: Loss = -10962.3955078125
13
Iteration 29800: Loss = -10962.39453125
14
Iteration 29900: Loss = -10962.3955078125
15
Stopping early at iteration 29900 due to no improvement.
pi: tensor([[8.4027e-06, 9.9999e-01],
        [3.4836e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5862, 0.4138], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2695, 0.0988],
         [0.0866, 0.1619]],

        [[0.0320, 0.1443],
         [0.0082, 0.9735]],

        [[0.9869, 0.2089],
         [0.0156, 0.1386]],

        [[0.0083, 0.2112],
         [0.0312, 0.0302]],

        [[0.3127, 0.1951],
         [0.9889, 0.0069]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0007797093862132294
Average Adjusted Rand Index: 0.1764764615189401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41760.2265625
Iteration 100: Loss = -24428.20703125
Iteration 200: Loss = -14865.9970703125
Iteration 300: Loss = -12473.3671875
Iteration 400: Loss = -11742.2978515625
Iteration 500: Loss = -11487.935546875
Iteration 600: Loss = -11359.630859375
Iteration 700: Loss = -11239.7373046875
Iteration 800: Loss = -11188.6552734375
Iteration 900: Loss = -11156.2158203125
Iteration 1000: Loss = -11125.916015625
Iteration 1100: Loss = -11111.015625
Iteration 1200: Loss = -11099.4833984375
Iteration 1300: Loss = -11090.072265625
Iteration 1400: Loss = -11081.6572265625
Iteration 1500: Loss = -11059.931640625
Iteration 1600: Loss = -11053.5185546875
Iteration 1700: Loss = -11049.1279296875
Iteration 1800: Loss = -11045.56640625
Iteration 1900: Loss = -11039.212890625
Iteration 2000: Loss = -11036.8095703125
Iteration 2100: Loss = -11034.8232421875
Iteration 2200: Loss = -11033.1767578125
Iteration 2300: Loss = -11031.73828125
Iteration 2400: Loss = -11030.46875
Iteration 2500: Loss = -11029.3291015625
Iteration 2600: Loss = -11028.3115234375
Iteration 2700: Loss = -11027.40625
Iteration 2800: Loss = -11026.5966796875
Iteration 2900: Loss = -11025.859375
Iteration 3000: Loss = -11025.1953125
Iteration 3100: Loss = -11024.5859375
Iteration 3200: Loss = -11024.0322265625
Iteration 3300: Loss = -11023.529296875
Iteration 3400: Loss = -11023.0625
Iteration 3500: Loss = -11022.63671875
Iteration 3600: Loss = -11022.244140625
Iteration 3700: Loss = -11021.880859375
Iteration 3800: Loss = -11021.546875
Iteration 3900: Loss = -11021.2333984375
Iteration 4000: Loss = -11020.9462890625
Iteration 4100: Loss = -11020.677734375
Iteration 4200: Loss = -11020.42578125
Iteration 4300: Loss = -11020.1962890625
Iteration 4400: Loss = -11019.9765625
Iteration 4500: Loss = -11019.7734375
Iteration 4600: Loss = -11019.5830078125
Iteration 4700: Loss = -11019.4052734375
Iteration 4800: Loss = -11019.2353515625
Iteration 4900: Loss = -11019.0791015625
Iteration 5000: Loss = -11018.931640625
Iteration 5100: Loss = -11018.7900390625
Iteration 5200: Loss = -11018.658203125
Iteration 5300: Loss = -11018.53125
Iteration 5400: Loss = -11018.4111328125
Iteration 5500: Loss = -11018.2958984375
Iteration 5600: Loss = -11018.1884765625
Iteration 5700: Loss = -11018.0849609375
Iteration 5800: Loss = -11017.9833984375
Iteration 5900: Loss = -11017.8876953125
Iteration 6000: Loss = -11017.794921875
Iteration 6100: Loss = -11017.7080078125
Iteration 6200: Loss = -11017.62109375
Iteration 6300: Loss = -11017.5400390625
Iteration 6400: Loss = -11017.4599609375
Iteration 6500: Loss = -11017.3828125
Iteration 6600: Loss = -11017.3076171875
Iteration 6700: Loss = -11017.232421875
Iteration 6800: Loss = -11017.16015625
Iteration 6900: Loss = -11017.091796875
Iteration 7000: Loss = -11017.01953125
Iteration 7100: Loss = -11016.9541015625
Iteration 7200: Loss = -11016.8876953125
Iteration 7300: Loss = -11016.8212890625
Iteration 7400: Loss = -11016.7578125
Iteration 7500: Loss = -11016.6962890625
Iteration 7600: Loss = -11016.6357421875
Iteration 7700: Loss = -11016.5732421875
Iteration 7800: Loss = -11016.513671875
Iteration 7900: Loss = -11016.4541015625
Iteration 8000: Loss = -11016.3935546875
Iteration 8100: Loss = -11016.111328125
Iteration 8200: Loss = -11011.4873046875
Iteration 8300: Loss = -11011.39453125
Iteration 8400: Loss = -11011.34765625
Iteration 8500: Loss = -11011.3134765625
Iteration 8600: Loss = -11011.2822265625
Iteration 8700: Loss = -11011.2548828125
Iteration 8800: Loss = -11011.224609375
Iteration 8900: Loss = -11011.1953125
Iteration 9000: Loss = -11007.9716796875
Iteration 9100: Loss = -11005.8857421875
Iteration 9200: Loss = -11005.70703125
Iteration 9300: Loss = -11005.37109375
Iteration 9400: Loss = -11005.1171875
Iteration 9500: Loss = -11004.40625
Iteration 9600: Loss = -11003.6298828125
Iteration 9700: Loss = -11003.390625
Iteration 9800: Loss = -11003.109375
Iteration 9900: Loss = -11002.1123046875
Iteration 10000: Loss = -11001.27734375
Iteration 10100: Loss = -11001.146484375
Iteration 10200: Loss = -10999.13671875
Iteration 10300: Loss = -10998.8173828125
Iteration 10400: Loss = -10998.326171875
Iteration 10500: Loss = -10997.5517578125
Iteration 10600: Loss = -10995.3720703125
Iteration 10700: Loss = -10994.7734375
Iteration 10800: Loss = -10994.705078125
Iteration 10900: Loss = -10994.462890625
Iteration 11000: Loss = -10994.2958984375
Iteration 11100: Loss = -10993.994140625
Iteration 11200: Loss = -10993.6201171875
Iteration 11300: Loss = -10993.5087890625
Iteration 11400: Loss = -10993.4482421875
Iteration 11500: Loss = -10993.431640625
Iteration 11600: Loss = -10993.40625
Iteration 11700: Loss = -10993.296875
Iteration 11800: Loss = -10993.115234375
Iteration 11900: Loss = -10992.8828125
Iteration 12000: Loss = -10988.87890625
Iteration 12100: Loss = -10988.8154296875
Iteration 12200: Loss = -10988.7646484375
Iteration 12300: Loss = -10988.69140625
Iteration 12400: Loss = -10988.6005859375
Iteration 12500: Loss = -10988.3525390625
Iteration 12600: Loss = -10979.16015625
Iteration 12700: Loss = -10978.4169921875
Iteration 12800: Loss = -10977.6298828125
Iteration 12900: Loss = -10977.4267578125
Iteration 13000: Loss = -10977.404296875
Iteration 13100: Loss = -10977.3916015625
Iteration 13200: Loss = -10977.3759765625
Iteration 13300: Loss = -10977.3330078125
Iteration 13400: Loss = -10977.32421875
Iteration 13500: Loss = -10977.3212890625
Iteration 13600: Loss = -10977.3173828125
Iteration 13700: Loss = -10977.3125
Iteration 13800: Loss = -10977.3095703125
Iteration 13900: Loss = -10977.3056640625
Iteration 14000: Loss = -10977.30078125
Iteration 14100: Loss = -10977.296875
Iteration 14200: Loss = -10977.255859375
Iteration 14300: Loss = -10974.8994140625
Iteration 14400: Loss = -10970.40625
Iteration 14500: Loss = -10970.119140625
Iteration 14600: Loss = -10970.099609375
Iteration 14700: Loss = -10970.091796875
Iteration 14800: Loss = -10970.0849609375
Iteration 14900: Loss = -10968.4501953125
Iteration 15000: Loss = -10966.435546875
Iteration 15100: Loss = -10966.4130859375
Iteration 15200: Loss = -10966.404296875
Iteration 15300: Loss = -10966.3994140625
Iteration 15400: Loss = -10966.396484375
Iteration 15500: Loss = -10966.39453125
Iteration 15600: Loss = -10966.3818359375
Iteration 15700: Loss = -10965.435546875
Iteration 15800: Loss = -10965.427734375
Iteration 15900: Loss = -10965.4248046875
Iteration 16000: Loss = -10965.4228515625
Iteration 16100: Loss = -10965.4228515625
Iteration 16200: Loss = -10965.4208984375
Iteration 16300: Loss = -10965.421875
1
Iteration 16400: Loss = -10965.4208984375
Iteration 16500: Loss = -10965.4189453125
Iteration 16600: Loss = -10965.396484375
Iteration 16700: Loss = -10965.396484375
Iteration 16800: Loss = -10965.3955078125
Iteration 16900: Loss = -10965.3955078125
Iteration 17000: Loss = -10965.3955078125
Iteration 17100: Loss = -10965.3955078125
Iteration 17200: Loss = -10965.3955078125
Iteration 17300: Loss = -10965.3935546875
Iteration 17400: Loss = -10965.39453125
1
Iteration 17500: Loss = -10965.39453125
2
Iteration 17600: Loss = -10965.3935546875
Iteration 17700: Loss = -10965.392578125
Iteration 17800: Loss = -10965.3935546875
1
Iteration 17900: Loss = -10965.3935546875
2
Iteration 18000: Loss = -10965.3916015625
Iteration 18100: Loss = -10965.3916015625
Iteration 18200: Loss = -10965.3916015625
Iteration 18300: Loss = -10965.3916015625
Iteration 18400: Loss = -10965.3916015625
Iteration 18500: Loss = -10965.3916015625
Iteration 18600: Loss = -10965.392578125
1
Iteration 18700: Loss = -10965.3916015625
Iteration 18800: Loss = -10965.3916015625
Iteration 18900: Loss = -10965.3916015625
Iteration 19000: Loss = -10965.392578125
1
Iteration 19100: Loss = -10965.3916015625
Iteration 19200: Loss = -10965.3935546875
1
Iteration 19300: Loss = -10965.3916015625
Iteration 19400: Loss = -10965.3896484375
Iteration 19500: Loss = -10965.3896484375
Iteration 19600: Loss = -10965.390625
1
Iteration 19700: Loss = -10965.3896484375
Iteration 19800: Loss = -10965.3896484375
Iteration 19900: Loss = -10965.390625
1
Iteration 20000: Loss = -10965.388671875
Iteration 20100: Loss = -10965.3896484375
1
Iteration 20200: Loss = -10965.3896484375
2
Iteration 20300: Loss = -10965.3896484375
3
Iteration 20400: Loss = -10965.390625
4
Iteration 20500: Loss = -10965.3896484375
5
Iteration 20600: Loss = -10965.3896484375
6
Iteration 20700: Loss = -10965.3896484375
7
Iteration 20800: Loss = -10965.3896484375
8
Iteration 20900: Loss = -10965.3896484375
9
Iteration 21000: Loss = -10965.388671875
Iteration 21100: Loss = -10965.3896484375
1
Iteration 21200: Loss = -10965.3896484375
2
Iteration 21300: Loss = -10965.388671875
Iteration 21400: Loss = -10965.3916015625
1
Iteration 21500: Loss = -10965.390625
2
Iteration 21600: Loss = -10965.3896484375
3
Iteration 21700: Loss = -10965.3896484375
4
Iteration 21800: Loss = -10965.3896484375
5
Iteration 21900: Loss = -10965.390625
6
Iteration 22000: Loss = -10965.3896484375
7
Iteration 22100: Loss = -10965.3896484375
8
Iteration 22200: Loss = -10965.388671875
Iteration 22300: Loss = -10965.388671875
Iteration 22400: Loss = -10965.3896484375
1
Iteration 22500: Loss = -10965.3896484375
2
Iteration 22600: Loss = -10965.3896484375
3
Iteration 22700: Loss = -10965.388671875
Iteration 22800: Loss = -10965.388671875
Iteration 22900: Loss = -10965.388671875
Iteration 23000: Loss = -10965.3896484375
1
Iteration 23100: Loss = -10965.388671875
Iteration 23200: Loss = -10965.388671875
Iteration 23300: Loss = -10965.3896484375
1
Iteration 23400: Loss = -10965.3896484375
2
Iteration 23500: Loss = -10965.3896484375
3
Iteration 23600: Loss = -10965.388671875
Iteration 23700: Loss = -10965.390625
1
Iteration 23800: Loss = -10965.390625
2
Iteration 23900: Loss = -10965.388671875
Iteration 24000: Loss = -10965.3896484375
1
Iteration 24100: Loss = -10965.3896484375
2
Iteration 24200: Loss = -10965.3896484375
3
Iteration 24300: Loss = -10965.388671875
Iteration 24400: Loss = -10965.390625
1
Iteration 24500: Loss = -10965.3896484375
2
Iteration 24600: Loss = -10965.3896484375
3
Iteration 24700: Loss = -10965.388671875
Iteration 24800: Loss = -10965.390625
1
Iteration 24900: Loss = -10965.388671875
Iteration 25000: Loss = -10965.390625
1
Iteration 25100: Loss = -10965.390625
2
Iteration 25200: Loss = -10965.3896484375
3
Iteration 25300: Loss = -10965.3896484375
4
Iteration 25400: Loss = -10965.3916015625
5
Iteration 25500: Loss = -10965.3896484375
6
Iteration 25600: Loss = -10965.3896484375
7
Iteration 25700: Loss = -10965.3896484375
8
Iteration 25800: Loss = -10965.390625
9
Iteration 25900: Loss = -10965.388671875
Iteration 26000: Loss = -10965.3896484375
1
Iteration 26100: Loss = -10965.392578125
2
Iteration 26200: Loss = -10965.3896484375
3
Iteration 26300: Loss = -10965.3935546875
4
Iteration 26400: Loss = -10965.3896484375
5
Iteration 26500: Loss = -10965.3935546875
6
Iteration 26600: Loss = -10965.3896484375
7
Iteration 26700: Loss = -10965.3896484375
8
Iteration 26800: Loss = -10965.3896484375
9
Iteration 26900: Loss = -10965.390625
10
Iteration 27000: Loss = -10965.390625
11
Iteration 27100: Loss = -10965.3916015625
12
Iteration 27200: Loss = -10965.390625
13
Iteration 27300: Loss = -10965.388671875
Iteration 27400: Loss = -10965.3916015625
1
Iteration 27500: Loss = -10965.388671875
Iteration 27600: Loss = -10965.3896484375
1
Iteration 27700: Loss = -10965.38671875
Iteration 27800: Loss = -10965.38671875
Iteration 27900: Loss = -10965.38671875
Iteration 28000: Loss = -10965.38671875
Iteration 28100: Loss = -10965.3896484375
1
Iteration 28200: Loss = -10965.3876953125
2
Iteration 28300: Loss = -10965.3876953125
3
Iteration 28400: Loss = -10965.3876953125
4
Iteration 28500: Loss = -10965.38671875
Iteration 28600: Loss = -10965.3876953125
1
Iteration 28700: Loss = -10965.38671875
Iteration 28800: Loss = -10965.38671875
Iteration 28900: Loss = -10965.38671875
Iteration 29000: Loss = -10965.3876953125
1
Iteration 29100: Loss = -10965.3876953125
2
Iteration 29200: Loss = -10965.388671875
3
Iteration 29300: Loss = -10965.38671875
Iteration 29400: Loss = -10965.3896484375
1
Iteration 29500: Loss = -10965.38671875
Iteration 29600: Loss = -10965.38671875
Iteration 29700: Loss = -10965.0693359375
Iteration 29800: Loss = -10964.169921875
Iteration 29900: Loss = -10963.5224609375
pi: tensor([[5.4997e-06, 9.9999e-01],
        [5.4794e-02, 9.4521e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5684, 0.4316], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2737, 0.1017],
         [0.0491, 0.1606]],

        [[0.9919, 0.1139],
         [0.0097, 0.0129]],

        [[0.4926, 0.2124],
         [0.8752, 0.0089]],

        [[0.7684, 0.1235],
         [0.9548, 0.8500]],

        [[0.0855, 0.1982],
         [0.1094, 0.4582]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0023054636989580843
Average Adjusted Rand Index: 0.19199877740731
[0.0007797093862132294, 0.0023054636989580843] [0.1764764615189401, 0.19199877740731] [10962.3955078125, 10963.46484375]
-------------------------------------
This iteration is 66
True Objective function: Loss = -10875.874711418353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21339.283203125
Iteration 100: Loss = -16849.00390625
Iteration 200: Loss = -12446.806640625
Iteration 300: Loss = -11295.974609375
Iteration 400: Loss = -11123.8935546875
Iteration 500: Loss = -11056.7314453125
Iteration 600: Loss = -11021.7109375
Iteration 700: Loss = -11000.5810546875
Iteration 800: Loss = -10986.611328125
Iteration 900: Loss = -10976.7685546875
Iteration 1000: Loss = -10969.5107421875
Iteration 1100: Loss = -10963.97265625
Iteration 1200: Loss = -10959.63671875
Iteration 1300: Loss = -10956.1728515625
Iteration 1400: Loss = -10953.3583984375
Iteration 1500: Loss = -10951.0390625
Iteration 1600: Loss = -10949.1015625
Iteration 1700: Loss = -10947.466796875
Iteration 1800: Loss = -10946.072265625
Iteration 1900: Loss = -10944.8740234375
Iteration 2000: Loss = -10943.8349609375
Iteration 2100: Loss = -10942.9287109375
Iteration 2200: Loss = -10942.130859375
Iteration 2300: Loss = -10941.4267578125
Iteration 2400: Loss = -10940.8046875
Iteration 2500: Loss = -10940.244140625
Iteration 2600: Loss = -10939.7451171875
Iteration 2700: Loss = -10939.296875
Iteration 2800: Loss = -10938.890625
Iteration 2900: Loss = -10938.5234375
Iteration 3000: Loss = -10938.1875
Iteration 3100: Loss = -10937.8828125
Iteration 3200: Loss = -10937.6044921875
Iteration 3300: Loss = -10937.345703125
Iteration 3400: Loss = -10937.109375
Iteration 3500: Loss = -10936.888671875
Iteration 3600: Loss = -10936.68359375
Iteration 3700: Loss = -10936.4912109375
Iteration 3800: Loss = -10936.3076171875
Iteration 3900: Loss = -10936.1318359375
Iteration 4000: Loss = -10935.9658203125
Iteration 4100: Loss = -10935.8115234375
Iteration 4200: Loss = -10935.6650390625
Iteration 4300: Loss = -10935.5341796875
Iteration 4400: Loss = -10935.4091796875
Iteration 4500: Loss = -10935.294921875
Iteration 4600: Loss = -10935.1904296875
Iteration 4700: Loss = -10935.091796875
Iteration 4800: Loss = -10935.0
Iteration 4900: Loss = -10934.9150390625
Iteration 5000: Loss = -10934.8349609375
Iteration 5100: Loss = -10934.759765625
Iteration 5200: Loss = -10934.6904296875
Iteration 5300: Loss = -10934.626953125
Iteration 5400: Loss = -10934.5634765625
Iteration 5500: Loss = -10934.5087890625
Iteration 5600: Loss = -10934.455078125
Iteration 5700: Loss = -10934.404296875
Iteration 5800: Loss = -10934.357421875
Iteration 5900: Loss = -10934.3134765625
Iteration 6000: Loss = -10934.271484375
Iteration 6100: Loss = -10934.2333984375
Iteration 6200: Loss = -10934.1943359375
Iteration 6300: Loss = -10934.1611328125
Iteration 6400: Loss = -10934.1279296875
Iteration 6500: Loss = -10934.0966796875
Iteration 6600: Loss = -10934.068359375
Iteration 6700: Loss = -10934.0400390625
Iteration 6800: Loss = -10934.013671875
Iteration 6900: Loss = -10933.990234375
Iteration 7000: Loss = -10933.966796875
Iteration 7100: Loss = -10933.9462890625
Iteration 7200: Loss = -10933.9248046875
Iteration 7300: Loss = -10933.904296875
Iteration 7400: Loss = -10933.8857421875
Iteration 7500: Loss = -10933.869140625
Iteration 7600: Loss = -10933.853515625
Iteration 7700: Loss = -10933.837890625
Iteration 7800: Loss = -10933.8232421875
Iteration 7900: Loss = -10933.8076171875
Iteration 8000: Loss = -10933.794921875
Iteration 8100: Loss = -10933.783203125
Iteration 8200: Loss = -10933.76953125
Iteration 8300: Loss = -10933.7578125
Iteration 8400: Loss = -10933.74609375
Iteration 8500: Loss = -10933.736328125
Iteration 8600: Loss = -10933.7275390625
Iteration 8700: Loss = -10933.716796875
Iteration 8800: Loss = -10933.712890625
Iteration 8900: Loss = -10933.7001953125
Iteration 9000: Loss = -10933.6943359375
Iteration 9100: Loss = -10933.6865234375
Iteration 9200: Loss = -10933.6787109375
Iteration 9300: Loss = -10933.6728515625
Iteration 9400: Loss = -10933.6640625
Iteration 9500: Loss = -10933.66015625
Iteration 9600: Loss = -10933.6533203125
Iteration 9700: Loss = -10933.6494140625
Iteration 9800: Loss = -10933.64453125
Iteration 9900: Loss = -10933.638671875
Iteration 10000: Loss = -10933.6328125
Iteration 10100: Loss = -10933.6298828125
Iteration 10200: Loss = -10933.6259765625
Iteration 10300: Loss = -10933.6220703125
Iteration 10400: Loss = -10933.6181640625
Iteration 10500: Loss = -10933.6142578125
Iteration 10600: Loss = -10933.611328125
Iteration 10700: Loss = -10933.6083984375
Iteration 10800: Loss = -10933.6044921875
Iteration 10900: Loss = -10933.6015625
Iteration 11000: Loss = -10933.5986328125
Iteration 11100: Loss = -10933.595703125
Iteration 11200: Loss = -10933.5927734375
Iteration 11300: Loss = -10933.5908203125
Iteration 11400: Loss = -10933.5888671875
Iteration 11500: Loss = -10933.5869140625
Iteration 11600: Loss = -10933.5849609375
Iteration 11700: Loss = -10933.5830078125
Iteration 11800: Loss = -10933.5810546875
Iteration 11900: Loss = -10933.580078125
Iteration 12000: Loss = -10933.578125
Iteration 12100: Loss = -10933.5751953125
Iteration 12200: Loss = -10933.57421875
Iteration 12300: Loss = -10933.57421875
Iteration 12400: Loss = -10933.5712890625
Iteration 12500: Loss = -10933.5712890625
Iteration 12600: Loss = -10933.5693359375
Iteration 12700: Loss = -10933.5673828125
Iteration 12800: Loss = -10933.5673828125
Iteration 12900: Loss = -10933.56640625
Iteration 13000: Loss = -10933.564453125
Iteration 13100: Loss = -10933.5634765625
Iteration 13200: Loss = -10933.5625
Iteration 13300: Loss = -10933.5615234375
Iteration 13400: Loss = -10933.560546875
Iteration 13500: Loss = -10933.560546875
Iteration 13600: Loss = -10933.560546875
Iteration 13700: Loss = -10933.5595703125
Iteration 13800: Loss = -10933.55859375
Iteration 13900: Loss = -10933.5576171875
Iteration 14000: Loss = -10933.5576171875
Iteration 14100: Loss = -10933.5576171875
Iteration 14200: Loss = -10933.5556640625
Iteration 14300: Loss = -10933.5546875
Iteration 14400: Loss = -10933.5556640625
1
Iteration 14500: Loss = -10933.5556640625
2
Iteration 14600: Loss = -10933.5546875
Iteration 14700: Loss = -10933.5546875
Iteration 14800: Loss = -10933.5546875
Iteration 14900: Loss = -10933.552734375
Iteration 15000: Loss = -10933.5556640625
1
Iteration 15100: Loss = -10933.552734375
Iteration 15200: Loss = -10933.552734375
Iteration 15300: Loss = -10933.5537109375
1
Iteration 15400: Loss = -10933.5517578125
Iteration 15500: Loss = -10933.5546875
1
Iteration 15600: Loss = -10933.55078125
Iteration 15700: Loss = -10933.55078125
Iteration 15800: Loss = -10933.55078125
Iteration 15900: Loss = -10933.5498046875
Iteration 16000: Loss = -10933.55078125
1
Iteration 16100: Loss = -10933.5498046875
Iteration 16200: Loss = -10933.55078125
1
Iteration 16300: Loss = -10933.548828125
Iteration 16400: Loss = -10933.5498046875
1
Iteration 16500: Loss = -10933.548828125
Iteration 16600: Loss = -10933.548828125
Iteration 16700: Loss = -10933.548828125
Iteration 16800: Loss = -10933.5498046875
1
Iteration 16900: Loss = -10933.5478515625
Iteration 17000: Loss = -10933.548828125
1
Iteration 17100: Loss = -10933.5478515625
Iteration 17200: Loss = -10933.5478515625
Iteration 17300: Loss = -10933.548828125
1
Iteration 17400: Loss = -10933.548828125
2
Iteration 17500: Loss = -10933.5478515625
Iteration 17600: Loss = -10933.548828125
1
Iteration 17700: Loss = -10933.5478515625
Iteration 17800: Loss = -10933.5478515625
Iteration 17900: Loss = -10933.5478515625
Iteration 18000: Loss = -10933.5478515625
Iteration 18100: Loss = -10933.5478515625
Iteration 18200: Loss = -10933.548828125
1
Iteration 18300: Loss = -10933.548828125
2
Iteration 18400: Loss = -10933.546875
Iteration 18500: Loss = -10933.546875
Iteration 18600: Loss = -10933.5478515625
1
Iteration 18700: Loss = -10933.5478515625
2
Iteration 18800: Loss = -10933.5478515625
3
Iteration 18900: Loss = -10933.546875
Iteration 19000: Loss = -10933.546875
Iteration 19100: Loss = -10933.546875
Iteration 19200: Loss = -10933.546875
Iteration 19300: Loss = -10933.5458984375
Iteration 19400: Loss = -10933.5478515625
1
Iteration 19500: Loss = -10933.5478515625
2
Iteration 19600: Loss = -10933.5478515625
3
Iteration 19700: Loss = -10933.5478515625
4
Iteration 19800: Loss = -10933.546875
5
Iteration 19900: Loss = -10933.5478515625
6
Iteration 20000: Loss = -10933.5458984375
Iteration 20100: Loss = -10933.5458984375
Iteration 20200: Loss = -10933.544921875
Iteration 20300: Loss = -10933.546875
1
Iteration 20400: Loss = -10933.546875
2
Iteration 20500: Loss = -10933.5458984375
3
Iteration 20600: Loss = -10933.546875
4
Iteration 20700: Loss = -10933.546875
5
Iteration 20800: Loss = -10933.546875
6
Iteration 20900: Loss = -10933.546875
7
Iteration 21000: Loss = -10933.5498046875
8
Iteration 21100: Loss = -10933.546875
9
Iteration 21200: Loss = -10933.5478515625
10
Iteration 21300: Loss = -10933.5458984375
11
Iteration 21400: Loss = -10933.5478515625
12
Iteration 21500: Loss = -10933.546875
13
Iteration 21600: Loss = -10933.546875
14
Iteration 21700: Loss = -10933.546875
15
Stopping early at iteration 21700 due to no improvement.
pi: tensor([[9.9999e-01, 1.1830e-05],
        [4.8562e-02, 9.5144e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.7682e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1837, 0.4137],
         [0.9102, 0.1598]],

        [[0.9587, 0.2173],
         [0.9865, 0.9801]],

        [[0.9819, 0.1119],
         [0.2813, 0.5558]],

        [[0.8964, 0.1773],
         [0.7748, 0.4715]],

        [[0.8791, 0.1762],
         [0.9749, 0.9762]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: -0.00016386303299890036
Average Adjusted Rand Index: -0.0009185013746896802
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29884.8984375
Iteration 100: Loss = -18910.01953125
Iteration 200: Loss = -12876.8857421875
Iteration 300: Loss = -11386.0751953125
Iteration 400: Loss = -11189.6591796875
Iteration 500: Loss = -11093.3515625
Iteration 600: Loss = -11038.421875
Iteration 700: Loss = -11002.3095703125
Iteration 800: Loss = -10982.896484375
Iteration 900: Loss = -10971.634765625
Iteration 1000: Loss = -10961.9443359375
Iteration 1100: Loss = -10957.6220703125
Iteration 1200: Loss = -10954.4248046875
Iteration 1300: Loss = -10951.9365234375
Iteration 1400: Loss = -10949.9462890625
Iteration 1500: Loss = -10948.330078125
Iteration 1600: Loss = -10946.9921875
Iteration 1700: Loss = -10945.8671875
Iteration 1800: Loss = -10944.912109375
Iteration 1900: Loss = -10944.0927734375
Iteration 2000: Loss = -10943.3837890625
Iteration 2100: Loss = -10942.763671875
Iteration 2200: Loss = -10942.21875
Iteration 2300: Loss = -10941.73828125
Iteration 2400: Loss = -10941.3095703125
Iteration 2500: Loss = -10940.9267578125
Iteration 2600: Loss = -10940.583984375
Iteration 2700: Loss = -10940.2744140625
Iteration 2800: Loss = -10939.9951171875
Iteration 2900: Loss = -10939.7412109375
Iteration 3000: Loss = -10939.51171875
Iteration 3100: Loss = -10939.3017578125
Iteration 3200: Loss = -10939.1103515625
Iteration 3300: Loss = -10938.93359375
Iteration 3400: Loss = -10938.7705078125
Iteration 3500: Loss = -10938.6220703125
Iteration 3600: Loss = -10938.482421875
Iteration 3700: Loss = -10938.3564453125
Iteration 3800: Loss = -10938.23828125
Iteration 3900: Loss = -10938.1279296875
Iteration 4000: Loss = -10938.025390625
Iteration 4100: Loss = -10937.9306640625
Iteration 4200: Loss = -10937.841796875
Iteration 4300: Loss = -10937.7578125
Iteration 4400: Loss = -10937.681640625
Iteration 4500: Loss = -10937.609375
Iteration 4600: Loss = -10937.5419921875
Iteration 4700: Loss = -10937.4775390625
Iteration 4800: Loss = -10937.419921875
Iteration 4900: Loss = -10937.36328125
Iteration 5000: Loss = -10937.310546875
Iteration 5100: Loss = -10937.259765625
Iteration 5200: Loss = -10937.21484375
Iteration 5300: Loss = -10937.171875
Iteration 5400: Loss = -10937.130859375
Iteration 5500: Loss = -10937.0927734375
Iteration 5600: Loss = -10937.0546875
Iteration 5700: Loss = -10937.01953125
Iteration 5800: Loss = -10936.98828125
Iteration 5900: Loss = -10936.9560546875
Iteration 6000: Loss = -10936.9267578125
Iteration 6100: Loss = -10936.900390625
Iteration 6200: Loss = -10936.8740234375
Iteration 6300: Loss = -10936.8486328125
Iteration 6400: Loss = -10936.826171875
Iteration 6500: Loss = -10936.8037109375
Iteration 6600: Loss = -10936.78125
Iteration 6700: Loss = -10936.76171875
Iteration 6800: Loss = -10936.7431640625
Iteration 6900: Loss = -10936.724609375
Iteration 7000: Loss = -10936.70703125
Iteration 7100: Loss = -10936.6923828125
Iteration 7200: Loss = -10936.6767578125
Iteration 7300: Loss = -10936.662109375
Iteration 7400: Loss = -10936.6474609375
Iteration 7500: Loss = -10936.6337890625
Iteration 7600: Loss = -10936.6220703125
Iteration 7700: Loss = -10936.609375
Iteration 7800: Loss = -10936.5966796875
Iteration 7900: Loss = -10936.5869140625
Iteration 8000: Loss = -10936.576171875
Iteration 8100: Loss = -10936.5654296875
Iteration 8200: Loss = -10936.5546875
Iteration 8300: Loss = -10936.5458984375
Iteration 8400: Loss = -10936.537109375
Iteration 8500: Loss = -10936.52734375
Iteration 8600: Loss = -10936.517578125
Iteration 8700: Loss = -10936.5078125
Iteration 8800: Loss = -10936.5
Iteration 8900: Loss = -10936.4912109375
Iteration 9000: Loss = -10936.4814453125
Iteration 9100: Loss = -10936.47265625
Iteration 9200: Loss = -10936.4619140625
Iteration 9300: Loss = -10936.4501953125
Iteration 9400: Loss = -10936.4365234375
Iteration 9500: Loss = -10936.4169921875
Iteration 9600: Loss = -10936.3857421875
Iteration 9700: Loss = -10936.33203125
Iteration 9800: Loss = -10936.236328125
Iteration 9900: Loss = -10936.1484375
Iteration 10000: Loss = -10936.0927734375
Iteration 10100: Loss = -10936.0400390625
Iteration 10200: Loss = -10936.0048828125
Iteration 10300: Loss = -10935.98046875
Iteration 10400: Loss = -10935.958984375
Iteration 10500: Loss = -10935.94140625
Iteration 10600: Loss = -10935.9248046875
Iteration 10700: Loss = -10935.9072265625
Iteration 10800: Loss = -10935.873046875
Iteration 10900: Loss = -10935.85546875
Iteration 11000: Loss = -10935.828125
Iteration 11100: Loss = -10935.7939453125
Iteration 11200: Loss = -10935.7626953125
Iteration 11300: Loss = -10935.734375
Iteration 11400: Loss = -10935.7138671875
Iteration 11500: Loss = -10935.689453125
Iteration 11600: Loss = -10935.6630859375
Iteration 11700: Loss = -10935.625
Iteration 11800: Loss = -10935.5810546875
Iteration 11900: Loss = -10935.5380859375
Iteration 12000: Loss = -10935.4912109375
Iteration 12100: Loss = -10935.443359375
Iteration 12200: Loss = -10935.392578125
Iteration 12300: Loss = -10935.341796875
Iteration 12400: Loss = -10935.28125
Iteration 12500: Loss = -10935.22265625
Iteration 12600: Loss = -10935.1787109375
Iteration 12700: Loss = -10935.1416015625
Iteration 12800: Loss = -10935.1142578125
Iteration 12900: Loss = -10935.0869140625
Iteration 13000: Loss = -10935.0634765625
Iteration 13100: Loss = -10935.0419921875
Iteration 13200: Loss = -10935.0244140625
Iteration 13300: Loss = -10935.005859375
Iteration 13400: Loss = -10934.9970703125
Iteration 13500: Loss = -10934.986328125
Iteration 13600: Loss = -10934.9765625
Iteration 13700: Loss = -10934.96484375
Iteration 13800: Loss = -10934.9521484375
Iteration 13900: Loss = -10934.94140625
Iteration 14000: Loss = -10934.9296875
Iteration 14100: Loss = -10934.9228515625
Iteration 14200: Loss = -10934.912109375
Iteration 14300: Loss = -10934.904296875
Iteration 14400: Loss = -10934.8984375
Iteration 14500: Loss = -10934.890625
Iteration 14600: Loss = -10934.8759765625
Iteration 14700: Loss = -10934.8525390625
Iteration 14800: Loss = -10934.81640625
Iteration 14900: Loss = -10934.80078125
Iteration 15000: Loss = -10934.7646484375
Iteration 15100: Loss = -10934.4970703125
Iteration 15200: Loss = -10933.97265625
Iteration 15300: Loss = -10933.7734375
Iteration 15400: Loss = -10933.7275390625
Iteration 15500: Loss = -10933.712890625
Iteration 15600: Loss = -10933.69921875
Iteration 15700: Loss = -10933.6884765625
Iteration 15800: Loss = -10933.681640625
Iteration 15900: Loss = -10933.677734375
Iteration 16000: Loss = -10933.6728515625
Iteration 16100: Loss = -10933.6591796875
Iteration 16200: Loss = -10933.6513671875
Iteration 16300: Loss = -10933.6484375
Iteration 16400: Loss = -10933.64453125
Iteration 16500: Loss = -10933.6416015625
Iteration 16600: Loss = -10933.6396484375
Iteration 16700: Loss = -10933.615234375
Iteration 16800: Loss = -10933.591796875
Iteration 16900: Loss = -10933.58984375
Iteration 17000: Loss = -10933.5908203125
1
Iteration 17100: Loss = -10933.5888671875
Iteration 17200: Loss = -10933.587890625
Iteration 17300: Loss = -10933.5888671875
1
Iteration 17400: Loss = -10933.5869140625
Iteration 17500: Loss = -10933.5869140625
Iteration 17600: Loss = -10933.5869140625
Iteration 17700: Loss = -10933.587890625
1
Iteration 17800: Loss = -10933.587890625
2
Iteration 17900: Loss = -10933.5869140625
Iteration 18000: Loss = -10933.5859375
Iteration 18100: Loss = -10933.5869140625
1
Iteration 18200: Loss = -10933.5859375
Iteration 18300: Loss = -10933.5859375
Iteration 18400: Loss = -10933.5849609375
Iteration 18500: Loss = -10933.5849609375
Iteration 18600: Loss = -10933.5849609375
Iteration 18700: Loss = -10933.5859375
1
Iteration 18800: Loss = -10933.5849609375
Iteration 18900: Loss = -10933.5849609375
Iteration 19000: Loss = -10933.5849609375
Iteration 19100: Loss = -10933.583984375
Iteration 19200: Loss = -10933.5849609375
1
Iteration 19300: Loss = -10933.57421875
Iteration 19400: Loss = -10933.568359375
Iteration 19500: Loss = -10933.568359375
Iteration 19600: Loss = -10933.5693359375
1
Iteration 19700: Loss = -10933.568359375
Iteration 19800: Loss = -10933.5693359375
1
Iteration 19900: Loss = -10933.5673828125
Iteration 20000: Loss = -10933.56640625
Iteration 20100: Loss = -10933.564453125
Iteration 20200: Loss = -10933.564453125
Iteration 20300: Loss = -10933.5634765625
Iteration 20400: Loss = -10933.5654296875
1
Iteration 20500: Loss = -10933.5654296875
2
Iteration 20600: Loss = -10933.564453125
3
Iteration 20700: Loss = -10933.5634765625
Iteration 20800: Loss = -10933.5634765625
Iteration 20900: Loss = -10933.5625
Iteration 21000: Loss = -10933.5634765625
1
Iteration 21100: Loss = -10933.5634765625
2
Iteration 21200: Loss = -10933.564453125
3
Iteration 21300: Loss = -10933.5634765625
4
Iteration 21400: Loss = -10933.564453125
5
Iteration 21500: Loss = -10933.5634765625
6
Iteration 21600: Loss = -10933.5625
Iteration 21700: Loss = -10933.5625
Iteration 21800: Loss = -10933.5634765625
1
Iteration 21900: Loss = -10933.5634765625
2
Iteration 22000: Loss = -10933.5634765625
3
Iteration 22100: Loss = -10933.5634765625
4
Iteration 22200: Loss = -10933.5634765625
5
Iteration 22300: Loss = -10933.5634765625
6
Iteration 22400: Loss = -10933.5625
Iteration 22500: Loss = -10933.5634765625
1
Iteration 22600: Loss = -10933.5615234375
Iteration 22700: Loss = -10933.564453125
1
Iteration 22800: Loss = -10933.5625
2
Iteration 22900: Loss = -10933.5634765625
3
Iteration 23000: Loss = -10933.5625
4
Iteration 23100: Loss = -10933.5625
5
Iteration 23200: Loss = -10933.5625
6
Iteration 23300: Loss = -10933.5634765625
7
Iteration 23400: Loss = -10933.5634765625
8
Iteration 23500: Loss = -10933.5625
9
Iteration 23600: Loss = -10933.564453125
10
Iteration 23700: Loss = -10933.5625
11
Iteration 23800: Loss = -10933.564453125
12
Iteration 23900: Loss = -10933.564453125
13
Iteration 24000: Loss = -10933.560546875
Iteration 24100: Loss = -10933.556640625
Iteration 24200: Loss = -10933.5576171875
1
Iteration 24300: Loss = -10933.5537109375
Iteration 24400: Loss = -10933.556640625
1
Iteration 24500: Loss = -10933.5546875
2
Iteration 24600: Loss = -10933.5546875
3
Iteration 24700: Loss = -10933.5556640625
4
Iteration 24800: Loss = -10933.5546875
5
Iteration 24900: Loss = -10933.5546875
6
Iteration 25000: Loss = -10933.5537109375
Iteration 25100: Loss = -10933.5537109375
Iteration 25200: Loss = -10933.5537109375
Iteration 25300: Loss = -10933.552734375
Iteration 25400: Loss = -10933.552734375
Iteration 25500: Loss = -10933.5537109375
1
Iteration 25600: Loss = -10933.552734375
Iteration 25700: Loss = -10933.55078125
Iteration 25800: Loss = -10933.5537109375
1
Iteration 25900: Loss = -10933.5517578125
2
Iteration 26000: Loss = -10933.5517578125
3
Iteration 26100: Loss = -10933.5517578125
4
Iteration 26200: Loss = -10933.5517578125
5
Iteration 26300: Loss = -10933.55078125
Iteration 26400: Loss = -10933.552734375
1
Iteration 26500: Loss = -10933.552734375
2
Iteration 26600: Loss = -10933.5517578125
3
Iteration 26700: Loss = -10933.552734375
4
Iteration 26800: Loss = -10933.5498046875
Iteration 26900: Loss = -10933.548828125
Iteration 27000: Loss = -10933.548828125
Iteration 27100: Loss = -10933.548828125
Iteration 27200: Loss = -10933.55078125
1
Iteration 27300: Loss = -10933.548828125
Iteration 27400: Loss = -10933.548828125
Iteration 27500: Loss = -10933.5478515625
Iteration 27600: Loss = -10933.5478515625
Iteration 27700: Loss = -10933.5478515625
Iteration 27800: Loss = -10933.546875
Iteration 27900: Loss = -10933.5478515625
1
Iteration 28000: Loss = -10933.5458984375
Iteration 28100: Loss = -10933.546875
1
Iteration 28200: Loss = -10933.546875
2
Iteration 28300: Loss = -10933.5458984375
Iteration 28400: Loss = -10933.546875
1
Iteration 28500: Loss = -10933.546875
2
Iteration 28600: Loss = -10933.546875
3
Iteration 28700: Loss = -10933.5458984375
Iteration 28800: Loss = -10933.546875
1
Iteration 28900: Loss = -10933.546875
2
Iteration 29000: Loss = -10933.5478515625
3
Iteration 29100: Loss = -10933.546875
4
Iteration 29200: Loss = -10933.548828125
5
Iteration 29300: Loss = -10933.546875
6
Iteration 29400: Loss = -10933.546875
7
Iteration 29500: Loss = -10933.5458984375
Iteration 29600: Loss = -10933.546875
1
Iteration 29700: Loss = -10933.546875
2
Iteration 29800: Loss = -10933.5478515625
3
Iteration 29900: Loss = -10933.5458984375
pi: tensor([[9.9999e-01, 7.7262e-06],
        [4.8561e-02, 9.5144e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.2760e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1837, 0.1712],
         [0.3706, 0.1598]],

        [[0.0122, 0.2172],
         [0.9907, 0.0780]],

        [[0.7485, 0.1119],
         [0.3924, 0.0104]],

        [[0.0565, 0.1773],
         [0.9865, 0.6144]],

        [[0.3130, 0.1762],
         [0.8547, 0.9802]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: -0.00016386303299890036
Average Adjusted Rand Index: -0.0009185013746896802
[-0.00016386303299890036, -0.00016386303299890036] [-0.0009185013746896802, -0.0009185013746896802] [10933.546875, 10933.546875]
-------------------------------------
This iteration is 67
True Objective function: Loss = -10764.273087346859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38702.03515625
Iteration 100: Loss = -21858.306640625
Iteration 200: Loss = -13348.3779296875
Iteration 300: Loss = -11781.4189453125
Iteration 400: Loss = -11373.947265625
Iteration 500: Loss = -11189.7841796875
Iteration 600: Loss = -11101.5986328125
Iteration 700: Loss = -11023.2412109375
Iteration 800: Loss = -10970.33203125
Iteration 900: Loss = -10948.71875
Iteration 1000: Loss = -10935.8095703125
Iteration 1100: Loss = -10927.099609375
Iteration 1200: Loss = -10920.0859375
Iteration 1300: Loss = -10912.8271484375
Iteration 1400: Loss = -10902.3525390625
Iteration 1500: Loss = -10896.75
Iteration 1600: Loss = -10893.2783203125
Iteration 1700: Loss = -10890.48828125
Iteration 1800: Loss = -10888.3173828125
Iteration 1900: Loss = -10886.5966796875
Iteration 2000: Loss = -10885.17578125
Iteration 2100: Loss = -10883.9697265625
Iteration 2200: Loss = -10882.9248046875
Iteration 2300: Loss = -10882.0087890625
Iteration 2400: Loss = -10881.2001953125
Iteration 2500: Loss = -10880.4775390625
Iteration 2600: Loss = -10879.828125
Iteration 2700: Loss = -10879.2392578125
Iteration 2800: Loss = -10878.708984375
Iteration 2900: Loss = -10878.2275390625
Iteration 3000: Loss = -10877.7470703125
Iteration 3100: Loss = -10877.33203125
Iteration 3200: Loss = -10876.8544921875
Iteration 3300: Loss = -10872.0185546875
Iteration 3400: Loss = -10871.5302734375
Iteration 3500: Loss = -10871.1943359375
Iteration 3600: Loss = -10870.904296875
Iteration 3700: Loss = -10870.6435546875
Iteration 3800: Loss = -10870.4072265625
Iteration 3900: Loss = -10870.189453125
Iteration 4000: Loss = -10869.98828125
Iteration 4100: Loss = -10869.802734375
Iteration 4200: Loss = -10869.630859375
Iteration 4300: Loss = -10869.4697265625
Iteration 4400: Loss = -10869.318359375
Iteration 4500: Loss = -10869.1806640625
Iteration 4600: Loss = -10869.0478515625
Iteration 4700: Loss = -10868.92578125
Iteration 4800: Loss = -10868.8095703125
Iteration 4900: Loss = -10868.7001953125
Iteration 5000: Loss = -10868.5986328125
Iteration 5100: Loss = -10868.4990234375
Iteration 5200: Loss = -10868.40625
Iteration 5300: Loss = -10868.31640625
Iteration 5400: Loss = -10868.2294921875
Iteration 5500: Loss = -10868.1494140625
Iteration 5600: Loss = -10868.068359375
Iteration 5700: Loss = -10867.9921875
Iteration 5800: Loss = -10867.9189453125
Iteration 5900: Loss = -10867.8525390625
Iteration 6000: Loss = -10867.787109375
Iteration 6100: Loss = -10867.7275390625
Iteration 6200: Loss = -10867.673828125
Iteration 6300: Loss = -10867.623046875
Iteration 6400: Loss = -10867.5751953125
Iteration 6500: Loss = -10867.533203125
Iteration 6600: Loss = -10867.4912109375
Iteration 6700: Loss = -10867.455078125
Iteration 6800: Loss = -10867.41796875
Iteration 6900: Loss = -10867.384765625
Iteration 7000: Loss = -10867.353515625
Iteration 7100: Loss = -10867.3173828125
Iteration 7200: Loss = -10867.28515625
Iteration 7300: Loss = -10867.259765625
Iteration 7400: Loss = -10867.232421875
Iteration 7500: Loss = -10867.208984375
Iteration 7600: Loss = -10867.1875
Iteration 7700: Loss = -10867.166015625
Iteration 7800: Loss = -10867.1455078125
Iteration 7900: Loss = -10867.1298828125
Iteration 8000: Loss = -10867.1123046875
Iteration 8100: Loss = -10867.0966796875
Iteration 8200: Loss = -10867.0810546875
Iteration 8300: Loss = -10867.0673828125
Iteration 8400: Loss = -10867.0537109375
Iteration 8500: Loss = -10867.041015625
Iteration 8600: Loss = -10867.029296875
Iteration 8700: Loss = -10867.0166015625
Iteration 8800: Loss = -10867.0087890625
Iteration 8900: Loss = -10866.9970703125
Iteration 9000: Loss = -10866.9873046875
Iteration 9100: Loss = -10866.9775390625
Iteration 9200: Loss = -10866.970703125
Iteration 9300: Loss = -10866.9619140625
Iteration 9400: Loss = -10866.955078125
Iteration 9500: Loss = -10866.947265625
Iteration 9600: Loss = -10866.94140625
Iteration 9700: Loss = -10866.9345703125
Iteration 9800: Loss = -10866.927734375
Iteration 9900: Loss = -10866.923828125
Iteration 10000: Loss = -10866.91796875
Iteration 10100: Loss = -10866.9111328125
Iteration 10200: Loss = -10866.90625
Iteration 10300: Loss = -10866.8955078125
Iteration 10400: Loss = -10866.890625
Iteration 10500: Loss = -10866.8837890625
Iteration 10600: Loss = -10866.876953125
Iteration 10700: Loss = -10866.8701171875
Iteration 10800: Loss = -10866.861328125
Iteration 10900: Loss = -10866.8486328125
Iteration 11000: Loss = -10866.826171875
Iteration 11100: Loss = -10866.794921875
Iteration 11200: Loss = -10866.7587890625
Iteration 11300: Loss = -10866.720703125
Iteration 11400: Loss = -10866.6767578125
Iteration 11500: Loss = -10866.6123046875
Iteration 11600: Loss = -10866.5576171875
Iteration 11700: Loss = -10866.494140625
Iteration 11800: Loss = -10866.4052734375
Iteration 11900: Loss = -10866.271484375
Iteration 12000: Loss = -10866.0302734375
Iteration 12100: Loss = -10865.4482421875
Iteration 12200: Loss = -10864.3505859375
Iteration 12300: Loss = -10864.02734375
Iteration 12400: Loss = -10863.8173828125
Iteration 12500: Loss = -10863.4365234375
Iteration 12600: Loss = -10863.3583984375
Iteration 12700: Loss = -10863.3408203125
Iteration 12800: Loss = -10863.333984375
Iteration 12900: Loss = -10863.3232421875
Iteration 13000: Loss = -10863.318359375
Iteration 13100: Loss = -10863.31640625
Iteration 13200: Loss = -10863.314453125
Iteration 13300: Loss = -10863.3125
Iteration 13400: Loss = -10863.310546875
Iteration 13500: Loss = -10863.3095703125
Iteration 13600: Loss = -10863.3076171875
Iteration 13700: Loss = -10863.3046875
Iteration 13800: Loss = -10863.3017578125
Iteration 13900: Loss = -10863.2998046875
Iteration 14000: Loss = -10863.296875
Iteration 14100: Loss = -10863.294921875
Iteration 14200: Loss = -10863.2939453125
Iteration 14300: Loss = -10863.29296875
Iteration 14400: Loss = -10863.2919921875
Iteration 14500: Loss = -10863.2900390625
Iteration 14600: Loss = -10863.291015625
1
Iteration 14700: Loss = -10863.2890625
Iteration 14800: Loss = -10863.2880859375
Iteration 14900: Loss = -10863.287109375
Iteration 15000: Loss = -10863.2861328125
Iteration 15100: Loss = -10863.28515625
Iteration 15200: Loss = -10863.283203125
Iteration 15300: Loss = -10863.28125
Iteration 15400: Loss = -10863.2822265625
1
Iteration 15500: Loss = -10863.28125
Iteration 15600: Loss = -10863.28125
Iteration 15700: Loss = -10863.279296875
Iteration 15800: Loss = -10863.2802734375
1
Iteration 15900: Loss = -10863.2802734375
2
Iteration 16000: Loss = -10863.2783203125
Iteration 16100: Loss = -10863.2783203125
Iteration 16200: Loss = -10863.2763671875
Iteration 16300: Loss = -10863.27734375
1
Iteration 16400: Loss = -10863.27734375
2
Iteration 16500: Loss = -10863.2763671875
Iteration 16600: Loss = -10863.2763671875
Iteration 16700: Loss = -10863.2783203125
1
Iteration 16800: Loss = -10863.275390625
Iteration 16900: Loss = -10863.275390625
Iteration 17000: Loss = -10863.2744140625
Iteration 17100: Loss = -10863.275390625
1
Iteration 17200: Loss = -10863.2763671875
2
Iteration 17300: Loss = -10863.2744140625
Iteration 17400: Loss = -10863.2734375
Iteration 17500: Loss = -10863.2744140625
1
Iteration 17600: Loss = -10863.2724609375
Iteration 17700: Loss = -10863.2734375
1
Iteration 17800: Loss = -10863.2734375
2
Iteration 17900: Loss = -10863.2734375
3
Iteration 18000: Loss = -10863.2724609375
Iteration 18100: Loss = -10863.2734375
1
Iteration 18200: Loss = -10863.2724609375
Iteration 18300: Loss = -10863.2734375
1
Iteration 18400: Loss = -10863.2734375
2
Iteration 18500: Loss = -10863.2724609375
Iteration 18600: Loss = -10863.2724609375
Iteration 18700: Loss = -10863.2724609375
Iteration 18800: Loss = -10863.2724609375
Iteration 18900: Loss = -10863.271484375
Iteration 19000: Loss = -10863.2724609375
1
Iteration 19100: Loss = -10863.2724609375
2
Iteration 19200: Loss = -10863.271484375
Iteration 19300: Loss = -10863.2724609375
1
Iteration 19400: Loss = -10863.271484375
Iteration 19500: Loss = -10863.2724609375
1
Iteration 19600: Loss = -10863.271484375
Iteration 19700: Loss = -10863.26953125
Iteration 19800: Loss = -10863.2705078125
1
Iteration 19900: Loss = -10863.2705078125
2
Iteration 20000: Loss = -10863.2724609375
3
Iteration 20100: Loss = -10863.271484375
4
Iteration 20200: Loss = -10863.26953125
Iteration 20300: Loss = -10863.271484375
1
Iteration 20400: Loss = -10863.271484375
2
Iteration 20500: Loss = -10863.2705078125
3
Iteration 20600: Loss = -10863.26953125
Iteration 20700: Loss = -10863.2705078125
1
Iteration 20800: Loss = -10863.26953125
Iteration 20900: Loss = -10863.26953125
Iteration 21000: Loss = -10863.271484375
1
Iteration 21100: Loss = -10863.26953125
Iteration 21200: Loss = -10863.2705078125
1
Iteration 21300: Loss = -10863.2705078125
2
Iteration 21400: Loss = -10863.26953125
Iteration 21500: Loss = -10863.26953125
Iteration 21600: Loss = -10863.2705078125
1
Iteration 21700: Loss = -10863.2705078125
2
Iteration 21800: Loss = -10863.2705078125
3
Iteration 21900: Loss = -10863.2705078125
4
Iteration 22000: Loss = -10863.2705078125
5
Iteration 22100: Loss = -10863.2724609375
6
Iteration 22200: Loss = -10863.26953125
Iteration 22300: Loss = -10863.2705078125
1
Iteration 22400: Loss = -10863.2626953125
Iteration 22500: Loss = -10863.2646484375
1
Iteration 22600: Loss = -10863.26171875
Iteration 22700: Loss = -10863.26171875
Iteration 22800: Loss = -10863.2607421875
Iteration 22900: Loss = -10863.24609375
Iteration 23000: Loss = -10863.240234375
Iteration 23100: Loss = -10863.224609375
Iteration 23200: Loss = -10863.21875
Iteration 23300: Loss = -10863.2158203125
Iteration 23400: Loss = -10863.2119140625
Iteration 23500: Loss = -10863.203125
Iteration 23600: Loss = -10863.19140625
Iteration 23700: Loss = -10863.173828125
Iteration 23800: Loss = -10863.0380859375
Iteration 23900: Loss = -10861.318359375
Iteration 24000: Loss = -10814.73046875
Iteration 24100: Loss = -10812.5595703125
Iteration 24200: Loss = -10809.6025390625
Iteration 24300: Loss = -10803.6689453125
Iteration 24400: Loss = -10802.798828125
Iteration 24500: Loss = -10799.28515625
Iteration 24600: Loss = -10792.751953125
Iteration 24700: Loss = -10790.720703125
Iteration 24800: Loss = -10789.9091796875
Iteration 24900: Loss = -10789.31640625
Iteration 25000: Loss = -10783.4931640625
Iteration 25100: Loss = -10780.685546875
Iteration 25200: Loss = -10780.626953125
Iteration 25300: Loss = -10780.4775390625
Iteration 25400: Loss = -10780.33984375
Iteration 25500: Loss = -10780.2392578125
Iteration 25600: Loss = -10780.2353515625
Iteration 25700: Loss = -10777.8662109375
Iteration 25800: Loss = -10777.8359375
Iteration 25900: Loss = -10777.8330078125
Iteration 26000: Loss = -10777.7919921875
Iteration 26100: Loss = -10775.6552734375
Iteration 26200: Loss = -10775.4052734375
Iteration 26300: Loss = -10775.4033203125
Iteration 26400: Loss = -10775.4013671875
Iteration 26500: Loss = -10775.3916015625
Iteration 26600: Loss = -10775.3916015625
Iteration 26700: Loss = -10775.3916015625
Iteration 26800: Loss = -10775.3896484375
Iteration 26900: Loss = -10775.3486328125
Iteration 27000: Loss = -10774.462890625
Iteration 27100: Loss = -10774.458984375
Iteration 27200: Loss = -10774.4580078125
Iteration 27300: Loss = -10774.458984375
1
Iteration 27400: Loss = -10774.4580078125
Iteration 27500: Loss = -10774.45703125
Iteration 27600: Loss = -10773.47265625
Iteration 27700: Loss = -10766.931640625
Iteration 27800: Loss = -10766.3125
Iteration 27900: Loss = -10766.291015625
Iteration 28000: Loss = -10761.833984375
Iteration 28100: Loss = -10761.794921875
Iteration 28200: Loss = -10761.78515625
Iteration 28300: Loss = -10761.78125
Iteration 28400: Loss = -10761.7783203125
Iteration 28500: Loss = -10761.775390625
Iteration 28600: Loss = -10761.7744140625
Iteration 28700: Loss = -10761.7744140625
Iteration 28800: Loss = -10761.7724609375
Iteration 28900: Loss = -10761.7724609375
Iteration 29000: Loss = -10761.7724609375
Iteration 29100: Loss = -10761.7724609375
Iteration 29200: Loss = -10761.771484375
Iteration 29300: Loss = -10761.6826171875
Iteration 29400: Loss = -10761.6728515625
Iteration 29500: Loss = -10761.6572265625
Iteration 29600: Loss = -10757.8740234375
Iteration 29700: Loss = -10757.861328125
Iteration 29800: Loss = -10757.859375
Iteration 29900: Loss = -10757.8583984375
pi: tensor([[0.7507, 0.2493],
        [0.2423, 0.7577]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4167, 0.5833], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.1114],
         [0.9605, 0.2330]],

        [[0.4175, 0.0953],
         [0.5278, 0.9073]],

        [[0.8863, 0.0955],
         [0.3953, 0.9539]],

        [[0.0249, 0.1023],
         [0.0804, 0.7071]],

        [[0.3919, 0.0880],
         [0.1982, 0.8789]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 87
Adjusted Rand Index: 0.5431645430176624
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8077803169987683
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721016799725718
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 89
Adjusted Rand Index: 0.6044118965160035
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080620079101718
Global Adjusted Rand Index: 0.7050124425582093
Average Adjusted Rand Index: 0.7071040888830356
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34177.828125
Iteration 100: Loss = -21836.29296875
Iteration 200: Loss = -13723.046875
Iteration 300: Loss = -11942.9482421875
Iteration 400: Loss = -11400.693359375
Iteration 500: Loss = -11243.875
Iteration 600: Loss = -11154.708984375
Iteration 700: Loss = -11118.8955078125
Iteration 800: Loss = -11092.8583984375
Iteration 900: Loss = -11074.7880859375
Iteration 1000: Loss = -11060.13671875
Iteration 1100: Loss = -11047.6484375
Iteration 1200: Loss = -11037.3994140625
Iteration 1300: Loss = -11024.734375
Iteration 1400: Loss = -11013.931640625
Iteration 1500: Loss = -11005.908203125
Iteration 1600: Loss = -11000.4345703125
Iteration 1700: Loss = -10995.5859375
Iteration 1800: Loss = -10988.8154296875
Iteration 1900: Loss = -10985.421875
Iteration 2000: Loss = -10981.748046875
Iteration 2100: Loss = -10979.1416015625
Iteration 2200: Loss = -10976.8193359375
Iteration 2300: Loss = -10974.51171875
Iteration 2400: Loss = -10972.125
Iteration 2500: Loss = -10969.2255859375
Iteration 2600: Loss = -10967.2177734375
Iteration 2700: Loss = -10965.6103515625
Iteration 2800: Loss = -10964.1298828125
Iteration 2900: Loss = -10962.8955078125
Iteration 3000: Loss = -10961.740234375
Iteration 3100: Loss = -10960.4052734375
Iteration 3200: Loss = -10958.837890625
Iteration 3300: Loss = -10957.0615234375
Iteration 3400: Loss = -10955.83984375
Iteration 3500: Loss = -10954.8603515625
Iteration 3600: Loss = -10953.99609375
Iteration 3700: Loss = -10953.30078125
Iteration 3800: Loss = -10952.7421875
Iteration 3900: Loss = -10952.142578125
Iteration 4000: Loss = -10951.330078125
Iteration 4100: Loss = -10950.779296875
Iteration 4200: Loss = -10950.1669921875
Iteration 4300: Loss = -10949.2421875
Iteration 4400: Loss = -10948.7919921875
Iteration 4500: Loss = -10948.5517578125
Iteration 4600: Loss = -10948.349609375
Iteration 4700: Loss = -10948.1689453125
Iteration 4800: Loss = -10948.0048828125
Iteration 4900: Loss = -10947.85546875
Iteration 5000: Loss = -10947.712890625
Iteration 5100: Loss = -10947.5771484375
Iteration 5200: Loss = -10947.435546875
Iteration 5300: Loss = -10947.2880859375
Iteration 5400: Loss = -10947.1474609375
Iteration 5500: Loss = -10946.9951171875
Iteration 5600: Loss = -10946.765625
Iteration 5700: Loss = -10946.1796875
Iteration 5800: Loss = -10945.8349609375
Iteration 5900: Loss = -10945.509765625
Iteration 6000: Loss = -10945.1123046875
Iteration 6100: Loss = -10944.9931640625
Iteration 6200: Loss = -10944.890625
Iteration 6300: Loss = -10944.8046875
Iteration 6400: Loss = -10944.728515625
Iteration 6500: Loss = -10944.6611328125
Iteration 6600: Loss = -10944.6025390625
Iteration 6700: Loss = -10944.5478515625
Iteration 6800: Loss = -10944.4951171875
Iteration 6900: Loss = -10944.443359375
Iteration 7000: Loss = -10944.375
Iteration 7100: Loss = -10944.1181640625
Iteration 7200: Loss = -10943.74609375
Iteration 7300: Loss = -10943.666015625
Iteration 7400: Loss = -10943.6201171875
Iteration 7500: Loss = -10943.58203125
Iteration 7600: Loss = -10943.5478515625
Iteration 7700: Loss = -10943.5126953125
Iteration 7800: Loss = -10943.47265625
Iteration 7900: Loss = -10943.2568359375
Iteration 8000: Loss = -10942.81640625
Iteration 8100: Loss = -10942.7314453125
Iteration 8200: Loss = -10942.1748046875
Iteration 8300: Loss = -10942.1416015625
Iteration 8400: Loss = -10942.119140625
Iteration 8500: Loss = -10942.1005859375
Iteration 8600: Loss = -10942.08203125
Iteration 8700: Loss = -10942.0673828125
Iteration 8800: Loss = -10942.052734375
Iteration 8900: Loss = -10942.0380859375
Iteration 9000: Loss = -10942.025390625
Iteration 9100: Loss = -10942.0126953125
Iteration 9200: Loss = -10942.0009765625
Iteration 9300: Loss = -10941.990234375
Iteration 9400: Loss = -10941.9794921875
Iteration 9500: Loss = -10941.970703125
Iteration 9600: Loss = -10941.9599609375
Iteration 9700: Loss = -10941.951171875
Iteration 9800: Loss = -10941.943359375
Iteration 9900: Loss = -10941.9345703125
Iteration 10000: Loss = -10941.9267578125
Iteration 10100: Loss = -10941.9208984375
Iteration 10200: Loss = -10941.9130859375
Iteration 10300: Loss = -10941.9072265625
Iteration 10400: Loss = -10941.9013671875
Iteration 10500: Loss = -10941.8955078125
Iteration 10600: Loss = -10941.890625
Iteration 10700: Loss = -10941.884765625
Iteration 10800: Loss = -10941.87890625
Iteration 10900: Loss = -10941.875
Iteration 11000: Loss = -10941.8701171875
Iteration 11100: Loss = -10941.8662109375
Iteration 11200: Loss = -10941.861328125
Iteration 11300: Loss = -10941.85546875
Iteration 11400: Loss = -10941.8173828125
Iteration 11500: Loss = -10941.30859375
Iteration 11600: Loss = -10940.5283203125
Iteration 11700: Loss = -10940.5234375
Iteration 11800: Loss = -10940.51953125
Iteration 11900: Loss = -10940.517578125
Iteration 12000: Loss = -10940.5146484375
Iteration 12100: Loss = -10940.51171875
Iteration 12200: Loss = -10940.5078125
Iteration 12300: Loss = -10940.5068359375
Iteration 12400: Loss = -10940.501953125
Iteration 12500: Loss = -10940.501953125
Iteration 12600: Loss = -10940.5
Iteration 12700: Loss = -10940.4990234375
Iteration 12800: Loss = -10940.4970703125
Iteration 12900: Loss = -10940.49609375
Iteration 13000: Loss = -10940.4931640625
Iteration 13100: Loss = -10940.4912109375
Iteration 13200: Loss = -10940.48828125
Iteration 13300: Loss = -10940.48828125
Iteration 13400: Loss = -10940.4873046875
Iteration 13500: Loss = -10940.4853515625
Iteration 13600: Loss = -10940.484375
Iteration 13700: Loss = -10940.4833984375
Iteration 13800: Loss = -10940.4814453125
Iteration 13900: Loss = -10940.482421875
1
Iteration 14000: Loss = -10940.4794921875
Iteration 14100: Loss = -10940.478515625
Iteration 14200: Loss = -10940.4775390625
Iteration 14300: Loss = -10940.4775390625
Iteration 14400: Loss = -10940.4775390625
Iteration 14500: Loss = -10940.4755859375
Iteration 14600: Loss = -10940.474609375
Iteration 14700: Loss = -10940.474609375
Iteration 14800: Loss = -10940.47265625
Iteration 14900: Loss = -10940.47265625
Iteration 15000: Loss = -10940.47265625
Iteration 15100: Loss = -10940.4736328125
1
Iteration 15200: Loss = -10940.470703125
Iteration 15300: Loss = -10940.470703125
Iteration 15400: Loss = -10940.4697265625
Iteration 15500: Loss = -10940.46875
Iteration 15600: Loss = -10940.4697265625
1
Iteration 15700: Loss = -10940.4677734375
Iteration 15800: Loss = -10940.46875
1
Iteration 15900: Loss = -10940.46875
2
Iteration 16000: Loss = -10940.4677734375
Iteration 16100: Loss = -10940.4658203125
Iteration 16200: Loss = -10940.4677734375
1
Iteration 16300: Loss = -10940.466796875
2
Iteration 16400: Loss = -10940.466796875
3
Iteration 16500: Loss = -10940.4658203125
Iteration 16600: Loss = -10940.4658203125
Iteration 16700: Loss = -10940.46484375
Iteration 16800: Loss = -10940.46484375
Iteration 16900: Loss = -10940.46484375
Iteration 17000: Loss = -10940.4638671875
Iteration 17100: Loss = -10940.46484375
1
Iteration 17200: Loss = -10940.466796875
2
Iteration 17300: Loss = -10940.4638671875
Iteration 17400: Loss = -10940.4638671875
Iteration 17500: Loss = -10940.4638671875
Iteration 17600: Loss = -10940.4638671875
Iteration 17700: Loss = -10940.462890625
Iteration 17800: Loss = -10940.462890625
Iteration 17900: Loss = -10940.4619140625
Iteration 18000: Loss = -10940.4638671875
1
Iteration 18100: Loss = -10940.4619140625
Iteration 18200: Loss = -10940.4619140625
Iteration 18300: Loss = -10939.9140625
Iteration 18400: Loss = -10939.8974609375
Iteration 18500: Loss = -10939.896484375
Iteration 18600: Loss = -10939.896484375
Iteration 18700: Loss = -10939.89453125
Iteration 18800: Loss = -10939.8955078125
1
Iteration 18900: Loss = -10939.8955078125
2
Iteration 19000: Loss = -10939.5185546875
Iteration 19100: Loss = -10935.978515625
Iteration 19200: Loss = -10934.5869140625
Iteration 19300: Loss = -10932.8359375
Iteration 19400: Loss = -10930.1298828125
Iteration 19500: Loss = -10930.09375
Iteration 19600: Loss = -10930.0546875
Iteration 19700: Loss = -10929.3173828125
Iteration 19800: Loss = -10929.2900390625
Iteration 19900: Loss = -10928.5888671875
Iteration 20000: Loss = -10927.9814453125
Iteration 20100: Loss = -10926.9208984375
Iteration 20200: Loss = -10926.8623046875
Iteration 20300: Loss = -10926.6611328125
Iteration 20400: Loss = -10924.6376953125
Iteration 20500: Loss = -10924.6220703125
Iteration 20600: Loss = -10924.482421875
Iteration 20700: Loss = -10924.0751953125
Iteration 20800: Loss = -10923.7021484375
Iteration 20900: Loss = -10923.189453125
Iteration 21000: Loss = -10922.6845703125
Iteration 21100: Loss = -10921.5859375
Iteration 21200: Loss = -10921.3720703125
Iteration 21300: Loss = -10920.9541015625
Iteration 21400: Loss = -10917.4345703125
Iteration 21500: Loss = -10916.2685546875
Iteration 21600: Loss = -10915.3779296875
Iteration 21700: Loss = -10914.5625
Iteration 21800: Loss = -10914.1865234375
Iteration 21900: Loss = -10913.173828125
Iteration 22000: Loss = -10912.3095703125
Iteration 22100: Loss = -10912.2958984375
Iteration 22200: Loss = -10912.2958984375
Iteration 22300: Loss = -10912.2958984375
Iteration 22400: Loss = -10912.2958984375
Iteration 22500: Loss = -10912.2705078125
Iteration 22600: Loss = -10912.0888671875
Iteration 22700: Loss = -10912.087890625
Iteration 22800: Loss = -10910.966796875
Iteration 22900: Loss = -10910.9013671875
Iteration 23000: Loss = -10910.8896484375
Iteration 23100: Loss = -10910.408203125
Iteration 23200: Loss = -10910.4091796875
1
Iteration 23300: Loss = -10910.40625
Iteration 23400: Loss = -10910.408203125
1
Iteration 23500: Loss = -10909.787109375
Iteration 23600: Loss = -10909.7724609375
Iteration 23700: Loss = -10909.7724609375
Iteration 23800: Loss = -10909.7724609375
Iteration 23900: Loss = -10909.7734375
1
Iteration 24000: Loss = -10909.7734375
2
Iteration 24100: Loss = -10909.7724609375
Iteration 24200: Loss = -10909.771484375
Iteration 24300: Loss = -10909.154296875
Iteration 24400: Loss = -10908.5185546875
Iteration 24500: Loss = -10908.51953125
1
Iteration 24600: Loss = -10908.51953125
2
Iteration 24700: Loss = -10908.5146484375
Iteration 24800: Loss = -10907.822265625
Iteration 24900: Loss = -10907.8212890625
Iteration 25000: Loss = -10907.8212890625
Iteration 25100: Loss = -10907.8212890625
Iteration 25200: Loss = -10907.822265625
1
Iteration 25300: Loss = -10907.822265625
2
Iteration 25400: Loss = -10907.8193359375
Iteration 25500: Loss = -10906.98828125
Iteration 25600: Loss = -10906.1962890625
Iteration 25700: Loss = -10905.4970703125
Iteration 25800: Loss = -10905.34765625
Iteration 25900: Loss = -10904.9677734375
Iteration 26000: Loss = -10904.966796875
Iteration 26100: Loss = -10904.966796875
Iteration 26200: Loss = -10904.96875
1
Iteration 26300: Loss = -10904.9658203125
Iteration 26400: Loss = -10904.951171875
Iteration 26500: Loss = -10904.2177734375
Iteration 26600: Loss = -10903.642578125
Iteration 26700: Loss = -10903.64453125
1
Iteration 26800: Loss = -10903.0615234375
Iteration 26900: Loss = -10902.267578125
Iteration 27000: Loss = -10902.263671875
Iteration 27100: Loss = -10900.8544921875
Iteration 27200: Loss = -10900.845703125
Iteration 27300: Loss = -10900.845703125
Iteration 27400: Loss = -10900.8447265625
Iteration 27500: Loss = -10900.8447265625
Iteration 27600: Loss = -10900.8447265625
Iteration 27700: Loss = -10900.8466796875
1
Iteration 27800: Loss = -10900.845703125
2
Iteration 27900: Loss = -10900.845703125
3
Iteration 28000: Loss = -10900.8466796875
4
Iteration 28100: Loss = -10900.845703125
5
Iteration 28200: Loss = -10900.845703125
6
Iteration 28300: Loss = -10900.8447265625
Iteration 28400: Loss = -10900.8486328125
1
Iteration 28500: Loss = -10900.845703125
2
Iteration 28600: Loss = -10900.794921875
Iteration 28700: Loss = -10900.2529296875
Iteration 28800: Loss = -10900.251953125
Iteration 28900: Loss = -10900.00390625
Iteration 29000: Loss = -10899.3798828125
Iteration 29100: Loss = -10898.9462890625
Iteration 29200: Loss = -10897.4892578125
Iteration 29300: Loss = -10897.1904296875
Iteration 29400: Loss = -10896.9501953125
Iteration 29500: Loss = -10896.6337890625
Iteration 29600: Loss = -10895.9384765625
Iteration 29700: Loss = -10894.849609375
Iteration 29800: Loss = -10893.8515625
Iteration 29900: Loss = -10893.3583984375
pi: tensor([[1.0000e+00, 1.3662e-07],
        [3.7491e-01, 6.2509e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.8542e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1459, 0.1678],
         [0.6982, 0.1691]],

        [[0.0329, 0.1631],
         [0.0480, 0.0159]],

        [[0.3146, 0.1607],
         [0.2623, 0.9759]],

        [[0.9804, 0.1649],
         [0.9835, 0.9809]],

        [[0.9120, 0.1822],
         [0.5244, 0.9829]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.033448153944155117
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06963631237665632
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.014415779130572131
Average Adjusted Rand Index: 0.02085646325745085
[0.7050124425582093, 0.014415779130572131] [0.7071040888830356, 0.02085646325745085] [10757.8583984375, 10892.779296875]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11008.738259478168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41403.2578125
Iteration 100: Loss = -23892.16796875
Iteration 200: Loss = -14360.658203125
Iteration 300: Loss = -12255.5615234375
Iteration 400: Loss = -11722.0419921875
Iteration 500: Loss = -11548.9345703125
Iteration 600: Loss = -11435.1416015625
Iteration 700: Loss = -11377.970703125
Iteration 800: Loss = -11321.90625
Iteration 900: Loss = -11270.7021484375
Iteration 1000: Loss = -11241.08984375
Iteration 1100: Loss = -11218.4462890625
Iteration 1200: Loss = -11205.4345703125
Iteration 1300: Loss = -11197.072265625
Iteration 1400: Loss = -11190.62109375
Iteration 1500: Loss = -11185.3759765625
Iteration 1600: Loss = -11180.783203125
Iteration 1700: Loss = -11172.896484375
Iteration 1800: Loss = -11166.2470703125
Iteration 1900: Loss = -11163.267578125
Iteration 2000: Loss = -11160.943359375
Iteration 2100: Loss = -11158.974609375
Iteration 2200: Loss = -11157.265625
Iteration 2300: Loss = -11155.755859375
Iteration 2400: Loss = -11154.41015625
Iteration 2500: Loss = -11153.203125
Iteration 2600: Loss = -11152.107421875
Iteration 2700: Loss = -11151.09765625
Iteration 2800: Loss = -11143.31640625
Iteration 2900: Loss = -11142.4609375
Iteration 3000: Loss = -11141.7373046875
Iteration 3100: Loss = -11141.072265625
Iteration 3200: Loss = -11140.447265625
Iteration 3300: Loss = -11139.8876953125
Iteration 3400: Loss = -11139.3720703125
Iteration 3500: Loss = -11138.892578125
Iteration 3600: Loss = -11138.4443359375
Iteration 3700: Loss = -11138.025390625
Iteration 3800: Loss = -11137.634765625
Iteration 3900: Loss = -11137.265625
Iteration 4000: Loss = -11136.916015625
Iteration 4100: Loss = -11136.5771484375
Iteration 4200: Loss = -11136.2294921875
Iteration 4300: Loss = -11135.81640625
Iteration 4400: Loss = -11135.28515625
Iteration 4500: Loss = -11134.822265625
Iteration 4600: Loss = -11134.4130859375
Iteration 4700: Loss = -11134.029296875
Iteration 4800: Loss = -11133.6982421875
Iteration 4900: Loss = -11133.4287109375
Iteration 5000: Loss = -11133.19140625
Iteration 5100: Loss = -11132.98046875
Iteration 5200: Loss = -11132.7880859375
Iteration 5300: Loss = -11132.609375
Iteration 5400: Loss = -11132.443359375
Iteration 5500: Loss = -11132.2900390625
Iteration 5600: Loss = -11132.1513671875
Iteration 5700: Loss = -11132.015625
Iteration 5800: Loss = -11131.8916015625
Iteration 5900: Loss = -11131.7744140625
Iteration 6000: Loss = -11131.662109375
Iteration 6100: Loss = -11131.5556640625
Iteration 6200: Loss = -11131.4541015625
Iteration 6300: Loss = -11130.97265625
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [55:16:22<26:28:06, 3073.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [56:02:48<24:53:40, 2987.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [56:52:47<24:05:37, 2990.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [57:43:33<23:23:28, 3007.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [58:29:27<21:59:02, 2931.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [59:17:54<21:07:04, 2924.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [60:05:13<20:07:41, 2898.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [60:47:43<18:37:32, 2793.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [61:33:20<17:44:31, 2777.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [62:22:03<17:14:16, 2820.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [63:03:16<15:50:42, 2716.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [63:43:37<14:35:54, 2627.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [64:38:11<14:53:31, 2821.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [65:24:21<14:01:53, 2806.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [66:20:04<14:00:42, 2967.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 84%|████████▍ | 84/100 [67:15:29<13:39:52, 3074.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [68:03:47<12:35:24, 3021.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [68:57:11<11:57:48, 3076.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [69:45:56<10:56:41, 3030.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [70:41:07<10:22:58, 3114.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [71:35:17<9:38:31, 3155.57s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [72:16:10<8:10:47, 2944.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [73:04:19<7:19:12, 2928.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [73:59:02<6:44:35, 3034.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [74:47:13<5:49:00, 2991.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [75:28:07<4:43:01, 2830.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [76:16:18<3:57:22, 2848.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [77:05:27<3:11:55, 2878.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [77:56:50<2:26:59, 2939.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [78:42:20<1:35:53, 2876.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [79:21:03<45:10, 2710.71s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [80:03:14<00:00, 2656.99s/it]100%|██████████| 100/100 [80:03:14<00:00, 2881.95s/it]
Iteration 6400: Loss = -11128.9697265625
Iteration 6500: Loss = -11128.83984375
Iteration 6600: Loss = -11128.7431640625
Iteration 6700: Loss = -11128.6474609375
Iteration 6800: Loss = -11128.5615234375
Iteration 6900: Loss = -11128.4892578125
Iteration 7000: Loss = -11128.42578125
Iteration 7100: Loss = -11128.36328125
Iteration 7200: Loss = -11128.306640625
Iteration 7300: Loss = -11128.2490234375
Iteration 7400: Loss = -11128.1943359375
Iteration 7500: Loss = -11124.396484375
Iteration 7600: Loss = -11123.650390625
Iteration 7700: Loss = -11123.5439453125
Iteration 7800: Loss = -11123.4814453125
Iteration 7900: Loss = -11123.4326171875
Iteration 8000: Loss = -11123.390625
Iteration 8100: Loss = -11123.35546875
Iteration 8200: Loss = -11123.32421875
Iteration 8300: Loss = -11123.294921875
Iteration 8400: Loss = -11123.2109375
Iteration 8500: Loss = -11119.349609375
Iteration 8600: Loss = -11119.23046875
Iteration 8700: Loss = -11119.177734375
Iteration 8800: Loss = -11119.1435546875
Iteration 8900: Loss = -11119.11328125
Iteration 9000: Loss = -11119.09375
Iteration 9100: Loss = -11119.072265625
Iteration 9200: Loss = -11119.05859375
Iteration 9300: Loss = -11119.041015625
Iteration 9400: Loss = -11119.0283203125
Iteration 9500: Loss = -11119.0146484375
Iteration 9600: Loss = -11119.0029296875
Iteration 9700: Loss = -11118.994140625
Iteration 9800: Loss = -11118.9833984375
Iteration 9900: Loss = -11118.9765625
Iteration 10000: Loss = -11118.966796875
Iteration 10100: Loss = -11118.9599609375
Iteration 10200: Loss = -11118.9521484375
Iteration 10300: Loss = -11118.9462890625
Iteration 10400: Loss = -11118.939453125
Iteration 10500: Loss = -11118.93359375
Iteration 10600: Loss = -11118.927734375
Iteration 10700: Loss = -11118.9208984375
Iteration 10800: Loss = -11118.9169921875
Iteration 10900: Loss = -11118.9111328125
Iteration 11000: Loss = -11118.90625
Iteration 11100: Loss = -11118.9013671875
Iteration 11200: Loss = -11118.896484375
Iteration 11300: Loss = -11118.8916015625
Iteration 11400: Loss = -11118.8876953125
Iteration 11500: Loss = -11118.8837890625
Iteration 11600: Loss = -11118.8798828125
Iteration 11700: Loss = -11118.875
Iteration 11800: Loss = -11118.8720703125
Iteration 11900: Loss = -11118.8662109375
Iteration 12000: Loss = -11118.859375
Iteration 12100: Loss = -11118.8544921875
Iteration 12200: Loss = -11118.8486328125
Iteration 12300: Loss = -11118.8427734375
Iteration 12400: Loss = -11118.8408203125
Iteration 12500: Loss = -11118.82421875
Iteration 12600: Loss = -11118.814453125
Iteration 12700: Loss = -11118.80859375
Iteration 12800: Loss = -11118.802734375
Iteration 12900: Loss = -11118.7998046875
Iteration 13000: Loss = -11118.794921875
Iteration 13100: Loss = -11118.7890625
Iteration 13200: Loss = -11118.78515625
Iteration 13300: Loss = -11118.779296875
Iteration 13400: Loss = -11118.7744140625
Iteration 13500: Loss = -11118.76953125
Iteration 13600: Loss = -11118.7666015625
Iteration 13700: Loss = -11118.7626953125
Iteration 13800: Loss = -11118.759765625
Iteration 13900: Loss = -11118.7451171875
Iteration 14000: Loss = -11118.7431640625
Iteration 14100: Loss = -11118.740234375
Iteration 14200: Loss = -11118.736328125
Iteration 14300: Loss = -11118.732421875
Iteration 14400: Loss = -11118.73046875
Iteration 14500: Loss = -11118.734375
1
Iteration 14600: Loss = -11118.7265625
Iteration 14700: Loss = -11118.724609375
Iteration 14800: Loss = -11118.724609375
Iteration 14900: Loss = -11118.72265625
Iteration 15000: Loss = -11118.7216796875
Iteration 15100: Loss = -11118.720703125
Iteration 15200: Loss = -11118.720703125
Iteration 15300: Loss = -11118.7177734375
Iteration 15400: Loss = -11118.7177734375
Iteration 15500: Loss = -11118.7158203125
Iteration 15600: Loss = -11118.71484375
Iteration 15700: Loss = -11118.71484375
Iteration 15800: Loss = -11118.71484375
Iteration 15900: Loss = -11118.7138671875
Iteration 16000: Loss = -11118.71484375
1
Iteration 16100: Loss = -11118.7138671875
Iteration 16200: Loss = -11118.712890625
Iteration 16300: Loss = -11118.712890625
Iteration 16400: Loss = -11118.712890625
Iteration 16500: Loss = -11118.7119140625
Iteration 16600: Loss = -11118.7109375
Iteration 16700: Loss = -11118.7119140625
1
Iteration 16800: Loss = -11118.7099609375
Iteration 16900: Loss = -11118.7099609375
Iteration 17000: Loss = -11118.7099609375
Iteration 17100: Loss = -11118.7109375
1
Iteration 17200: Loss = -11118.7109375
2
Iteration 17300: Loss = -11118.7060546875
Iteration 17400: Loss = -11118.7041015625
Iteration 17500: Loss = -11118.701171875
Iteration 17600: Loss = -11118.701171875
Iteration 17700: Loss = -11118.685546875
Iteration 17800: Loss = -11118.6630859375
Iteration 17900: Loss = -11117.958984375
Iteration 18000: Loss = -11117.9384765625
Iteration 18100: Loss = -11117.92578125
Iteration 18200: Loss = -11117.86328125
Iteration 18300: Loss = -11117.86328125
Iteration 18400: Loss = -11117.859375
Iteration 18500: Loss = -11117.859375
Iteration 18600: Loss = -11117.857421875
Iteration 18700: Loss = -11117.857421875
Iteration 18800: Loss = -11117.857421875
Iteration 18900: Loss = -11117.81640625
Iteration 19000: Loss = -11117.7978515625
Iteration 19100: Loss = -11117.794921875
Iteration 19200: Loss = -11117.755859375
Iteration 19300: Loss = -11117.740234375
Iteration 19400: Loss = -11117.7158203125
Iteration 19500: Loss = -11117.67578125
Iteration 19600: Loss = -11117.6748046875
Iteration 19700: Loss = -11117.6376953125
Iteration 19800: Loss = -11117.625
Iteration 19900: Loss = -11117.6044921875
Iteration 20000: Loss = -11117.5927734375
Iteration 20100: Loss = -11117.5888671875
Iteration 20200: Loss = -11117.583984375
Iteration 20300: Loss = -11117.564453125
Iteration 20400: Loss = -11117.5234375
Iteration 20500: Loss = -11117.49609375
Iteration 20600: Loss = -11117.47265625
Iteration 20700: Loss = -11117.0712890625
Iteration 20800: Loss = -11116.958984375
Iteration 20900: Loss = -11116.68359375
Iteration 21000: Loss = -11116.4931640625
Iteration 21100: Loss = -11116.3291015625
Iteration 21200: Loss = -11115.87890625
Iteration 21300: Loss = -11115.626953125
Iteration 21400: Loss = -11115.462890625
Iteration 21500: Loss = -11040.7236328125
Iteration 21600: Loss = -10980.890625
Iteration 21700: Loss = -10978.3427734375
Iteration 21800: Loss = -10978.083984375
Iteration 21900: Loss = -10977.98828125
Iteration 22000: Loss = -10977.3642578125
Iteration 22100: Loss = -10975.671875
Iteration 22200: Loss = -10975.6328125
Iteration 22300: Loss = -10975.6103515625
Iteration 22400: Loss = -10975.5986328125
Iteration 22500: Loss = -10975.5888671875
Iteration 22600: Loss = -10975.5810546875
Iteration 22700: Loss = -10975.5751953125
Iteration 22800: Loss = -10975.5703125
Iteration 22900: Loss = -10975.5654296875
Iteration 23000: Loss = -10975.5615234375
Iteration 23100: Loss = -10975.5517578125
Iteration 23200: Loss = -10975.548828125
Iteration 23300: Loss = -10975.5458984375
Iteration 23400: Loss = -10975.5439453125
Iteration 23500: Loss = -10975.54296875
Iteration 23600: Loss = -10975.5400390625
Iteration 23700: Loss = -10975.537109375
Iteration 23800: Loss = -10975.5361328125
Iteration 23900: Loss = -10975.53515625
Iteration 24000: Loss = -10975.53515625
Iteration 24100: Loss = -10975.5322265625
Iteration 24200: Loss = -10975.533203125
1
Iteration 24300: Loss = -10975.533203125
2
Iteration 24400: Loss = -10975.5322265625
Iteration 24500: Loss = -10975.5302734375
Iteration 24600: Loss = -10975.53125
1
Iteration 24700: Loss = -10975.5302734375
Iteration 24800: Loss = -10975.529296875
Iteration 24900: Loss = -10975.529296875
Iteration 25000: Loss = -10975.529296875
Iteration 25100: Loss = -10975.5302734375
1
Iteration 25200: Loss = -10975.529296875
Iteration 25300: Loss = -10975.52734375
Iteration 25400: Loss = -10975.52734375
Iteration 25500: Loss = -10975.52734375
Iteration 25600: Loss = -10975.52734375
Iteration 25700: Loss = -10975.5263671875
Iteration 25800: Loss = -10975.52734375
1
Iteration 25900: Loss = -10975.4697265625
Iteration 26000: Loss = -10975.4677734375
Iteration 26100: Loss = -10975.46875
1
Iteration 26200: Loss = -10975.46484375
Iteration 26300: Loss = -10975.458984375
Iteration 26400: Loss = -10975.4033203125
Iteration 26500: Loss = -10975.400390625
Iteration 26600: Loss = -10975.400390625
Iteration 26700: Loss = -10975.400390625
Iteration 26800: Loss = -10975.4013671875
1
Iteration 26900: Loss = -10975.4013671875
2
Iteration 27000: Loss = -10975.4013671875
3
Iteration 27100: Loss = -10975.400390625
Iteration 27200: Loss = -10975.400390625
Iteration 27300: Loss = -10975.3994140625
Iteration 27400: Loss = -10975.4013671875
1
Iteration 27500: Loss = -10975.400390625
2
Iteration 27600: Loss = -10975.400390625
3
Iteration 27700: Loss = -10975.400390625
4
Iteration 27800: Loss = -10975.3994140625
Iteration 27900: Loss = -10975.400390625
1
Iteration 28000: Loss = -10975.400390625
2
Iteration 28100: Loss = -10975.400390625
3
Iteration 28200: Loss = -10975.3994140625
Iteration 28300: Loss = -10975.3994140625
Iteration 28400: Loss = -10975.3994140625
Iteration 28500: Loss = -10975.3994140625
Iteration 28600: Loss = -10975.3984375
Iteration 28700: Loss = -10975.3994140625
1
Iteration 28800: Loss = -10975.3994140625
2
Iteration 28900: Loss = -10975.3994140625
3
Iteration 29000: Loss = -10975.400390625
4
Iteration 29100: Loss = -10975.400390625
5
Iteration 29200: Loss = -10975.3994140625
6
Iteration 29300: Loss = -10975.400390625
7
Iteration 29400: Loss = -10975.3994140625
8
Iteration 29500: Loss = -10975.3994140625
9
Iteration 29600: Loss = -10975.3984375
Iteration 29700: Loss = -10975.3994140625
1
Iteration 29800: Loss = -10975.400390625
2
Iteration 29900: Loss = -10975.3994140625
3
pi: tensor([[0.7818, 0.2182],
        [0.2633, 0.7367]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4226, 0.5774], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2042, 0.1020],
         [0.0277, 0.2549]],

        [[0.9088, 0.0964],
         [0.9777, 0.9844]],

        [[0.0459, 0.1076],
         [0.0327, 0.0980]],

        [[0.0295, 0.1111],
         [0.8507, 0.0586]],

        [[0.3284, 0.0939],
         [0.2077, 0.9345]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
Global Adjusted Rand Index: 0.9061158614302716
Average Adjusted Rand Index: 0.9056105478489489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20093.046875
Iteration 100: Loss = -14462.5166015625
Iteration 200: Loss = -11963.5
Iteration 300: Loss = -11408.7919921875
Iteration 400: Loss = -11304.5625
Iteration 500: Loss = -11258.1904296875
Iteration 600: Loss = -11227.9736328125
Iteration 700: Loss = -11205.3232421875
Iteration 800: Loss = -11183.5791015625
Iteration 900: Loss = -11169.1884765625
Iteration 1000: Loss = -11158.138671875
Iteration 1100: Loss = -11149.056640625
Iteration 1200: Loss = -11142.8330078125
Iteration 1300: Loss = -11138.02734375
Iteration 1400: Loss = -11134.25390625
Iteration 1500: Loss = -11131.4521484375
Iteration 1600: Loss = -11129.2958984375
Iteration 1700: Loss = -11127.529296875
Iteration 1800: Loss = -11126.0791015625
Iteration 1900: Loss = -11124.9775390625
Iteration 2000: Loss = -11124.103515625
Iteration 2100: Loss = -11123.392578125
Iteration 2200: Loss = -11122.8115234375
Iteration 2300: Loss = -11122.3271484375
Iteration 2400: Loss = -11121.9208984375
Iteration 2500: Loss = -11121.5791015625
Iteration 2600: Loss = -11121.2841796875
Iteration 2700: Loss = -11121.033203125
Iteration 2800: Loss = -11120.8115234375
Iteration 2900: Loss = -11120.6201171875
Iteration 3000: Loss = -11120.4482421875
Iteration 3100: Loss = -11120.298828125
Iteration 3200: Loss = -11120.162109375
Iteration 3300: Loss = -11120.0419921875
Iteration 3400: Loss = -11119.9375
Iteration 3500: Loss = -11119.84375
Iteration 3600: Loss = -11119.7578125
Iteration 3700: Loss = -11119.68359375
Iteration 3800: Loss = -11119.6142578125
Iteration 3900: Loss = -11119.55078125
Iteration 4000: Loss = -11119.4951171875
Iteration 4100: Loss = -11119.4423828125
Iteration 4200: Loss = -11119.3955078125
Iteration 4300: Loss = -11119.353515625
Iteration 4400: Loss = -11119.3134765625
Iteration 4500: Loss = -11119.27734375
Iteration 4600: Loss = -11119.2431640625
Iteration 4700: Loss = -11119.212890625
Iteration 4800: Loss = -11119.18359375
Iteration 4900: Loss = -11119.1572265625
Iteration 5000: Loss = -11119.1328125
Iteration 5100: Loss = -11119.109375
Iteration 5200: Loss = -11119.0869140625
Iteration 5300: Loss = -11119.06640625
Iteration 5400: Loss = -11119.0478515625
Iteration 5500: Loss = -11119.0302734375
Iteration 5600: Loss = -11119.0126953125
Iteration 5700: Loss = -11118.9951171875
Iteration 5800: Loss = -11118.9794921875
Iteration 5900: Loss = -11118.9658203125
Iteration 6000: Loss = -11118.951171875
Iteration 6100: Loss = -11118.9375
Iteration 6200: Loss = -11118.9248046875
Iteration 6300: Loss = -11118.9111328125
Iteration 6400: Loss = -11118.9013671875
Iteration 6500: Loss = -11118.8896484375
Iteration 6600: Loss = -11118.87890625
Iteration 6700: Loss = -11118.87109375
Iteration 6800: Loss = -11118.859375
Iteration 6900: Loss = -11118.849609375
Iteration 7000: Loss = -11118.83984375
Iteration 7100: Loss = -11118.830078125
Iteration 7200: Loss = -11118.8203125
Iteration 7300: Loss = -11118.810546875
Iteration 7400: Loss = -11118.80078125
Iteration 7500: Loss = -11118.7890625
Iteration 7600: Loss = -11118.7763671875
Iteration 7700: Loss = -11118.76171875
Iteration 7800: Loss = -11118.7421875
Iteration 7900: Loss = -11118.71875
Iteration 8000: Loss = -11118.6806640625
Iteration 8100: Loss = -11118.615234375
Iteration 8200: Loss = -11118.517578125
Iteration 8300: Loss = -11118.3974609375
Iteration 8400: Loss = -11118.3193359375
Iteration 8500: Loss = -11118.205078125
Iteration 8600: Loss = -11118.1669921875
Iteration 8700: Loss = -11118.1357421875
Iteration 8800: Loss = -11118.1044921875
Iteration 8900: Loss = -11118.072265625
Iteration 9000: Loss = -11118.029296875
Iteration 9100: Loss = -11117.974609375
Iteration 9200: Loss = -11117.9384765625
Iteration 9300: Loss = -11117.9189453125
Iteration 9400: Loss = -11117.8974609375
Iteration 9500: Loss = -11117.8330078125
Iteration 9600: Loss = -11117.7578125
Iteration 9700: Loss = -11117.6689453125
Iteration 9800: Loss = -11117.619140625
Iteration 9900: Loss = -11117.609375
Iteration 10000: Loss = -11117.5849609375
Iteration 10100: Loss = -11117.5244140625
Iteration 10200: Loss = -11117.4990234375
Iteration 10300: Loss = -11117.462890625
Iteration 10400: Loss = -11117.4404296875
Iteration 10500: Loss = -11117.416015625
Iteration 10600: Loss = -11117.3759765625
Iteration 10700: Loss = -11117.3125
Iteration 10800: Loss = -11117.294921875
Iteration 10900: Loss = -11117.283203125
Iteration 11000: Loss = -11117.2685546875
Iteration 11100: Loss = -11117.2275390625
Iteration 11200: Loss = -11117.1982421875
Iteration 11300: Loss = -11117.142578125
Iteration 11400: Loss = -11116.9560546875
Iteration 11500: Loss = -11116.6259765625
Iteration 11600: Loss = -11116.4921875
Iteration 11700: Loss = -11116.375
Iteration 11800: Loss = -11116.314453125
Iteration 11900: Loss = -11116.2294921875
Iteration 12000: Loss = -11116.1904296875
Iteration 12100: Loss = -11116.16015625
Iteration 12200: Loss = -11116.1318359375
Iteration 12300: Loss = -11116.1025390625
Iteration 12400: Loss = -11116.08984375
Iteration 12500: Loss = -11116.08203125
Iteration 12600: Loss = -11116.0771484375
Iteration 12700: Loss = -11116.052734375
Iteration 12800: Loss = -11116.052734375
Iteration 12900: Loss = -11116.04296875
Iteration 13000: Loss = -11116.033203125
Iteration 13100: Loss = -11116.0341796875
1
Iteration 13200: Loss = -11116.029296875
Iteration 13300: Loss = -11116.0185546875
Iteration 13400: Loss = -11116.0048828125
Iteration 13500: Loss = -11115.9931640625
Iteration 13600: Loss = -11115.978515625
Iteration 13700: Loss = -11115.9658203125
Iteration 13800: Loss = -11115.9619140625
Iteration 13900: Loss = -11115.9609375
Iteration 14000: Loss = -11115.9560546875
Iteration 14100: Loss = -11115.9482421875
Iteration 14200: Loss = -11115.9443359375
Iteration 14300: Loss = -11115.9208984375
Iteration 14400: Loss = -11115.87109375
Iteration 14500: Loss = -11115.8623046875
Iteration 14600: Loss = -11115.82421875
Iteration 14700: Loss = -11115.8134765625
Iteration 14800: Loss = -11115.6298828125
Iteration 14900: Loss = -11115.5908203125
Iteration 15000: Loss = -11115.5654296875
Iteration 15100: Loss = -11115.541015625
Iteration 15200: Loss = -11115.498046875
Iteration 15300: Loss = -11115.439453125
Iteration 15400: Loss = -11115.4033203125
Iteration 15500: Loss = -11115.3046875
Iteration 15600: Loss = -11115.0830078125
Iteration 15700: Loss = -11113.8037109375
Iteration 15800: Loss = -10996.205078125
Iteration 15900: Loss = -10992.658203125
Iteration 16000: Loss = -10992.185546875
Iteration 16100: Loss = -10990.7236328125
Iteration 16200: Loss = -10990.6708984375
Iteration 16300: Loss = -10990.642578125
Iteration 16400: Loss = -10990.3056640625
Iteration 16500: Loss = -10990.279296875
Iteration 16600: Loss = -10989.435546875
Iteration 16700: Loss = -10988.009765625
Iteration 16800: Loss = -10986.9462890625
Iteration 16900: Loss = -10978.5361328125
Iteration 17000: Loss = -10978.4677734375
Iteration 17100: Loss = -10978.44140625
Iteration 17200: Loss = -10976.919921875
Iteration 17300: Loss = -10975.478515625
Iteration 17400: Loss = -10975.4677734375
Iteration 17500: Loss = -10975.4599609375
Iteration 17600: Loss = -10975.4560546875
Iteration 17700: Loss = -10975.4521484375
Iteration 17800: Loss = -10975.44921875
Iteration 17900: Loss = -10975.4404296875
Iteration 18000: Loss = -10975.4384765625
Iteration 18100: Loss = -10975.4365234375
Iteration 18200: Loss = -10975.4345703125
Iteration 18300: Loss = -10975.4345703125
Iteration 18400: Loss = -10975.431640625
Iteration 18500: Loss = -10975.431640625
Iteration 18600: Loss = -10975.4306640625
Iteration 18700: Loss = -10975.412109375
Iteration 18800: Loss = -10975.4111328125
Iteration 18900: Loss = -10975.4111328125
Iteration 19000: Loss = -10975.41015625
Iteration 19100: Loss = -10975.41015625
Iteration 19200: Loss = -10975.408203125
Iteration 19300: Loss = -10975.4091796875
1
Iteration 19400: Loss = -10975.4091796875
2
Iteration 19500: Loss = -10975.408203125
Iteration 19600: Loss = -10975.4072265625
Iteration 19700: Loss = -10975.408203125
1
Iteration 19800: Loss = -10975.40625
Iteration 19900: Loss = -10975.40625
Iteration 20000: Loss = -10975.40625
Iteration 20100: Loss = -10975.40625
Iteration 20200: Loss = -10975.4052734375
Iteration 20300: Loss = -10975.40625
1
Iteration 20400: Loss = -10975.40625
2
Iteration 20500: Loss = -10975.4052734375
Iteration 20600: Loss = -10975.4052734375
Iteration 20700: Loss = -10975.404296875
Iteration 20800: Loss = -10975.4052734375
1
Iteration 20900: Loss = -10975.404296875
Iteration 21000: Loss = -10975.404296875
Iteration 21100: Loss = -10975.4052734375
1
Iteration 21200: Loss = -10975.404296875
Iteration 21300: Loss = -10975.404296875
Iteration 21400: Loss = -10975.404296875
Iteration 21500: Loss = -10975.4033203125
Iteration 21600: Loss = -10975.4033203125
Iteration 21700: Loss = -10975.404296875
1
Iteration 21800: Loss = -10975.404296875
2
Iteration 21900: Loss = -10975.4033203125
Iteration 22000: Loss = -10975.4033203125
Iteration 22100: Loss = -10975.4033203125
Iteration 22200: Loss = -10975.4033203125
Iteration 22300: Loss = -10975.4033203125
Iteration 22400: Loss = -10975.4033203125
Iteration 22500: Loss = -10975.404296875
1
Iteration 22600: Loss = -10975.4033203125
Iteration 22700: Loss = -10975.4033203125
Iteration 22800: Loss = -10975.4033203125
Iteration 22900: Loss = -10975.40234375
Iteration 23000: Loss = -10975.40234375
Iteration 23100: Loss = -10975.4033203125
1
Iteration 23200: Loss = -10975.4033203125
2
Iteration 23300: Loss = -10975.40234375
Iteration 23400: Loss = -10975.4033203125
1
Iteration 23500: Loss = -10975.40234375
Iteration 23600: Loss = -10975.4033203125
1
Iteration 23700: Loss = -10975.40234375
Iteration 23800: Loss = -10975.4033203125
1
Iteration 23900: Loss = -10975.40234375
Iteration 24000: Loss = -10975.4033203125
1
Iteration 24100: Loss = -10975.404296875
2
Iteration 24200: Loss = -10975.404296875
3
Iteration 24300: Loss = -10975.40234375
Iteration 24400: Loss = -10975.4033203125
1
Iteration 24500: Loss = -10975.40234375
Iteration 24600: Loss = -10975.4033203125
1
Iteration 24700: Loss = -10975.40234375
Iteration 24800: Loss = -10975.40234375
Iteration 24900: Loss = -10975.4033203125
1
Iteration 25000: Loss = -10975.4033203125
2
Iteration 25100: Loss = -10975.4033203125
3
Iteration 25200: Loss = -10975.40234375
Iteration 25300: Loss = -10975.40234375
Iteration 25400: Loss = -10975.40234375
Iteration 25500: Loss = -10975.4013671875
Iteration 25600: Loss = -10975.404296875
1
Iteration 25700: Loss = -10975.40234375
2
Iteration 25800: Loss = -10975.4033203125
3
Iteration 25900: Loss = -10975.4033203125
4
Iteration 26000: Loss = -10975.40234375
5
Iteration 26100: Loss = -10975.4033203125
6
Iteration 26200: Loss = -10975.40234375
7
Iteration 26300: Loss = -10975.40234375
8
Iteration 26400: Loss = -10975.4033203125
9
Iteration 26500: Loss = -10975.40234375
10
Iteration 26600: Loss = -10975.4033203125
11
Iteration 26700: Loss = -10975.4033203125
12
Iteration 26800: Loss = -10975.40234375
13
Iteration 26900: Loss = -10975.40234375
14
Iteration 27000: Loss = -10975.40234375
15
Stopping early at iteration 27000 due to no improvement.
pi: tensor([[0.7367, 0.2633],
        [0.2180, 0.7820]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5774, 0.4226], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2549, 0.1020],
         [0.0135, 0.2042]],

        [[0.9817, 0.0964],
         [0.2356, 0.9913]],

        [[0.9862, 0.1077],
         [0.4878, 0.7713]],

        [[0.9850, 0.1111],
         [0.0356, 0.6611]],

        [[0.0359, 0.0939],
         [0.8281, 0.9888]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
Global Adjusted Rand Index: 0.9061158614302716
Average Adjusted Rand Index: 0.9056105478489489
[0.9061158614302716, 0.9061158614302716] [0.9056105478489489, 0.9056105478489489] [10975.3994140625, 10975.40234375]
-------------------------------------
This iteration is 69
True Objective function: Loss = -10964.978206519521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36420.81640625
Iteration 100: Loss = -20260.263671875
Iteration 200: Loss = -12862.7890625
Iteration 300: Loss = -11560.4501953125
Iteration 400: Loss = -11323.1640625
Iteration 500: Loss = -11217.3369140625
Iteration 600: Loss = -11175.7568359375
Iteration 700: Loss = -11152.59375
Iteration 800: Loss = -11135.90625
Iteration 900: Loss = -11125.1201171875
Iteration 1000: Loss = -11117.6728515625
Iteration 1100: Loss = -11112.0908203125
Iteration 1200: Loss = -11107.736328125
Iteration 1300: Loss = -11104.25390625
Iteration 1400: Loss = -11101.4140625
Iteration 1500: Loss = -11099.078125
Iteration 1600: Loss = -11097.1337890625
Iteration 1700: Loss = -11095.5
Iteration 1800: Loss = -11094.1103515625
Iteration 1900: Loss = -11092.91796875
Iteration 2000: Loss = -11091.8837890625
Iteration 2100: Loss = -11090.9794921875
Iteration 2200: Loss = -11090.185546875
Iteration 2300: Loss = -11089.4873046875
Iteration 2400: Loss = -11088.8642578125
Iteration 2500: Loss = -11088.302734375
Iteration 2600: Loss = -11087.7998046875
Iteration 2700: Loss = -11087.3486328125
Iteration 2800: Loss = -11086.9404296875
Iteration 2900: Loss = -11086.5703125
Iteration 3000: Loss = -11086.2333984375
Iteration 3100: Loss = -11085.92578125
Iteration 3200: Loss = -11085.64453125
Iteration 3300: Loss = -11085.384765625
Iteration 3400: Loss = -11085.146484375
Iteration 3500: Loss = -11084.9267578125
Iteration 3600: Loss = -11084.7216796875
Iteration 3700: Loss = -11084.5283203125
Iteration 3800: Loss = -11084.3408203125
Iteration 3900: Loss = -11084.162109375
Iteration 4000: Loss = -11084.0009765625
Iteration 4100: Loss = -11083.8515625
Iteration 4200: Loss = -11083.7119140625
Iteration 4300: Loss = -11083.580078125
Iteration 4400: Loss = -11083.4560546875
Iteration 4500: Loss = -11083.3408203125
Iteration 4600: Loss = -11083.234375
Iteration 4700: Loss = -11083.1318359375
Iteration 4800: Loss = -11083.0400390625
Iteration 4900: Loss = -11082.951171875
Iteration 5000: Loss = -11082.865234375
Iteration 5100: Loss = -11082.7861328125
Iteration 5200: Loss = -11082.7138671875
Iteration 5300: Loss = -11082.6494140625
Iteration 5400: Loss = -11082.5908203125
Iteration 5500: Loss = -11082.5341796875
Iteration 5600: Loss = -11082.4833984375
Iteration 5700: Loss = -11082.4375
Iteration 5800: Loss = -11082.39453125
Iteration 5900: Loss = -11082.35546875
Iteration 6000: Loss = -11082.3173828125
Iteration 6100: Loss = -11082.2841796875
Iteration 6200: Loss = -11082.251953125
Iteration 6300: Loss = -11082.2197265625
Iteration 6400: Loss = -11082.1904296875
Iteration 6500: Loss = -11082.1630859375
Iteration 6600: Loss = -11082.1376953125
Iteration 6700: Loss = -11082.1123046875
Iteration 6800: Loss = -11082.08984375
Iteration 6900: Loss = -11082.068359375
Iteration 7000: Loss = -11082.046875
Iteration 7100: Loss = -11082.02734375
Iteration 7200: Loss = -11082.0087890625
Iteration 7300: Loss = -11081.990234375
Iteration 7400: Loss = -11081.97265625
Iteration 7500: Loss = -11081.95703125
Iteration 7600: Loss = -11081.94140625
Iteration 7700: Loss = -11081.9267578125
Iteration 7800: Loss = -11081.912109375
Iteration 7900: Loss = -11081.8974609375
Iteration 8000: Loss = -11081.8828125
Iteration 8100: Loss = -11081.87109375
Iteration 8200: Loss = -11081.857421875
Iteration 8300: Loss = -11081.8466796875
Iteration 8400: Loss = -11081.8330078125
Iteration 8500: Loss = -11081.8232421875
Iteration 8600: Loss = -11081.80859375
Iteration 8700: Loss = -11081.796875
Iteration 8800: Loss = -11081.7841796875
Iteration 8900: Loss = -11081.7734375
Iteration 9000: Loss = -11081.7587890625
Iteration 9100: Loss = -11081.74609375
Iteration 9200: Loss = -11081.7314453125
Iteration 9300: Loss = -11081.7158203125
Iteration 9400: Loss = -11081.701171875
Iteration 9500: Loss = -11081.68359375
Iteration 9600: Loss = -11081.662109375
Iteration 9700: Loss = -11081.6416015625
Iteration 9800: Loss = -11081.619140625
Iteration 9900: Loss = -11081.595703125
Iteration 10000: Loss = -11081.5703125
Iteration 10100: Loss = -11081.5439453125
Iteration 10200: Loss = -11081.5185546875
Iteration 10300: Loss = -11081.4892578125
Iteration 10400: Loss = -11081.4560546875
Iteration 10500: Loss = -11081.4169921875
Iteration 10600: Loss = -11081.3642578125
Iteration 10700: Loss = -11080.96484375
Iteration 10800: Loss = -11080.7841796875
Iteration 10900: Loss = -11080.623046875
Iteration 11000: Loss = -11080.453125
Iteration 11100: Loss = -11080.26953125
Iteration 11200: Loss = -11080.078125
Iteration 11300: Loss = -11079.8876953125
Iteration 11400: Loss = -11079.712890625
Iteration 11500: Loss = -11079.55859375
Iteration 11600: Loss = -11079.4375
Iteration 11700: Loss = -11079.345703125
Iteration 11800: Loss = -11079.2744140625
Iteration 11900: Loss = -11079.2158203125
Iteration 12000: Loss = -11079.1650390625
Iteration 12100: Loss = -11079.1259765625
Iteration 12200: Loss = -11079.091796875
Iteration 12300: Loss = -11079.060546875
Iteration 12400: Loss = -11079.0302734375
Iteration 12500: Loss = -11078.9970703125
Iteration 12600: Loss = -11078.9345703125
Iteration 12700: Loss = -11078.8896484375
Iteration 12800: Loss = -11078.8642578125
Iteration 12900: Loss = -11078.8486328125
Iteration 13000: Loss = -11078.833984375
Iteration 13100: Loss = -11078.8251953125
Iteration 13200: Loss = -11078.8154296875
Iteration 13300: Loss = -11078.80859375
Iteration 13400: Loss = -11078.802734375
Iteration 13500: Loss = -11078.794921875
Iteration 13600: Loss = -11078.7958984375
1
Iteration 13700: Loss = -11078.7919921875
Iteration 13800: Loss = -11078.7900390625
Iteration 13900: Loss = -11078.787109375
Iteration 14000: Loss = -11078.7861328125
Iteration 14100: Loss = -11078.7861328125
Iteration 14200: Loss = -11078.7841796875
Iteration 14300: Loss = -11078.7841796875
Iteration 14400: Loss = -11078.783203125
Iteration 14500: Loss = -11078.7822265625
Iteration 14600: Loss = -11078.7822265625
Iteration 14700: Loss = -11078.7802734375
Iteration 14800: Loss = -11078.7802734375
Iteration 14900: Loss = -11078.7802734375
Iteration 15000: Loss = -11078.7802734375
Iteration 15100: Loss = -11078.7802734375
Iteration 15200: Loss = -11078.779296875
Iteration 15300: Loss = -11078.77734375
Iteration 15400: Loss = -11078.779296875
1
Iteration 15500: Loss = -11078.779296875
2
Iteration 15600: Loss = -11078.7783203125
3
Iteration 15700: Loss = -11078.775390625
Iteration 15800: Loss = -11078.77734375
1
Iteration 15900: Loss = -11078.7783203125
2
Iteration 16000: Loss = -11078.775390625
Iteration 16100: Loss = -11078.7763671875
1
Iteration 16200: Loss = -11078.775390625
Iteration 16300: Loss = -11078.77734375
1
Iteration 16400: Loss = -11078.775390625
Iteration 16500: Loss = -11078.7744140625
Iteration 16600: Loss = -11078.7744140625
Iteration 16700: Loss = -11078.7744140625
Iteration 16800: Loss = -11078.775390625
1
Iteration 16900: Loss = -11078.775390625
2
Iteration 17000: Loss = -11078.7744140625
Iteration 17100: Loss = -11078.7734375
Iteration 17200: Loss = -11078.7744140625
1
Iteration 17300: Loss = -11078.7744140625
2
Iteration 17400: Loss = -11078.775390625
3
Iteration 17500: Loss = -11078.775390625
4
Iteration 17600: Loss = -11078.7744140625
5
Iteration 17700: Loss = -11078.7763671875
6
Iteration 17800: Loss = -11078.7744140625
7
Iteration 17900: Loss = -11078.775390625
8
Iteration 18000: Loss = -11078.7744140625
9
Iteration 18100: Loss = -11078.7724609375
Iteration 18200: Loss = -11078.7734375
1
Iteration 18300: Loss = -11078.7744140625
2
Iteration 18400: Loss = -11078.7734375
3
Iteration 18500: Loss = -11078.7724609375
Iteration 18600: Loss = -11078.7734375
1
Iteration 18700: Loss = -11078.7744140625
2
Iteration 18800: Loss = -11078.7744140625
3
Iteration 18900: Loss = -11078.7724609375
Iteration 19000: Loss = -11078.7734375
1
Iteration 19100: Loss = -11078.7734375
2
Iteration 19200: Loss = -11078.7724609375
Iteration 19300: Loss = -11078.7724609375
Iteration 19400: Loss = -11078.7734375
1
Iteration 19500: Loss = -11078.7734375
2
Iteration 19600: Loss = -11078.7724609375
Iteration 19700: Loss = -11078.7734375
1
Iteration 19800: Loss = -11078.771484375
Iteration 19900: Loss = -11078.7724609375
1
Iteration 20000: Loss = -11078.7734375
2
Iteration 20100: Loss = -11078.7734375
3
Iteration 20200: Loss = -11078.7734375
4
Iteration 20300: Loss = -11078.7734375
5
Iteration 20400: Loss = -11078.7734375
6
Iteration 20500: Loss = -11078.7734375
7
Iteration 20600: Loss = -11078.7734375
8
Iteration 20700: Loss = -11078.7734375
9
Iteration 20800: Loss = -11078.7724609375
10
Iteration 20900: Loss = -11078.775390625
11
Iteration 21000: Loss = -11078.7734375
12
Iteration 21100: Loss = -11078.7763671875
13
Iteration 21200: Loss = -11078.7724609375
14
Iteration 21300: Loss = -11078.7734375
15
Stopping early at iteration 21300 due to no improvement.
pi: tensor([[6.7143e-05, 9.9993e-01],
        [5.2731e-02, 9.4727e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0092, 0.9908], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2680, 0.0870],
         [0.5452, 0.1634]],

        [[0.5936, 0.2257],
         [0.0224, 0.0974]],

        [[0.5626, 0.2004],
         [0.0879, 0.1164]],

        [[0.9234, 0.1952],
         [0.9713, 0.0761]],

        [[0.5208, 0.0906],
         [0.9853, 0.1525]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: -0.0003203894517489664
Average Adjusted Rand Index: 0.00025988589812057233
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29989.982421875
Iteration 100: Loss = -19526.796875
Iteration 200: Loss = -13449.4169921875
Iteration 300: Loss = -12031.525390625
Iteration 400: Loss = -11579.177734375
Iteration 500: Loss = -11383.333984375
Iteration 600: Loss = -11296.685546875
Iteration 700: Loss = -11254.3330078125
Iteration 800: Loss = -11231.2236328125
Iteration 900: Loss = -11211.1982421875
Iteration 1000: Loss = -11199.86328125
Iteration 1100: Loss = -11187.5703125
Iteration 1200: Loss = -11174.8642578125
Iteration 1300: Loss = -11164.951171875
Iteration 1400: Loss = -11158.1611328125
Iteration 1500: Loss = -11153.7119140625
Iteration 1600: Loss = -11149.4541015625
Iteration 1700: Loss = -11144.8115234375
Iteration 1800: Loss = -11140.5732421875
Iteration 1900: Loss = -11135.701171875
Iteration 2000: Loss = -11132.1767578125
Iteration 2100: Loss = -11128.7333984375
Iteration 2200: Loss = -11123.9404296875
Iteration 2300: Loss = -11120.7958984375
Iteration 2400: Loss = -11118.7646484375
Iteration 2500: Loss = -11116.859375
Iteration 2600: Loss = -11115.0908203125
Iteration 2700: Loss = -11113.5888671875
Iteration 2800: Loss = -11112.4541015625
Iteration 2900: Loss = -11111.458984375
Iteration 3000: Loss = -11110.591796875
Iteration 3100: Loss = -11109.9140625
Iteration 3200: Loss = -11109.4072265625
Iteration 3300: Loss = -11108.970703125
Iteration 3400: Loss = -11108.5634765625
Iteration 3500: Loss = -11108.150390625
Iteration 3600: Loss = -11107.6650390625
Iteration 3700: Loss = -11107.0068359375
Iteration 3800: Loss = -11106.3828125
Iteration 3900: Loss = -11105.9970703125
Iteration 4000: Loss = -11105.734375
Iteration 4100: Loss = -11105.5166015625
Iteration 4200: Loss = -11105.310546875
Iteration 4300: Loss = -11105.0810546875
Iteration 4400: Loss = -11104.7724609375
Iteration 4500: Loss = -11104.279296875
Iteration 4600: Loss = -11103.6875
Iteration 4700: Loss = -11103.275390625
Iteration 4800: Loss = -11102.9755859375
Iteration 4900: Loss = -11101.6923828125
Iteration 5000: Loss = -11099.6044921875
Iteration 5100: Loss = -11098.2978515625
Iteration 5200: Loss = -11096.2880859375
Iteration 5300: Loss = -11095.677734375
Iteration 5400: Loss = -11095.369140625
Iteration 5500: Loss = -11095.21484375
Iteration 5600: Loss = -11095.0986328125
Iteration 5700: Loss = -11095.00390625
Iteration 5800: Loss = -11094.9228515625
Iteration 5900: Loss = -11094.849609375
Iteration 6000: Loss = -11094.775390625
Iteration 6100: Loss = -11094.607421875
Iteration 6200: Loss = -11094.158203125
Iteration 6300: Loss = -11093.9609375
Iteration 6400: Loss = -11093.8603515625
Iteration 6500: Loss = -11093.7900390625
Iteration 6600: Loss = -11093.734375
Iteration 6700: Loss = -11093.6787109375
Iteration 6800: Loss = -11093.626953125
Iteration 6900: Loss = -11093.5693359375
Iteration 7000: Loss = -11093.4892578125
Iteration 7100: Loss = -11093.3662109375
Iteration 7200: Loss = -11093.1962890625
Iteration 7300: Loss = -11093.03515625
Iteration 7400: Loss = -11092.9189453125
Iteration 7500: Loss = -11092.8369140625
Iteration 7600: Loss = -11092.7900390625
Iteration 7700: Loss = -11092.755859375
Iteration 7800: Loss = -11092.7041015625
Iteration 7900: Loss = -11092.2734375
Iteration 8000: Loss = -11092.0166015625
Iteration 8100: Loss = -11091.9384765625
Iteration 8200: Loss = -11091.89453125
Iteration 8300: Loss = -11091.859375
Iteration 8400: Loss = -11091.822265625
Iteration 8500: Loss = -11091.611328125
Iteration 8600: Loss = -11086.92578125
Iteration 8700: Loss = -11074.4287109375
Iteration 8800: Loss = -11074.09375
Iteration 8900: Loss = -11074.0068359375
Iteration 9000: Loss = -11073.92578125
Iteration 9100: Loss = -11073.8974609375
Iteration 9200: Loss = -11073.7001953125
Iteration 9300: Loss = -11073.5478515625
Iteration 9400: Loss = -11073.5185546875
Iteration 9500: Loss = -11073.5029296875
Iteration 9600: Loss = -11073.490234375
Iteration 9700: Loss = -11073.482421875
Iteration 9800: Loss = -11073.4736328125
Iteration 9900: Loss = -11073.4658203125
Iteration 10000: Loss = -11073.4609375
Iteration 10100: Loss = -11073.4453125
Iteration 10200: Loss = -11069.9677734375
Iteration 10300: Loss = -11069.9169921875
Iteration 10400: Loss = -11069.90234375
Iteration 10500: Loss = -11069.8935546875
Iteration 10600: Loss = -11069.8876953125
Iteration 10700: Loss = -11069.8818359375
Iteration 10800: Loss = -11069.8779296875
Iteration 10900: Loss = -11069.8740234375
Iteration 11000: Loss = -11069.8720703125
Iteration 11100: Loss = -11069.8671875
Iteration 11200: Loss = -11069.8642578125
Iteration 11300: Loss = -11069.86328125
Iteration 11400: Loss = -11069.861328125
Iteration 11500: Loss = -11069.857421875
Iteration 11600: Loss = -11069.85546875
Iteration 11700: Loss = -11069.853515625
Iteration 11800: Loss = -11069.8525390625
Iteration 11900: Loss = -11069.8505859375
Iteration 12000: Loss = -11069.849609375
Iteration 12100: Loss = -11069.8486328125
Iteration 12200: Loss = -11069.8466796875
Iteration 12300: Loss = -11069.845703125
Iteration 12400: Loss = -11069.84375
Iteration 12500: Loss = -11069.84375
Iteration 12600: Loss = -11069.841796875
Iteration 12700: Loss = -11069.841796875
Iteration 12800: Loss = -11069.8388671875
Iteration 12900: Loss = -11069.83984375
1
Iteration 13000: Loss = -11069.8369140625
Iteration 13100: Loss = -11069.8369140625
Iteration 13200: Loss = -11069.8369140625
Iteration 13300: Loss = -11069.8369140625
Iteration 13400: Loss = -11069.8359375
Iteration 13500: Loss = -11069.833984375
Iteration 13600: Loss = -11069.833984375
Iteration 13700: Loss = -11069.8330078125
Iteration 13800: Loss = -11069.83203125
Iteration 13900: Loss = -11069.830078125
Iteration 14000: Loss = -11069.8291015625
Iteration 14100: Loss = -11069.8291015625
Iteration 14200: Loss = -11069.828125
Iteration 14300: Loss = -11069.8271484375
Iteration 14400: Loss = -11069.8271484375
Iteration 14500: Loss = -11069.826171875
Iteration 14600: Loss = -11069.8271484375
1
Iteration 14700: Loss = -11069.8271484375
2
Iteration 14800: Loss = -11069.8251953125
Iteration 14900: Loss = -11069.8251953125
Iteration 15000: Loss = -11069.8251953125
Iteration 15100: Loss = -11069.826171875
1
Iteration 15200: Loss = -11069.8232421875
Iteration 15300: Loss = -11069.82421875
1
Iteration 15400: Loss = -11069.82421875
2
Iteration 15500: Loss = -11069.822265625
Iteration 15600: Loss = -11069.822265625
Iteration 15700: Loss = -11069.8232421875
1
Iteration 15800: Loss = -11069.822265625
Iteration 15900: Loss = -11069.8212890625
Iteration 16000: Loss = -11069.822265625
1
Iteration 16100: Loss = -11069.822265625
2
Iteration 16200: Loss = -11069.822265625
3
Iteration 16300: Loss = -11069.8203125
Iteration 16400: Loss = -11069.8212890625
1
Iteration 16500: Loss = -11069.8212890625
2
Iteration 16600: Loss = -11069.8212890625
3
Iteration 16700: Loss = -11069.8203125
Iteration 16800: Loss = -11069.8203125
Iteration 16900: Loss = -11069.822265625
1
Iteration 17000: Loss = -11069.8203125
Iteration 17100: Loss = -11069.8203125
Iteration 17200: Loss = -11069.8193359375
Iteration 17300: Loss = -11069.8203125
1
Iteration 17400: Loss = -11069.8212890625
2
Iteration 17500: Loss = -11069.810546875
Iteration 17600: Loss = -11062.568359375
Iteration 17700: Loss = -11062.4853515625
Iteration 17800: Loss = -11062.466796875
Iteration 17900: Loss = -11062.458984375
Iteration 18000: Loss = -11062.453125
Iteration 18100: Loss = -11062.4501953125
Iteration 18200: Loss = -11062.4482421875
Iteration 18300: Loss = -11062.4462890625
Iteration 18400: Loss = -11062.4462890625
Iteration 18500: Loss = -11062.4443359375
Iteration 18600: Loss = -11062.443359375
Iteration 18700: Loss = -11062.4423828125
Iteration 18800: Loss = -11062.443359375
1
Iteration 18900: Loss = -11062.4423828125
Iteration 19000: Loss = -11062.4404296875
Iteration 19100: Loss = -11062.4404296875
Iteration 19200: Loss = -11062.4404296875
Iteration 19300: Loss = -11062.4404296875
Iteration 19400: Loss = -11062.439453125
Iteration 19500: Loss = -11062.439453125
Iteration 19600: Loss = -11062.4404296875
1
Iteration 19700: Loss = -11062.439453125
Iteration 19800: Loss = -11062.439453125
Iteration 19900: Loss = -11062.439453125
Iteration 20000: Loss = -11062.4384765625
Iteration 20100: Loss = -11062.439453125
1
Iteration 20200: Loss = -11062.439453125
2
Iteration 20300: Loss = -11062.439453125
3
Iteration 20400: Loss = -11062.4384765625
Iteration 20500: Loss = -11062.4384765625
Iteration 20600: Loss = -11062.4384765625
Iteration 20700: Loss = -11062.4384765625
Iteration 20800: Loss = -11062.4384765625
Iteration 20900: Loss = -11062.4384765625
Iteration 21000: Loss = -11062.4404296875
1
Iteration 21100: Loss = -11062.4375
Iteration 21200: Loss = -11062.4384765625
1
Iteration 21300: Loss = -11062.4375
Iteration 21400: Loss = -11062.4384765625
1
Iteration 21500: Loss = -11062.4384765625
2
Iteration 21600: Loss = -11062.4375
Iteration 21700: Loss = -11062.4375
Iteration 21800: Loss = -11062.4384765625
1
Iteration 21900: Loss = -11062.4365234375
Iteration 22000: Loss = -11062.4365234375
Iteration 22100: Loss = -11062.4365234375
Iteration 22200: Loss = -11062.4375
1
Iteration 22300: Loss = -11062.4365234375
Iteration 22400: Loss = -11062.4365234375
Iteration 22500: Loss = -11062.4404296875
1
Iteration 22600: Loss = -11062.4375
2
Iteration 22700: Loss = -11062.4375
3
Iteration 22800: Loss = -11062.4375
4
Iteration 22900: Loss = -11062.4375
5
Iteration 23000: Loss = -11062.4375
6
Iteration 23100: Loss = -11062.4375
7
Iteration 23200: Loss = -11062.4384765625
8
Iteration 23300: Loss = -11062.4375
9
Iteration 23400: Loss = -11062.4365234375
Iteration 23500: Loss = -11062.439453125
1
Iteration 23600: Loss = -11062.4375
2
Iteration 23700: Loss = -11062.4365234375
Iteration 23800: Loss = -11062.4365234375
Iteration 23900: Loss = -11062.4375
1
Iteration 24000: Loss = -11062.4384765625
2
Iteration 24100: Loss = -11062.4375
3
Iteration 24200: Loss = -11062.4375
4
Iteration 24300: Loss = -11062.4365234375
Iteration 24400: Loss = -11062.4375
1
Iteration 24500: Loss = -11062.4375
2
Iteration 24600: Loss = -11062.4384765625
3
Iteration 24700: Loss = -11062.4384765625
4
Iteration 24800: Loss = -11062.4375
5
Iteration 24900: Loss = -11062.4375
6
Iteration 25000: Loss = -11062.4375
7
Iteration 25100: Loss = -11062.4375
8
Iteration 25200: Loss = -11062.4375
9
Iteration 25300: Loss = -11062.4375
10
Iteration 25400: Loss = -11062.4365234375
Iteration 25500: Loss = -11062.4375
1
Iteration 25600: Loss = -11062.4375
2
Iteration 25700: Loss = -11062.4384765625
3
Iteration 25800: Loss = -11062.4375
4
Iteration 25900: Loss = -11062.4375
5
Iteration 26000: Loss = -11062.4384765625
6
Iteration 26100: Loss = -11062.4375
7
Iteration 26200: Loss = -11062.4365234375
Iteration 26300: Loss = -11062.4365234375
Iteration 26400: Loss = -11062.4365234375
Iteration 26500: Loss = -11062.4375
1
Iteration 26600: Loss = -11062.4365234375
Iteration 26700: Loss = -11062.4375
1
Iteration 26800: Loss = -11062.4375
2
Iteration 26900: Loss = -11062.4375
3
Iteration 27000: Loss = -11062.4365234375
Iteration 27100: Loss = -11062.4365234375
Iteration 27200: Loss = -11062.4375
1
Iteration 27300: Loss = -11062.4375
2
Iteration 27400: Loss = -11062.4365234375
Iteration 27500: Loss = -11062.439453125
1
Iteration 27600: Loss = -11062.4375
2
Iteration 27700: Loss = -11062.4365234375
Iteration 27800: Loss = -11062.4375
1
Iteration 27900: Loss = -11062.4365234375
Iteration 28000: Loss = -11062.4375
1
Iteration 28100: Loss = -11062.4375
2
Iteration 28200: Loss = -11062.4404296875
3
Iteration 28300: Loss = -11062.4365234375
Iteration 28400: Loss = -11062.4365234375
Iteration 28500: Loss = -11062.4384765625
1
Iteration 28600: Loss = -11062.4365234375
Iteration 28700: Loss = -11062.4375
1
Iteration 28800: Loss = -11062.4365234375
Iteration 28900: Loss = -11062.4375
1
Iteration 29000: Loss = -11062.4375
2
Iteration 29100: Loss = -11062.4375
3
Iteration 29200: Loss = -11062.4375
4
Iteration 29300: Loss = -11062.4365234375
Iteration 29400: Loss = -11062.4365234375
Iteration 29500: Loss = -11062.4365234375
Iteration 29600: Loss = -11062.4384765625
1
Iteration 29700: Loss = -11062.4375
2
Iteration 29800: Loss = -11062.4365234375
Iteration 29900: Loss = -11062.4375
1
pi: tensor([[1.3034e-07, 1.0000e+00],
        [3.9854e-01, 6.0146e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.9425e-06, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2671, 0.1288],
         [0.0957, 0.1728]],

        [[0.5120, 0.1085],
         [0.9524, 0.0134]],

        [[0.0632, 0.3229],
         [0.5232, 0.2080]],

        [[0.6778, 0.0985],
         [0.8666, 0.1869]],

        [[0.9867, 0.1027],
         [0.0345, 0.1625]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 71
Adjusted Rand Index: 0.1693792917298064
Global Adjusted Rand Index: 0.061729964896963564
Average Adjusted Rand Index: 0.32211542537984317
[-0.0003203894517489664, 0.061729964896963564] [0.00025988589812057233, 0.32211542537984317] [11078.7734375, 11062.4365234375]
-------------------------------------
This iteration is 70
True Objective function: Loss = -10869.629594230853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33109.6171875
Iteration 100: Loss = -21196.58203125
Iteration 200: Loss = -13477.3076171875
Iteration 300: Loss = -11883.26171875
Iteration 400: Loss = -11448.2666015625
Iteration 500: Loss = -11271.087890625
Iteration 600: Loss = -11204.66015625
Iteration 700: Loss = -11164.74609375
Iteration 800: Loss = -11137.13671875
Iteration 900: Loss = -11123.0625
Iteration 1000: Loss = -11111.37890625
Iteration 1100: Loss = -11103.8291015625
Iteration 1200: Loss = -11097.9423828125
Iteration 1300: Loss = -11093.154296875
Iteration 1400: Loss = -11089.068359375
Iteration 1500: Loss = -11084.291015625
Iteration 1600: Loss = -11080.44140625
Iteration 1700: Loss = -11077.0732421875
Iteration 1800: Loss = -11072.9716796875
Iteration 1900: Loss = -11068.5546875
Iteration 2000: Loss = -11058.865234375
Iteration 2100: Loss = -11054.0087890625
Iteration 2200: Loss = -11050.322265625
Iteration 2300: Loss = -11047.5205078125
Iteration 2400: Loss = -11044.78515625
Iteration 2500: Loss = -11042.74609375
Iteration 2600: Loss = -11041.091796875
Iteration 2700: Loss = -11037.8779296875
Iteration 2800: Loss = -11035.216796875
Iteration 2900: Loss = -11033.986328125
Iteration 3000: Loss = -11033.0732421875
Iteration 3100: Loss = -11031.8896484375
Iteration 3200: Loss = -11029.1455078125
Iteration 3300: Loss = -11027.734375
Iteration 3400: Loss = -11026.9609375
Iteration 3500: Loss = -11026.3203125
Iteration 3600: Loss = -11025.716796875
Iteration 3700: Loss = -11025.1123046875
Iteration 3800: Loss = -11024.455078125
Iteration 3900: Loss = -11023.4482421875
Iteration 4000: Loss = -11022.4072265625
Iteration 4100: Loss = -11021.88671875
Iteration 4200: Loss = -11021.5498046875
Iteration 4300: Loss = -11021.26953125
Iteration 4400: Loss = -11021.0078125
Iteration 4500: Loss = -11019.9189453125
Iteration 4600: Loss = -11017.3388671875
Iteration 4700: Loss = -11015.474609375
Iteration 4800: Loss = -11014.80078125
Iteration 4900: Loss = -11014.3994140625
Iteration 5000: Loss = -11014.083984375
Iteration 5100: Loss = -11013.8095703125
Iteration 5200: Loss = -11013.4775390625
Iteration 5300: Loss = -11012.9501953125
Iteration 5400: Loss = -11012.529296875
Iteration 5500: Loss = -11012.291015625
Iteration 5600: Loss = -11012.1298828125
Iteration 5700: Loss = -11012.001953125
Iteration 5800: Loss = -11011.8916015625
Iteration 5900: Loss = -11011.7958984375
Iteration 6000: Loss = -11011.7080078125
Iteration 6100: Loss = -11011.625
Iteration 6200: Loss = -11011.544921875
Iteration 6300: Loss = -11011.4609375
Iteration 6400: Loss = -11011.3779296875
Iteration 6500: Loss = -11011.2998046875
Iteration 6600: Loss = -11011.1142578125
Iteration 6700: Loss = -11011.0400390625
Iteration 6800: Loss = -11010.9775390625
Iteration 6900: Loss = -11008.5341796875
Iteration 7000: Loss = -11007.7783203125
Iteration 7100: Loss = -11007.6630859375
Iteration 7200: Loss = -11007.5908203125
Iteration 7300: Loss = -11007.537109375
Iteration 7400: Loss = -11007.4912109375
Iteration 7500: Loss = -11007.453125
Iteration 7600: Loss = -11007.4189453125
Iteration 7700: Loss = -11007.3876953125
Iteration 7800: Loss = -11007.3603515625
Iteration 7900: Loss = -11007.3349609375
Iteration 8000: Loss = -11007.310546875
Iteration 8100: Loss = -11007.287109375
Iteration 8200: Loss = -11007.265625
Iteration 8300: Loss = -11007.2451171875
Iteration 8400: Loss = -11007.224609375
Iteration 8500: Loss = -11007.2021484375
Iteration 8600: Loss = -11007.1796875
Iteration 8700: Loss = -11007.1513671875
Iteration 8800: Loss = -11007.11328125
Iteration 8900: Loss = -11007.060546875
Iteration 9000: Loss = -11006.99609375
Iteration 9100: Loss = -11006.9443359375
Iteration 9200: Loss = -11006.9140625
Iteration 9300: Loss = -11006.8984375
Iteration 9400: Loss = -11006.8837890625
Iteration 9500: Loss = -11006.859375
Iteration 9600: Loss = -11006.8447265625
Iteration 9700: Loss = -11006.833984375
Iteration 9800: Loss = -11006.8232421875
Iteration 9900: Loss = -11006.8125
Iteration 10000: Loss = -11006.8017578125
Iteration 10100: Loss = -11006.7890625
Iteration 10200: Loss = -11006.7763671875
Iteration 10300: Loss = -11006.759765625
Iteration 10400: Loss = -11006.7470703125
Iteration 10500: Loss = -11006.736328125
Iteration 10600: Loss = -11006.7275390625
Iteration 10700: Loss = -11006.7177734375
Iteration 10800: Loss = -11006.7099609375
Iteration 10900: Loss = -11006.69921875
Iteration 11000: Loss = -11006.6904296875
Iteration 11100: Loss = -11006.681640625
Iteration 11200: Loss = -11006.6748046875
Iteration 11300: Loss = -11006.6669921875
Iteration 11400: Loss = -11006.6572265625
Iteration 11500: Loss = -11006.650390625
Iteration 11600: Loss = -11006.642578125
Iteration 11700: Loss = -11006.6357421875
Iteration 11800: Loss = -11006.6279296875
Iteration 11900: Loss = -11006.6220703125
Iteration 12000: Loss = -11006.615234375
Iteration 12100: Loss = -11006.609375
Iteration 12200: Loss = -11006.6044921875
Iteration 12300: Loss = -11006.5986328125
Iteration 12400: Loss = -11006.59375
Iteration 12500: Loss = -11006.591796875
Iteration 12600: Loss = -11006.5859375
Iteration 12700: Loss = -11006.580078125
Iteration 12800: Loss = -11006.572265625
Iteration 12900: Loss = -11006.5634765625
Iteration 13000: Loss = -11006.5439453125
Iteration 13100: Loss = -11006.5
Iteration 13200: Loss = -11006.46875
Iteration 13300: Loss = -11006.4404296875
Iteration 13400: Loss = -11006.3701171875
Iteration 13500: Loss = -11006.3681640625
Iteration 13600: Loss = -11006.3671875
Iteration 13700: Loss = -11006.3681640625
1
Iteration 13800: Loss = -11006.3662109375
Iteration 13900: Loss = -11006.3642578125
Iteration 14000: Loss = -11006.3662109375
1
Iteration 14100: Loss = -11006.365234375
2
Iteration 14200: Loss = -11006.36328125
Iteration 14300: Loss = -11006.36328125
Iteration 14400: Loss = -11006.3623046875
Iteration 14500: Loss = -11006.361328125
Iteration 14600: Loss = -11006.359375
Iteration 14700: Loss = -11006.3603515625
1
Iteration 14800: Loss = -11006.359375
Iteration 14900: Loss = -11006.357421875
Iteration 15000: Loss = -11006.36328125
1
Iteration 15100: Loss = -11006.3583984375
2
Iteration 15200: Loss = -11006.3564453125
Iteration 15300: Loss = -11006.3564453125
Iteration 15400: Loss = -11006.365234375
1
Iteration 15500: Loss = -11006.3564453125
Iteration 15600: Loss = -11006.357421875
1
Iteration 15700: Loss = -11006.3564453125
Iteration 15800: Loss = -11006.3544921875
Iteration 15900: Loss = -11006.35546875
1
Iteration 16000: Loss = -11006.35546875
2
Iteration 16100: Loss = -11006.35546875
3
Iteration 16200: Loss = -11006.3544921875
Iteration 16300: Loss = -11006.3544921875
Iteration 16400: Loss = -11006.353515625
Iteration 16500: Loss = -11006.3544921875
1
Iteration 16600: Loss = -11006.3525390625
Iteration 16700: Loss = -11006.353515625
1
Iteration 16800: Loss = -11006.3525390625
Iteration 16900: Loss = -11006.3525390625
Iteration 17000: Loss = -11006.353515625
1
Iteration 17100: Loss = -11006.353515625
2
Iteration 17200: Loss = -11006.3525390625
Iteration 17300: Loss = -11006.3525390625
Iteration 17400: Loss = -11006.3515625
Iteration 17500: Loss = -11006.3525390625
1
Iteration 17600: Loss = -11006.3515625
Iteration 17700: Loss = -11006.3515625
Iteration 17800: Loss = -11006.3515625
Iteration 17900: Loss = -11006.349609375
Iteration 18000: Loss = -11006.3486328125
Iteration 18100: Loss = -11006.3486328125
Iteration 18200: Loss = -11006.349609375
1
Iteration 18300: Loss = -11006.3515625
2
Iteration 18400: Loss = -11006.3486328125
Iteration 18500: Loss = -11006.3486328125
Iteration 18600: Loss = -11006.3486328125
Iteration 18700: Loss = -11006.34765625
Iteration 18800: Loss = -11006.3486328125
1
Iteration 18900: Loss = -11006.34765625
Iteration 19000: Loss = -11006.34765625
Iteration 19100: Loss = -11006.3486328125
1
Iteration 19200: Loss = -11006.3486328125
2
Iteration 19300: Loss = -11006.3486328125
3
Iteration 19400: Loss = -11006.3486328125
4
Iteration 19500: Loss = -11006.34765625
Iteration 19600: Loss = -11006.3486328125
1
Iteration 19700: Loss = -11006.34765625
Iteration 19800: Loss = -11006.3466796875
Iteration 19900: Loss = -11006.3486328125
1
Iteration 20000: Loss = -11006.3486328125
2
Iteration 20100: Loss = -11006.34765625
3
Iteration 20200: Loss = -11006.3466796875
Iteration 20300: Loss = -11006.3466796875
Iteration 20400: Loss = -11006.3486328125
1
Iteration 20500: Loss = -11006.34765625
2
Iteration 20600: Loss = -11006.34765625
3
Iteration 20700: Loss = -11006.34765625
4
Iteration 20800: Loss = -11006.3466796875
Iteration 20900: Loss = -11006.3466796875
Iteration 21000: Loss = -11006.34375
Iteration 21100: Loss = -11006.296875
Iteration 21200: Loss = -11006.2431640625
Iteration 21300: Loss = -11006.1806640625
Iteration 21400: Loss = -11006.1484375
Iteration 21500: Loss = -11006.1435546875
Iteration 21600: Loss = -11006.1435546875
Iteration 21700: Loss = -11006.140625
Iteration 21800: Loss = -11006.1376953125
Iteration 21900: Loss = -11006.1298828125
Iteration 22000: Loss = -11006.12109375
Iteration 22100: Loss = -11006.1201171875
Iteration 22200: Loss = -11006.12109375
1
Iteration 22300: Loss = -11006.119140625
Iteration 22400: Loss = -11006.119140625
Iteration 22500: Loss = -11006.119140625
Iteration 22600: Loss = -11006.1201171875
1
Iteration 22700: Loss = -11006.119140625
Iteration 22800: Loss = -11006.1201171875
1
Iteration 22900: Loss = -11006.1201171875
2
Iteration 23000: Loss = -11006.119140625
Iteration 23100: Loss = -11006.1201171875
1
Iteration 23200: Loss = -11006.119140625
Iteration 23300: Loss = -11006.1201171875
1
Iteration 23400: Loss = -11006.1201171875
2
Iteration 23500: Loss = -11006.119140625
Iteration 23600: Loss = -11006.1181640625
Iteration 23700: Loss = -11006.111328125
Iteration 23800: Loss = -11006.009765625
Iteration 23900: Loss = -11006.0078125
Iteration 24000: Loss = -11006.0107421875
1
Iteration 24100: Loss = -11006.009765625
2
Iteration 24200: Loss = -11006.0087890625
3
Iteration 24300: Loss = -11006.0087890625
4
Iteration 24400: Loss = -11006.0107421875
5
Iteration 24500: Loss = -11006.009765625
6
Iteration 24600: Loss = -11006.0107421875
7
Iteration 24700: Loss = -11006.0087890625
8
Iteration 24800: Loss = -11006.0087890625
9
Iteration 24900: Loss = -11006.009765625
10
Iteration 25000: Loss = -11006.0107421875
11
Iteration 25100: Loss = -11006.009765625
12
Iteration 25200: Loss = -11006.0087890625
13
Iteration 25300: Loss = -11006.009765625
14
Iteration 25400: Loss = -11006.009765625
15
Stopping early at iteration 25400 due to no improvement.
pi: tensor([[0.0068, 0.9932],
        [0.0078, 0.9922]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9501, 0.0499], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1635, 0.2109],
         [0.9906, 0.1631]],

        [[0.6389, 0.1016],
         [0.9683, 0.0415]],

        [[0.0219, 0.5611],
         [0.5841, 0.2905]],

        [[0.6345, 0.0494],
         [0.9906, 0.3103]],

        [[0.0087, 0.1100],
         [0.5880, 0.9381]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0028065860232498806
Average Adjusted Rand Index: 0.0010907143204947167
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33494.1328125
Iteration 100: Loss = -20417.578125
Iteration 200: Loss = -14139.0615234375
Iteration 300: Loss = -12224.1484375
Iteration 400: Loss = -11540.8388671875
Iteration 500: Loss = -11304.1396484375
Iteration 600: Loss = -11215.68359375
Iteration 700: Loss = -11161.36328125
Iteration 800: Loss = -11121.310546875
Iteration 900: Loss = -11100.76171875
Iteration 1000: Loss = -11087.8369140625
Iteration 1100: Loss = -11073.7255859375
Iteration 1200: Loss = -11066.2783203125
Iteration 1300: Loss = -11059.0390625
Iteration 1400: Loss = -11049.337890625
Iteration 1500: Loss = -11043.23046875
Iteration 1600: Loss = -11039.3896484375
Iteration 1700: Loss = -11035.5771484375
Iteration 1800: Loss = -11032.873046875
Iteration 1900: Loss = -11030.8984375
Iteration 2000: Loss = -11028.0849609375
Iteration 2100: Loss = -11024.2900390625
Iteration 2200: Loss = -11022.97265625
Iteration 2300: Loss = -11021.93359375
Iteration 2400: Loss = -11021.0595703125
Iteration 2500: Loss = -11017.43359375
Iteration 2600: Loss = -11016.3603515625
Iteration 2700: Loss = -11015.6396484375
Iteration 2800: Loss = -11015.033203125
Iteration 2900: Loss = -11014.5048828125
Iteration 3000: Loss = -11014.041015625
Iteration 3100: Loss = -11013.625
Iteration 3200: Loss = -11013.255859375
Iteration 3300: Loss = -11012.9169921875
Iteration 3400: Loss = -11012.6103515625
Iteration 3500: Loss = -11012.3330078125
Iteration 3600: Loss = -11012.0810546875
Iteration 3700: Loss = -11011.849609375
Iteration 3800: Loss = -11011.6376953125
Iteration 3900: Loss = -11011.439453125
Iteration 4000: Loss = -11011.2587890625
Iteration 4100: Loss = -11011.0888671875
Iteration 4200: Loss = -11010.9306640625
Iteration 4300: Loss = -11010.7822265625
Iteration 4400: Loss = -11010.64453125
Iteration 4500: Loss = -11010.5146484375
Iteration 4600: Loss = -11010.392578125
Iteration 4700: Loss = -11010.2783203125
Iteration 4800: Loss = -11010.1689453125
Iteration 4900: Loss = -11010.06640625
Iteration 5000: Loss = -11009.966796875
Iteration 5100: Loss = -11009.8720703125
Iteration 5200: Loss = -11009.77734375
Iteration 5300: Loss = -11009.6884765625
Iteration 5400: Loss = -11009.59765625
Iteration 5500: Loss = -11009.5107421875
Iteration 5600: Loss = -11009.41796875
Iteration 5700: Loss = -11009.32421875
Iteration 5800: Loss = -11009.2275390625
Iteration 5900: Loss = -11009.1298828125
Iteration 6000: Loss = -11009.0322265625
Iteration 6100: Loss = -11008.9365234375
Iteration 6200: Loss = -11008.84765625
Iteration 6300: Loss = -11008.767578125
Iteration 6400: Loss = -11008.693359375
Iteration 6500: Loss = -11008.623046875
Iteration 6600: Loss = -11008.5546875
Iteration 6700: Loss = -11008.4921875
Iteration 6800: Loss = -11008.4306640625
Iteration 6900: Loss = -11008.3681640625
Iteration 7000: Loss = -11008.3056640625
Iteration 7100: Loss = -11008.244140625
Iteration 7200: Loss = -11008.17578125
Iteration 7300: Loss = -11008.1103515625
Iteration 7400: Loss = -11008.0419921875
Iteration 7500: Loss = -11007.966796875
Iteration 7600: Loss = -11007.8876953125
Iteration 7700: Loss = -11007.7978515625
Iteration 7800: Loss = -11007.6953125
Iteration 7900: Loss = -11007.556640625
Iteration 8000: Loss = -11007.2236328125
Iteration 8100: Loss = -11002.3232421875
Iteration 8200: Loss = -10996.5263671875
Iteration 8300: Loss = -10991.3720703125
Iteration 8400: Loss = -10988.470703125
Iteration 8500: Loss = -10980.4443359375
Iteration 8600: Loss = -10976.5576171875
Iteration 8700: Loss = -10970.2080078125
Iteration 8800: Loss = -10966.810546875
Iteration 8900: Loss = -10966.1455078125
Iteration 9000: Loss = -10965.083984375
Iteration 9100: Loss = -10963.87109375
Iteration 9200: Loss = -10962.1025390625
Iteration 9300: Loss = -10960.51171875
Iteration 9400: Loss = -10959.435546875
Iteration 9500: Loss = -10959.2099609375
Iteration 9600: Loss = -10958.8134765625
Iteration 9700: Loss = -10958.490234375
Iteration 9800: Loss = -10958.357421875
Iteration 9900: Loss = -10958.1748046875
Iteration 10000: Loss = -10957.4501953125
Iteration 10100: Loss = -10956.7265625
Iteration 10200: Loss = -10955.255859375
Iteration 10300: Loss = -10946.3173828125
Iteration 10400: Loss = -10937.0244140625
Iteration 10500: Loss = -10934.482421875
Iteration 10600: Loss = -10932.796875
Iteration 10700: Loss = -10931.7861328125
Iteration 10800: Loss = -10918.9853515625
Iteration 10900: Loss = -10891.4873046875
Iteration 11000: Loss = -10884.4638671875
Iteration 11100: Loss = -10878.5322265625
Iteration 11200: Loss = -10872.306640625
Iteration 11300: Loss = -10860.3037109375
Iteration 11400: Loss = -10856.8603515625
Iteration 11500: Loss = -10848.802734375
Iteration 11600: Loss = -10847.748046875
Iteration 11700: Loss = -10847.33203125
Iteration 11800: Loss = -10847.2333984375
Iteration 11900: Loss = -10847.1748046875
Iteration 12000: Loss = -10847.14453125
Iteration 12100: Loss = -10847.1279296875
Iteration 12200: Loss = -10847.115234375
Iteration 12300: Loss = -10847.10546875
Iteration 12400: Loss = -10847.0966796875
Iteration 12500: Loss = -10847.0888671875
Iteration 12600: Loss = -10847.0830078125
Iteration 12700: Loss = -10847.076171875
Iteration 12800: Loss = -10847.068359375
Iteration 12900: Loss = -10847.041015625
Iteration 13000: Loss = -10846.8486328125
Iteration 13100: Loss = -10846.845703125
Iteration 13200: Loss = -10846.841796875
Iteration 13300: Loss = -10846.8388671875
Iteration 13400: Loss = -10846.8173828125
Iteration 13500: Loss = -10846.814453125
Iteration 13600: Loss = -10846.8095703125
Iteration 13700: Loss = -10846.7958984375
Iteration 13800: Loss = -10846.7939453125
Iteration 13900: Loss = -10846.748046875
Iteration 14000: Loss = -10845.904296875
Iteration 14100: Loss = -10845.89453125
Iteration 14200: Loss = -10845.8876953125
Iteration 14300: Loss = -10845.88671875
Iteration 14400: Loss = -10845.8837890625
Iteration 14500: Loss = -10845.8818359375
Iteration 14600: Loss = -10845.8818359375
Iteration 14700: Loss = -10845.8818359375
Iteration 14800: Loss = -10845.8798828125
Iteration 14900: Loss = -10845.87890625
Iteration 15000: Loss = -10845.87890625
Iteration 15100: Loss = -10845.8740234375
Iteration 15200: Loss = -10845.869140625
Iteration 15300: Loss = -10845.8515625
Iteration 15400: Loss = -10845.8515625
Iteration 15500: Loss = -10845.8515625
Iteration 15600: Loss = -10845.8505859375
Iteration 15700: Loss = -10845.8505859375
Iteration 15800: Loss = -10845.849609375
Iteration 15900: Loss = -10845.8486328125
Iteration 16000: Loss = -10845.8486328125
Iteration 16100: Loss = -10845.84765625
Iteration 16200: Loss = -10845.8486328125
1
Iteration 16300: Loss = -10845.8466796875
Iteration 16400: Loss = -10845.84765625
1
Iteration 16500: Loss = -10845.84765625
2
Iteration 16600: Loss = -10845.8466796875
Iteration 16700: Loss = -10845.84375
Iteration 16800: Loss = -10845.84375
Iteration 16900: Loss = -10845.8427734375
Iteration 17000: Loss = -10845.8427734375
Iteration 17100: Loss = -10845.841796875
Iteration 17200: Loss = -10845.8427734375
1
Iteration 17300: Loss = -10845.841796875
Iteration 17400: Loss = -10845.841796875
Iteration 17500: Loss = -10845.8408203125
Iteration 17600: Loss = -10845.841796875
1
Iteration 17700: Loss = -10845.8408203125
Iteration 17800: Loss = -10845.8408203125
Iteration 17900: Loss = -10845.8388671875
Iteration 18000: Loss = -10845.83984375
1
Iteration 18100: Loss = -10845.8369140625
Iteration 18200: Loss = -10845.8310546875
Iteration 18300: Loss = -10845.83203125
1
Iteration 18400: Loss = -10845.8310546875
Iteration 18500: Loss = -10845.83203125
1
Iteration 18600: Loss = -10845.8310546875
Iteration 18700: Loss = -10845.8310546875
Iteration 18800: Loss = -10845.830078125
Iteration 18900: Loss = -10845.8310546875
1
Iteration 19000: Loss = -10845.8310546875
2
Iteration 19100: Loss = -10845.8310546875
3
Iteration 19200: Loss = -10845.8310546875
4
Iteration 19300: Loss = -10845.8310546875
5
Iteration 19400: Loss = -10845.830078125
Iteration 19500: Loss = -10845.8310546875
1
Iteration 19600: Loss = -10845.8310546875
2
Iteration 19700: Loss = -10845.693359375
Iteration 19800: Loss = -10845.59765625
Iteration 19900: Loss = -10845.59765625
Iteration 20000: Loss = -10845.599609375
1
Iteration 20100: Loss = -10845.59765625
Iteration 20200: Loss = -10845.3623046875
Iteration 20300: Loss = -10845.353515625
Iteration 20400: Loss = -10845.353515625
Iteration 20500: Loss = -10845.3505859375
Iteration 20600: Loss = -10845.3515625
1
Iteration 20700: Loss = -10845.3505859375
Iteration 20800: Loss = -10845.3505859375
Iteration 20900: Loss = -10845.349609375
Iteration 21000: Loss = -10845.3505859375
1
Iteration 21100: Loss = -10845.3505859375
2
Iteration 21200: Loss = -10845.349609375
Iteration 21300: Loss = -10845.349609375
Iteration 21400: Loss = -10845.349609375
Iteration 21500: Loss = -10845.349609375
Iteration 21600: Loss = -10845.34765625
Iteration 21700: Loss = -10842.142578125
Iteration 21800: Loss = -10842.1328125
Iteration 21900: Loss = -10842.1298828125
Iteration 22000: Loss = -10842.130859375
1
Iteration 22100: Loss = -10842.12890625
Iteration 22200: Loss = -10842.1298828125
1
Iteration 22300: Loss = -10842.1298828125
2
Iteration 22400: Loss = -10842.12890625
Iteration 22500: Loss = -10842.126953125
Iteration 22600: Loss = -10842.126953125
Iteration 22700: Loss = -10842.1279296875
1
Iteration 22800: Loss = -10842.1279296875
2
Iteration 22900: Loss = -10842.126953125
Iteration 23000: Loss = -10842.1240234375
Iteration 23100: Loss = -10842.107421875
Iteration 23200: Loss = -10841.998046875
Iteration 23300: Loss = -10841.96875
Iteration 23400: Loss = -10841.9404296875
Iteration 23500: Loss = -10841.9404296875
Iteration 23600: Loss = -10841.8359375
Iteration 23700: Loss = -10841.4736328125
Iteration 23800: Loss = -10839.48828125
Iteration 23900: Loss = -10839.486328125
Iteration 24000: Loss = -10839.4873046875
1
Iteration 24100: Loss = -10839.486328125
Iteration 24200: Loss = -10839.4853515625
Iteration 24300: Loss = -10839.486328125
1
Iteration 24400: Loss = -10839.486328125
2
Iteration 24500: Loss = -10839.484375
Iteration 24600: Loss = -10839.486328125
1
Iteration 24700: Loss = -10839.484375
Iteration 24800: Loss = -10839.484375
Iteration 24900: Loss = -10839.4140625
Iteration 25000: Loss = -10839.2587890625
Iteration 25100: Loss = -10839.255859375
Iteration 25200: Loss = -10839.2041015625
Iteration 25300: Loss = -10839.203125
Iteration 25400: Loss = -10839.203125
Iteration 25500: Loss = -10839.2021484375
Iteration 25600: Loss = -10839.201171875
Iteration 25700: Loss = -10835.49609375
Iteration 25800: Loss = -10835.4794921875
Iteration 25900: Loss = -10835.4755859375
Iteration 26000: Loss = -10835.474609375
Iteration 26100: Loss = -10835.474609375
Iteration 26200: Loss = -10835.412109375
Iteration 26300: Loss = -10835.412109375
Iteration 26400: Loss = -10835.412109375
Iteration 26500: Loss = -10835.412109375
Iteration 26600: Loss = -10835.4111328125
Iteration 26700: Loss = -10835.412109375
1
Iteration 26800: Loss = -10835.4072265625
Iteration 26900: Loss = -10835.408203125
1
Iteration 27000: Loss = -10835.4052734375
Iteration 27100: Loss = -10835.3662109375
Iteration 27200: Loss = -10835.3623046875
Iteration 27300: Loss = -10835.3583984375
Iteration 27400: Loss = -10835.32421875
Iteration 27500: Loss = -10833.908203125
Iteration 27600: Loss = -10833.9072265625
Iteration 27700: Loss = -10833.90625
Iteration 27800: Loss = -10833.9072265625
1
Iteration 27900: Loss = -10833.9072265625
2
Iteration 28000: Loss = -10833.9072265625
3
Iteration 28100: Loss = -10833.90625
Iteration 28200: Loss = -10833.90625
Iteration 28300: Loss = -10833.90625
Iteration 28400: Loss = -10833.9052734375
Iteration 28500: Loss = -10833.9033203125
Iteration 28600: Loss = -10833.904296875
1
Iteration 28700: Loss = -10833.7041015625
Iteration 28800: Loss = -10833.3564453125
Iteration 28900: Loss = -10833.357421875
1
Iteration 29000: Loss = -10833.349609375
Iteration 29100: Loss = -10833.34375
Iteration 29200: Loss = -10833.3427734375
Iteration 29300: Loss = -10833.3427734375
Iteration 29400: Loss = -10833.34375
1
Iteration 29500: Loss = -10833.337890625
Iteration 29600: Loss = -10833.328125
Iteration 29700: Loss = -10833.31640625
Iteration 29800: Loss = -10833.3154296875
Iteration 29900: Loss = -10833.3154296875
pi: tensor([[0.7327, 0.2673],
        [0.2419, 0.7581]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4198, 0.5802], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1863, 0.0944],
         [0.0244, 0.2544]],

        [[0.2116, 0.1051],
         [0.8473, 0.0144]],

        [[0.9697, 0.0975],
         [0.6706, 0.8292]],

        [[0.8071, 0.0917],
         [0.8163, 0.6956]],

        [[0.9616, 0.1002],
         [0.1278, 0.0701]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7720044903306482
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
Global Adjusted Rand Index: 0.8909058618272272
Average Adjusted Rand Index: 0.892172235569989
[-0.0028065860232498806, 0.8909058618272272] [0.0010907143204947167, 0.892172235569989] [11006.009765625, 10833.294921875]
-------------------------------------
This iteration is 71
True Objective function: Loss = -10878.406166656672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21186.97265625
Iteration 100: Loss = -14202.009765625
Iteration 200: Loss = -11680.8095703125
Iteration 300: Loss = -11108.5
Iteration 400: Loss = -10996.1513671875
Iteration 500: Loss = -10971.26171875
Iteration 600: Loss = -10959.783203125
Iteration 700: Loss = -10952.2294921875
Iteration 800: Loss = -10948.68359375
Iteration 900: Loss = -10946.6005859375
Iteration 1000: Loss = -10945.15234375
Iteration 1100: Loss = -10943.5283203125
Iteration 1200: Loss = -10941.4658203125
Iteration 1300: Loss = -10939.8203125
Iteration 1400: Loss = -10938.111328125
Iteration 1500: Loss = -10936.72265625
Iteration 1600: Loss = -10936.1318359375
Iteration 1700: Loss = -10935.546875
Iteration 1800: Loss = -10934.9892578125
Iteration 1900: Loss = -10934.68359375
Iteration 2000: Loss = -10934.412109375
Iteration 2100: Loss = -10934.15234375
Iteration 2200: Loss = -10933.794921875
Iteration 2300: Loss = -10932.90234375
Iteration 2400: Loss = -10930.8818359375
Iteration 2500: Loss = -10929.2685546875
Iteration 2600: Loss = -10928.61328125
Iteration 2700: Loss = -10928.3232421875
Iteration 2800: Loss = -10928.146484375
Iteration 2900: Loss = -10928.0166015625
Iteration 3000: Loss = -10927.9140625
Iteration 3100: Loss = -10927.828125
Iteration 3200: Loss = -10927.7568359375
Iteration 3300: Loss = -10927.6943359375
Iteration 3400: Loss = -10927.6416015625
Iteration 3500: Loss = -10927.59375
Iteration 3600: Loss = -10927.55078125
Iteration 3700: Loss = -10927.5126953125
Iteration 3800: Loss = -10927.4765625
Iteration 3900: Loss = -10927.4453125
Iteration 4000: Loss = -10927.4150390625
Iteration 4100: Loss = -10927.3896484375
Iteration 4200: Loss = -10927.36328125
Iteration 4300: Loss = -10927.33984375
Iteration 4400: Loss = -10927.3173828125
Iteration 4500: Loss = -10927.2958984375
Iteration 4600: Loss = -10927.2763671875
Iteration 4700: Loss = -10927.255859375
Iteration 4800: Loss = -10927.236328125
Iteration 4900: Loss = -10927.21875
Iteration 5000: Loss = -10927.2001953125
Iteration 5100: Loss = -10927.18359375
Iteration 5200: Loss = -10927.1650390625
Iteration 5300: Loss = -10927.1484375
Iteration 5400: Loss = -10927.1318359375
Iteration 5500: Loss = -10927.1162109375
Iteration 5600: Loss = -10927.0986328125
Iteration 5700: Loss = -10927.078125
Iteration 5800: Loss = -10927.0263671875
Iteration 5900: Loss = -10922.548828125
Iteration 6000: Loss = -10916.4853515625
Iteration 6100: Loss = -10914.951171875
Iteration 6200: Loss = -10914.7783203125
Iteration 6300: Loss = -10914.7197265625
Iteration 6400: Loss = -10914.6337890625
Iteration 6500: Loss = -10914.5810546875
Iteration 6600: Loss = -10914.4775390625
Iteration 6700: Loss = -10914.037109375
Iteration 6800: Loss = -10911.8154296875
Iteration 6900: Loss = -10910.458984375
Iteration 7000: Loss = -10909.0810546875
Iteration 7100: Loss = -10902.873046875
Iteration 7200: Loss = -10898.189453125
Iteration 7300: Loss = -10896.0908203125
Iteration 7400: Loss = -10893.4794921875
Iteration 7500: Loss = -10891.3662109375
Iteration 7600: Loss = -10886.1826171875
Iteration 7700: Loss = -10886.107421875
Iteration 7800: Loss = -10885.27734375
Iteration 7900: Loss = -10881.828125
Iteration 8000: Loss = -10881.2568359375
Iteration 8100: Loss = -10881.21875
Iteration 8200: Loss = -10881.20703125
Iteration 8300: Loss = -10879.037109375
Iteration 8400: Loss = -10877.994140625
Iteration 8500: Loss = -10874.0224609375
Iteration 8600: Loss = -10873.978515625
Iteration 8700: Loss = -10873.966796875
Iteration 8800: Loss = -10873.958984375
Iteration 8900: Loss = -10873.9521484375
Iteration 9000: Loss = -10873.8505859375
Iteration 9100: Loss = -10869.2392578125
Iteration 9200: Loss = -10869.091796875
Iteration 9300: Loss = -10868.990234375
Iteration 9400: Loss = -10868.982421875
Iteration 9500: Loss = -10868.9775390625
Iteration 9600: Loss = -10868.970703125
Iteration 9700: Loss = -10868.7275390625
Iteration 9800: Loss = -10868.7109375
Iteration 9900: Loss = -10868.56640625
Iteration 10000: Loss = -10868.564453125
Iteration 10100: Loss = -10868.5625
Iteration 10200: Loss = -10868.5625
Iteration 10300: Loss = -10868.5048828125
Iteration 10400: Loss = -10868.50390625
Iteration 10500: Loss = -10868.501953125
Iteration 10600: Loss = -10868.4794921875
Iteration 10700: Loss = -10868.34375
Iteration 10800: Loss = -10868.310546875
Iteration 10900: Loss = -10868.30859375
Iteration 11000: Loss = -10868.3076171875
Iteration 11100: Loss = -10868.3076171875
Iteration 11200: Loss = -10868.3076171875
Iteration 11300: Loss = -10868.3056640625
Iteration 11400: Loss = -10867.8359375
Iteration 11500: Loss = -10865.8701171875
Iteration 11600: Loss = -10865.861328125
Iteration 11700: Loss = -10865.859375
Iteration 11800: Loss = -10865.8466796875
Iteration 11900: Loss = -10865.654296875
Iteration 12000: Loss = -10865.6513671875
Iteration 12100: Loss = -10865.650390625
Iteration 12200: Loss = -10865.6494140625
Iteration 12300: Loss = -10865.482421875
Iteration 12400: Loss = -10863.5390625
Iteration 12500: Loss = -10863.537109375
Iteration 12600: Loss = -10863.5361328125
Iteration 12700: Loss = -10863.533203125
Iteration 12800: Loss = -10863.404296875
Iteration 12900: Loss = -10863.3701171875
Iteration 13000: Loss = -10863.37109375
1
Iteration 13100: Loss = -10863.3701171875
Iteration 13200: Loss = -10863.3701171875
Iteration 13300: Loss = -10863.369140625
Iteration 13400: Loss = -10863.369140625
Iteration 13500: Loss = -10863.3681640625
Iteration 13600: Loss = -10863.369140625
1
Iteration 13700: Loss = -10863.369140625
2
Iteration 13800: Loss = -10863.3681640625
Iteration 13900: Loss = -10863.3671875
Iteration 14000: Loss = -10863.3671875
Iteration 14100: Loss = -10863.3662109375
Iteration 14200: Loss = -10863.365234375
Iteration 14300: Loss = -10863.3583984375
Iteration 14400: Loss = -10863.357421875
Iteration 14500: Loss = -10863.35546875
Iteration 14600: Loss = -10863.3544921875
Iteration 14700: Loss = -10863.35546875
1
Iteration 14800: Loss = -10863.3564453125
2
Iteration 14900: Loss = -10863.3544921875
Iteration 15000: Loss = -10863.3544921875
Iteration 15100: Loss = -10863.353515625
Iteration 15200: Loss = -10863.3544921875
1
Iteration 15300: Loss = -10863.3544921875
2
Iteration 15400: Loss = -10863.3544921875
3
Iteration 15500: Loss = -10863.353515625
Iteration 15600: Loss = -10863.353515625
Iteration 15700: Loss = -10863.353515625
Iteration 15800: Loss = -10863.353515625
Iteration 15900: Loss = -10863.353515625
Iteration 16000: Loss = -10863.353515625
Iteration 16100: Loss = -10863.353515625
Iteration 16200: Loss = -10863.353515625
Iteration 16300: Loss = -10863.353515625
Iteration 16400: Loss = -10863.353515625
Iteration 16500: Loss = -10863.353515625
Iteration 16600: Loss = -10863.353515625
Iteration 16700: Loss = -10863.353515625
Iteration 16800: Loss = -10863.3525390625
Iteration 16900: Loss = -10863.353515625
1
Iteration 17000: Loss = -10863.353515625
2
Iteration 17100: Loss = -10863.3544921875
3
Iteration 17200: Loss = -10863.3359375
Iteration 17300: Loss = -10863.33203125
Iteration 17400: Loss = -10863.3310546875
Iteration 17500: Loss = -10863.33203125
1
Iteration 17600: Loss = -10863.3330078125
2
Iteration 17700: Loss = -10863.3330078125
3
Iteration 17800: Loss = -10863.33203125
4
Iteration 17900: Loss = -10863.33203125
5
Iteration 18000: Loss = -10863.330078125
Iteration 18100: Loss = -10863.3291015625
Iteration 18200: Loss = -10863.330078125
1
Iteration 18300: Loss = -10863.3291015625
Iteration 18400: Loss = -10863.328125
Iteration 18500: Loss = -10863.328125
Iteration 18600: Loss = -10863.3291015625
1
Iteration 18700: Loss = -10863.3271484375
Iteration 18800: Loss = -10863.328125
1
Iteration 18900: Loss = -10863.328125
2
Iteration 19000: Loss = -10863.328125
3
Iteration 19100: Loss = -10863.3271484375
Iteration 19200: Loss = -10863.328125
1
Iteration 19300: Loss = -10863.328125
2
Iteration 19400: Loss = -10863.3291015625
3
Iteration 19500: Loss = -10863.3291015625
4
Iteration 19600: Loss = -10863.259765625
Iteration 19700: Loss = -10863.259765625
Iteration 19800: Loss = -10863.259765625
Iteration 19900: Loss = -10863.2607421875
1
Iteration 20000: Loss = -10863.259765625
Iteration 20100: Loss = -10863.2607421875
1
Iteration 20200: Loss = -10863.2607421875
2
Iteration 20300: Loss = -10863.2587890625
Iteration 20400: Loss = -10863.2587890625
Iteration 20500: Loss = -10863.2568359375
Iteration 20600: Loss = -10863.2568359375
Iteration 20700: Loss = -10863.2568359375
Iteration 20800: Loss = -10863.2568359375
Iteration 20900: Loss = -10863.2529296875
Iteration 21000: Loss = -10863.251953125
Iteration 21100: Loss = -10863.25
Iteration 21200: Loss = -10863.248046875
Iteration 21300: Loss = -10863.248046875
Iteration 21400: Loss = -10863.24609375
Iteration 21500: Loss = -10863.2470703125
1
Iteration 21600: Loss = -10863.24609375
Iteration 21700: Loss = -10863.24609375
Iteration 21800: Loss = -10863.2470703125
1
Iteration 21900: Loss = -10863.248046875
2
Iteration 22000: Loss = -10863.2451171875
Iteration 22100: Loss = -10863.24609375
1
Iteration 22200: Loss = -10863.24609375
2
Iteration 22300: Loss = -10863.2470703125
3
Iteration 22400: Loss = -10863.2470703125
4
Iteration 22500: Loss = -10863.2470703125
5
Iteration 22600: Loss = -10863.24609375
6
Iteration 22700: Loss = -10863.2412109375
Iteration 22800: Loss = -10863.2412109375
Iteration 22900: Loss = -10863.2373046875
Iteration 23000: Loss = -10863.23828125
1
Iteration 23100: Loss = -10863.23828125
2
Iteration 23200: Loss = -10863.23828125
3
Iteration 23300: Loss = -10863.23828125
4
Iteration 23400: Loss = -10863.23828125
5
Iteration 23500: Loss = -10863.2373046875
Iteration 23600: Loss = -10863.2392578125
1
Iteration 23700: Loss = -10863.23828125
2
Iteration 23800: Loss = -10863.2373046875
Iteration 23900: Loss = -10863.23828125
1
Iteration 24000: Loss = -10863.23828125
2
Iteration 24100: Loss = -10863.2392578125
3
Iteration 24200: Loss = -10863.23828125
4
Iteration 24300: Loss = -10863.2392578125
5
Iteration 24400: Loss = -10863.23828125
6
Iteration 24500: Loss = -10863.2373046875
Iteration 24600: Loss = -10863.232421875
Iteration 24700: Loss = -10863.2314453125
Iteration 24800: Loss = -10863.23046875
Iteration 24900: Loss = -10863.2294921875
Iteration 25000: Loss = -10863.2294921875
Iteration 25100: Loss = -10863.228515625
Iteration 25200: Loss = -10863.2294921875
1
Iteration 25300: Loss = -10863.23046875
2
Iteration 25400: Loss = -10863.2294921875
3
Iteration 25500: Loss = -10863.228515625
Iteration 25600: Loss = -10863.228515625
Iteration 25700: Loss = -10863.228515625
Iteration 25800: Loss = -10863.2294921875
1
Iteration 25900: Loss = -10863.23046875
2
Iteration 26000: Loss = -10863.2294921875
3
Iteration 26100: Loss = -10863.2275390625
Iteration 26200: Loss = -10863.224609375
Iteration 26300: Loss = -10863.2236328125
Iteration 26400: Loss = -10863.2236328125
Iteration 26500: Loss = -10863.2236328125
Iteration 26600: Loss = -10863.22265625
Iteration 26700: Loss = -10863.224609375
1
Iteration 26800: Loss = -10863.2236328125
2
Iteration 26900: Loss = -10863.22265625
Iteration 27000: Loss = -10863.2236328125
1
Iteration 27100: Loss = -10863.2255859375
2
Iteration 27200: Loss = -10863.22265625
Iteration 27300: Loss = -10863.2236328125
1
Iteration 27400: Loss = -10863.2236328125
2
Iteration 27500: Loss = -10863.22265625
Iteration 27600: Loss = -10863.2236328125
1
Iteration 27700: Loss = -10863.2236328125
2
Iteration 27800: Loss = -10863.2236328125
3
Iteration 27900: Loss = -10863.2236328125
4
Iteration 28000: Loss = -10863.2236328125
5
Iteration 28100: Loss = -10863.22265625
Iteration 28200: Loss = -10863.2236328125
1
Iteration 28300: Loss = -10863.22265625
Iteration 28400: Loss = -10863.2236328125
1
Iteration 28500: Loss = -10863.22265625
Iteration 28600: Loss = -10863.224609375
1
Iteration 28700: Loss = -10863.2236328125
2
Iteration 28800: Loss = -10863.22265625
Iteration 28900: Loss = -10863.2236328125
1
Iteration 29000: Loss = -10863.2236328125
2
Iteration 29100: Loss = -10863.224609375
3
Iteration 29200: Loss = -10863.224609375
4
Iteration 29300: Loss = -10863.224609375
5
Iteration 29400: Loss = -10863.22265625
Iteration 29500: Loss = -10863.224609375
1
Iteration 29600: Loss = -10863.2236328125
2
Iteration 29700: Loss = -10863.2236328125
3
Iteration 29800: Loss = -10863.2236328125
4
Iteration 29900: Loss = -10863.2255859375
5
pi: tensor([[0.4445, 0.5555],
        [0.7184, 0.2816]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2088, 0.1021],
         [0.0514, 0.2357]],

        [[0.7510, 0.1078],
         [0.9913, 0.0300]],

        [[0.7607, 0.1056],
         [0.2308, 0.0106]],

        [[0.7924, 0.0897],
         [0.0230, 0.9805]],

        [[0.7744, 0.1026],
         [0.9519, 0.0235]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369480537608971
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 14
Adjusted Rand Index: 0.5138096559663675
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 90
Adjusted Rand Index: 0.6363636363636364
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080875752406894
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 14
Adjusted Rand Index: 0.5135632264964799
Global Adjusted Rand Index: 0.04824381999617845
Average Adjusted Rand Index: 0.641754429565614
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -66547.078125
Iteration 100: Loss = -48344.40625
Iteration 200: Loss = -31253.125
Iteration 300: Loss = -19514.970703125
Iteration 400: Loss = -14258.166015625
Iteration 500: Loss = -12272.4599609375
Iteration 600: Loss = -11504.8994140625
Iteration 700: Loss = -11208.5283203125
Iteration 800: Loss = -11094.3818359375
Iteration 900: Loss = -11048.4609375
Iteration 1000: Loss = -11020.6904296875
Iteration 1100: Loss = -10995.708984375
Iteration 1200: Loss = -10979.412109375
Iteration 1300: Loss = -10971.41796875
Iteration 1400: Loss = -10964.00390625
Iteration 1500: Loss = -10960.572265625
Iteration 1600: Loss = -10957.904296875
Iteration 1700: Loss = -10955.740234375
Iteration 1800: Loss = -10953.943359375
Iteration 1900: Loss = -10952.416015625
Iteration 2000: Loss = -10951.10546875
Iteration 2100: Loss = -10949.9482421875
Iteration 2200: Loss = -10948.892578125
Iteration 2300: Loss = -10947.6826171875
Iteration 2400: Loss = -10940.259765625
Iteration 2500: Loss = -10935.783203125
Iteration 2600: Loss = -10934.486328125
Iteration 2700: Loss = -10933.7021484375
Iteration 2800: Loss = -10933.0888671875
Iteration 2900: Loss = -10932.57421875
Iteration 3000: Loss = -10932.1220703125
Iteration 3100: Loss = -10931.720703125
Iteration 3200: Loss = -10931.357421875
Iteration 3300: Loss = -10931.029296875
Iteration 3400: Loss = -10930.73046875
Iteration 3500: Loss = -10930.4580078125
Iteration 3600: Loss = -10930.205078125
Iteration 3700: Loss = -10929.974609375
Iteration 3800: Loss = -10929.7607421875
Iteration 3900: Loss = -10929.5615234375
Iteration 4000: Loss = -10929.3779296875
Iteration 4100: Loss = -10929.2080078125
Iteration 4200: Loss = -10929.0498046875
Iteration 4300: Loss = -10928.9013671875
Iteration 4400: Loss = -10928.7646484375
Iteration 4500: Loss = -10928.6357421875
Iteration 4600: Loss = -10928.5146484375
Iteration 4700: Loss = -10928.4033203125
Iteration 4800: Loss = -10928.298828125
Iteration 4900: Loss = -10928.1982421875
Iteration 5000: Loss = -10928.10546875
Iteration 5100: Loss = -10928.0166015625
Iteration 5200: Loss = -10927.9375
Iteration 5300: Loss = -10927.8603515625
Iteration 5400: Loss = -10927.787109375
Iteration 5500: Loss = -10927.7197265625
Iteration 5600: Loss = -10927.6552734375
Iteration 5700: Loss = -10927.5947265625
Iteration 5800: Loss = -10927.5380859375
Iteration 5900: Loss = -10927.484375
Iteration 6000: Loss = -10927.43359375
Iteration 6100: Loss = -10927.384765625
Iteration 6200: Loss = -10927.33984375
Iteration 6300: Loss = -10927.2958984375
Iteration 6400: Loss = -10927.2568359375
Iteration 6500: Loss = -10927.216796875
Iteration 6600: Loss = -10927.1806640625
Iteration 6700: Loss = -10927.1484375
Iteration 6800: Loss = -10927.1142578125
Iteration 6900: Loss = -10927.0830078125
Iteration 7000: Loss = -10927.052734375
Iteration 7100: Loss = -10927.025390625
Iteration 7200: Loss = -10927.0
Iteration 7300: Loss = -10926.9736328125
Iteration 7400: Loss = -10926.951171875
Iteration 7500: Loss = -10926.927734375
Iteration 7600: Loss = -10926.9072265625
Iteration 7700: Loss = -10926.8857421875
Iteration 7800: Loss = -10926.8681640625
Iteration 7900: Loss = -10926.8486328125
Iteration 8000: Loss = -10926.83203125
Iteration 8100: Loss = -10926.814453125
Iteration 8200: Loss = -10926.7998046875
Iteration 8300: Loss = -10926.7822265625
Iteration 8400: Loss = -10926.7685546875
Iteration 8500: Loss = -10926.755859375
Iteration 8600: Loss = -10926.7431640625
Iteration 8700: Loss = -10926.7314453125
Iteration 8800: Loss = -10926.7197265625
Iteration 8900: Loss = -10926.708984375
Iteration 9000: Loss = -10926.6982421875
Iteration 9100: Loss = -10926.6884765625
Iteration 9200: Loss = -10926.6787109375
Iteration 9300: Loss = -10926.6689453125
Iteration 9400: Loss = -10926.6611328125
Iteration 9500: Loss = -10926.65234375
Iteration 9600: Loss = -10926.6455078125
Iteration 9700: Loss = -10926.6376953125
Iteration 9800: Loss = -10926.6298828125
Iteration 9900: Loss = -10926.6240234375
Iteration 10000: Loss = -10926.6171875
Iteration 10100: Loss = -10926.61328125
Iteration 10200: Loss = -10926.60546875
Iteration 10300: Loss = -10926.599609375
Iteration 10400: Loss = -10926.5947265625
Iteration 10500: Loss = -10926.58984375
Iteration 10600: Loss = -10926.5859375
Iteration 10700: Loss = -10926.5791015625
Iteration 10800: Loss = -10926.576171875
Iteration 10900: Loss = -10926.572265625
Iteration 11000: Loss = -10926.56640625
Iteration 11100: Loss = -10926.5654296875
Iteration 11200: Loss = -10926.5615234375
Iteration 11300: Loss = -10926.5576171875
Iteration 11400: Loss = -10926.5546875
Iteration 11500: Loss = -10926.55078125
Iteration 11600: Loss = -10926.5498046875
Iteration 11700: Loss = -10926.5439453125
Iteration 11800: Loss = -10926.544921875
1
Iteration 11900: Loss = -10926.541015625
Iteration 12000: Loss = -10926.5390625
Iteration 12100: Loss = -10926.5361328125
Iteration 12200: Loss = -10926.533203125
Iteration 12300: Loss = -10926.53125
Iteration 12400: Loss = -10926.5302734375
Iteration 12500: Loss = -10926.52734375
Iteration 12600: Loss = -10926.5263671875
Iteration 12700: Loss = -10926.5244140625
Iteration 12800: Loss = -10926.5224609375
Iteration 12900: Loss = -10926.521484375
Iteration 13000: Loss = -10926.51953125
Iteration 13100: Loss = -10926.51953125
Iteration 13200: Loss = -10926.51953125
Iteration 13300: Loss = -10926.515625
Iteration 13400: Loss = -10926.5166015625
1
Iteration 13500: Loss = -10926.513671875
Iteration 13600: Loss = -10926.51171875
Iteration 13700: Loss = -10926.51171875
Iteration 13800: Loss = -10926.51171875
Iteration 13900: Loss = -10926.5087890625
Iteration 14000: Loss = -10926.5087890625
Iteration 14100: Loss = -10926.5087890625
Iteration 14200: Loss = -10926.5087890625
Iteration 14300: Loss = -10926.505859375
Iteration 14400: Loss = -10926.5048828125
Iteration 14500: Loss = -10926.50390625
Iteration 14600: Loss = -10926.5048828125
1
Iteration 14700: Loss = -10926.5048828125
2
Iteration 14800: Loss = -10926.5029296875
Iteration 14900: Loss = -10926.501953125
Iteration 15000: Loss = -10926.5009765625
Iteration 15100: Loss = -10926.501953125
1
Iteration 15200: Loss = -10926.501953125
2
Iteration 15300: Loss = -10926.5
Iteration 15400: Loss = -10926.4990234375
Iteration 15500: Loss = -10926.498046875
Iteration 15600: Loss = -10926.4990234375
1
Iteration 15700: Loss = -10926.4990234375
2
Iteration 15800: Loss = -10926.4990234375
3
Iteration 15900: Loss = -10926.498046875
Iteration 16000: Loss = -10926.4970703125
Iteration 16100: Loss = -10926.4970703125
Iteration 16200: Loss = -10926.4970703125
Iteration 16300: Loss = -10926.4970703125
Iteration 16400: Loss = -10926.49609375
Iteration 16500: Loss = -10926.498046875
1
Iteration 16600: Loss = -10926.49609375
Iteration 16700: Loss = -10926.49609375
Iteration 16800: Loss = -10926.49609375
Iteration 16900: Loss = -10926.49609375
Iteration 17000: Loss = -10926.494140625
Iteration 17100: Loss = -10926.49609375
1
Iteration 17200: Loss = -10926.494140625
Iteration 17300: Loss = -10926.4951171875
1
Iteration 17400: Loss = -10926.494140625
Iteration 17500: Loss = -10926.494140625
Iteration 17600: Loss = -10926.4931640625
Iteration 17700: Loss = -10926.494140625
1
Iteration 17800: Loss = -10926.494140625
2
Iteration 17900: Loss = -10926.494140625
3
Iteration 18000: Loss = -10926.494140625
4
Iteration 18100: Loss = -10926.494140625
5
Iteration 18200: Loss = -10926.494140625
6
Iteration 18300: Loss = -10926.494140625
7
Iteration 18400: Loss = -10926.4931640625
Iteration 18500: Loss = -10926.4931640625
Iteration 18600: Loss = -10926.494140625
1
Iteration 18700: Loss = -10926.4921875
Iteration 18800: Loss = -10926.4931640625
1
Iteration 18900: Loss = -10926.4921875
Iteration 19000: Loss = -10926.4921875
Iteration 19100: Loss = -10926.4931640625
1
Iteration 19200: Loss = -10926.4921875
Iteration 19300: Loss = -10926.4921875
Iteration 19400: Loss = -10926.4912109375
Iteration 19500: Loss = -10926.4921875
1
Iteration 19600: Loss = -10926.4912109375
Iteration 19700: Loss = -10926.4921875
1
Iteration 19800: Loss = -10926.4921875
2
Iteration 19900: Loss = -10926.494140625
3
Iteration 20000: Loss = -10926.4931640625
4
Iteration 20100: Loss = -10926.4912109375
Iteration 20200: Loss = -10926.4931640625
1
Iteration 20300: Loss = -10926.4921875
2
Iteration 20400: Loss = -10926.4931640625
3
Iteration 20500: Loss = -10926.4931640625
4
Iteration 20600: Loss = -10926.4912109375
Iteration 20700: Loss = -10926.4931640625
1
Iteration 20800: Loss = -10926.4912109375
Iteration 20900: Loss = -10926.4912109375
Iteration 21000: Loss = -10926.494140625
1
Iteration 21100: Loss = -10926.494140625
2
Iteration 21200: Loss = -10926.4921875
3
Iteration 21300: Loss = -10926.4921875
4
Iteration 21400: Loss = -10926.4921875
5
Iteration 21500: Loss = -10926.4921875
6
Iteration 21600: Loss = -10926.4921875
7
Iteration 21700: Loss = -10926.4921875
8
Iteration 21800: Loss = -10926.4921875
9
Iteration 21900: Loss = -10926.4912109375
Iteration 22000: Loss = -10926.4912109375
Iteration 22100: Loss = -10926.4921875
1
Iteration 22200: Loss = -10926.4921875
2
Iteration 22300: Loss = -10926.4931640625
3
Iteration 22400: Loss = -10926.4921875
4
Iteration 22500: Loss = -10926.4931640625
5
Iteration 22600: Loss = -10926.4931640625
6
Iteration 22700: Loss = -10926.4921875
7
Iteration 22800: Loss = -10926.4912109375
Iteration 22900: Loss = -10926.4921875
1
Iteration 23000: Loss = -10926.4921875
2
Iteration 23100: Loss = -10926.4921875
3
Iteration 23200: Loss = -10926.4921875
4
Iteration 23300: Loss = -10926.4921875
5
Iteration 23400: Loss = -10926.494140625
6
Iteration 23500: Loss = -10926.4931640625
7
Iteration 23600: Loss = -10926.4931640625
8
Iteration 23700: Loss = -10926.4921875
9
Iteration 23800: Loss = -10926.490234375
Iteration 23900: Loss = -10926.4921875
1
Iteration 24000: Loss = -10926.4931640625
2
Iteration 24100: Loss = -10926.4921875
3
Iteration 24200: Loss = -10926.4921875
4
Iteration 24300: Loss = -10926.4921875
5
Iteration 24400: Loss = -10926.4931640625
6
Iteration 24500: Loss = -10926.4912109375
7
Iteration 24600: Loss = -10926.4921875
8
Iteration 24700: Loss = -10926.4921875
9
Iteration 24800: Loss = -10926.4921875
10
Iteration 24900: Loss = -10926.4921875
11
Iteration 25000: Loss = -10926.4931640625
12
Iteration 25100: Loss = -10926.4912109375
13
Iteration 25200: Loss = -10926.4912109375
14
Iteration 25300: Loss = -10926.4921875
15
Stopping early at iteration 25300 due to no improvement.
pi: tensor([[7.7714e-04, 9.9922e-01],
        [2.6147e-05, 9.9997e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.5605e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.8792, 0.1993],
         [0.6879, 0.1611]],

        [[0.5671, 0.2512],
         [0.9785, 0.9795]],

        [[0.9446, 0.8536],
         [0.0341, 0.3723]],

        [[0.8003, 0.1140],
         [0.9200, 0.9686]],

        [[0.1695, 0.2093],
         [0.7506, 0.2932]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.04824381999617845, 0.0] [0.641754429565614, 0.0] [10863.224609375, 10926.4921875]
-------------------------------------
This iteration is 72
True Objective function: Loss = -10814.543615683191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17009.373046875
Iteration 100: Loss = -12775.416015625
Iteration 200: Loss = -11264.6953125
Iteration 300: Loss = -10993.59765625
Iteration 400: Loss = -10941.4150390625
Iteration 500: Loss = -10920.853515625
Iteration 600: Loss = -10906.517578125
Iteration 700: Loss = -10896.64453125
Iteration 800: Loss = -10890.7763671875
Iteration 900: Loss = -10887.751953125
Iteration 1000: Loss = -10885.8203125
Iteration 1100: Loss = -10884.837890625
Iteration 1200: Loss = -10884.158203125
Iteration 1300: Loss = -10883.6474609375
Iteration 1400: Loss = -10883.2509765625
Iteration 1500: Loss = -10882.935546875
Iteration 1600: Loss = -10882.685546875
Iteration 1700: Loss = -10882.482421875
Iteration 1800: Loss = -10882.3134765625
Iteration 1900: Loss = -10882.1689453125
Iteration 2000: Loss = -10882.048828125
Iteration 2100: Loss = -10881.94140625
Iteration 2200: Loss = -10881.84765625
Iteration 2300: Loss = -10881.7646484375
Iteration 2400: Loss = -10881.6904296875
Iteration 2500: Loss = -10881.623046875
Iteration 2600: Loss = -10881.55859375
Iteration 2700: Loss = -10881.5
Iteration 2800: Loss = -10881.443359375
Iteration 2900: Loss = -10881.3857421875
Iteration 3000: Loss = -10881.330078125
Iteration 3100: Loss = -10881.2685546875
Iteration 3200: Loss = -10881.2041015625
Iteration 3300: Loss = -10881.1259765625
Iteration 3400: Loss = -10881.03125
Iteration 3500: Loss = -10880.9111328125
Iteration 3600: Loss = -10880.740234375
Iteration 3700: Loss = -10880.501953125
Iteration 3800: Loss = -10880.2255859375
Iteration 3900: Loss = -10880.0078125
Iteration 4000: Loss = -10879.87890625
Iteration 4100: Loss = -10879.79296875
Iteration 4200: Loss = -10879.720703125
Iteration 4300: Loss = -10879.6513671875
Iteration 4400: Loss = -10879.5830078125
Iteration 4500: Loss = -10879.515625
Iteration 4600: Loss = -10879.4619140625
Iteration 4700: Loss = -10879.4189453125
Iteration 4800: Loss = -10879.3779296875
Iteration 4900: Loss = -10879.33984375
Iteration 5000: Loss = -10879.306640625
Iteration 5100: Loss = -10879.2744140625
Iteration 5200: Loss = -10879.2431640625
Iteration 5300: Loss = -10879.2109375
Iteration 5400: Loss = -10879.1767578125
Iteration 5500: Loss = -10879.1357421875
Iteration 5600: Loss = -10879.09375
Iteration 5700: Loss = -10879.046875
Iteration 5800: Loss = -10878.9765625
Iteration 5900: Loss = -10878.8447265625
Iteration 6000: Loss = -10878.576171875
Iteration 6100: Loss = -10878.1171875
Iteration 6200: Loss = -10877.1552734375
Iteration 6300: Loss = -10875.3896484375
Iteration 6400: Loss = -10866.1533203125
Iteration 6500: Loss = -10815.638671875
Iteration 6600: Loss = -10803.7158203125
Iteration 6700: Loss = -10797.2353515625
Iteration 6800: Loss = -10796.70703125
Iteration 6900: Loss = -10796.4873046875
Iteration 7000: Loss = -10796.3662109375
Iteration 7100: Loss = -10795.7783203125
Iteration 7200: Loss = -10795.7314453125
Iteration 7300: Loss = -10795.7021484375
Iteration 7400: Loss = -10795.6796875
Iteration 7500: Loss = -10795.5712890625
Iteration 7600: Loss = -10795.501953125
Iteration 7700: Loss = -10795.4765625
Iteration 7800: Loss = -10795.37890625
Iteration 7900: Loss = -10795.3662109375
Iteration 8000: Loss = -10795.3525390625
Iteration 8100: Loss = -10795.337890625
Iteration 8200: Loss = -10795.318359375
Iteration 8300: Loss = -10795.287109375
Iteration 8400: Loss = -10795.2216796875
Iteration 8500: Loss = -10795.056640625
Iteration 8600: Loss = -10794.7158203125
Iteration 8700: Loss = -10794.564453125
Iteration 8800: Loss = -10794.46875
Iteration 8900: Loss = -10794.4453125
Iteration 9000: Loss = -10794.43359375
Iteration 9100: Loss = -10794.42578125
Iteration 9200: Loss = -10794.4189453125
Iteration 9300: Loss = -10794.416015625
Iteration 9400: Loss = -10794.4130859375
Iteration 9500: Loss = -10794.408203125
Iteration 9600: Loss = -10794.40234375
Iteration 9700: Loss = -10794.38671875
Iteration 9800: Loss = -10794.37109375
Iteration 9900: Loss = -10794.333984375
Iteration 10000: Loss = -10793.271484375
Iteration 10100: Loss = -10792.80859375
Iteration 10200: Loss = -10785.4755859375
Iteration 10300: Loss = -10779.4072265625
Iteration 10400: Loss = -10778.0537109375
Iteration 10500: Loss = -10777.88671875
Iteration 10600: Loss = -10777.875
Iteration 10700: Loss = -10777.3798828125
Iteration 10800: Loss = -10776.4482421875
Iteration 10900: Loss = -10776.4443359375
Iteration 11000: Loss = -10776.4423828125
Iteration 11100: Loss = -10776.4404296875
Iteration 11200: Loss = -10776.439453125
Iteration 11300: Loss = -10776.4384765625
Iteration 11400: Loss = -10776.4384765625
Iteration 11500: Loss = -10776.4365234375
Iteration 11600: Loss = -10770.54296875
Iteration 11700: Loss = -10769.8310546875
Iteration 11800: Loss = -10769.8046875
Iteration 11900: Loss = -10769.7841796875
Iteration 12000: Loss = -10769.7783203125
Iteration 12100: Loss = -10769.7744140625
Iteration 12200: Loss = -10769.7724609375
Iteration 12300: Loss = -10769.76953125
Iteration 12400: Loss = -10767.7734375
Iteration 12500: Loss = -10767.326171875
Iteration 12600: Loss = -10767.32421875
Iteration 12700: Loss = -10767.322265625
Iteration 12800: Loss = -10767.3212890625
Iteration 12900: Loss = -10767.3212890625
Iteration 13000: Loss = -10767.3212890625
Iteration 13100: Loss = -10767.3203125
Iteration 13200: Loss = -10767.3193359375
Iteration 13300: Loss = -10767.318359375
Iteration 13400: Loss = -10767.3173828125
Iteration 13500: Loss = -10767.3154296875
Iteration 13600: Loss = -10767.31640625
1
Iteration 13700: Loss = -10767.3154296875
Iteration 13800: Loss = -10767.3154296875
Iteration 13900: Loss = -10767.3154296875
Iteration 14000: Loss = -10767.31640625
1
Iteration 14100: Loss = -10767.3154296875
Iteration 14200: Loss = -10767.3154296875
Iteration 14300: Loss = -10767.314453125
Iteration 14400: Loss = -10767.3154296875
1
Iteration 14500: Loss = -10767.314453125
Iteration 14600: Loss = -10767.314453125
Iteration 14700: Loss = -10767.3154296875
1
Iteration 14800: Loss = -10767.314453125
Iteration 14900: Loss = -10767.314453125
Iteration 15000: Loss = -10767.3134765625
Iteration 15100: Loss = -10767.3125
Iteration 15200: Loss = -10767.3125
Iteration 15300: Loss = -10767.3115234375
Iteration 15400: Loss = -10767.3115234375
Iteration 15500: Loss = -10767.310546875
Iteration 15600: Loss = -10767.3115234375
1
Iteration 15700: Loss = -10767.3115234375
2
Iteration 15800: Loss = -10767.3115234375
3
Iteration 15900: Loss = -10767.310546875
Iteration 16000: Loss = -10767.3125
1
Iteration 16100: Loss = -10767.3115234375
2
Iteration 16200: Loss = -10767.3115234375
3
Iteration 16300: Loss = -10767.310546875
Iteration 16400: Loss = -10767.3115234375
1
Iteration 16500: Loss = -10767.310546875
Iteration 16600: Loss = -10767.3125
1
Iteration 16700: Loss = -10767.310546875
Iteration 16800: Loss = -10767.310546875
Iteration 16900: Loss = -10767.3115234375
1
Iteration 17000: Loss = -10767.310546875
Iteration 17100: Loss = -10767.3017578125
Iteration 17200: Loss = -10767.3017578125
Iteration 17300: Loss = -10767.3017578125
Iteration 17400: Loss = -10767.3017578125
Iteration 17500: Loss = -10767.30078125
Iteration 17600: Loss = -10767.3017578125
1
Iteration 17700: Loss = -10767.3017578125
2
Iteration 17800: Loss = -10767.302734375
3
Iteration 17900: Loss = -10767.3017578125
4
Iteration 18000: Loss = -10767.30078125
Iteration 18100: Loss = -10767.30078125
Iteration 18200: Loss = -10767.30078125
Iteration 18300: Loss = -10767.3017578125
1
Iteration 18400: Loss = -10767.302734375
2
Iteration 18500: Loss = -10767.298828125
Iteration 18600: Loss = -10767.30078125
1
Iteration 18700: Loss = -10767.2998046875
2
Iteration 18800: Loss = -10767.2998046875
3
Iteration 18900: Loss = -10767.2998046875
4
Iteration 19000: Loss = -10767.30078125
5
Iteration 19100: Loss = -10767.30078125
6
Iteration 19200: Loss = -10767.2998046875
7
Iteration 19300: Loss = -10767.30078125
8
Iteration 19400: Loss = -10767.30078125
9
Iteration 19500: Loss = -10767.30078125
10
Iteration 19600: Loss = -10767.2998046875
11
Iteration 19700: Loss = -10767.30078125
12
Iteration 19800: Loss = -10767.2998046875
13
Iteration 19900: Loss = -10767.30078125
14
Iteration 20000: Loss = -10767.2998046875
15
Stopping early at iteration 20000 due to no improvement.
pi: tensor([[0.7363, 0.2637],
        [0.2803, 0.7197]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4031, 0.5969], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2569, 0.1038],
         [0.9597, 0.1892]],

        [[0.1206, 0.0839],
         [0.8028, 0.0441]],

        [[0.0829, 0.1110],
         [0.9900, 0.5582]],

        [[0.0078, 0.0982],
         [0.9488, 0.0270]],

        [[0.9427, 0.1076],
         [0.9812, 0.7903]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6364272031776997
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6690831299615492
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.7810192124075866
Average Adjusted Rand Index: 0.7833614240443627
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41861.859375
Iteration 100: Loss = -24423.29296875
Iteration 200: Loss = -13789.984375
Iteration 300: Loss = -11960.017578125
Iteration 400: Loss = -11538.9423828125
Iteration 500: Loss = -11357.7294921875
Iteration 600: Loss = -11258.1796875
Iteration 700: Loss = -11192.6708984375
Iteration 800: Loss = -11126.75
Iteration 900: Loss = -11076.888671875
Iteration 1000: Loss = -11044.4892578125
Iteration 1100: Loss = -11009.888671875
Iteration 1200: Loss = -10988.4228515625
Iteration 1300: Loss = -10977.162109375
Iteration 1400: Loss = -10965.361328125
Iteration 1500: Loss = -10950.5478515625
Iteration 1600: Loss = -10944.865234375
Iteration 1700: Loss = -10939.953125
Iteration 1800: Loss = -10931.947265625
Iteration 1900: Loss = -10928.326171875
Iteration 2000: Loss = -10925.7724609375
Iteration 2100: Loss = -10923.6572265625
Iteration 2200: Loss = -10921.8388671875
Iteration 2300: Loss = -10920.244140625
Iteration 2400: Loss = -10918.81640625
Iteration 2500: Loss = -10911.599609375
Iteration 2600: Loss = -10910.0361328125
Iteration 2700: Loss = -10908.9267578125
Iteration 2800: Loss = -10907.9599609375
Iteration 2900: Loss = -10907.0830078125
Iteration 3000: Loss = -10906.25390625
Iteration 3100: Loss = -10905.3427734375
Iteration 3200: Loss = -10902.953125
Iteration 3300: Loss = -10900.1240234375
Iteration 3400: Loss = -10894.818359375
Iteration 3500: Loss = -10893.3623046875
Iteration 3600: Loss = -10892.6416015625
Iteration 3700: Loss = -10892.07421875
Iteration 3800: Loss = -10891.5830078125
Iteration 3900: Loss = -10891.1494140625
Iteration 4000: Loss = -10890.7578125
Iteration 4100: Loss = -10890.4013671875
Iteration 4200: Loss = -10890.0771484375
Iteration 4300: Loss = -10889.7783203125
Iteration 4400: Loss = -10889.5009765625
Iteration 4500: Loss = -10889.2451171875
Iteration 4600: Loss = -10889.005859375
Iteration 4700: Loss = -10888.78515625
Iteration 4800: Loss = -10888.578125
Iteration 4900: Loss = -10888.384765625
Iteration 5000: Loss = -10888.203125
Iteration 5100: Loss = -10888.03515625
Iteration 5200: Loss = -10887.875
Iteration 5300: Loss = -10887.7275390625
Iteration 5400: Loss = -10887.587890625
Iteration 5500: Loss = -10887.45703125
Iteration 5600: Loss = -10887.33203125
Iteration 5700: Loss = -10887.216796875
Iteration 5800: Loss = -10887.1044921875
Iteration 5900: Loss = -10887.0009765625
Iteration 6000: Loss = -10886.9033203125
Iteration 6100: Loss = -10886.8095703125
Iteration 6200: Loss = -10886.7197265625
Iteration 6300: Loss = -10886.638671875
Iteration 6400: Loss = -10886.5595703125
Iteration 6500: Loss = -10886.486328125
Iteration 6600: Loss = -10886.416015625
Iteration 6700: Loss = -10886.3515625
Iteration 6800: Loss = -10886.2880859375
Iteration 6900: Loss = -10886.2294921875
Iteration 7000: Loss = -10886.173828125
Iteration 7100: Loss = -10886.12109375
Iteration 7200: Loss = -10886.0712890625
Iteration 7300: Loss = -10886.0224609375
Iteration 7400: Loss = -10885.978515625
Iteration 7500: Loss = -10885.93359375
Iteration 7600: Loss = -10885.89453125
Iteration 7700: Loss = -10885.8564453125
Iteration 7800: Loss = -10885.818359375
Iteration 7900: Loss = -10885.783203125
Iteration 8000: Loss = -10885.75
Iteration 8100: Loss = -10885.7177734375
Iteration 8200: Loss = -10885.6904296875
Iteration 8300: Loss = -10885.662109375
Iteration 8400: Loss = -10885.6328125
Iteration 8500: Loss = -10885.6083984375
Iteration 8600: Loss = -10885.58203125
Iteration 8700: Loss = -10885.5595703125
Iteration 8800: Loss = -10885.5380859375
Iteration 8900: Loss = -10885.5166015625
Iteration 9000: Loss = -10885.4970703125
Iteration 9100: Loss = -10885.4775390625
Iteration 9200: Loss = -10885.458984375
Iteration 9300: Loss = -10885.4423828125
Iteration 9400: Loss = -10885.4248046875
Iteration 9500: Loss = -10885.4091796875
Iteration 9600: Loss = -10885.39453125
Iteration 9700: Loss = -10885.3798828125
Iteration 9800: Loss = -10885.3662109375
Iteration 9900: Loss = -10885.3525390625
Iteration 10000: Loss = -10885.3388671875
Iteration 10100: Loss = -10885.3291015625
Iteration 10200: Loss = -10885.31640625
Iteration 10300: Loss = -10885.3056640625
Iteration 10400: Loss = -10885.29296875
Iteration 10500: Loss = -10885.283203125
Iteration 10600: Loss = -10885.275390625
Iteration 10700: Loss = -10885.265625
Iteration 10800: Loss = -10885.2568359375
Iteration 10900: Loss = -10885.248046875
Iteration 11000: Loss = -10885.2392578125
Iteration 11100: Loss = -10885.232421875
Iteration 11200: Loss = -10885.224609375
Iteration 11300: Loss = -10885.21875
Iteration 11400: Loss = -10885.2109375
Iteration 11500: Loss = -10885.2041015625
Iteration 11600: Loss = -10885.2001953125
Iteration 11700: Loss = -10885.193359375
Iteration 11800: Loss = -10885.1884765625
Iteration 11900: Loss = -10885.181640625
Iteration 12000: Loss = -10885.1767578125
Iteration 12100: Loss = -10885.1708984375
Iteration 12200: Loss = -10885.1650390625
Iteration 12300: Loss = -10885.1611328125
Iteration 12400: Loss = -10885.1552734375
Iteration 12500: Loss = -10885.1494140625
Iteration 12600: Loss = -10885.1435546875
Iteration 12700: Loss = -10885.138671875
Iteration 12800: Loss = -10885.1337890625
Iteration 12900: Loss = -10885.12890625
Iteration 13000: Loss = -10885.1220703125
Iteration 13100: Loss = -10885.1162109375
Iteration 13200: Loss = -10885.107421875
Iteration 13300: Loss = -10885.099609375
Iteration 13400: Loss = -10885.091796875
Iteration 13500: Loss = -10885.08203125
Iteration 13600: Loss = -10885.06640625
Iteration 13700: Loss = -10885.048828125
Iteration 13800: Loss = -10885.0263671875
Iteration 13900: Loss = -10884.990234375
Iteration 14000: Loss = -10884.9326171875
Iteration 14100: Loss = -10884.7978515625
Iteration 14200: Loss = -10884.3642578125
Iteration 14300: Loss = -10883.673828125
Iteration 14400: Loss = -10883.294921875
Iteration 14500: Loss = -10882.001953125
Iteration 14600: Loss = -10881.59765625
Iteration 14700: Loss = -10880.9052734375
Iteration 14800: Loss = -10880.7998046875
Iteration 14900: Loss = -10876.625
Iteration 15000: Loss = -10876.345703125
Iteration 15100: Loss = -10876.3193359375
Iteration 15200: Loss = -10876.19921875
Iteration 15300: Loss = -10876.1875
Iteration 15400: Loss = -10876.1826171875
Iteration 15500: Loss = -10876.1806640625
Iteration 15600: Loss = -10876.1796875
Iteration 15700: Loss = -10876.1787109375
Iteration 15800: Loss = -10876.177734375
Iteration 15900: Loss = -10876.17578125
Iteration 16000: Loss = -10876.17578125
Iteration 16100: Loss = -10876.1748046875
Iteration 16200: Loss = -10876.1748046875
Iteration 16300: Loss = -10876.1748046875
Iteration 16400: Loss = -10876.1728515625
Iteration 16500: Loss = -10876.171875
Iteration 16600: Loss = -10876.1728515625
1
Iteration 16700: Loss = -10876.171875
Iteration 16800: Loss = -10876.169921875
Iteration 16900: Loss = -10876.169921875
Iteration 17000: Loss = -10876.169921875
Iteration 17100: Loss = -10876.169921875
Iteration 17200: Loss = -10876.169921875
Iteration 17300: Loss = -10876.1689453125
Iteration 17400: Loss = -10876.1689453125
Iteration 17500: Loss = -10876.1689453125
Iteration 17600: Loss = -10876.1689453125
Iteration 17700: Loss = -10876.1689453125
Iteration 17800: Loss = -10876.16796875
Iteration 17900: Loss = -10876.1689453125
1
Iteration 18000: Loss = -10876.1669921875
Iteration 18100: Loss = -10876.1669921875
Iteration 18200: Loss = -10876.16796875
1
Iteration 18300: Loss = -10876.16796875
2
Iteration 18400: Loss = -10876.16796875
3
Iteration 18500: Loss = -10876.1669921875
Iteration 18600: Loss = -10876.16796875
1
Iteration 18700: Loss = -10876.166015625
Iteration 18800: Loss = -10876.1689453125
1
Iteration 18900: Loss = -10876.1669921875
2
Iteration 19000: Loss = -10876.166015625
Iteration 19100: Loss = -10876.16796875
1
Iteration 19200: Loss = -10876.1669921875
2
Iteration 19300: Loss = -10876.166015625
Iteration 19400: Loss = -10876.166015625
Iteration 19500: Loss = -10876.1669921875
1
Iteration 19600: Loss = -10876.166015625
Iteration 19700: Loss = -10876.1669921875
1
Iteration 19800: Loss = -10876.166015625
Iteration 19900: Loss = -10876.166015625
Iteration 20000: Loss = -10876.1669921875
1
Iteration 20100: Loss = -10876.1669921875
2
Iteration 20200: Loss = -10876.166015625
Iteration 20300: Loss = -10876.1650390625
Iteration 20400: Loss = -10876.1669921875
1
Iteration 20500: Loss = -10876.166015625
2
Iteration 20600: Loss = -10876.166015625
3
Iteration 20700: Loss = -10876.1669921875
4
Iteration 20800: Loss = -10876.166015625
5
Iteration 20900: Loss = -10876.1650390625
Iteration 21000: Loss = -10876.1640625
Iteration 21100: Loss = -10876.166015625
1
Iteration 21200: Loss = -10876.1650390625
2
Iteration 21300: Loss = -10876.1650390625
3
Iteration 21400: Loss = -10876.166015625
4
Iteration 21500: Loss = -10876.166015625
5
Iteration 21600: Loss = -10876.1630859375
Iteration 21700: Loss = -10876.1630859375
Iteration 21800: Loss = -10876.1162109375
Iteration 21900: Loss = -10875.904296875
Iteration 22000: Loss = -10875.470703125
Iteration 22100: Loss = -10875.4658203125
Iteration 22200: Loss = -10875.4150390625
Iteration 22300: Loss = -10874.9453125
Iteration 22400: Loss = -10874.9443359375
Iteration 22500: Loss = -10874.8310546875
Iteration 22600: Loss = -10874.3720703125
Iteration 22700: Loss = -10874.302734375
Iteration 22800: Loss = -10874.189453125
Iteration 22900: Loss = -10873.794921875
Iteration 23000: Loss = -10873.189453125
Iteration 23100: Loss = -10870.5986328125
Iteration 23200: Loss = -10866.7451171875
Iteration 23300: Loss = -10864.328125
Iteration 23400: Loss = -10858.8876953125
Iteration 23500: Loss = -10858.83984375
Iteration 23600: Loss = -10858.83203125
Iteration 23700: Loss = -10858.828125
Iteration 23800: Loss = -10858.82421875
Iteration 23900: Loss = -10856.43359375
Iteration 24000: Loss = -10856.41015625
Iteration 24100: Loss = -10855.6435546875
Iteration 24200: Loss = -10854.412109375
Iteration 24300: Loss = -10854.4052734375
Iteration 24400: Loss = -10854.4033203125
Iteration 24500: Loss = -10854.4033203125
Iteration 24600: Loss = -10854.40234375
Iteration 24700: Loss = -10854.400390625
Iteration 24800: Loss = -10850.7919921875
Iteration 24900: Loss = -10850.7041015625
Iteration 25000: Loss = -10850.6953125
Iteration 25100: Loss = -10850.6904296875
Iteration 25200: Loss = -10850.6904296875
Iteration 25300: Loss = -10850.6875
Iteration 25400: Loss = -10850.6826171875
Iteration 25500: Loss = -10850.68359375
1
Iteration 25600: Loss = -10850.6826171875
Iteration 25700: Loss = -10850.6826171875
Iteration 25800: Loss = -10850.576171875
Iteration 25900: Loss = -10850.5732421875
Iteration 26000: Loss = -10850.572265625
Iteration 26100: Loss = -10850.5712890625
Iteration 26200: Loss = -10850.572265625
1
Iteration 26300: Loss = -10850.572265625
2
Iteration 26400: Loss = -10850.572265625
3
Iteration 26500: Loss = -10850.560546875
Iteration 26600: Loss = -10850.560546875
Iteration 26700: Loss = -10850.560546875
Iteration 26800: Loss = -10850.5595703125
Iteration 26900: Loss = -10850.5595703125
Iteration 27000: Loss = -10850.560546875
1
Iteration 27100: Loss = -10850.5595703125
Iteration 27200: Loss = -10850.5595703125
Iteration 27300: Loss = -10850.560546875
1
Iteration 27400: Loss = -10850.560546875
2
Iteration 27500: Loss = -10850.5595703125
Iteration 27600: Loss = -10850.5595703125
Iteration 27700: Loss = -10850.5595703125
Iteration 27800: Loss = -10850.560546875
1
Iteration 27900: Loss = -10850.5595703125
Iteration 28000: Loss = -10850.55859375
Iteration 28100: Loss = -10850.552734375
Iteration 28200: Loss = -10850.5537109375
1
Iteration 28300: Loss = -10850.5537109375
2
Iteration 28400: Loss = -10850.552734375
Iteration 28500: Loss = -10850.552734375
Iteration 28600: Loss = -10850.5537109375
1
Iteration 28700: Loss = -10850.5537109375
2
Iteration 28800: Loss = -10850.5537109375
3
Iteration 28900: Loss = -10850.552734375
Iteration 29000: Loss = -10850.5537109375
1
Iteration 29100: Loss = -10850.5537109375
2
Iteration 29200: Loss = -10850.552734375
Iteration 29300: Loss = -10850.5537109375
1
Iteration 29400: Loss = -10850.5537109375
2
Iteration 29500: Loss = -10850.552734375
Iteration 29600: Loss = -10850.5537109375
1
Iteration 29700: Loss = -10850.5537109375
2
Iteration 29800: Loss = -10850.5537109375
3
Iteration 29900: Loss = -10850.5556640625
4
pi: tensor([[8.2164e-01, 1.7836e-01],
        [1.0135e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6909, 0.3091], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1717, 0.1196],
         [0.9926, 0.2145]],

        [[0.1253, 0.1030],
         [0.9931, 0.1411]],

        [[0.9738, 0.1452],
         [0.9506, 0.9845]],

        [[0.2316, 0.1180],
         [0.9718, 0.3556]],

        [[0.9879, 0.1221],
         [0.5523, 0.0374]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 81
Adjusted Rand Index: 0.3784886359130699
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733577800500688
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 75
Adjusted Rand Index: 0.2423747699064116
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.3537331701346389
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 85
Adjusted Rand Index: 0.48510267107807936
Global Adjusted Rand Index: 0.4033011948405766
Average Adjusted Rand Index: 0.4066114054164537
[0.7810192124075866, 0.4033011948405766] [0.7833614240443627, 0.4066114054164537] [10767.2998046875, 10850.552734375]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11062.538999680504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27826.76171875
Iteration 100: Loss = -17629.1875
Iteration 200: Loss = -12498.693359375
Iteration 300: Loss = -11660.2578125
Iteration 400: Loss = -11523.8955078125
Iteration 500: Loss = -11474.6845703125
Iteration 600: Loss = -11445.3408203125
Iteration 700: Loss = -11426.5576171875
Iteration 800: Loss = -11411.013671875
Iteration 900: Loss = -11398.58984375
Iteration 1000: Loss = -11390.3681640625
Iteration 1100: Loss = -11382.212890625
Iteration 1200: Loss = -11372.5986328125
Iteration 1300: Loss = -11363.6005859375
Iteration 1400: Loss = -11354.0048828125
Iteration 1500: Loss = -11345.4951171875
Iteration 1600: Loss = -11339.41796875
Iteration 1700: Loss = -11332.09375
Iteration 1800: Loss = -11321.166015625
Iteration 1900: Loss = -11309.0068359375
Iteration 2000: Loss = -11301.7177734375
Iteration 2100: Loss = -11294.5078125
Iteration 2200: Loss = -11286.62890625
Iteration 2300: Loss = -11280.900390625
Iteration 2400: Loss = -11273.5576171875
Iteration 2500: Loss = -11263.6044921875
Iteration 2600: Loss = -11258.4755859375
Iteration 2700: Loss = -11254.2080078125
Iteration 2800: Loss = -11245.76171875
Iteration 2900: Loss = -11234.83984375
Iteration 3000: Loss = -11224.78515625
Iteration 3100: Loss = -11220.26171875
Iteration 3200: Loss = -11218.0830078125
Iteration 3300: Loss = -11216.734375
Iteration 3400: Loss = -11215.748046875
Iteration 3500: Loss = -11209.5478515625
Iteration 3600: Loss = -11206.396484375
Iteration 3700: Loss = -11205.2587890625
Iteration 3800: Loss = -11204.505859375
Iteration 3900: Loss = -11203.9345703125
Iteration 4000: Loss = -11203.4697265625
Iteration 4100: Loss = -11203.0830078125
Iteration 4200: Loss = -11202.7509765625
Iteration 4300: Loss = -11202.4638671875
Iteration 4400: Loss = -11202.212890625
Iteration 4500: Loss = -11201.990234375
Iteration 4600: Loss = -11201.7939453125
Iteration 4700: Loss = -11201.6171875
Iteration 4800: Loss = -11201.45703125
Iteration 4900: Loss = -11201.3125
Iteration 5000: Loss = -11201.1796875
Iteration 5100: Loss = -11201.060546875
Iteration 5200: Loss = -11200.9521484375
Iteration 5300: Loss = -11200.8505859375
Iteration 5400: Loss = -11200.7587890625
Iteration 5500: Loss = -11200.6728515625
Iteration 5600: Loss = -11200.5947265625
Iteration 5700: Loss = -11200.5224609375
Iteration 5800: Loss = -11200.453125
Iteration 5900: Loss = -11200.3916015625
Iteration 6000: Loss = -11200.3330078125
Iteration 6100: Loss = -11200.279296875
Iteration 6200: Loss = -11200.228515625
Iteration 6300: Loss = -11200.1796875
Iteration 6400: Loss = -11200.13671875
Iteration 6500: Loss = -11200.09375
Iteration 6600: Loss = -11200.056640625
Iteration 6700: Loss = -11200.0185546875
Iteration 6800: Loss = -11199.9853515625
Iteration 6900: Loss = -11199.953125
Iteration 7000: Loss = -11199.921875
Iteration 7100: Loss = -11199.8935546875
Iteration 7200: Loss = -11199.8681640625
Iteration 7300: Loss = -11199.8427734375
Iteration 7400: Loss = -11199.818359375
Iteration 7500: Loss = -11199.796875
Iteration 7600: Loss = -11199.775390625
Iteration 7700: Loss = -11199.755859375
Iteration 7800: Loss = -11199.736328125
Iteration 7900: Loss = -11199.71875
Iteration 8000: Loss = -11199.703125
Iteration 8100: Loss = -11199.6845703125
Iteration 8200: Loss = -11199.669921875
Iteration 8300: Loss = -11199.65625
Iteration 8400: Loss = -11199.6435546875
Iteration 8500: Loss = -11199.6298828125
Iteration 8600: Loss = -11199.619140625
Iteration 8700: Loss = -11199.6083984375
Iteration 8800: Loss = -11199.5966796875
Iteration 8900: Loss = -11199.5869140625
Iteration 9000: Loss = -11199.578125
Iteration 9100: Loss = -11199.568359375
Iteration 9200: Loss = -11199.560546875
Iteration 9300: Loss = -11199.5537109375
Iteration 9400: Loss = -11199.5458984375
Iteration 9500: Loss = -11199.5400390625
Iteration 9600: Loss = -11199.5341796875
Iteration 9700: Loss = -11199.52734375
Iteration 9800: Loss = -11199.521484375
Iteration 9900: Loss = -11199.5166015625
Iteration 10000: Loss = -11199.51171875
Iteration 10100: Loss = -11199.5078125
Iteration 10200: Loss = -11199.5029296875
Iteration 10300: Loss = -11199.5
Iteration 10400: Loss = -11199.4951171875
Iteration 10500: Loss = -11199.4931640625
Iteration 10600: Loss = -11199.4892578125
Iteration 10700: Loss = -11199.4873046875
Iteration 10800: Loss = -11199.482421875
Iteration 10900: Loss = -11199.48046875
Iteration 11000: Loss = -11199.4775390625
Iteration 11100: Loss = -11199.4755859375
Iteration 11200: Loss = -11199.47265625
Iteration 11300: Loss = -11199.4697265625
Iteration 11400: Loss = -11199.4697265625
Iteration 11500: Loss = -11199.466796875
Iteration 11600: Loss = -11199.4638671875
Iteration 11700: Loss = -11199.462890625
Iteration 11800: Loss = -11199.4609375
Iteration 11900: Loss = -11199.4599609375
Iteration 12000: Loss = -11199.4580078125
Iteration 12100: Loss = -11199.4560546875
Iteration 12200: Loss = -11199.4541015625
Iteration 12300: Loss = -11199.4541015625
Iteration 12400: Loss = -11199.4521484375
Iteration 12500: Loss = -11199.451171875
Iteration 12600: Loss = -11199.44921875
Iteration 12700: Loss = -11199.447265625
Iteration 12800: Loss = -11199.4462890625
Iteration 12900: Loss = -11199.4462890625
Iteration 13000: Loss = -11199.4443359375
Iteration 13100: Loss = -11199.443359375
Iteration 13200: Loss = -11199.44140625
Iteration 13300: Loss = -11199.4423828125
1
Iteration 13400: Loss = -11199.44140625
Iteration 13500: Loss = -11199.44140625
Iteration 13600: Loss = -11199.439453125
Iteration 13700: Loss = -11199.439453125
Iteration 13800: Loss = -11199.439453125
Iteration 13900: Loss = -11199.4375
Iteration 14000: Loss = -11199.4375
Iteration 14100: Loss = -11199.4365234375
Iteration 14200: Loss = -11199.4365234375
Iteration 14300: Loss = -11199.4345703125
Iteration 14400: Loss = -11199.4345703125
Iteration 14500: Loss = -11199.4345703125
Iteration 14600: Loss = -11199.43359375
Iteration 14700: Loss = -11199.4326171875
Iteration 14800: Loss = -11199.4326171875
Iteration 14900: Loss = -11199.4326171875
Iteration 15000: Loss = -11199.4326171875
Iteration 15100: Loss = -11199.431640625
Iteration 15200: Loss = -11199.431640625
Iteration 15300: Loss = -11199.4296875
Iteration 15400: Loss = -11199.4296875
Iteration 15500: Loss = -11199.4287109375
Iteration 15600: Loss = -11199.4296875
1
Iteration 15700: Loss = -11199.4296875
2
Iteration 15800: Loss = -11199.4296875
3
Iteration 15900: Loss = -11199.4296875
4
Iteration 16000: Loss = -11199.427734375
Iteration 16100: Loss = -11199.4267578125
Iteration 16200: Loss = -11199.427734375
1
Iteration 16300: Loss = -11199.4287109375
2
Iteration 16400: Loss = -11199.427734375
3
Iteration 16500: Loss = -11199.427734375
4
Iteration 16600: Loss = -11199.427734375
5
Iteration 16700: Loss = -11199.4267578125
Iteration 16800: Loss = -11199.427734375
1
Iteration 16900: Loss = -11199.4267578125
Iteration 17000: Loss = -11199.427734375
1
Iteration 17100: Loss = -11199.42578125
Iteration 17200: Loss = -11199.4267578125
1
Iteration 17300: Loss = -11199.4267578125
2
Iteration 17400: Loss = -11199.42578125
Iteration 17500: Loss = -11199.427734375
1
Iteration 17600: Loss = -11199.42578125
Iteration 17700: Loss = -11199.4267578125
1
Iteration 17800: Loss = -11199.42578125
Iteration 17900: Loss = -11199.423828125
Iteration 18000: Loss = -11199.42578125
1
Iteration 18100: Loss = -11199.4267578125
2
Iteration 18200: Loss = -11199.42578125
3
Iteration 18300: Loss = -11199.4248046875
4
Iteration 18400: Loss = -11199.42578125
5
Iteration 18500: Loss = -11199.42578125
6
Iteration 18600: Loss = -11199.42578125
7
Iteration 18700: Loss = -11199.423828125
Iteration 18800: Loss = -11199.4248046875
1
Iteration 18900: Loss = -11199.4248046875
2
Iteration 19000: Loss = -11199.42578125
3
Iteration 19100: Loss = -11199.4248046875
4
Iteration 19200: Loss = -11199.4248046875
5
Iteration 19300: Loss = -11199.423828125
Iteration 19400: Loss = -11199.423828125
Iteration 19500: Loss = -11199.4267578125
1
Iteration 19600: Loss = -11199.4248046875
2
Iteration 19700: Loss = -11199.42578125
3
Iteration 19800: Loss = -11199.4248046875
4
Iteration 19900: Loss = -11199.4248046875
5
Iteration 20000: Loss = -11199.423828125
Iteration 20100: Loss = -11199.423828125
Iteration 20200: Loss = -11199.423828125
Iteration 20300: Loss = -11199.423828125
Iteration 20400: Loss = -11199.423828125
Iteration 20500: Loss = -11199.423828125
Iteration 20600: Loss = -11199.423828125
Iteration 20700: Loss = -11199.423828125
Iteration 20800: Loss = -11199.423828125
Iteration 20900: Loss = -11199.4228515625
Iteration 21000: Loss = -11199.423828125
1
Iteration 21100: Loss = -11199.4248046875
2
Iteration 21200: Loss = -11199.423828125
3
Iteration 21300: Loss = -11199.4228515625
Iteration 21400: Loss = -11199.4228515625
Iteration 21500: Loss = -11199.4248046875
1
Iteration 21600: Loss = -11199.423828125
2
Iteration 21700: Loss = -11199.4228515625
Iteration 21800: Loss = -11199.4248046875
1
Iteration 21900: Loss = -11199.42578125
2
Iteration 22000: Loss = -11199.4248046875
3
Iteration 22100: Loss = -11199.421875
Iteration 22200: Loss = -11199.42578125
1
Iteration 22300: Loss = -11199.423828125
2
Iteration 22400: Loss = -11199.423828125
3
Iteration 22500: Loss = -11199.4248046875
4
Iteration 22600: Loss = -11199.4228515625
5
Iteration 22700: Loss = -11199.423828125
6
Iteration 22800: Loss = -11199.423828125
7
Iteration 22900: Loss = -11199.423828125
8
Iteration 23000: Loss = -11199.423828125
9
Iteration 23100: Loss = -11199.4248046875
10
Iteration 23200: Loss = -11199.423828125
11
Iteration 23300: Loss = -11199.423828125
12
Iteration 23400: Loss = -11199.4248046875
13
Iteration 23500: Loss = -11199.423828125
14
Iteration 23600: Loss = -11199.4248046875
15
Stopping early at iteration 23600 due to no improvement.
pi: tensor([[8.9546e-07, 1.0000e+00],
        [1.0000e+00, 2.6790e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.7039e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1693, 0.1638],
         [0.5549, 0.1669]],

        [[0.0210, 0.1666],
         [0.2020, 0.0563]],

        [[0.0142, 0.8198],
         [0.9206, 0.3139]],

        [[0.9649, 0.2035],
         [0.9886, 0.7036]],

        [[0.0291, 0.1458],
         [0.0072, 0.9904]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0010583936399217598
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -54630.4765625
Iteration 100: Loss = -30615.7734375
Iteration 200: Loss = -15511.5986328125
Iteration 300: Loss = -12474.4560546875
Iteration 400: Loss = -11922.1025390625
Iteration 500: Loss = -11698.26953125
Iteration 600: Loss = -11560.92578125
Iteration 700: Loss = -11475.4287109375
Iteration 800: Loss = -11428.7099609375
Iteration 900: Loss = -11379.9794921875
Iteration 1000: Loss = -11335.5693359375
Iteration 1100: Loss = -11303.712890625
Iteration 1200: Loss = -11276.302734375
Iteration 1300: Loss = -11265.9599609375
Iteration 1400: Loss = -11258.37890625
Iteration 1500: Loss = -11252.3681640625
Iteration 1600: Loss = -11247.4287109375
Iteration 1700: Loss = -11243.2841796875
Iteration 1800: Loss = -11239.759765625
Iteration 1900: Loss = -11236.7275390625
Iteration 2000: Loss = -11234.0947265625
Iteration 2100: Loss = -11231.787109375
Iteration 2200: Loss = -11229.7568359375
Iteration 2300: Loss = -11227.9482421875
Iteration 2400: Loss = -11226.28515625
Iteration 2500: Loss = -11224.740234375
Iteration 2600: Loss = -11223.447265625
Iteration 2700: Loss = -11222.28125
Iteration 2800: Loss = -11221.212890625
Iteration 2900: Loss = -11217.6591796875
Iteration 3000: Loss = -11216.5107421875
Iteration 3100: Loss = -11215.64453125
Iteration 3200: Loss = -11214.912109375
Iteration 3300: Loss = -11214.259765625
Iteration 3400: Loss = -11213.669921875
Iteration 3500: Loss = -11213.1328125
Iteration 3600: Loss = -11212.6396484375
Iteration 3700: Loss = -11212.185546875
Iteration 3800: Loss = -11211.767578125
Iteration 3900: Loss = -11211.376953125
Iteration 4000: Loss = -11211.013671875
Iteration 4100: Loss = -11210.6015625
Iteration 4200: Loss = -11205.2919921875
Iteration 4300: Loss = -11204.7509765625
Iteration 4400: Loss = -11204.3759765625
Iteration 4500: Loss = -11204.0625
Iteration 4600: Loss = -11203.7822265625
Iteration 4700: Loss = -11203.5302734375
Iteration 4800: Loss = -11203.294921875
Iteration 4900: Loss = -11203.076171875
Iteration 5000: Loss = -11202.875
Iteration 5100: Loss = -11202.6904296875
Iteration 5200: Loss = -11202.515625
Iteration 5300: Loss = -11202.3544921875
Iteration 5400: Loss = -11202.2021484375
Iteration 5500: Loss = -11202.056640625
Iteration 5600: Loss = -11201.923828125
Iteration 5700: Loss = -11201.796875
Iteration 5800: Loss = -11201.6796875
Iteration 5900: Loss = -11201.56640625
Iteration 6000: Loss = -11201.4599609375
Iteration 6100: Loss = -11201.359375
Iteration 6200: Loss = -11201.267578125
Iteration 6300: Loss = -11201.1767578125
Iteration 6400: Loss = -11201.09375
Iteration 6500: Loss = -11201.013671875
Iteration 6600: Loss = -11200.9384765625
Iteration 6700: Loss = -11200.8681640625
Iteration 6800: Loss = -11200.80078125
Iteration 6900: Loss = -11200.736328125
Iteration 7000: Loss = -11200.67578125
Iteration 7100: Loss = -11200.6171875
Iteration 7200: Loss = -11200.564453125
Iteration 7300: Loss = -11200.5126953125
Iteration 7400: Loss = -11200.4638671875
Iteration 7500: Loss = -11200.4169921875
Iteration 7600: Loss = -11200.3740234375
Iteration 7700: Loss = -11200.3310546875
Iteration 7800: Loss = -11200.2919921875
Iteration 7900: Loss = -11200.2548828125
Iteration 8000: Loss = -11200.216796875
Iteration 8100: Loss = -11200.18359375
Iteration 8200: Loss = -11200.150390625
Iteration 8300: Loss = -11200.1201171875
Iteration 8400: Loss = -11200.0908203125
Iteration 8500: Loss = -11200.0634765625
Iteration 8600: Loss = -11200.0361328125
Iteration 8700: Loss = -11200.01171875
Iteration 8800: Loss = -11199.9873046875
Iteration 8900: Loss = -11199.96484375
Iteration 9000: Loss = -11199.943359375
Iteration 9100: Loss = -11199.9228515625
Iteration 9200: Loss = -11199.904296875
Iteration 9300: Loss = -11199.8857421875
Iteration 9400: Loss = -11199.8681640625
Iteration 9500: Loss = -11199.8515625
Iteration 9600: Loss = -11199.8349609375
Iteration 9700: Loss = -11199.8212890625
Iteration 9800: Loss = -11199.806640625
Iteration 9900: Loss = -11199.79296875
Iteration 10000: Loss = -11199.7802734375
Iteration 10100: Loss = -11199.767578125
Iteration 10200: Loss = -11199.755859375
Iteration 10300: Loss = -11199.7451171875
Iteration 10400: Loss = -11199.7353515625
Iteration 10500: Loss = -11199.7236328125
Iteration 10600: Loss = -11199.7158203125
Iteration 10700: Loss = -11199.705078125
Iteration 10800: Loss = -11199.6953125
Iteration 10900: Loss = -11199.6884765625
Iteration 11000: Loss = -11199.681640625
Iteration 11100: Loss = -11199.673828125
Iteration 11200: Loss = -11199.6650390625
Iteration 11300: Loss = -11199.6591796875
Iteration 11400: Loss = -11199.6513671875
Iteration 11500: Loss = -11199.6474609375
Iteration 11600: Loss = -11199.6416015625
Iteration 11700: Loss = -11199.634765625
Iteration 11800: Loss = -11199.62890625
Iteration 11900: Loss = -11199.625
Iteration 12000: Loss = -11199.62109375
Iteration 12100: Loss = -11199.6162109375
Iteration 12200: Loss = -11199.611328125
Iteration 12300: Loss = -11199.6083984375
Iteration 12400: Loss = -11199.6044921875
Iteration 12500: Loss = -11199.6015625
Iteration 12600: Loss = -11199.59765625
Iteration 12700: Loss = -11199.5947265625
Iteration 12800: Loss = -11199.5908203125
Iteration 12900: Loss = -11199.58984375
Iteration 13000: Loss = -11199.5859375
Iteration 13100: Loss = -11199.5830078125
Iteration 13200: Loss = -11199.58203125
Iteration 13300: Loss = -11199.578125
Iteration 13400: Loss = -11199.576171875
Iteration 13500: Loss = -11199.57421875
Iteration 13600: Loss = -11199.572265625
Iteration 13700: Loss = -11199.5712890625
Iteration 13800: Loss = -11199.5703125
Iteration 13900: Loss = -11199.568359375
Iteration 14000: Loss = -11199.56640625
Iteration 14100: Loss = -11199.56640625
Iteration 14200: Loss = -11199.5634765625
Iteration 14300: Loss = -11199.5625
Iteration 14400: Loss = -11199.5615234375
Iteration 14500: Loss = -11199.5615234375
Iteration 14600: Loss = -11199.560546875
Iteration 14700: Loss = -11199.5576171875
Iteration 14800: Loss = -11199.5595703125
1
Iteration 14900: Loss = -11199.556640625
Iteration 15000: Loss = -11199.5556640625
Iteration 15100: Loss = -11199.5537109375
Iteration 15200: Loss = -11199.552734375
Iteration 15300: Loss = -11199.5546875
1
Iteration 15400: Loss = -11199.55078125
Iteration 15500: Loss = -11199.55078125
Iteration 15600: Loss = -11199.5498046875
Iteration 15700: Loss = -11199.5498046875
Iteration 15800: Loss = -11199.5478515625
Iteration 15900: Loss = -11199.546875
Iteration 16000: Loss = -11199.544921875
Iteration 16100: Loss = -11199.544921875
Iteration 16200: Loss = -11199.544921875
Iteration 16300: Loss = -11199.54296875
Iteration 16400: Loss = -11199.541015625
Iteration 16500: Loss = -11199.5400390625
Iteration 16600: Loss = -11199.5380859375
Iteration 16700: Loss = -11199.5380859375
Iteration 16800: Loss = -11199.5361328125
Iteration 16900: Loss = -11199.5361328125
Iteration 17000: Loss = -11199.53515625
Iteration 17100: Loss = -11199.53515625
Iteration 17200: Loss = -11199.53515625
Iteration 17300: Loss = -11199.5322265625
Iteration 17400: Loss = -11199.533203125
1
Iteration 17500: Loss = -11199.5322265625
Iteration 17600: Loss = -11199.533203125
1
Iteration 17700: Loss = -11199.53125
Iteration 17800: Loss = -11199.53125
Iteration 17900: Loss = -11199.5302734375
Iteration 18000: Loss = -11199.5302734375
Iteration 18100: Loss = -11199.529296875
Iteration 18200: Loss = -11199.529296875
Iteration 18300: Loss = -11199.529296875
Iteration 18400: Loss = -11199.52734375
Iteration 18500: Loss = -11199.52734375
Iteration 18600: Loss = -11199.5263671875
Iteration 18700: Loss = -11199.525390625
Iteration 18800: Loss = -11199.5234375
Iteration 18900: Loss = -11199.5244140625
1
Iteration 19000: Loss = -11199.521484375
Iteration 19100: Loss = -11199.51953125
Iteration 19200: Loss = -11199.515625
Iteration 19300: Loss = -11199.5146484375
Iteration 19400: Loss = -11199.51171875
Iteration 19500: Loss = -11199.505859375
Iteration 19600: Loss = -11199.5
Iteration 19700: Loss = -11199.48828125
Iteration 19800: Loss = -11199.4580078125
Iteration 19900: Loss = -11199.380859375
Iteration 20000: Loss = -11199.203125
Iteration 20100: Loss = -11199.18359375
Iteration 20200: Loss = -11199.169921875
Iteration 20300: Loss = -11199.1611328125
Iteration 20400: Loss = -11199.15625
Iteration 20500: Loss = -11199.150390625
Iteration 20600: Loss = -11199.138671875
Iteration 20700: Loss = -11199.140625
1
Iteration 20800: Loss = -11199.1171875
Iteration 20900: Loss = -11199.115234375
Iteration 21000: Loss = -11199.1162109375
1
Iteration 21100: Loss = -11199.115234375
Iteration 21200: Loss = -11199.111328125
Iteration 21300: Loss = -11199.10546875
Iteration 21400: Loss = -11199.103515625
Iteration 21500: Loss = -11199.0869140625
Iteration 21600: Loss = -11199.0791015625
Iteration 21700: Loss = -11199.06640625
Iteration 21800: Loss = -11199.0654296875
Iteration 21900: Loss = -11199.056640625
Iteration 22000: Loss = -11199.052734375
Iteration 22100: Loss = -11199.04296875
Iteration 22200: Loss = -11199.0419921875
Iteration 22300: Loss = -11199.033203125
Iteration 22400: Loss = -11199.03125
Iteration 22500: Loss = -11199.0302734375
Iteration 22600: Loss = -11199.0283203125
Iteration 22700: Loss = -11199.02734375
Iteration 22800: Loss = -11199.0263671875
Iteration 22900: Loss = -11199.02734375
1
Iteration 23000: Loss = -11199.0234375
Iteration 23100: Loss = -11199.021484375
Iteration 23200: Loss = -11199.01953125
Iteration 23300: Loss = -11199.0185546875
Iteration 23400: Loss = -11199.015625
Iteration 23500: Loss = -11199.0146484375
Iteration 23600: Loss = -11199.013671875
Iteration 23700: Loss = -11199.0126953125
Iteration 23800: Loss = -11199.0068359375
Iteration 23900: Loss = -11199.0068359375
Iteration 24000: Loss = -11199.0068359375
Iteration 24100: Loss = -11199.0048828125
Iteration 24200: Loss = -11199.005859375
1
Iteration 24300: Loss = -11199.00390625
Iteration 24400: Loss = -11199.00390625
Iteration 24500: Loss = -11199.001953125
Iteration 24600: Loss = -11199.001953125
Iteration 24700: Loss = -11198.9990234375
Iteration 24800: Loss = -11199.0
1
Iteration 24900: Loss = -11198.9990234375
Iteration 25000: Loss = -11199.0
1
Iteration 25100: Loss = -11198.9990234375
Iteration 25200: Loss = -11199.0
1
Iteration 25300: Loss = -11198.9990234375
Iteration 25400: Loss = -11198.9990234375
Iteration 25500: Loss = -11199.0
1
Iteration 25600: Loss = -11198.9990234375
Iteration 25700: Loss = -11198.9990234375
Iteration 25800: Loss = -11199.0
1
Iteration 25900: Loss = -11199.0009765625
2
Iteration 26000: Loss = -11198.9990234375
Iteration 26100: Loss = -11199.0
1
Iteration 26200: Loss = -11198.99609375
Iteration 26300: Loss = -11198.99609375
Iteration 26400: Loss = -11198.99609375
Iteration 26500: Loss = -11198.9951171875
Iteration 26600: Loss = -11198.9951171875
Iteration 26700: Loss = -11198.9921875
Iteration 26800: Loss = -11198.990234375
Iteration 26900: Loss = -11198.98828125
Iteration 27000: Loss = -11198.9755859375
Iteration 27100: Loss = -11198.9755859375
Iteration 27200: Loss = -11198.9482421875
Iteration 27300: Loss = -11198.943359375
Iteration 27400: Loss = -11198.8056640625
Iteration 27500: Loss = -11198.76953125
Iteration 27600: Loss = -11198.7021484375
Iteration 27700: Loss = -11198.6640625
Iteration 27800: Loss = -11198.6611328125
Iteration 27900: Loss = -11198.66015625
Iteration 28000: Loss = -11198.6455078125
Iteration 28100: Loss = -11198.646484375
1
Iteration 28200: Loss = -11198.6298828125
Iteration 28300: Loss = -11198.630859375
1
Iteration 28400: Loss = -11198.62890625
Iteration 28500: Loss = -11198.62890625
Iteration 28600: Loss = -11198.6298828125
1
Iteration 28700: Loss = -11198.62890625
Iteration 28800: Loss = -11198.6279296875
Iteration 28900: Loss = -11198.6298828125
1
Iteration 29000: Loss = -11198.62890625
2
Iteration 29100: Loss = -11198.62890625
3
Iteration 29200: Loss = -11198.6318359375
4
Iteration 29300: Loss = -11198.6298828125
5
Iteration 29400: Loss = -11198.59765625
Iteration 29500: Loss = -11198.5732421875
Iteration 29600: Loss = -11198.572265625
Iteration 29700: Loss = -11198.572265625
Iteration 29800: Loss = -11198.572265625
Iteration 29900: Loss = -11198.5712890625
pi: tensor([[0.0047, 0.9953],
        [0.0246, 0.9754]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0100, 0.9900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2707, 0.1078],
         [0.0106, 0.1668]],

        [[0.9779, 0.1955],
         [0.9600, 0.1042]],

        [[0.2326, 0.2288],
         [0.1071, 0.9549]],

        [[0.0454, 0.2353],
         [0.7418, 0.0127]],

        [[0.6758, 0.1282],
         [0.0883, 0.7446]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0010583936399217598, 0.0] [0.0, 0.0] [11199.4248046875, 11198.53515625]
-------------------------------------
This iteration is 74
True Objective function: Loss = -10888.416703605853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -55470.75
Iteration 100: Loss = -37520.7421875
Iteration 200: Loss = -22296.009765625
Iteration 300: Loss = -14721.44921875
Iteration 400: Loss = -12177.396484375
Iteration 500: Loss = -11435.439453125
Iteration 600: Loss = -11217.4287109375
Iteration 700: Loss = -11146.1455078125
Iteration 800: Loss = -11104.6181640625
Iteration 900: Loss = -11067.68359375
Iteration 1000: Loss = -11054.75390625
Iteration 1100: Loss = -11041.90625
Iteration 1200: Loss = -11036.9130859375
Iteration 1300: Loss = -11033.0556640625
Iteration 1400: Loss = -11029.9599609375
Iteration 1500: Loss = -11027.4208984375
Iteration 1600: Loss = -11025.3037109375
Iteration 1700: Loss = -11023.5146484375
Iteration 1800: Loss = -11021.984375
Iteration 1900: Loss = -11020.640625
Iteration 2000: Loss = -11019.4853515625
Iteration 2100: Loss = -11018.4326171875
Iteration 2200: Loss = -11017.5263671875
Iteration 2300: Loss = -11016.7138671875
Iteration 2400: Loss = -11015.9658203125
Iteration 2500: Loss = -11015.2763671875
Iteration 2600: Loss = -11014.6806640625
Iteration 2700: Loss = -11014.1376953125
Iteration 2800: Loss = -11013.638671875
Iteration 2900: Loss = -11013.18359375
Iteration 3000: Loss = -11012.767578125
Iteration 3100: Loss = -11012.390625
Iteration 3200: Loss = -11012.046875
Iteration 3300: Loss = -11011.734375
Iteration 3400: Loss = -11011.4501953125
Iteration 3500: Loss = -11011.1875
Iteration 3600: Loss = -11010.9453125
Iteration 3700: Loss = -11010.71875
Iteration 3800: Loss = -11010.509765625
Iteration 3900: Loss = -11010.318359375
Iteration 4000: Loss = -11010.1396484375
Iteration 4100: Loss = -11009.97265625
Iteration 4200: Loss = -11009.8173828125
Iteration 4300: Loss = -11009.671875
Iteration 4400: Loss = -11009.533203125
Iteration 4500: Loss = -11009.40234375
Iteration 4600: Loss = -11009.27734375
Iteration 4700: Loss = -11009.1611328125
Iteration 4800: Loss = -11009.0498046875
Iteration 4900: Loss = -11008.9404296875
Iteration 5000: Loss = -11008.8359375
Iteration 5100: Loss = -11008.734375
Iteration 5200: Loss = -11008.638671875
Iteration 5300: Loss = -11008.544921875
Iteration 5400: Loss = -11008.4560546875
Iteration 5500: Loss = -11008.3671875
Iteration 5600: Loss = -11008.2822265625
Iteration 5700: Loss = -11008.19921875
Iteration 5800: Loss = -11008.1201171875
Iteration 5900: Loss = -11008.0439453125
Iteration 6000: Loss = -11007.966796875
Iteration 6100: Loss = -11005.025390625
Iteration 6200: Loss = -11004.638671875
Iteration 6300: Loss = -11004.5283203125
Iteration 6400: Loss = -11004.455078125
Iteration 6500: Loss = -11004.39453125
Iteration 6600: Loss = -11004.34375
Iteration 6700: Loss = -11004.302734375
Iteration 6800: Loss = -11004.2626953125
Iteration 6900: Loss = -11004.228515625
Iteration 7000: Loss = -11004.197265625
Iteration 7100: Loss = -11004.16796875
Iteration 7200: Loss = -11004.140625
Iteration 7300: Loss = -11004.1181640625
Iteration 7400: Loss = -11004.0947265625
Iteration 7500: Loss = -11004.0732421875
Iteration 7600: Loss = -11004.0537109375
Iteration 7700: Loss = -11004.03515625
Iteration 7800: Loss = -11004.017578125
Iteration 7900: Loss = -11004.0029296875
Iteration 8000: Loss = -11003.986328125
Iteration 8100: Loss = -11003.9736328125
Iteration 8200: Loss = -11003.9599609375
Iteration 8300: Loss = -11003.9462890625
Iteration 8400: Loss = -11003.935546875
Iteration 8500: Loss = -11003.923828125
Iteration 8600: Loss = -11003.9130859375
Iteration 8700: Loss = -11003.9033203125
Iteration 8800: Loss = -11003.8935546875
Iteration 8900: Loss = -11003.884765625
Iteration 9000: Loss = -11003.8759765625
Iteration 9100: Loss = -11003.8681640625
Iteration 9200: Loss = -11003.859375
Iteration 9300: Loss = -11003.853515625
Iteration 9400: Loss = -11003.845703125
Iteration 9500: Loss = -11003.83984375
Iteration 9600: Loss = -11003.8349609375
Iteration 9700: Loss = -11003.826171875
Iteration 9800: Loss = -11003.8212890625
Iteration 9900: Loss = -11003.8154296875
Iteration 10000: Loss = -11003.810546875
Iteration 10100: Loss = -11003.806640625
Iteration 10200: Loss = -11003.802734375
Iteration 10300: Loss = -11003.796875
Iteration 10400: Loss = -11003.7939453125
Iteration 10500: Loss = -11003.7900390625
Iteration 10600: Loss = -11003.78515625
Iteration 10700: Loss = -11003.7822265625
Iteration 10800: Loss = -11003.779296875
Iteration 10900: Loss = -11003.775390625
Iteration 11000: Loss = -11003.771484375
Iteration 11100: Loss = -11003.76953125
Iteration 11200: Loss = -11003.767578125
Iteration 11300: Loss = -11003.7646484375
Iteration 11400: Loss = -11003.76171875
Iteration 11500: Loss = -11003.7578125
Iteration 11600: Loss = -11003.7568359375
Iteration 11700: Loss = -11003.7548828125
Iteration 11800: Loss = -11003.751953125
Iteration 11900: Loss = -11003.7490234375
Iteration 12000: Loss = -11003.748046875
Iteration 12100: Loss = -11003.74609375
Iteration 12200: Loss = -11003.7431640625
Iteration 12300: Loss = -11003.7412109375
Iteration 12400: Loss = -11003.740234375
Iteration 12500: Loss = -11003.7392578125
Iteration 12600: Loss = -11003.736328125
Iteration 12700: Loss = -11003.7353515625
Iteration 12800: Loss = -11003.7353515625
Iteration 12900: Loss = -11003.7333984375
Iteration 13000: Loss = -11003.7314453125
Iteration 13100: Loss = -11003.7314453125
Iteration 13200: Loss = -11003.7294921875
Iteration 13300: Loss = -11003.728515625
Iteration 13400: Loss = -11003.7255859375
Iteration 13500: Loss = -11003.724609375
Iteration 13600: Loss = -11003.7216796875
Iteration 13700: Loss = -11003.7158203125
Iteration 13800: Loss = -11003.70703125
Iteration 13900: Loss = -11003.701171875
Iteration 14000: Loss = -11003.6982421875
Iteration 14100: Loss = -11003.6943359375
Iteration 14200: Loss = -11003.69140625
Iteration 14300: Loss = -11003.6904296875
Iteration 14400: Loss = -11003.6875
Iteration 14500: Loss = -11003.685546875
Iteration 14600: Loss = -11003.685546875
Iteration 14700: Loss = -11003.6845703125
Iteration 14800: Loss = -11003.68359375
Iteration 14900: Loss = -11003.6826171875
Iteration 15000: Loss = -11003.6806640625
Iteration 15100: Loss = -11003.6806640625
Iteration 15200: Loss = -11003.6787109375
Iteration 15300: Loss = -11003.6796875
1
Iteration 15400: Loss = -11003.6767578125
Iteration 15500: Loss = -11003.6767578125
Iteration 15600: Loss = -11003.6767578125
Iteration 15700: Loss = -11003.673828125
Iteration 15800: Loss = -11003.673828125
Iteration 15900: Loss = -11003.671875
Iteration 16000: Loss = -11003.6728515625
1
Iteration 16100: Loss = -11003.6708984375
Iteration 16200: Loss = -11003.671875
1
Iteration 16300: Loss = -11003.669921875
Iteration 16400: Loss = -11003.6669921875
Iteration 16500: Loss = -11003.6669921875
Iteration 16600: Loss = -11003.6630859375
Iteration 16700: Loss = -11003.662109375
Iteration 16800: Loss = -11003.662109375
Iteration 16900: Loss = -11003.6591796875
Iteration 17000: Loss = -11003.6572265625
Iteration 17100: Loss = -11003.65234375
Iteration 17200: Loss = -11003.6494140625
Iteration 17300: Loss = -11003.646484375
Iteration 17400: Loss = -11003.640625
Iteration 17500: Loss = -11003.63671875
Iteration 17600: Loss = -11003.62890625
Iteration 17700: Loss = -11003.6201171875
Iteration 17800: Loss = -11003.607421875
Iteration 17900: Loss = -11003.5986328125
Iteration 18000: Loss = -11003.5859375
Iteration 18100: Loss = -11003.5751953125
Iteration 18200: Loss = -11003.5673828125
Iteration 18300: Loss = -11003.5615234375
Iteration 18400: Loss = -11003.5595703125
Iteration 18500: Loss = -11003.556640625
Iteration 18600: Loss = -11003.5546875
Iteration 18700: Loss = -11003.5546875
Iteration 18800: Loss = -11003.5537109375
Iteration 18900: Loss = -11003.5556640625
1
Iteration 19000: Loss = -11003.5546875
2
Iteration 19100: Loss = -11003.5546875
3
Iteration 19200: Loss = -11003.552734375
Iteration 19300: Loss = -11003.5537109375
1
Iteration 19400: Loss = -11003.5537109375
2
Iteration 19500: Loss = -11003.52734375
Iteration 19600: Loss = -11003.52734375
Iteration 19700: Loss = -11003.52734375
Iteration 19800: Loss = -11003.52734375
Iteration 19900: Loss = -11003.5263671875
Iteration 20000: Loss = -11003.52734375
1
Iteration 20100: Loss = -11003.52734375
2
Iteration 20200: Loss = -11003.52734375
3
Iteration 20300: Loss = -11003.5283203125
4
Iteration 20400: Loss = -11003.52734375
5
Iteration 20500: Loss = -11003.52734375
6
Iteration 20600: Loss = -11003.5263671875
Iteration 20700: Loss = -11003.5263671875
Iteration 20800: Loss = -11003.5263671875
Iteration 20900: Loss = -11003.52734375
1
Iteration 21000: Loss = -11003.525390625
Iteration 21100: Loss = -11003.5263671875
1
Iteration 21200: Loss = -11003.5244140625
Iteration 21300: Loss = -11003.5244140625
Iteration 21400: Loss = -11003.5244140625
Iteration 21500: Loss = -11003.525390625
1
Iteration 21600: Loss = -11003.5224609375
Iteration 21700: Loss = -11002.951171875
Iteration 21800: Loss = -11002.91015625
Iteration 21900: Loss = -11002.8994140625
Iteration 22000: Loss = -11002.8818359375
Iteration 22100: Loss = -11002.86328125
Iteration 22200: Loss = -11002.732421875
Iteration 22300: Loss = -11002.693359375
Iteration 22400: Loss = -11002.693359375
Iteration 22500: Loss = -11002.6943359375
1
Iteration 22600: Loss = -11002.693359375
Iteration 22700: Loss = -11002.6796875
Iteration 22800: Loss = -11002.662109375
Iteration 22900: Loss = -11002.6435546875
Iteration 23000: Loss = -11002.611328125
Iteration 23100: Loss = -11002.59375
Iteration 23200: Loss = -11002.580078125
Iteration 23300: Loss = -11002.5595703125
Iteration 23400: Loss = -11002.5576171875
Iteration 23500: Loss = -11002.55078125
Iteration 23600: Loss = -11002.5400390625
Iteration 23700: Loss = -11002.537109375
Iteration 23800: Loss = -11002.5380859375
1
Iteration 23900: Loss = -11002.537109375
Iteration 24000: Loss = -11002.5341796875
Iteration 24100: Loss = -11002.462890625
Iteration 24200: Loss = -11002.455078125
Iteration 24300: Loss = -11002.4375
Iteration 24400: Loss = -11002.4375
Iteration 24500: Loss = -11002.439453125
1
Iteration 24600: Loss = -11002.435546875
Iteration 24700: Loss = -11002.4365234375
1
Iteration 24800: Loss = -11002.4365234375
2
Iteration 24900: Loss = -11002.4384765625
3
Iteration 25000: Loss = -11002.4365234375
4
Iteration 25100: Loss = -11002.435546875
Iteration 25200: Loss = -11002.435546875
Iteration 25300: Loss = -11002.4375
1
Iteration 25400: Loss = -11002.4345703125
Iteration 25500: Loss = -11002.4326171875
Iteration 25600: Loss = -11002.43359375
1
Iteration 25700: Loss = -11002.43359375
2
Iteration 25800: Loss = -11002.435546875
3
Iteration 25900: Loss = -11002.43359375
4
Iteration 26000: Loss = -11002.4345703125
5
Iteration 26100: Loss = -11002.435546875
6
Iteration 26200: Loss = -11002.4345703125
7
Iteration 26300: Loss = -11002.43359375
8
Iteration 26400: Loss = -11002.435546875
9
Iteration 26500: Loss = -11002.43359375
10
Iteration 26600: Loss = -11002.43359375
11
Iteration 26700: Loss = -11002.4345703125
12
Iteration 26800: Loss = -11002.4326171875
Iteration 26900: Loss = -11002.43359375
1
Iteration 27000: Loss = -11002.4326171875
Iteration 27100: Loss = -11002.4326171875
Iteration 27200: Loss = -11002.4326171875
Iteration 27300: Loss = -11002.4345703125
1
Iteration 27400: Loss = -11002.4326171875
Iteration 27500: Loss = -11002.43359375
1
Iteration 27600: Loss = -11002.43359375
2
Iteration 27700: Loss = -11002.43359375
3
Iteration 27800: Loss = -11002.4326171875
Iteration 27900: Loss = -11002.431640625
Iteration 28000: Loss = -11002.43359375
1
Iteration 28100: Loss = -11002.4326171875
2
Iteration 28200: Loss = -11002.431640625
Iteration 28300: Loss = -11002.4345703125
1
Iteration 28400: Loss = -11002.4326171875
2
Iteration 28500: Loss = -11002.4326171875
3
Iteration 28600: Loss = -11002.431640625
Iteration 28700: Loss = -11002.4326171875
1
Iteration 28800: Loss = -11002.4326171875
2
Iteration 28900: Loss = -11002.4326171875
3
Iteration 29000: Loss = -11002.4326171875
4
Iteration 29100: Loss = -11002.43359375
5
Iteration 29200: Loss = -11002.4296875
Iteration 29300: Loss = -11002.4326171875
1
Iteration 29400: Loss = -11002.431640625
2
Iteration 29500: Loss = -11002.4326171875
3
Iteration 29600: Loss = -11002.4326171875
4
Iteration 29700: Loss = -11002.431640625
5
Iteration 29800: Loss = -11002.431640625
6
Iteration 29900: Loss = -11002.4326171875
7
pi: tensor([[9.8426e-01, 1.5741e-02],
        [1.0000e+00, 2.9082e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9489, 0.0511], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1625, 0.1872],
         [0.0304, 0.2206]],

        [[0.9461, 0.2484],
         [0.0290, 0.0138]],

        [[0.9206, 0.0797],
         [0.0422, 0.0114]],

        [[0.0927, 0.0991],
         [0.1163, 0.9903]],

        [[0.1773, 0.2457],
         [0.0165, 0.9437]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0006956275301036035
Average Adjusted Rand Index: 0.0008888888888888889
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43450.0234375
Iteration 100: Loss = -26841.078125
Iteration 200: Loss = -14488.216796875
Iteration 300: Loss = -12065.455078125
Iteration 400: Loss = -11613.36328125
Iteration 500: Loss = -11442.0322265625
Iteration 600: Loss = -11346.0615234375
Iteration 700: Loss = -11277.55078125
Iteration 800: Loss = -11223.8974609375
Iteration 900: Loss = -11191.5546875
Iteration 1000: Loss = -11168.61328125
Iteration 1100: Loss = -11150.5830078125
Iteration 1200: Loss = -11134.0205078125
Iteration 1300: Loss = -11119.8291015625
Iteration 1400: Loss = -11108.107421875
Iteration 1500: Loss = -11095.1181640625
Iteration 1600: Loss = -11088.4365234375
Iteration 1700: Loss = -11081.9560546875
Iteration 1800: Loss = -11070.4697265625
Iteration 1900: Loss = -11063.4609375
Iteration 2000: Loss = -11059.076171875
Iteration 2100: Loss = -11051.509765625
Iteration 2200: Loss = -11048.033203125
Iteration 2300: Loss = -11039.5126953125
Iteration 2400: Loss = -11036.4609375
Iteration 2500: Loss = -11033.4228515625
Iteration 2600: Loss = -11030.7255859375
Iteration 2700: Loss = -11029.169921875
Iteration 2800: Loss = -11027.56640625
Iteration 2900: Loss = -11025.96875
Iteration 3000: Loss = -11024.8203125
Iteration 3100: Loss = -11023.9638671875
Iteration 3200: Loss = -11023.25
Iteration 3300: Loss = -11022.634765625
Iteration 3400: Loss = -11022.087890625
Iteration 3500: Loss = -11021.5986328125
Iteration 3600: Loss = -11021.1552734375
Iteration 3700: Loss = -11020.7529296875
Iteration 3800: Loss = -11020.384765625
Iteration 3900: Loss = -11020.046875
Iteration 4000: Loss = -11019.73828125
Iteration 4100: Loss = -11019.453125
Iteration 4200: Loss = -11019.1875
Iteration 4300: Loss = -11018.9443359375
Iteration 4400: Loss = -11018.71875
Iteration 4500: Loss = -11018.5068359375
Iteration 4600: Loss = -11018.3125
Iteration 4700: Loss = -11018.1279296875
Iteration 4800: Loss = -11017.9580078125
Iteration 4900: Loss = -11017.798828125
Iteration 5000: Loss = -11017.6474609375
Iteration 5100: Loss = -11017.5078125
Iteration 5200: Loss = -11017.3759765625
Iteration 5300: Loss = -11017.2509765625
Iteration 5400: Loss = -11017.134765625
Iteration 5500: Loss = -11017.021484375
Iteration 5600: Loss = -11016.9189453125
Iteration 5700: Loss = -11016.8203125
Iteration 5800: Loss = -11016.7275390625
Iteration 5900: Loss = -11016.640625
Iteration 6000: Loss = -11016.5576171875
Iteration 6100: Loss = -11016.48046875
Iteration 6200: Loss = -11016.4052734375
Iteration 6300: Loss = -11016.333984375
Iteration 6400: Loss = -11016.2666015625
Iteration 6500: Loss = -11016.205078125
Iteration 6600: Loss = -11016.1455078125
Iteration 6700: Loss = -11016.0888671875
Iteration 6800: Loss = -11016.03515625
Iteration 6900: Loss = -11015.984375
Iteration 7000: Loss = -11015.93359375
Iteration 7100: Loss = -11015.88671875
Iteration 7200: Loss = -11015.841796875
Iteration 7300: Loss = -11015.798828125
Iteration 7400: Loss = -11015.755859375
Iteration 7500: Loss = -11015.71484375
Iteration 7600: Loss = -11015.6767578125
Iteration 7700: Loss = -11015.640625
Iteration 7800: Loss = -11015.6064453125
Iteration 7900: Loss = -11015.5732421875
Iteration 8000: Loss = -11015.541015625
Iteration 8100: Loss = -11015.509765625
Iteration 8200: Loss = -11015.48046875
Iteration 8300: Loss = -11015.453125
Iteration 8400: Loss = -11015.423828125
Iteration 8500: Loss = -11015.400390625
Iteration 8600: Loss = -11015.3740234375
Iteration 8700: Loss = -11015.349609375
Iteration 8800: Loss = -11015.3271484375
Iteration 8900: Loss = -11015.306640625
Iteration 9000: Loss = -11015.2822265625
Iteration 9100: Loss = -11015.2626953125
Iteration 9200: Loss = -11015.2412109375
Iteration 9300: Loss = -11009.98046875
Iteration 9400: Loss = -11009.716796875
Iteration 9500: Loss = -11009.6806640625
Iteration 9600: Loss = -11009.65625
Iteration 9700: Loss = -11009.6396484375
Iteration 9800: Loss = -11009.6220703125
Iteration 9900: Loss = -11009.6064453125
Iteration 10000: Loss = -11009.591796875
Iteration 10100: Loss = -11009.5791015625
Iteration 10200: Loss = -11009.5654296875
Iteration 10300: Loss = -11009.55078125
Iteration 10400: Loss = -11009.537109375
Iteration 10500: Loss = -11009.5244140625
Iteration 10600: Loss = -11009.5126953125
Iteration 10700: Loss = -11009.498046875
Iteration 10800: Loss = -11009.4833984375
Iteration 10900: Loss = -11009.470703125
Iteration 11000: Loss = -11009.45703125
Iteration 11100: Loss = -11009.443359375
Iteration 11200: Loss = -11009.4287109375
Iteration 11300: Loss = -11009.4140625
Iteration 11400: Loss = -11009.3984375
Iteration 11500: Loss = -11004.001953125
Iteration 11600: Loss = -11003.7861328125
Iteration 11700: Loss = -11003.728515625
Iteration 11800: Loss = -11003.6953125
Iteration 11900: Loss = -11003.671875
Iteration 12000: Loss = -11003.65234375
Iteration 12100: Loss = -11003.640625
Iteration 12200: Loss = -11003.6259765625
Iteration 12300: Loss = -11003.6162109375
Iteration 12400: Loss = -11003.6064453125
Iteration 12500: Loss = -11003.59765625
Iteration 12600: Loss = -11003.5908203125
Iteration 12700: Loss = -11003.583984375
Iteration 12800: Loss = -11003.5771484375
Iteration 12900: Loss = -11003.572265625
Iteration 13000: Loss = -11003.5654296875
Iteration 13100: Loss = -11003.5625
Iteration 13200: Loss = -11003.55859375
Iteration 13300: Loss = -11003.552734375
Iteration 13400: Loss = -11003.5498046875
Iteration 13500: Loss = -11003.546875
Iteration 13600: Loss = -11003.544921875
Iteration 13700: Loss = -11003.541015625
Iteration 13800: Loss = -11003.5400390625
Iteration 13900: Loss = -11003.53515625
Iteration 14000: Loss = -11003.5341796875
Iteration 14100: Loss = -11003.5322265625
Iteration 14200: Loss = -11003.5302734375
Iteration 14300: Loss = -11003.5283203125
Iteration 14400: Loss = -11003.5263671875
Iteration 14500: Loss = -11003.5263671875
Iteration 14600: Loss = -11003.5234375
Iteration 14700: Loss = -11003.5234375
Iteration 14800: Loss = -11003.521484375
Iteration 14900: Loss = -11003.5205078125
Iteration 15000: Loss = -11003.51953125
Iteration 15100: Loss = -11003.51953125
Iteration 15200: Loss = -11003.517578125
Iteration 15300: Loss = -11003.5166015625
Iteration 15400: Loss = -11003.515625
Iteration 15500: Loss = -11003.5146484375
Iteration 15600: Loss = -11003.5146484375
Iteration 15700: Loss = -11003.513671875
Iteration 15800: Loss = -11003.5126953125
Iteration 15900: Loss = -11003.513671875
1
Iteration 16000: Loss = -11003.51171875
Iteration 16100: Loss = -11003.51171875
Iteration 16200: Loss = -11003.5107421875
Iteration 16300: Loss = -11003.51171875
1
Iteration 16400: Loss = -11003.509765625
Iteration 16500: Loss = -11003.5107421875
1
Iteration 16600: Loss = -11003.509765625
Iteration 16700: Loss = -11003.5078125
Iteration 16800: Loss = -11003.5068359375
Iteration 16900: Loss = -11003.5078125
1
Iteration 17000: Loss = -11003.5087890625
2
Iteration 17100: Loss = -11003.5078125
3
Iteration 17200: Loss = -11003.5078125
4
Iteration 17300: Loss = -11003.5078125
5
Iteration 17400: Loss = -11003.50390625
Iteration 17500: Loss = -11003.505859375
1
Iteration 17600: Loss = -11003.505859375
2
Iteration 17700: Loss = -11003.5068359375
3
Iteration 17800: Loss = -11003.5068359375
4
Iteration 17900: Loss = -11003.505859375
5
Iteration 18000: Loss = -11003.50390625
Iteration 18100: Loss = -11003.50390625
Iteration 18200: Loss = -11003.505859375
1
Iteration 18300: Loss = -11003.5048828125
2
Iteration 18400: Loss = -11003.5048828125
3
Iteration 18500: Loss = -11003.5048828125
4
Iteration 18600: Loss = -11003.5029296875
Iteration 18700: Loss = -11003.5048828125
1
Iteration 18800: Loss = -11003.5048828125
2
Iteration 18900: Loss = -11003.50390625
3
Iteration 19000: Loss = -11003.50390625
4
Iteration 19100: Loss = -11003.5048828125
5
Iteration 19200: Loss = -11003.5048828125
6
Iteration 19300: Loss = -11003.5029296875
Iteration 19400: Loss = -11003.50390625
1
Iteration 19500: Loss = -11003.501953125
Iteration 19600: Loss = -11003.50390625
1
Iteration 19700: Loss = -11003.50390625
2
Iteration 19800: Loss = -11003.5029296875
3
Iteration 19900: Loss = -11003.50390625
4
Iteration 20000: Loss = -11003.5048828125
5
Iteration 20100: Loss = -11003.50390625
6
Iteration 20200: Loss = -11003.5029296875
7
Iteration 20300: Loss = -11003.50390625
8
Iteration 20400: Loss = -11003.50390625
9
Iteration 20500: Loss = -11003.5
Iteration 20600: Loss = -11003.501953125
1
Iteration 20700: Loss = -11003.501953125
2
Iteration 20800: Loss = -11003.5009765625
3
Iteration 20900: Loss = -11003.5029296875
4
Iteration 21000: Loss = -11003.5029296875
5
Iteration 21100: Loss = -11003.5029296875
6
Iteration 21200: Loss = -11003.5029296875
7
Iteration 21300: Loss = -11003.5029296875
8
Iteration 21400: Loss = -11003.5029296875
9
Iteration 21500: Loss = -11003.501953125
10
Iteration 21600: Loss = -11003.501953125
11
Iteration 21700: Loss = -11003.5029296875
12
Iteration 21800: Loss = -11003.5029296875
13
Iteration 21900: Loss = -11003.50390625
14
Iteration 22000: Loss = -11003.501953125
15
Stopping early at iteration 22000 due to no improvement.
pi: tensor([[1.0000e+00, 1.3636e-06],
        [1.0000e+00, 2.0036e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7133, 0.2867], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1622, 0.1689],
         [0.6530, 0.1763]],

        [[0.0375, 0.2495],
         [0.9338, 0.8735]],

        [[0.4255, 0.1857],
         [0.8134, 0.1379]],

        [[0.8241, 0.4858],
         [0.8022, 0.0277]],

        [[0.9759, 0.2344],
         [0.0674, 0.9800]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0006956275301036035, 0.0] [0.0008888888888888889, 0.0] [11002.4306640625, 11003.501953125]
-------------------------------------
This iteration is 75
True Objective function: Loss = -10955.327733799195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41106.203125
Iteration 100: Loss = -26841.41796875
Iteration 200: Loss = -14736.3173828125
Iteration 300: Loss = -12317.501953125
Iteration 400: Loss = -11626.5927734375
Iteration 500: Loss = -11442.06640625
Iteration 600: Loss = -11358.484375
Iteration 700: Loss = -11308.7998046875
Iteration 800: Loss = -11272.3740234375
Iteration 900: Loss = -11248.646484375
Iteration 1000: Loss = -11230.3681640625
Iteration 1100: Loss = -11215.2236328125
Iteration 1200: Loss = -11201.841796875
Iteration 1300: Loss = -11190.15234375
Iteration 1400: Loss = -11179.046875
Iteration 1500: Loss = -11166.78515625
Iteration 1600: Loss = -11159.048828125
Iteration 1700: Loss = -11150.01953125
Iteration 1800: Loss = -11143.794921875
Iteration 1900: Loss = -11137.6474609375
Iteration 2000: Loss = -11132.51171875
Iteration 2100: Loss = -11124.7119140625
Iteration 2200: Loss = -11118.8818359375
Iteration 2300: Loss = -11113.345703125
Iteration 2400: Loss = -11108.3095703125
Iteration 2500: Loss = -11103.373046875
Iteration 2600: Loss = -11098.9169921875
Iteration 2700: Loss = -11094.634765625
Iteration 2800: Loss = -11088.091796875
Iteration 2900: Loss = -11085.16015625
Iteration 3000: Loss = -11079.1630859375
Iteration 3100: Loss = -11076.349609375
Iteration 3200: Loss = -11073.7041015625
Iteration 3300: Loss = -11070.4169921875
Iteration 3400: Loss = -11069.37890625
Iteration 3500: Loss = -11068.5810546875
Iteration 3600: Loss = -11067.91796875
Iteration 3700: Loss = -11067.34765625
Iteration 3800: Loss = -11066.84765625
Iteration 3900: Loss = -11066.4013671875
Iteration 4000: Loss = -11065.9970703125
Iteration 4100: Loss = -11065.638671875
Iteration 4200: Loss = -11065.3046875
Iteration 4300: Loss = -11065.001953125
Iteration 4400: Loss = -11064.7216796875
Iteration 4500: Loss = -11064.4638671875
Iteration 4600: Loss = -11064.2265625
Iteration 4700: Loss = -11064.00390625
Iteration 4800: Loss = -11063.7958984375
Iteration 4900: Loss = -11063.6025390625
Iteration 5000: Loss = -11063.4140625
Iteration 5100: Loss = -11063.2333984375
Iteration 5200: Loss = -11063.0498046875
Iteration 5300: Loss = -11062.8544921875
Iteration 5400: Loss = -11062.630859375
Iteration 5500: Loss = -11062.3828125
Iteration 5600: Loss = -11062.15625
Iteration 5700: Loss = -11061.9619140625
Iteration 5800: Loss = -11061.7626953125
Iteration 5900: Loss = -11061.34765625
Iteration 6000: Loss = -11060.623046875
Iteration 6100: Loss = -11060.3720703125
Iteration 6200: Loss = -11060.2265625
Iteration 6300: Loss = -11060.1123046875
Iteration 6400: Loss = -11060.017578125
Iteration 6500: Loss = -11059.9306640625
Iteration 6600: Loss = -11059.8525390625
Iteration 6700: Loss = -11059.779296875
Iteration 6800: Loss = -11056.298828125
Iteration 6900: Loss = -11055.7841796875
Iteration 7000: Loss = -11055.6708984375
Iteration 7100: Loss = -11055.5966796875
Iteration 7200: Loss = -11055.5380859375
Iteration 7300: Loss = -11055.490234375
Iteration 7400: Loss = -11055.4462890625
Iteration 7500: Loss = -11055.408203125
Iteration 7600: Loss = -11055.3701171875
Iteration 7700: Loss = -11050.2587890625
Iteration 7800: Loss = -11049.4853515625
Iteration 7900: Loss = -11049.294921875
Iteration 8000: Loss = -11049.1796875
Iteration 8100: Loss = -11049.0947265625
Iteration 8200: Loss = -11049.02734375
Iteration 8300: Loss = -11048.9736328125
Iteration 8400: Loss = -11048.923828125
Iteration 8500: Loss = -11048.8837890625
Iteration 8600: Loss = -11048.84375
Iteration 8700: Loss = -11048.8115234375
Iteration 8800: Loss = -11048.779296875
Iteration 8900: Loss = -11048.7529296875
Iteration 9000: Loss = -11048.7265625
Iteration 9100: Loss = -11048.7021484375
Iteration 9200: Loss = -11048.6806640625
Iteration 9300: Loss = -11048.6611328125
Iteration 9400: Loss = -11048.6416015625
Iteration 9500: Loss = -11048.625
Iteration 9600: Loss = -11048.6064453125
Iteration 9700: Loss = -11048.5927734375
Iteration 9800: Loss = -11048.578125
Iteration 9900: Loss = -11048.564453125
Iteration 10000: Loss = -11048.5517578125
Iteration 10100: Loss = -11048.541015625
Iteration 10200: Loss = -11048.5283203125
Iteration 10300: Loss = -11048.5185546875
Iteration 10400: Loss = -11048.5068359375
Iteration 10500: Loss = -11048.4990234375
Iteration 10600: Loss = -11048.490234375
Iteration 10700: Loss = -11048.48046875
Iteration 10800: Loss = -11048.4736328125
Iteration 10900: Loss = -11048.466796875
Iteration 11000: Loss = -11048.458984375
Iteration 11100: Loss = -11048.4521484375
Iteration 11200: Loss = -11048.447265625
Iteration 11300: Loss = -11048.4404296875
Iteration 11400: Loss = -11048.4345703125
Iteration 11500: Loss = -11048.4287109375
Iteration 11600: Loss = -11048.42578125
Iteration 11700: Loss = -11048.4208984375
Iteration 11800: Loss = -11048.4150390625
Iteration 11900: Loss = -11048.4130859375
Iteration 12000: Loss = -11048.408203125
Iteration 12100: Loss = -11048.4033203125
Iteration 12200: Loss = -11048.400390625
Iteration 12300: Loss = -11048.3955078125
Iteration 12400: Loss = -11048.392578125
Iteration 12500: Loss = -11048.3896484375
Iteration 12600: Loss = -11048.388671875
Iteration 12700: Loss = -11048.3837890625
Iteration 12800: Loss = -11048.3798828125
Iteration 12900: Loss = -11048.3779296875
Iteration 13000: Loss = -11048.3759765625
Iteration 13100: Loss = -11048.3740234375
Iteration 13200: Loss = -11048.3720703125
Iteration 13300: Loss = -11048.369140625
Iteration 13400: Loss = -11048.3681640625
Iteration 13500: Loss = -11048.3642578125
Iteration 13600: Loss = -11048.36328125
Iteration 13700: Loss = -11048.3623046875
Iteration 13800: Loss = -11048.361328125
Iteration 13900: Loss = -11048.3603515625
Iteration 14000: Loss = -11048.3583984375
Iteration 14100: Loss = -11048.35546875
Iteration 14200: Loss = -11048.3564453125
1
Iteration 14300: Loss = -11048.3515625
Iteration 14400: Loss = -11048.3525390625
1
Iteration 14500: Loss = -11048.3525390625
2
Iteration 14600: Loss = -11048.3515625
Iteration 14700: Loss = -11048.3505859375
Iteration 14800: Loss = -11048.349609375
Iteration 14900: Loss = -11048.349609375
Iteration 15000: Loss = -11048.3486328125
Iteration 15100: Loss = -11048.345703125
Iteration 15200: Loss = -11048.345703125
Iteration 15300: Loss = -11048.345703125
Iteration 15400: Loss = -11048.34375
Iteration 15500: Loss = -11048.34375
Iteration 15600: Loss = -11048.34375
Iteration 15700: Loss = -11048.3427734375
Iteration 15800: Loss = -11048.3408203125
Iteration 15900: Loss = -11048.3427734375
1
Iteration 16000: Loss = -11048.3408203125
Iteration 16100: Loss = -11048.3408203125
Iteration 16200: Loss = -11048.3408203125
Iteration 16300: Loss = -11048.33984375
Iteration 16400: Loss = -11048.33984375
Iteration 16500: Loss = -11048.33984375
Iteration 16600: Loss = -11048.33984375
Iteration 16700: Loss = -11048.3369140625
Iteration 16800: Loss = -11048.3388671875
1
Iteration 16900: Loss = -11048.3369140625
Iteration 17000: Loss = -11048.337890625
1
Iteration 17100: Loss = -11048.3369140625
Iteration 17200: Loss = -11048.3359375
Iteration 17300: Loss = -11048.3349609375
Iteration 17400: Loss = -11048.3359375
1
Iteration 17500: Loss = -11048.337890625
2
Iteration 17600: Loss = -11048.3349609375
Iteration 17700: Loss = -11048.3359375
1
Iteration 17800: Loss = -11048.333984375
Iteration 17900: Loss = -11048.3349609375
1
Iteration 18000: Loss = -11048.3369140625
2
Iteration 18100: Loss = -11048.3349609375
3
Iteration 18200: Loss = -11048.3349609375
4
Iteration 18300: Loss = -11048.333984375
Iteration 18400: Loss = -11048.3330078125
Iteration 18500: Loss = -11048.333984375
1
Iteration 18600: Loss = -11048.3330078125
Iteration 18700: Loss = -11048.333984375
1
Iteration 18800: Loss = -11048.3330078125
Iteration 18900: Loss = -11048.333984375
1
Iteration 19000: Loss = -11048.33203125
Iteration 19100: Loss = -11048.3330078125
1
Iteration 19200: Loss = -11048.33203125
Iteration 19300: Loss = -11048.33203125
Iteration 19400: Loss = -11048.333984375
1
Iteration 19500: Loss = -11048.333984375
2
Iteration 19600: Loss = -11048.3349609375
3
Iteration 19700: Loss = -11048.33203125
Iteration 19800: Loss = -11048.333984375
1
Iteration 19900: Loss = -11048.33203125
Iteration 20000: Loss = -11048.333984375
1
Iteration 20100: Loss = -11048.3330078125
2
Iteration 20200: Loss = -11048.333984375
3
Iteration 20300: Loss = -11048.333984375
4
Iteration 20400: Loss = -11048.3330078125
5
Iteration 20500: Loss = -11048.3330078125
6
Iteration 20600: Loss = -11048.33203125
Iteration 20700: Loss = -11048.33203125
Iteration 20800: Loss = -11048.33203125
Iteration 20900: Loss = -11048.3330078125
1
Iteration 21000: Loss = -11048.33203125
Iteration 21100: Loss = -11048.333984375
1
Iteration 21200: Loss = -11048.3330078125
2
Iteration 21300: Loss = -11048.33203125
Iteration 21400: Loss = -11048.3310546875
Iteration 21500: Loss = -11048.33203125
1
Iteration 21600: Loss = -11048.3330078125
2
Iteration 21700: Loss = -11048.333984375
3
Iteration 21800: Loss = -11048.3310546875
Iteration 21900: Loss = -11048.3310546875
Iteration 22000: Loss = -11048.33203125
1
Iteration 22100: Loss = -11048.33203125
2
Iteration 22200: Loss = -11048.3310546875
Iteration 22300: Loss = -11048.33203125
1
Iteration 22400: Loss = -11048.3330078125
2
Iteration 22500: Loss = -11048.3330078125
3
Iteration 22600: Loss = -11048.3310546875
Iteration 22700: Loss = -11048.3330078125
1
Iteration 22800: Loss = -11048.33203125
2
Iteration 22900: Loss = -11048.333984375
3
Iteration 23000: Loss = -11048.33203125
4
Iteration 23100: Loss = -11048.33203125
5
Iteration 23200: Loss = -11048.33203125
6
Iteration 23300: Loss = -11048.3310546875
Iteration 23400: Loss = -11048.33203125
1
Iteration 23500: Loss = -11048.33203125
2
Iteration 23600: Loss = -11048.33203125
3
Iteration 23700: Loss = -11048.3330078125
4
Iteration 23800: Loss = -11048.33203125
5
Iteration 23900: Loss = -11048.3330078125
6
Iteration 24000: Loss = -11048.3310546875
Iteration 24100: Loss = -11048.33203125
1
Iteration 24200: Loss = -11048.3310546875
Iteration 24300: Loss = -11048.333984375
1
Iteration 24400: Loss = -11048.3310546875
Iteration 24500: Loss = -11048.3330078125
1
Iteration 24600: Loss = -11048.3310546875
Iteration 24700: Loss = -11048.33203125
1
Iteration 24800: Loss = -11048.33203125
2
Iteration 24900: Loss = -11048.3330078125
3
Iteration 25000: Loss = -11048.330078125
Iteration 25100: Loss = -11048.33203125
1
Iteration 25200: Loss = -11048.3330078125
2
Iteration 25300: Loss = -11048.33203125
3
Iteration 25400: Loss = -11048.33203125
4
Iteration 25500: Loss = -11048.333984375
5
Iteration 25600: Loss = -11048.33203125
6
Iteration 25700: Loss = -11048.330078125
Iteration 25800: Loss = -11048.3330078125
1
Iteration 25900: Loss = -11048.3330078125
2
Iteration 26000: Loss = -11048.33203125
3
Iteration 26100: Loss = -11048.33203125
4
Iteration 26200: Loss = -11048.33203125
5
Iteration 26300: Loss = -11048.33203125
6
Iteration 26400: Loss = -11048.33203125
7
Iteration 26500: Loss = -11048.33203125
8
Iteration 26600: Loss = -11048.33203125
9
Iteration 26700: Loss = -11048.3330078125
10
Iteration 26800: Loss = -11048.3330078125
11
Iteration 26900: Loss = -11048.333984375
12
Iteration 27000: Loss = -11048.3310546875
13
Iteration 27100: Loss = -11048.3330078125
14
Iteration 27200: Loss = -11048.33203125
15
Stopping early at iteration 27200 due to no improvement.
pi: tensor([[1.0000e+00, 1.0129e-06],
        [1.0000e+00, 1.5437e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5654, 0.4346], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1690, 0.0983],
         [0.9502, 0.2287]],

        [[0.7036, 0.1875],
         [0.4632, 0.7799]],

        [[0.4998, 0.1520],
         [0.9607, 0.7481]],

        [[0.9735, 0.2855],
         [0.0261, 0.5719]],

        [[0.9495, 0.2491],
         [0.8204, 0.0175]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 12
Adjusted Rand Index: 0.5733734495489129
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06160762587046823
Average Adjusted Rand Index: 0.11467468990978258
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17959.759765625
Iteration 100: Loss = -12827.291015625
Iteration 200: Loss = -11367.94140625
Iteration 300: Loss = -11155.20703125
Iteration 400: Loss = -11116.1064453125
Iteration 500: Loss = -11094.7939453125
Iteration 600: Loss = -11081.66796875
Iteration 700: Loss = -11070.232421875
Iteration 800: Loss = -11063.2958984375
Iteration 900: Loss = -11058.685546875
Iteration 1000: Loss = -11055.9189453125
Iteration 1100: Loss = -11054.49609375
Iteration 1200: Loss = -11053.626953125
Iteration 1300: Loss = -11053.0361328125
Iteration 1400: Loss = -11052.642578125
Iteration 1500: Loss = -11052.3564453125
Iteration 1600: Loss = -11052.1279296875
Iteration 1700: Loss = -11051.93359375
Iteration 1800: Loss = -11051.78125
Iteration 1900: Loss = -11051.6669921875
Iteration 2000: Loss = -11051.5615234375
Iteration 2100: Loss = -11051.474609375
Iteration 2200: Loss = -11051.41015625
Iteration 2300: Loss = -11051.359375
Iteration 2400: Loss = -11051.3125
Iteration 2500: Loss = -11051.2685546875
Iteration 2600: Loss = -11051.2353515625
Iteration 2700: Loss = -11051.203125
Iteration 2800: Loss = -11051.173828125
Iteration 2900: Loss = -11051.1455078125
Iteration 3000: Loss = -11051.1181640625
Iteration 3100: Loss = -11051.0908203125
Iteration 3200: Loss = -11051.060546875
Iteration 3300: Loss = -11051.0302734375
Iteration 3400: Loss = -11050.998046875
Iteration 3500: Loss = -11050.958984375
Iteration 3600: Loss = -11050.9111328125
Iteration 3700: Loss = -11050.845703125
Iteration 3800: Loss = -11050.7392578125
Iteration 3900: Loss = -11050.62890625
Iteration 4000: Loss = -11050.5166015625
Iteration 4100: Loss = -11050.3759765625
Iteration 4200: Loss = -11050.2138671875
Iteration 4300: Loss = -11050.0166015625
Iteration 4400: Loss = -11049.7607421875
Iteration 4500: Loss = -11049.4267578125
Iteration 4600: Loss = -11048.927734375
Iteration 4700: Loss = -11047.6943359375
Iteration 4800: Loss = -10999.55859375
Iteration 4900: Loss = -10960.2255859375
Iteration 5000: Loss = -10955.5
Iteration 5100: Loss = -10953.5537109375
Iteration 5200: Loss = -10952.7431640625
Iteration 5300: Loss = -10952.291015625
Iteration 5400: Loss = -10951.88671875
Iteration 5500: Loss = -10951.7841796875
Iteration 5600: Loss = -10951.7216796875
Iteration 5700: Loss = -10951.6796875
Iteration 5800: Loss = -10951.6533203125
Iteration 5900: Loss = -10951.634765625
Iteration 6000: Loss = -10951.619140625
Iteration 6100: Loss = -10951.60546875
Iteration 6200: Loss = -10951.5888671875
Iteration 6300: Loss = -10951.5634765625
Iteration 6400: Loss = -10951.4013671875
Iteration 6500: Loss = -10951.0830078125
Iteration 6600: Loss = -10937.5048828125
Iteration 6700: Loss = -10919.474609375
Iteration 6800: Loss = -10915.4208984375
Iteration 6900: Loss = -10915.27734375
Iteration 7000: Loss = -10914.8818359375
Iteration 7100: Loss = -10914.8447265625
Iteration 7200: Loss = -10914.8271484375
Iteration 7300: Loss = -10914.814453125
Iteration 7400: Loss = -10914.8056640625
Iteration 7500: Loss = -10914.7978515625
Iteration 7600: Loss = -10914.7919921875
Iteration 7700: Loss = -10914.7890625
Iteration 7800: Loss = -10914.78515625
Iteration 7900: Loss = -10914.7822265625
Iteration 8000: Loss = -10914.7802734375
Iteration 8100: Loss = -10914.77734375
Iteration 8200: Loss = -10914.7763671875
Iteration 8300: Loss = -10914.7744140625
Iteration 8400: Loss = -10914.771484375
Iteration 8500: Loss = -10914.7724609375
1
Iteration 8600: Loss = -10914.76953125
Iteration 8700: Loss = -10914.7685546875
Iteration 8800: Loss = -10914.765625
Iteration 8900: Loss = -10914.765625
Iteration 9000: Loss = -10914.763671875
Iteration 9100: Loss = -10914.7626953125
Iteration 9200: Loss = -10914.76171875
Iteration 9300: Loss = -10914.76171875
Iteration 9400: Loss = -10914.76171875
Iteration 9500: Loss = -10914.76171875
Iteration 9600: Loss = -10914.7607421875
Iteration 9700: Loss = -10914.7587890625
Iteration 9800: Loss = -10914.7578125
Iteration 9900: Loss = -10914.7587890625
1
Iteration 10000: Loss = -10914.7578125
Iteration 10100: Loss = -10914.7587890625
1
Iteration 10200: Loss = -10914.7568359375
Iteration 10300: Loss = -10914.7568359375
Iteration 10400: Loss = -10914.7568359375
Iteration 10500: Loss = -10914.7568359375
Iteration 10600: Loss = -10914.7568359375
Iteration 10700: Loss = -10914.7568359375
Iteration 10800: Loss = -10914.755859375
Iteration 10900: Loss = -10914.755859375
Iteration 11000: Loss = -10914.7548828125
Iteration 11100: Loss = -10914.755859375
1
Iteration 11200: Loss = -10914.755859375
2
Iteration 11300: Loss = -10914.755859375
3
Iteration 11400: Loss = -10914.7548828125
Iteration 11500: Loss = -10914.75390625
Iteration 11600: Loss = -10914.7548828125
1
Iteration 11700: Loss = -10914.7548828125
2
Iteration 11800: Loss = -10914.7548828125
3
Iteration 11900: Loss = -10914.755859375
4
Iteration 12000: Loss = -10914.75390625
Iteration 12100: Loss = -10914.7548828125
1
Iteration 12200: Loss = -10914.7548828125
2
Iteration 12300: Loss = -10914.75390625
Iteration 12400: Loss = -10914.75390625
Iteration 12500: Loss = -10914.75390625
Iteration 12600: Loss = -10914.75390625
Iteration 12700: Loss = -10914.75390625
Iteration 12800: Loss = -10914.75390625
Iteration 12900: Loss = -10914.7548828125
1
Iteration 13000: Loss = -10914.7548828125
2
Iteration 13100: Loss = -10914.75390625
Iteration 13200: Loss = -10914.7529296875
Iteration 13300: Loss = -10914.75390625
1
Iteration 13400: Loss = -10914.75390625
2
Iteration 13500: Loss = -10914.75390625
3
Iteration 13600: Loss = -10914.75390625
4
Iteration 13700: Loss = -10914.7529296875
Iteration 13800: Loss = -10914.75390625
1
Iteration 13900: Loss = -10914.751953125
Iteration 14000: Loss = -10914.7529296875
1
Iteration 14100: Loss = -10914.75390625
2
Iteration 14200: Loss = -10914.75390625
3
Iteration 14300: Loss = -10914.7529296875
4
Iteration 14400: Loss = -10914.75390625
5
Iteration 14500: Loss = -10914.75390625
6
Iteration 14600: Loss = -10914.75390625
7
Iteration 14700: Loss = -10914.75390625
8
Iteration 14800: Loss = -10914.7529296875
9
Iteration 14900: Loss = -10914.751953125
Iteration 15000: Loss = -10914.7529296875
1
Iteration 15100: Loss = -10914.751953125
Iteration 15200: Loss = -10914.751953125
Iteration 15300: Loss = -10914.7529296875
1
Iteration 15400: Loss = -10914.7529296875
2
Iteration 15500: Loss = -10914.751953125
Iteration 15600: Loss = -10914.75390625
1
Iteration 15700: Loss = -10914.75390625
2
Iteration 15800: Loss = -10914.7529296875
3
Iteration 15900: Loss = -10914.7529296875
4
Iteration 16000: Loss = -10914.7529296875
5
Iteration 16100: Loss = -10914.7529296875
6
Iteration 16200: Loss = -10914.75390625
7
Iteration 16300: Loss = -10914.75390625
8
Iteration 16400: Loss = -10914.751953125
Iteration 16500: Loss = -10914.7529296875
1
Iteration 16600: Loss = -10914.7529296875
2
Iteration 16700: Loss = -10914.75390625
3
Iteration 16800: Loss = -10914.7529296875
4
Iteration 16900: Loss = -10914.75390625
5
Iteration 17000: Loss = -10914.755859375
6
Iteration 17100: Loss = -10914.75390625
7
Iteration 17200: Loss = -10914.75390625
8
Iteration 17300: Loss = -10914.7529296875
9
Iteration 17400: Loss = -10914.7529296875
10
Iteration 17500: Loss = -10914.7529296875
11
Iteration 17600: Loss = -10914.7529296875
12
Iteration 17700: Loss = -10914.751953125
Iteration 17800: Loss = -10914.7529296875
1
Iteration 17900: Loss = -10914.751953125
Iteration 18000: Loss = -10914.75390625
1
Iteration 18100: Loss = -10914.751953125
Iteration 18200: Loss = -10914.7529296875
1
Iteration 18300: Loss = -10914.7529296875
2
Iteration 18400: Loss = -10914.7529296875
3
Iteration 18500: Loss = -10914.7529296875
4
Iteration 18600: Loss = -10914.7529296875
5
Iteration 18700: Loss = -10914.7529296875
6
Iteration 18800: Loss = -10914.7529296875
7
Iteration 18900: Loss = -10914.7529296875
8
Iteration 19000: Loss = -10914.75390625
9
Iteration 19100: Loss = -10914.75390625
10
Iteration 19200: Loss = -10914.75390625
11
Iteration 19300: Loss = -10914.75390625
12
Iteration 19400: Loss = -10914.75390625
13
Iteration 19500: Loss = -10914.75390625
14
Iteration 19600: Loss = -10914.7529296875
15
Stopping early at iteration 19600 due to no improvement.
pi: tensor([[0.6184, 0.3816],
        [0.2295, 0.7705]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5124, 0.4876], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2092, 0.0960],
         [0.9809, 0.2378]],

        [[0.9898, 0.0963],
         [0.9816, 0.7733]],

        [[0.1741, 0.0976],
         [0.7136, 0.9595]],

        [[0.9846, 0.0963],
         [0.3243, 0.9902]],

        [[0.0942, 0.0978],
         [0.1446, 0.9791]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8079046942578025
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6689365546862865
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9206925302859384
Global Adjusted Rand Index: 0.8168006676967421
Average Adjusted Rand Index: 0.8174451767833112
[0.06160762587046823, 0.8168006676967421] [0.11467468990978258, 0.8174451767833112] [11048.33203125, 10914.7529296875]
-------------------------------------
This iteration is 76
True Objective function: Loss = -10822.3505846895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29263.544921875
Iteration 100: Loss = -18128.421875
Iteration 200: Loss = -12308.1923828125
Iteration 300: Loss = -11472.2783203125
Iteration 400: Loss = -11265.5048828125
Iteration 500: Loss = -11162.1123046875
Iteration 600: Loss = -11097.7666015625
Iteration 700: Loss = -11059.1474609375
Iteration 800: Loss = -11028.7958984375
Iteration 900: Loss = -11010.2099609375
Iteration 1000: Loss = -10997.849609375
Iteration 1100: Loss = -10988.4990234375
Iteration 1200: Loss = -10979.673828125
Iteration 1300: Loss = -10970.9150390625
Iteration 1400: Loss = -10963.4521484375
Iteration 1500: Loss = -10957.189453125
Iteration 1600: Loss = -10952.8115234375
Iteration 1700: Loss = -10949.458984375
Iteration 1800: Loss = -10946.6630859375
Iteration 1900: Loss = -10944.5087890625
Iteration 2000: Loss = -10942.609375
Iteration 2100: Loss = -10940.7099609375
Iteration 2200: Loss = -10938.474609375
Iteration 2300: Loss = -10935.771484375
Iteration 2400: Loss = -10933.5244140625
Iteration 2500: Loss = -10931.541015625
Iteration 2600: Loss = -10929.490234375
Iteration 2700: Loss = -10927.5302734375
Iteration 2800: Loss = -10925.99609375
Iteration 2900: Loss = -10920.3935546875
Iteration 3000: Loss = -10919.3583984375
Iteration 3100: Loss = -10918.609375
Iteration 3200: Loss = -10915.4931640625
Iteration 3300: Loss = -10914.2529296875
Iteration 3400: Loss = -10913.791015625
Iteration 3500: Loss = -10913.4287109375
Iteration 3600: Loss = -10913.1240234375
Iteration 3700: Loss = -10912.865234375
Iteration 3800: Loss = -10912.63671875
Iteration 3900: Loss = -10912.435546875
Iteration 4000: Loss = -10912.2578125
Iteration 4100: Loss = -10912.0947265625
Iteration 4200: Loss = -10911.9482421875
Iteration 4300: Loss = -10911.8134765625
Iteration 4400: Loss = -10911.6904296875
Iteration 4500: Loss = -10911.5771484375
Iteration 4600: Loss = -10911.470703125
Iteration 4700: Loss = -10911.375
Iteration 4800: Loss = -10911.28515625
Iteration 4900: Loss = -10911.203125
Iteration 5000: Loss = -10911.125
Iteration 5100: Loss = -10911.0546875
Iteration 5200: Loss = -10910.986328125
Iteration 5300: Loss = -10910.9267578125
Iteration 5400: Loss = -10910.8671875
Iteration 5500: Loss = -10910.8115234375
Iteration 5600: Loss = -10910.7509765625
Iteration 5700: Loss = -10905.77734375
Iteration 5800: Loss = -10905.5341796875
Iteration 5900: Loss = -10905.3994140625
Iteration 6000: Loss = -10905.2998046875
Iteration 6100: Loss = -10905.2177734375
Iteration 6200: Loss = -10905.1474609375
Iteration 6300: Loss = -10905.0859375
Iteration 6400: Loss = -10905.029296875
Iteration 6500: Loss = -10904.9814453125
Iteration 6600: Loss = -10904.935546875
Iteration 6700: Loss = -10904.89453125
Iteration 6800: Loss = -10904.8564453125
Iteration 6900: Loss = -10904.822265625
Iteration 7000: Loss = -10904.7939453125
Iteration 7100: Loss = -10904.7607421875
Iteration 7200: Loss = -10904.7373046875
Iteration 7300: Loss = -10904.70703125
Iteration 7400: Loss = -10904.685546875
Iteration 7500: Loss = -10904.6630859375
Iteration 7600: Loss = -10904.6416015625
Iteration 7700: Loss = -10904.6220703125
Iteration 7800: Loss = -10904.603515625
Iteration 7900: Loss = -10904.5869140625
Iteration 8000: Loss = -10904.5703125
Iteration 8100: Loss = -10904.5556640625
Iteration 8200: Loss = -10904.5419921875
Iteration 8300: Loss = -10904.52734375
Iteration 8400: Loss = -10904.5166015625
Iteration 8500: Loss = -10904.505859375
Iteration 8600: Loss = -10904.4931640625
Iteration 8700: Loss = -10904.4814453125
Iteration 8800: Loss = -10904.4716796875
Iteration 8900: Loss = -10904.46484375
Iteration 9000: Loss = -10904.455078125
Iteration 9100: Loss = -10904.4462890625
Iteration 9200: Loss = -10904.4384765625
Iteration 9300: Loss = -10904.431640625
Iteration 9400: Loss = -10904.423828125
Iteration 9500: Loss = -10904.416015625
Iteration 9600: Loss = -10904.4111328125
Iteration 9700: Loss = -10904.404296875
Iteration 9800: Loss = -10904.3984375
Iteration 9900: Loss = -10904.3935546875
Iteration 10000: Loss = -10904.388671875
Iteration 10100: Loss = -10904.3828125
Iteration 10200: Loss = -10904.376953125
Iteration 10300: Loss = -10904.375
Iteration 10400: Loss = -10904.369140625
Iteration 10500: Loss = -10904.3642578125
Iteration 10600: Loss = -10904.361328125
Iteration 10700: Loss = -10904.357421875
Iteration 10800: Loss = -10904.35546875
Iteration 10900: Loss = -10904.3515625
Iteration 11000: Loss = -10904.34765625
Iteration 11100: Loss = -10904.345703125
Iteration 11200: Loss = -10904.3427734375
Iteration 11300: Loss = -10904.33984375
Iteration 11400: Loss = -10904.337890625
Iteration 11500: Loss = -10904.3330078125
Iteration 11600: Loss = -10904.3330078125
Iteration 11700: Loss = -10904.3310546875
Iteration 11800: Loss = -10904.326171875
Iteration 11900: Loss = -10904.326171875
Iteration 12000: Loss = -10904.3251953125
Iteration 12100: Loss = -10904.3212890625
Iteration 12200: Loss = -10904.322265625
1
Iteration 12300: Loss = -10904.3193359375
Iteration 12400: Loss = -10904.318359375
Iteration 12500: Loss = -10904.31640625
Iteration 12600: Loss = -10904.31640625
Iteration 12700: Loss = -10904.3134765625
Iteration 12800: Loss = -10904.3115234375
Iteration 12900: Loss = -10904.3095703125
Iteration 13000: Loss = -10904.310546875
1
Iteration 13100: Loss = -10904.30859375
Iteration 13200: Loss = -10904.30859375
Iteration 13300: Loss = -10904.306640625
Iteration 13400: Loss = -10904.3056640625
Iteration 13500: Loss = -10904.3046875
Iteration 13600: Loss = -10904.3046875
Iteration 13700: Loss = -10904.3037109375
Iteration 13800: Loss = -10904.3056640625
1
Iteration 13900: Loss = -10904.3017578125
Iteration 14000: Loss = -10904.3017578125
Iteration 14100: Loss = -10904.30078125
Iteration 14200: Loss = -10904.30078125
Iteration 14300: Loss = -10904.298828125
Iteration 14400: Loss = -10904.2998046875
1
Iteration 14500: Loss = -10904.2978515625
Iteration 14600: Loss = -10904.296875
Iteration 14700: Loss = -10904.296875
Iteration 14800: Loss = -10904.2978515625
1
Iteration 14900: Loss = -10904.296875
Iteration 15000: Loss = -10904.2978515625
1
Iteration 15100: Loss = -10904.298828125
2
Iteration 15200: Loss = -10904.2978515625
3
Iteration 15300: Loss = -10904.296875
Iteration 15400: Loss = -10904.296875
Iteration 15500: Loss = -10904.294921875
Iteration 15600: Loss = -10904.294921875
Iteration 15700: Loss = -10904.294921875
Iteration 15800: Loss = -10904.29296875
Iteration 15900: Loss = -10904.2958984375
1
Iteration 16000: Loss = -10904.294921875
2
Iteration 16100: Loss = -10904.2939453125
3
Iteration 16200: Loss = -10904.2939453125
4
Iteration 16300: Loss = -10904.2939453125
5
Iteration 16400: Loss = -10904.2939453125
6
Iteration 16500: Loss = -10904.2939453125
7
Iteration 16600: Loss = -10904.294921875
8
Iteration 16700: Loss = -10904.2939453125
9
Iteration 16800: Loss = -10904.29296875
Iteration 16900: Loss = -10904.29296875
Iteration 17000: Loss = -10904.2939453125
1
Iteration 17100: Loss = -10904.2919921875
Iteration 17200: Loss = -10904.29296875
1
Iteration 17300: Loss = -10904.2919921875
Iteration 17400: Loss = -10904.2919921875
Iteration 17500: Loss = -10904.291015625
Iteration 17600: Loss = -10904.29296875
1
Iteration 17700: Loss = -10904.2919921875
2
Iteration 17800: Loss = -10904.2919921875
3
Iteration 17900: Loss = -10904.29296875
4
Iteration 18000: Loss = -10904.2900390625
Iteration 18100: Loss = -10904.2939453125
1
Iteration 18200: Loss = -10904.29296875
2
Iteration 18300: Loss = -10904.2919921875
3
Iteration 18400: Loss = -10904.291015625
4
Iteration 18500: Loss = -10904.291015625
5
Iteration 18600: Loss = -10904.2919921875
6
Iteration 18700: Loss = -10904.2939453125
7
Iteration 18800: Loss = -10904.291015625
8
Iteration 18900: Loss = -10904.291015625
9
Iteration 19000: Loss = -10904.2919921875
10
Iteration 19100: Loss = -10904.2919921875
11
Iteration 19200: Loss = -10904.2919921875
12
Iteration 19300: Loss = -10904.2919921875
13
Iteration 19400: Loss = -10904.291015625
14
Iteration 19500: Loss = -10904.291015625
15
Stopping early at iteration 19500 due to no improvement.
pi: tensor([[9.9999e-01, 1.0620e-05],
        [1.0000e+00, 2.2494e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6840, 0.3160], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1604, 0.1229],
         [0.9750, 0.3479]],

        [[0.2131, 0.3681],
         [0.0091, 0.0127]],

        [[0.9905, 0.1633],
         [0.8268, 0.9933]],

        [[0.0110, 0.2093],
         [0.9343, 0.9125]],

        [[0.9743, 0.1205],
         [0.0296, 0.0580]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 80
Adjusted Rand Index: 0.3543852647240511
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.01486839854035048
Average Adjusted Rand Index: 0.07087705294481021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -52831.46875
Iteration 100: Loss = -35670.23046875
Iteration 200: Loss = -22763.056640625
Iteration 300: Loss = -15041.2978515625
Iteration 400: Loss = -12218.8779296875
Iteration 500: Loss = -11340.5615234375
Iteration 600: Loss = -11072.689453125
Iteration 700: Loss = -10982.7705078125
Iteration 800: Loss = -10951.9814453125
Iteration 900: Loss = -10942.390625
Iteration 1000: Loss = -10937.234375
Iteration 1100: Loss = -10933.7236328125
Iteration 1200: Loss = -10931.07421875
Iteration 1300: Loss = -10928.9873046875
Iteration 1400: Loss = -10927.2978515625
Iteration 1500: Loss = -10925.9052734375
Iteration 1600: Loss = -10924.7353515625
Iteration 1700: Loss = -10923.740234375
Iteration 1800: Loss = -10922.8896484375
Iteration 1900: Loss = -10922.16015625
Iteration 2000: Loss = -10921.5244140625
Iteration 2100: Loss = -10920.9638671875
Iteration 2200: Loss = -10920.4697265625
Iteration 2300: Loss = -10920.029296875
Iteration 2400: Loss = -10919.634765625
Iteration 2500: Loss = -10919.28125
Iteration 2600: Loss = -10918.9638671875
Iteration 2700: Loss = -10918.6767578125
Iteration 2800: Loss = -10918.4150390625
Iteration 2900: Loss = -10918.1767578125
Iteration 3000: Loss = -10917.958984375
Iteration 3100: Loss = -10917.759765625
Iteration 3200: Loss = -10917.578125
Iteration 3300: Loss = -10917.4091796875
Iteration 3400: Loss = -10917.2529296875
Iteration 3500: Loss = -10917.1083984375
Iteration 3600: Loss = -10916.9765625
Iteration 3700: Loss = -10916.8544921875
Iteration 3800: Loss = -10916.7392578125
Iteration 3900: Loss = -10916.634765625
Iteration 4000: Loss = -10916.5341796875
Iteration 4100: Loss = -10916.443359375
Iteration 4200: Loss = -10916.3564453125
Iteration 4300: Loss = -10916.27734375
Iteration 4400: Loss = -10916.2001953125
Iteration 4500: Loss = -10916.1318359375
Iteration 4600: Loss = -10916.06640625
Iteration 4700: Loss = -10916.00390625
Iteration 4800: Loss = -10915.9443359375
Iteration 4900: Loss = -10915.890625
Iteration 5000: Loss = -10915.83984375
Iteration 5100: Loss = -10915.7900390625
Iteration 5200: Loss = -10915.74609375
Iteration 5300: Loss = -10915.7041015625
Iteration 5400: Loss = -10915.662109375
Iteration 5500: Loss = -10915.6259765625
Iteration 5600: Loss = -10915.587890625
Iteration 5700: Loss = -10915.5556640625
Iteration 5800: Loss = -10915.5224609375
Iteration 5900: Loss = -10915.494140625
Iteration 6000: Loss = -10915.4638671875
Iteration 6100: Loss = -10915.4365234375
Iteration 6200: Loss = -10915.4111328125
Iteration 6300: Loss = -10915.38671875
Iteration 6400: Loss = -10915.36328125
Iteration 6500: Loss = -10915.3427734375
Iteration 6600: Loss = -10915.3212890625
Iteration 6700: Loss = -10915.30078125
Iteration 6800: Loss = -10915.2841796875
Iteration 6900: Loss = -10915.265625
Iteration 7000: Loss = -10915.2490234375
Iteration 7100: Loss = -10915.232421875
Iteration 7200: Loss = -10915.216796875
Iteration 7300: Loss = -10915.2021484375
Iteration 7400: Loss = -10915.189453125
Iteration 7500: Loss = -10915.177734375
Iteration 7600: Loss = -10915.1630859375
Iteration 7700: Loss = -10915.1533203125
Iteration 7800: Loss = -10915.142578125
Iteration 7900: Loss = -10915.1318359375
Iteration 8000: Loss = -10915.119140625
Iteration 8100: Loss = -10915.1103515625
Iteration 8200: Loss = -10915.1015625
Iteration 8300: Loss = -10915.0927734375
Iteration 8400: Loss = -10915.0830078125
Iteration 8500: Loss = -10915.0771484375
Iteration 8600: Loss = -10915.0693359375
Iteration 8700: Loss = -10915.0595703125
Iteration 8800: Loss = -10915.0537109375
Iteration 8900: Loss = -10915.0478515625
Iteration 9000: Loss = -10915.041015625
Iteration 9100: Loss = -10915.03515625
Iteration 9200: Loss = -10915.029296875
Iteration 9300: Loss = -10915.0224609375
Iteration 9400: Loss = -10915.0166015625
Iteration 9500: Loss = -10915.0107421875
Iteration 9600: Loss = -10915.0068359375
Iteration 9700: Loss = -10915.0009765625
Iteration 9800: Loss = -10914.998046875
Iteration 9900: Loss = -10914.9912109375
Iteration 10000: Loss = -10914.986328125
Iteration 10100: Loss = -10914.9814453125
Iteration 10200: Loss = -10914.978515625
Iteration 10300: Loss = -10914.97265625
Iteration 10400: Loss = -10914.96875
Iteration 10500: Loss = -10914.9658203125
Iteration 10600: Loss = -10914.9619140625
Iteration 10700: Loss = -10914.9580078125
Iteration 10800: Loss = -10914.9560546875
Iteration 10900: Loss = -10914.953125
Iteration 11000: Loss = -10914.9501953125
Iteration 11100: Loss = -10914.947265625
Iteration 11200: Loss = -10914.9453125
Iteration 11300: Loss = -10914.9423828125
Iteration 11400: Loss = -10914.9404296875
Iteration 11500: Loss = -10914.9384765625
Iteration 11600: Loss = -10914.9375
Iteration 11700: Loss = -10914.93359375
Iteration 11800: Loss = -10914.93359375
Iteration 11900: Loss = -10914.931640625
Iteration 12000: Loss = -10914.9296875
Iteration 12100: Loss = -10914.9267578125
Iteration 12200: Loss = -10914.9267578125
Iteration 12300: Loss = -10914.9267578125
Iteration 12400: Loss = -10914.923828125
Iteration 12500: Loss = -10914.921875
Iteration 12600: Loss = -10914.9208984375
Iteration 12700: Loss = -10914.921875
1
Iteration 12800: Loss = -10914.9208984375
Iteration 12900: Loss = -10914.9169921875
Iteration 13000: Loss = -10914.9169921875
Iteration 13100: Loss = -10914.9169921875
Iteration 13200: Loss = -10914.9169921875
Iteration 13300: Loss = -10914.9150390625
Iteration 13400: Loss = -10914.9150390625
Iteration 13500: Loss = -10914.9130859375
Iteration 13600: Loss = -10914.9130859375
Iteration 13700: Loss = -10914.912109375
Iteration 13800: Loss = -10914.9111328125
Iteration 13900: Loss = -10914.9111328125
Iteration 14000: Loss = -10914.9111328125
Iteration 14100: Loss = -10914.908203125
Iteration 14200: Loss = -10914.908203125
Iteration 14300: Loss = -10914.908203125
Iteration 14400: Loss = -10914.9072265625
Iteration 14500: Loss = -10914.9072265625
Iteration 14600: Loss = -10914.9072265625
Iteration 14700: Loss = -10914.90625
Iteration 14800: Loss = -10914.9052734375
Iteration 14900: Loss = -10914.904296875
Iteration 15000: Loss = -10914.9033203125
Iteration 15100: Loss = -10914.904296875
1
Iteration 15200: Loss = -10914.9013671875
Iteration 15300: Loss = -10914.90234375
1
Iteration 15400: Loss = -10914.900390625
Iteration 15500: Loss = -10914.9013671875
1
Iteration 15600: Loss = -10914.900390625
Iteration 15700: Loss = -10914.8994140625
Iteration 15800: Loss = -10914.896484375
Iteration 15900: Loss = -10914.8974609375
1
Iteration 16000: Loss = -10914.8955078125
Iteration 16100: Loss = -10914.89453125
Iteration 16200: Loss = -10914.892578125
Iteration 16300: Loss = -10914.8916015625
Iteration 16400: Loss = -10914.888671875
Iteration 16500: Loss = -10914.8876953125
Iteration 16600: Loss = -10914.8857421875
Iteration 16700: Loss = -10914.8828125
Iteration 16800: Loss = -10914.87890625
Iteration 16900: Loss = -10914.8740234375
Iteration 17000: Loss = -10914.8681640625
Iteration 17100: Loss = -10914.8583984375
Iteration 17200: Loss = -10914.841796875
Iteration 17300: Loss = -10914.8056640625
Iteration 17400: Loss = -10914.6650390625
Iteration 17500: Loss = -10914.1826171875
Iteration 17600: Loss = -10913.775390625
Iteration 17700: Loss = -10906.125
Iteration 17800: Loss = -10876.001953125
Iteration 17900: Loss = -10875.806640625
Iteration 18000: Loss = -10875.73046875
Iteration 18100: Loss = -10875.7060546875
Iteration 18200: Loss = -10875.689453125
Iteration 18300: Loss = -10875.6806640625
Iteration 18400: Loss = -10875.6748046875
Iteration 18500: Loss = -10875.6689453125
Iteration 18600: Loss = -10875.666015625
Iteration 18700: Loss = -10875.662109375
Iteration 18800: Loss = -10875.6611328125
Iteration 18900: Loss = -10875.6572265625
Iteration 19000: Loss = -10875.658203125
1
Iteration 19100: Loss = -10875.6552734375
Iteration 19200: Loss = -10875.654296875
Iteration 19300: Loss = -10875.654296875
Iteration 19400: Loss = -10875.65234375
Iteration 19500: Loss = -10875.65234375
Iteration 19600: Loss = -10875.650390625
Iteration 19700: Loss = -10875.6494140625
Iteration 19800: Loss = -10875.6494140625
Iteration 19900: Loss = -10875.6484375
Iteration 20000: Loss = -10875.6484375
Iteration 20100: Loss = -10875.6484375
Iteration 20200: Loss = -10875.6474609375
Iteration 20300: Loss = -10875.6474609375
Iteration 20400: Loss = -10875.6474609375
Iteration 20500: Loss = -10875.646484375
Iteration 20600: Loss = -10875.646484375
Iteration 20700: Loss = -10875.6474609375
1
Iteration 20800: Loss = -10875.6474609375
2
Iteration 20900: Loss = -10875.6455078125
Iteration 21000: Loss = -10875.6474609375
1
Iteration 21100: Loss = -10875.646484375
2
Iteration 21200: Loss = -10875.646484375
3
Iteration 21300: Loss = -10875.6455078125
Iteration 21400: Loss = -10875.646484375
1
Iteration 21500: Loss = -10875.6455078125
Iteration 21600: Loss = -10875.6455078125
Iteration 21700: Loss = -10875.6455078125
Iteration 21800: Loss = -10875.646484375
1
Iteration 21900: Loss = -10875.64453125
Iteration 22000: Loss = -10875.6455078125
1
Iteration 22100: Loss = -10875.64453125
Iteration 22200: Loss = -10875.64453125
Iteration 22300: Loss = -10875.6455078125
1
Iteration 22400: Loss = -10875.64453125
Iteration 22500: Loss = -10875.64453125
Iteration 22600: Loss = -10875.64453125
Iteration 22700: Loss = -10875.64453125
Iteration 22800: Loss = -10875.6435546875
Iteration 22900: Loss = -10875.642578125
Iteration 23000: Loss = -10875.63671875
Iteration 23100: Loss = -10810.9150390625
Iteration 23200: Loss = -10785.1728515625
Iteration 23300: Loss = -10783.5986328125
Iteration 23400: Loss = -10783.474609375
Iteration 23500: Loss = -10783.42578125
Iteration 23600: Loss = -10783.3974609375
Iteration 23700: Loss = -10783.3779296875
Iteration 23800: Loss = -10783.3603515625
Iteration 23900: Loss = -10783.34375
Iteration 24000: Loss = -10783.3359375
Iteration 24100: Loss = -10783.3251953125
Iteration 24200: Loss = -10783.3125
Iteration 24300: Loss = -10783.27734375
Iteration 24400: Loss = -10782.9658203125
Iteration 24500: Loss = -10782.9619140625
Iteration 24600: Loss = -10782.9599609375
Iteration 24700: Loss = -10782.958984375
Iteration 24800: Loss = -10782.955078125
Iteration 24900: Loss = -10782.9541015625
Iteration 25000: Loss = -10782.9521484375
Iteration 25100: Loss = -10782.951171875
Iteration 25200: Loss = -10782.951171875
Iteration 25300: Loss = -10782.94921875
Iteration 25400: Loss = -10782.9482421875
Iteration 25500: Loss = -10782.9482421875
Iteration 25600: Loss = -10782.9462890625
Iteration 25700: Loss = -10782.9462890625
Iteration 25800: Loss = -10782.9453125
Iteration 25900: Loss = -10782.9453125
Iteration 26000: Loss = -10782.9443359375
Iteration 26100: Loss = -10782.9443359375
Iteration 26200: Loss = -10782.943359375
Iteration 26300: Loss = -10782.943359375
Iteration 26400: Loss = -10782.943359375
Iteration 26500: Loss = -10782.943359375
Iteration 26600: Loss = -10782.9423828125
Iteration 26700: Loss = -10782.8466796875
Iteration 26800: Loss = -10782.8447265625
Iteration 26900: Loss = -10782.8447265625
Iteration 27000: Loss = -10782.8447265625
Iteration 27100: Loss = -10782.8447265625
Iteration 27200: Loss = -10782.845703125
1
Iteration 27300: Loss = -10782.84375
Iteration 27400: Loss = -10782.84375
Iteration 27500: Loss = -10782.8447265625
1
Iteration 27600: Loss = -10782.84375
Iteration 27700: Loss = -10782.8447265625
1
Iteration 27800: Loss = -10782.8447265625
2
Iteration 27900: Loss = -10782.8427734375
Iteration 28000: Loss = -10782.8427734375
Iteration 28100: Loss = -10782.845703125
1
Iteration 28200: Loss = -10782.8427734375
Iteration 28300: Loss = -10782.8349609375
Iteration 28400: Loss = -10782.8271484375
Iteration 28500: Loss = -10782.82421875
Iteration 28600: Loss = -10782.82421875
Iteration 28700: Loss = -10782.8232421875
Iteration 28800: Loss = -10782.822265625
Iteration 28900: Loss = -10782.82421875
1
Iteration 29000: Loss = -10782.82421875
2
Iteration 29100: Loss = -10782.82421875
3
Iteration 29200: Loss = -10782.82421875
4
Iteration 29300: Loss = -10782.82421875
5
Iteration 29400: Loss = -10782.8232421875
6
Iteration 29500: Loss = -10782.8232421875
7
Iteration 29600: Loss = -10782.82421875
8
Iteration 29700: Loss = -10782.82421875
9
Iteration 29800: Loss = -10782.82421875
10
Iteration 29900: Loss = -10782.82421875
11
pi: tensor([[0.7787, 0.2213],
        [0.2138, 0.7862]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4758, 0.5242], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.1128],
         [0.9787, 0.2462]],

        [[0.0831, 0.0973],
         [0.1366, 0.9923]],

        [[0.3721, 0.0943],
         [0.5116, 0.0807]],

        [[0.3901, 0.1003],
         [0.8075, 0.0372]],

        [[0.7083, 0.0954],
         [0.2788, 0.9930]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363408394869687
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.8460917583623252
Average Adjusted Rand Index: 0.8488795992504823
[0.01486839854035048, 0.8460917583623252] [0.07087705294481021, 0.8488795992504823] [10904.291015625, 10782.822265625]
-------------------------------------
This iteration is 77
True Objective function: Loss = -10794.583839460178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25272.8046875
Iteration 100: Loss = -17188.509765625
Iteration 200: Loss = -12845.7666015625
Iteration 300: Loss = -11778.0986328125
Iteration 400: Loss = -11354.4580078125
Iteration 500: Loss = -11154.4296875
Iteration 600: Loss = -11057.533203125
Iteration 700: Loss = -11008.71484375
Iteration 800: Loss = -10981.0927734375
Iteration 900: Loss = -10961.7109375
Iteration 1000: Loss = -10943.5234375
Iteration 1100: Loss = -10935.353515625
Iteration 1200: Loss = -10928.916015625
Iteration 1300: Loss = -10921.474609375
Iteration 1400: Loss = -10917.5673828125
Iteration 1500: Loss = -10912.3212890625
Iteration 1600: Loss = -10906.5888671875
Iteration 1700: Loss = -10903.486328125
Iteration 1800: Loss = -10900.4453125
Iteration 1900: Loss = -10896.9443359375
Iteration 2000: Loss = -10894.7822265625
Iteration 2100: Loss = -10893.373046875
Iteration 2200: Loss = -10892.125
Iteration 2300: Loss = -10891.029296875
Iteration 2400: Loss = -10889.056640625
Iteration 2500: Loss = -10885.57421875
Iteration 2600: Loss = -10883.46875
Iteration 2700: Loss = -10879.791015625
Iteration 2800: Loss = -10876.9765625
Iteration 2900: Loss = -10871.29296875
Iteration 3000: Loss = -10869.2783203125
Iteration 3100: Loss = -10867.6728515625
Iteration 3200: Loss = -10866.6318359375
Iteration 3300: Loss = -10866.0556640625
Iteration 3400: Loss = -10865.564453125
Iteration 3500: Loss = -10865.0419921875
Iteration 3600: Loss = -10864.6201171875
Iteration 3700: Loss = -10864.3095703125
Iteration 3800: Loss = -10864.0556640625
Iteration 3900: Loss = -10863.8349609375
Iteration 4000: Loss = -10863.619140625
Iteration 4100: Loss = -10863.447265625
Iteration 4200: Loss = -10863.2958984375
Iteration 4300: Loss = -10863.162109375
Iteration 4400: Loss = -10863.0390625
Iteration 4500: Loss = -10862.92578125
Iteration 4600: Loss = -10862.82421875
Iteration 4700: Loss = -10862.7294921875
Iteration 4800: Loss = -10862.638671875
Iteration 4900: Loss = -10862.5546875
Iteration 5000: Loss = -10862.4775390625
Iteration 5100: Loss = -10862.40625
Iteration 5200: Loss = -10862.3388671875
Iteration 5300: Loss = -10862.2744140625
Iteration 5400: Loss = -10862.2158203125
Iteration 5500: Loss = -10862.154296875
Iteration 5600: Loss = -10862.060546875
Iteration 5700: Loss = -10861.9990234375
Iteration 5800: Loss = -10861.9345703125
Iteration 5900: Loss = -10861.8759765625
Iteration 6000: Loss = -10861.8212890625
Iteration 6100: Loss = -10861.7734375
Iteration 6200: Loss = -10861.73046875
Iteration 6300: Loss = -10861.6904296875
Iteration 6400: Loss = -10861.6513671875
Iteration 6500: Loss = -10861.611328125
Iteration 6600: Loss = -10861.5673828125
Iteration 6700: Loss = -10861.5107421875
Iteration 6800: Loss = -10861.4140625
Iteration 6900: Loss = -10861.3173828125
Iteration 7000: Loss = -10861.271484375
Iteration 7100: Loss = -10861.236328125
Iteration 7200: Loss = -10861.1962890625
Iteration 7300: Loss = -10861.150390625
Iteration 7400: Loss = -10861.0791015625
Iteration 7500: Loss = -10861.0234375
Iteration 7600: Loss = -10860.9755859375
Iteration 7700: Loss = -10860.9345703125
Iteration 7800: Loss = -10860.8984375
Iteration 7900: Loss = -10860.8740234375
Iteration 8000: Loss = -10860.845703125
Iteration 8100: Loss = -10860.7705078125
Iteration 8200: Loss = -10860.7412109375
Iteration 8300: Loss = -10860.7109375
Iteration 8400: Loss = -10860.6845703125
Iteration 8500: Loss = -10860.63671875
Iteration 8600: Loss = -10859.2998046875
Iteration 8700: Loss = -10859.166015625
Iteration 8800: Loss = -10859.1435546875
Iteration 8900: Loss = -10859.1279296875
Iteration 9000: Loss = -10859.1142578125
Iteration 9100: Loss = -10859.0673828125
Iteration 9200: Loss = -10859.044921875
Iteration 9300: Loss = -10859.0361328125
Iteration 9400: Loss = -10859.02734375
Iteration 9500: Loss = -10859.0205078125
Iteration 9600: Loss = -10859.013671875
Iteration 9700: Loss = -10859.005859375
Iteration 9800: Loss = -10859.001953125
Iteration 9900: Loss = -10858.9951171875
Iteration 10000: Loss = -10858.9892578125
Iteration 10100: Loss = -10858.984375
Iteration 10200: Loss = -10858.9794921875
Iteration 10300: Loss = -10858.974609375
Iteration 10400: Loss = -10858.9716796875
Iteration 10500: Loss = -10858.9658203125
Iteration 10600: Loss = -10858.962890625
Iteration 10700: Loss = -10858.9599609375
Iteration 10800: Loss = -10858.9521484375
Iteration 10900: Loss = -10858.9404296875
Iteration 11000: Loss = -10858.9345703125
Iteration 11100: Loss = -10858.931640625
Iteration 11200: Loss = -10858.9267578125
Iteration 11300: Loss = -10858.9248046875
Iteration 11400: Loss = -10858.921875
Iteration 11500: Loss = -10858.91796875
Iteration 11600: Loss = -10858.9140625
Iteration 11700: Loss = -10858.9091796875
Iteration 11800: Loss = -10858.9052734375
Iteration 11900: Loss = -10858.90234375
Iteration 12000: Loss = -10858.8974609375
Iteration 12100: Loss = -10858.89453125
Iteration 12200: Loss = -10858.8916015625
Iteration 12300: Loss = -10858.8896484375
Iteration 12400: Loss = -10857.3564453125
Iteration 12500: Loss = -10857.32421875
Iteration 12600: Loss = -10857.3154296875
Iteration 12700: Loss = -10857.310546875
Iteration 12800: Loss = -10857.3056640625
Iteration 12900: Loss = -10857.3056640625
Iteration 13000: Loss = -10857.302734375
Iteration 13100: Loss = -10857.30078125
Iteration 13200: Loss = -10857.2998046875
Iteration 13300: Loss = -10857.2978515625
Iteration 13400: Loss = -10857.296875
Iteration 13500: Loss = -10857.2958984375
Iteration 13600: Loss = -10857.294921875
Iteration 13700: Loss = -10857.2939453125
Iteration 13800: Loss = -10857.2939453125
Iteration 13900: Loss = -10857.2939453125
Iteration 14000: Loss = -10857.2919921875
Iteration 14100: Loss = -10857.2900390625
Iteration 14200: Loss = -10857.291015625
1
Iteration 14300: Loss = -10857.2900390625
Iteration 14400: Loss = -10857.2880859375
Iteration 14500: Loss = -10857.287109375
Iteration 14600: Loss = -10857.287109375
Iteration 14700: Loss = -10857.287109375
Iteration 14800: Loss = -10857.287109375
Iteration 14900: Loss = -10857.287109375
Iteration 15000: Loss = -10857.28515625
Iteration 15100: Loss = -10857.2861328125
1
Iteration 15200: Loss = -10857.28515625
Iteration 15300: Loss = -10857.28515625
Iteration 15400: Loss = -10857.2841796875
Iteration 15500: Loss = -10857.2841796875
Iteration 15600: Loss = -10857.279296875
Iteration 15700: Loss = -10857.27734375
Iteration 15800: Loss = -10857.275390625
Iteration 15900: Loss = -10857.275390625
Iteration 16000: Loss = -10857.2734375
Iteration 16100: Loss = -10857.2734375
Iteration 16200: Loss = -10857.2734375
Iteration 16300: Loss = -10857.2734375
Iteration 16400: Loss = -10857.2734375
Iteration 16500: Loss = -10857.2734375
Iteration 16600: Loss = -10857.2724609375
Iteration 16700: Loss = -10857.2724609375
Iteration 16800: Loss = -10857.2724609375
Iteration 16900: Loss = -10857.2734375
1
Iteration 17000: Loss = -10857.2724609375
Iteration 17100: Loss = -10857.271484375
Iteration 17200: Loss = -10857.271484375
Iteration 17300: Loss = -10857.2724609375
1
Iteration 17400: Loss = -10857.2705078125
Iteration 17500: Loss = -10857.271484375
1
Iteration 17600: Loss = -10857.271484375
2
Iteration 17700: Loss = -10857.2724609375
3
Iteration 17800: Loss = -10857.2724609375
4
Iteration 17900: Loss = -10857.2705078125
Iteration 18000: Loss = -10857.271484375
1
Iteration 18100: Loss = -10857.2705078125
Iteration 18200: Loss = -10857.26953125
Iteration 18300: Loss = -10857.2705078125
1
Iteration 18400: Loss = -10857.26953125
Iteration 18500: Loss = -10857.271484375
1
Iteration 18600: Loss = -10857.2705078125
2
Iteration 18700: Loss = -10857.271484375
3
Iteration 18800: Loss = -10857.271484375
4
Iteration 18900: Loss = -10857.2705078125
5
Iteration 19000: Loss = -10857.271484375
6
Iteration 19100: Loss = -10857.271484375
7
Iteration 19200: Loss = -10857.2705078125
8
Iteration 19300: Loss = -10857.271484375
9
Iteration 19400: Loss = -10857.26953125
Iteration 19500: Loss = -10857.2705078125
1
Iteration 19600: Loss = -10857.26953125
Iteration 19700: Loss = -10857.2705078125
1
Iteration 19800: Loss = -10857.2705078125
2
Iteration 19900: Loss = -10857.26953125
Iteration 20000: Loss = -10857.2705078125
1
Iteration 20100: Loss = -10857.2705078125
2
Iteration 20200: Loss = -10857.271484375
3
Iteration 20300: Loss = -10857.2705078125
4
Iteration 20400: Loss = -10857.26953125
Iteration 20500: Loss = -10857.2685546875
Iteration 20600: Loss = -10857.26953125
1
Iteration 20700: Loss = -10857.2705078125
2
Iteration 20800: Loss = -10857.2685546875
Iteration 20900: Loss = -10857.26953125
1
Iteration 21000: Loss = -10857.26953125
2
Iteration 21100: Loss = -10857.26953125
3
Iteration 21200: Loss = -10857.2685546875
Iteration 21300: Loss = -10857.2705078125
1
Iteration 21400: Loss = -10857.2685546875
Iteration 21500: Loss = -10857.26953125
1
Iteration 21600: Loss = -10857.2685546875
Iteration 21700: Loss = -10857.26953125
1
Iteration 21800: Loss = -10857.26953125
2
Iteration 21900: Loss = -10857.271484375
3
Iteration 22000: Loss = -10857.26953125
4
Iteration 22100: Loss = -10856.134765625
Iteration 22200: Loss = -10853.953125
Iteration 22300: Loss = -10853.9521484375
Iteration 22400: Loss = -10853.9521484375
Iteration 22500: Loss = -10853.953125
1
Iteration 22600: Loss = -10853.953125
2
Iteration 22700: Loss = -10853.9521484375
Iteration 22800: Loss = -10853.953125
1
Iteration 22900: Loss = -10853.9521484375
Iteration 23000: Loss = -10853.9521484375
Iteration 23100: Loss = -10853.951171875
Iteration 23200: Loss = -10853.951171875
Iteration 23300: Loss = -10853.951171875
Iteration 23400: Loss = -10853.953125
1
Iteration 23500: Loss = -10853.9521484375
2
Iteration 23600: Loss = -10853.9521484375
3
Iteration 23700: Loss = -10853.951171875
Iteration 23800: Loss = -10853.951171875
Iteration 23900: Loss = -10853.9521484375
1
Iteration 24000: Loss = -10853.9521484375
2
Iteration 24100: Loss = -10853.9521484375
3
Iteration 24200: Loss = -10853.9560546875
4
Iteration 24300: Loss = -10853.9521484375
5
Iteration 24400: Loss = -10853.951171875
Iteration 24500: Loss = -10853.951171875
Iteration 24600: Loss = -10853.951171875
Iteration 24700: Loss = -10853.9501953125
Iteration 24800: Loss = -10853.94921875
Iteration 24900: Loss = -10853.9501953125
1
Iteration 25000: Loss = -10853.9501953125
2
Iteration 25100: Loss = -10853.947265625
Iteration 25200: Loss = -10853.947265625
Iteration 25300: Loss = -10853.943359375
Iteration 25400: Loss = -10853.935546875
Iteration 25500: Loss = -10853.931640625
Iteration 25600: Loss = -10853.931640625
Iteration 25700: Loss = -10853.9296875
Iteration 25800: Loss = -10853.9296875
Iteration 25900: Loss = -10853.9306640625
1
Iteration 26000: Loss = -10853.927734375
Iteration 26100: Loss = -10853.9306640625
1
Iteration 26200: Loss = -10853.9306640625
2
Iteration 26300: Loss = -10853.9296875
3
Iteration 26400: Loss = -10853.9296875
4
Iteration 26500: Loss = -10853.9296875
5
Iteration 26600: Loss = -10853.9306640625
6
Iteration 26700: Loss = -10853.9296875
7
Iteration 26800: Loss = -10853.9296875
8
Iteration 26900: Loss = -10853.9296875
9
Iteration 27000: Loss = -10853.9296875
10
Iteration 27100: Loss = -10853.931640625
11
Iteration 27200: Loss = -10853.9306640625
12
Iteration 27300: Loss = -10853.9296875
13
Iteration 27400: Loss = -10853.9306640625
14
Iteration 27500: Loss = -10853.9287109375
15
Stopping early at iteration 27500 due to no improvement.
pi: tensor([[1.0000e+00, 8.9362e-07],
        [9.9463e-01, 5.3722e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0278, 0.9722], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1602, 0.2455],
         [0.9418, 0.1501]],

        [[0.1600, 0.2231],
         [0.9475, 0.0147]],

        [[0.2233, 0.1929],
         [0.3851, 0.0069]],

        [[0.2148, 0.1749],
         [0.9826, 0.1143]],

        [[0.3327, 0.3445],
         [0.2330, 0.9546]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015895362736001513
Average Adjusted Rand Index: 0.0020182875964866232
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21228.220703125
Iteration 100: Loss = -15539.71875
Iteration 200: Loss = -12106.86328125
Iteration 300: Loss = -11216.7373046875
Iteration 400: Loss = -11121.2763671875
Iteration 500: Loss = -11090.51171875
Iteration 600: Loss = -11070.3603515625
Iteration 700: Loss = -11055.7529296875
Iteration 800: Loss = -11043.6044921875
Iteration 900: Loss = -11032.4990234375
Iteration 1000: Loss = -11025.0341796875
Iteration 1100: Loss = -11018.4189453125
Iteration 1200: Loss = -11013.0087890625
Iteration 1300: Loss = -11006.97265625
Iteration 1400: Loss = -11003.3720703125
Iteration 1500: Loss = -11000.34765625
Iteration 1600: Loss = -10997.00390625
Iteration 1700: Loss = -10992.826171875
Iteration 1800: Loss = -10988.595703125
Iteration 1900: Loss = -10982.595703125
Iteration 2000: Loss = -10974.4736328125
Iteration 2100: Loss = -10963.328125
Iteration 2200: Loss = -10950.94140625
Iteration 2300: Loss = -10938.2373046875
Iteration 2400: Loss = -10922.3095703125
Iteration 2500: Loss = -10911.3046875
Iteration 2600: Loss = -10905.171875
Iteration 2700: Loss = -10894.166015625
Iteration 2800: Loss = -10879.77734375
Iteration 2900: Loss = -10870.080078125
Iteration 3000: Loss = -10865.7587890625
Iteration 3100: Loss = -10864.357421875
Iteration 3200: Loss = -10860.1748046875
Iteration 3300: Loss = -10857.94921875
Iteration 3400: Loss = -10857.333984375
Iteration 3500: Loss = -10856.919921875
Iteration 3600: Loss = -10856.6162109375
Iteration 3700: Loss = -10856.3818359375
Iteration 3800: Loss = -10856.193359375
Iteration 3900: Loss = -10856.0380859375
Iteration 4000: Loss = -10855.9091796875
Iteration 4100: Loss = -10855.798828125
Iteration 4200: Loss = -10855.7041015625
Iteration 4300: Loss = -10855.6240234375
Iteration 4400: Loss = -10855.5546875
Iteration 4500: Loss = -10855.4931640625
Iteration 4600: Loss = -10855.439453125
Iteration 4700: Loss = -10855.3916015625
Iteration 4800: Loss = -10855.3505859375
Iteration 4900: Loss = -10855.3115234375
Iteration 5000: Loss = -10855.2763671875
Iteration 5100: Loss = -10855.2451171875
Iteration 5200: Loss = -10855.21875
Iteration 5300: Loss = -10855.19140625
Iteration 5400: Loss = -10855.1708984375
Iteration 5500: Loss = -10855.1474609375
Iteration 5600: Loss = -10855.12890625
Iteration 5700: Loss = -10855.111328125
Iteration 5800: Loss = -10855.0966796875
Iteration 5900: Loss = -10855.0810546875
Iteration 6000: Loss = -10855.06640625
Iteration 6100: Loss = -10855.0546875
Iteration 6200: Loss = -10855.0419921875
Iteration 6300: Loss = -10855.0302734375
Iteration 6400: Loss = -10855.0205078125
Iteration 6500: Loss = -10855.01171875
Iteration 6600: Loss = -10855.001953125
Iteration 6700: Loss = -10854.994140625
Iteration 6800: Loss = -10854.986328125
Iteration 6900: Loss = -10854.978515625
Iteration 7000: Loss = -10854.97265625
Iteration 7100: Loss = -10854.9658203125
Iteration 7200: Loss = -10854.958984375
Iteration 7300: Loss = -10854.953125
Iteration 7400: Loss = -10854.9482421875
Iteration 7500: Loss = -10854.9423828125
Iteration 7600: Loss = -10854.939453125
Iteration 7700: Loss = -10854.9345703125
Iteration 7800: Loss = -10854.9296875
Iteration 7900: Loss = -10854.92578125
Iteration 8000: Loss = -10854.9228515625
Iteration 8100: Loss = -10854.91796875
Iteration 8200: Loss = -10854.9150390625
Iteration 8300: Loss = -10854.912109375
Iteration 8400: Loss = -10854.91015625
Iteration 8500: Loss = -10854.90625
Iteration 8600: Loss = -10854.9033203125
Iteration 8700: Loss = -10854.9013671875
Iteration 8800: Loss = -10854.8994140625
Iteration 8900: Loss = -10854.896484375
Iteration 9000: Loss = -10854.89453125
Iteration 9100: Loss = -10854.892578125
Iteration 9200: Loss = -10854.890625
Iteration 9300: Loss = -10854.888671875
Iteration 9400: Loss = -10854.888671875
Iteration 9500: Loss = -10854.884765625
Iteration 9600: Loss = -10854.884765625
Iteration 9700: Loss = -10854.8828125
Iteration 9800: Loss = -10854.880859375
Iteration 9900: Loss = -10854.8798828125
Iteration 10000: Loss = -10854.8779296875
Iteration 10100: Loss = -10854.87890625
1
Iteration 10200: Loss = -10854.8759765625
Iteration 10300: Loss = -10854.8759765625
Iteration 10400: Loss = -10854.875
Iteration 10500: Loss = -10854.8740234375
Iteration 10600: Loss = -10854.873046875
Iteration 10700: Loss = -10854.8720703125
Iteration 10800: Loss = -10854.87109375
Iteration 10900: Loss = -10854.8701171875
Iteration 11000: Loss = -10854.87109375
1
Iteration 11100: Loss = -10854.8681640625
Iteration 11200: Loss = -10854.8681640625
Iteration 11300: Loss = -10854.8681640625
Iteration 11400: Loss = -10854.8681640625
Iteration 11500: Loss = -10854.865234375
Iteration 11600: Loss = -10854.8662109375
1
Iteration 11700: Loss = -10854.8662109375
2
Iteration 11800: Loss = -10854.8642578125
Iteration 11900: Loss = -10854.865234375
1
Iteration 12000: Loss = -10854.865234375
2
Iteration 12100: Loss = -10854.86328125
Iteration 12200: Loss = -10854.86328125
Iteration 12300: Loss = -10854.861328125
Iteration 12400: Loss = -10854.86328125
1
Iteration 12500: Loss = -10854.8623046875
2
Iteration 12600: Loss = -10854.861328125
Iteration 12700: Loss = -10854.861328125
Iteration 12800: Loss = -10854.8623046875
1
Iteration 12900: Loss = -10854.86328125
2
Iteration 13000: Loss = -10854.861328125
Iteration 13100: Loss = -10854.859375
Iteration 13200: Loss = -10854.8603515625
1
Iteration 13300: Loss = -10854.859375
Iteration 13400: Loss = -10854.8603515625
1
Iteration 13500: Loss = -10854.8603515625
2
Iteration 13600: Loss = -10854.859375
Iteration 13700: Loss = -10854.859375
Iteration 13800: Loss = -10854.859375
Iteration 13900: Loss = -10854.8583984375
Iteration 14000: Loss = -10854.85546875
Iteration 14100: Loss = -10854.830078125
Iteration 14200: Loss = -10854.828125
Iteration 14300: Loss = -10854.830078125
1
Iteration 14400: Loss = -10854.8291015625
2
Iteration 14500: Loss = -10854.828125
Iteration 14600: Loss = -10854.8291015625
1
Iteration 14700: Loss = -10854.828125
Iteration 14800: Loss = -10854.8271484375
Iteration 14900: Loss = -10854.828125
1
Iteration 15000: Loss = -10854.8271484375
Iteration 15100: Loss = -10854.8271484375
Iteration 15200: Loss = -10854.828125
1
Iteration 15300: Loss = -10854.8271484375
Iteration 15400: Loss = -10854.8271484375
Iteration 15500: Loss = -10854.828125
1
Iteration 15600: Loss = -10854.826171875
Iteration 15700: Loss = -10854.8291015625
1
Iteration 15800: Loss = -10854.8271484375
2
Iteration 15900: Loss = -10854.826171875
Iteration 16000: Loss = -10854.826171875
Iteration 16100: Loss = -10854.8271484375
1
Iteration 16200: Loss = -10854.8271484375
2
Iteration 16300: Loss = -10854.8271484375
3
Iteration 16400: Loss = -10854.8271484375
4
Iteration 16500: Loss = -10854.826171875
Iteration 16600: Loss = -10854.828125
1
Iteration 16700: Loss = -10854.8271484375
2
Iteration 16800: Loss = -10854.8251953125
Iteration 16900: Loss = -10854.826171875
1
Iteration 17000: Loss = -10854.8251953125
Iteration 17100: Loss = -10854.826171875
1
Iteration 17200: Loss = -10854.826171875
2
Iteration 17300: Loss = -10854.826171875
3
Iteration 17400: Loss = -10854.8271484375
4
Iteration 17500: Loss = -10854.8251953125
Iteration 17600: Loss = -10854.8271484375
1
Iteration 17700: Loss = -10854.8271484375
2
Iteration 17800: Loss = -10854.8271484375
3
Iteration 17900: Loss = -10854.826171875
4
Iteration 18000: Loss = -10854.826171875
5
Iteration 18100: Loss = -10854.8251953125
Iteration 18200: Loss = -10854.826171875
1
Iteration 18300: Loss = -10854.826171875
2
Iteration 18400: Loss = -10854.826171875
3
Iteration 18500: Loss = -10854.826171875
4
Iteration 18600: Loss = -10854.826171875
5
Iteration 18700: Loss = -10854.826171875
6
Iteration 18800: Loss = -10854.826171875
7
Iteration 18900: Loss = -10854.8271484375
8
Iteration 19000: Loss = -10854.8251953125
Iteration 19100: Loss = -10854.826171875
1
Iteration 19200: Loss = -10854.8271484375
2
Iteration 19300: Loss = -10854.826171875
3
Iteration 19400: Loss = -10854.826171875
4
Iteration 19500: Loss = -10854.826171875
5
Iteration 19600: Loss = -10854.826171875
6
Iteration 19700: Loss = -10854.82421875
Iteration 19800: Loss = -10854.826171875
1
Iteration 19900: Loss = -10854.8212890625
Iteration 20000: Loss = -10854.8232421875
1
Iteration 20100: Loss = -10854.8212890625
Iteration 20200: Loss = -10854.8193359375
Iteration 20300: Loss = -10854.818359375
Iteration 20400: Loss = -10854.8193359375
1
Iteration 20500: Loss = -10854.8193359375
2
Iteration 20600: Loss = -10854.8173828125
Iteration 20700: Loss = -10854.81640625
Iteration 20800: Loss = -10854.8173828125
1
Iteration 20900: Loss = -10854.8154296875
Iteration 21000: Loss = -10854.8154296875
Iteration 21100: Loss = -10854.814453125
Iteration 21200: Loss = -10854.814453125
Iteration 21300: Loss = -10854.8125
Iteration 21400: Loss = -10854.8115234375
Iteration 21500: Loss = -10854.8115234375
Iteration 21600: Loss = -10854.8125
1
Iteration 21700: Loss = -10854.8125
2
Iteration 21800: Loss = -10854.810546875
Iteration 21900: Loss = -10854.8115234375
1
Iteration 22000: Loss = -10854.8115234375
2
Iteration 22100: Loss = -10854.8095703125
Iteration 22200: Loss = -10854.810546875
1
Iteration 22300: Loss = -10854.8095703125
Iteration 22400: Loss = -10854.80859375
Iteration 22500: Loss = -10854.8076171875
Iteration 22600: Loss = -10854.810546875
1
Iteration 22700: Loss = -10854.80859375
2
Iteration 22800: Loss = -10854.8095703125
3
Iteration 22900: Loss = -10854.80859375
4
Iteration 23000: Loss = -10854.8095703125
5
Iteration 23100: Loss = -10854.810546875
6
Iteration 23200: Loss = -10854.80859375
7
Iteration 23300: Loss = -10854.80859375
8
Iteration 23400: Loss = -10854.8095703125
9
Iteration 23500: Loss = -10854.80859375
10
Iteration 23600: Loss = -10854.80859375
11
Iteration 23700: Loss = -10854.80859375
12
Iteration 23800: Loss = -10854.8076171875
Iteration 23900: Loss = -10854.8095703125
1
Iteration 24000: Loss = -10854.8095703125
2
Iteration 24100: Loss = -10854.8095703125
3
Iteration 24200: Loss = -10854.8076171875
Iteration 24300: Loss = -10854.80859375
1
Iteration 24400: Loss = -10854.80859375
2
Iteration 24500: Loss = -10854.80859375
3
Iteration 24600: Loss = -10854.8095703125
4
Iteration 24700: Loss = -10854.8095703125
5
Iteration 24800: Loss = -10854.80859375
6
Iteration 24900: Loss = -10854.80859375
7
Iteration 25000: Loss = -10854.80859375
8
Iteration 25100: Loss = -10854.80859375
9
Iteration 25200: Loss = -10854.810546875
10
Iteration 25300: Loss = -10854.80859375
11
Iteration 25400: Loss = -10854.80859375
12
Iteration 25500: Loss = -10854.810546875
13
Iteration 25600: Loss = -10854.80859375
14
Iteration 25700: Loss = -10854.80859375
15
Stopping early at iteration 25700 due to no improvement.
pi: tensor([[1.0000e+00, 2.1623e-06],
        [9.4002e-01, 5.9982e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8464, 0.1536], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1602, 0.1439],
         [0.9274, 0.1287]],

        [[0.9910, 0.2011],
         [0.8633, 0.9885]],

        [[0.1395, 0.1791],
         [0.9445, 0.5461]],

        [[0.9897, 0.1987],
         [0.8278, 0.7044]],

        [[0.0792, 0.1534],
         [0.2482, 0.6329]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.0015895362736001513, 0.0] [0.0020182875964866232, 0.0] [10853.9287109375, 10854.80859375]
-------------------------------------
This iteration is 78
True Objective function: Loss = -10961.535011335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -62920.4140625
Iteration 100: Loss = -40359.02734375
Iteration 200: Loss = -22185.736328125
Iteration 300: Loss = -14301.7529296875
Iteration 400: Loss = -12148.26953125
Iteration 500: Loss = -11519.013671875
Iteration 600: Loss = -11305.0263671875
Iteration 700: Loss = -11220.904296875
Iteration 800: Loss = -11185.1318359375
Iteration 900: Loss = -11155.1298828125
Iteration 1000: Loss = -11130.2509765625
Iteration 1100: Loss = -11114.078125
Iteration 1200: Loss = -11104.236328125
Iteration 1300: Loss = -11097.525390625
Iteration 1400: Loss = -11092.4755859375
Iteration 1500: Loss = -11088.4482421875
Iteration 1600: Loss = -11085.138671875
Iteration 1700: Loss = -11082.3623046875
Iteration 1800: Loss = -11080.0185546875
Iteration 1900: Loss = -11078.005859375
Iteration 2000: Loss = -11076.263671875
Iteration 2100: Loss = -11074.7431640625
Iteration 2200: Loss = -11073.4052734375
Iteration 2300: Loss = -11072.22265625
Iteration 2400: Loss = -11071.1689453125
Iteration 2500: Loss = -11070.228515625
Iteration 2600: Loss = -11069.3828125
Iteration 2700: Loss = -11068.623046875
Iteration 2800: Loss = -11067.9345703125
Iteration 2900: Loss = -11067.3095703125
Iteration 3000: Loss = -11066.72265625
Iteration 3100: Loss = -11066.205078125
Iteration 3200: Loss = -11065.734375
Iteration 3300: Loss = -11065.2998046875
Iteration 3400: Loss = -11064.9013671875
Iteration 3500: Loss = -11064.533203125
Iteration 3600: Loss = -11064.1943359375
Iteration 3700: Loss = -11063.8818359375
Iteration 3800: Loss = -11063.5908203125
Iteration 3900: Loss = -11063.322265625
Iteration 4000: Loss = -11063.0712890625
Iteration 4100: Loss = -11062.8408203125
Iteration 4200: Loss = -11062.623046875
Iteration 4300: Loss = -11062.4208984375
Iteration 4400: Loss = -11062.234375
Iteration 4500: Loss = -11062.0595703125
Iteration 4600: Loss = -11061.8935546875
Iteration 4700: Loss = -11061.7392578125
Iteration 4800: Loss = -11061.595703125
Iteration 4900: Loss = -11061.4619140625
Iteration 5000: Loss = -11061.333984375
Iteration 5100: Loss = -11061.2138671875
Iteration 5200: Loss = -11061.1025390625
Iteration 5300: Loss = -11060.9970703125
Iteration 5400: Loss = -11060.8984375
Iteration 5500: Loss = -11060.8046875
Iteration 5600: Loss = -11060.716796875
Iteration 5700: Loss = -11060.6337890625
Iteration 5800: Loss = -11060.5576171875
Iteration 5900: Loss = -11060.484375
Iteration 6000: Loss = -11060.4130859375
Iteration 6100: Loss = -11060.3486328125
Iteration 6200: Loss = -11060.2861328125
Iteration 6300: Loss = -11060.2255859375
Iteration 6400: Loss = -11060.1728515625
Iteration 6500: Loss = -11060.1201171875
Iteration 6600: Loss = -11060.0703125
Iteration 6700: Loss = -11060.0234375
Iteration 6800: Loss = -11059.9794921875
Iteration 6900: Loss = -11059.9375
Iteration 7000: Loss = -11059.8984375
Iteration 7100: Loss = -11059.861328125
Iteration 7200: Loss = -11059.826171875
Iteration 7300: Loss = -11059.7919921875
Iteration 7400: Loss = -11059.7607421875
Iteration 7500: Loss = -11059.728515625
Iteration 7600: Loss = -11059.701171875
Iteration 7700: Loss = -11059.673828125
Iteration 7800: Loss = -11059.6484375
Iteration 7900: Loss = -11059.625
Iteration 8000: Loss = -11059.6005859375
Iteration 8100: Loss = -11059.5791015625
Iteration 8200: Loss = -11059.5595703125
Iteration 8300: Loss = -11059.5380859375
Iteration 8400: Loss = -11059.5205078125
Iteration 8500: Loss = -11059.501953125
Iteration 8600: Loss = -11059.4853515625
Iteration 8700: Loss = -11059.4697265625
Iteration 8800: Loss = -11059.455078125
Iteration 8900: Loss = -11059.4404296875
Iteration 9000: Loss = -11059.4267578125
Iteration 9100: Loss = -11059.4130859375
Iteration 9200: Loss = -11059.4013671875
Iteration 9300: Loss = -11059.3916015625
Iteration 9400: Loss = -11059.3779296875
Iteration 9500: Loss = -11059.369140625
Iteration 9600: Loss = -11059.3583984375
Iteration 9700: Loss = -11059.34765625
Iteration 9800: Loss = -11059.33984375
Iteration 9900: Loss = -11059.330078125
Iteration 10000: Loss = -11059.3232421875
Iteration 10100: Loss = -11059.31640625
Iteration 10200: Loss = -11059.30859375
Iteration 10300: Loss = -11059.302734375
Iteration 10400: Loss = -11059.294921875
Iteration 10500: Loss = -11059.2890625
Iteration 10600: Loss = -11059.2841796875
Iteration 10700: Loss = -11059.279296875
Iteration 10800: Loss = -11059.2734375
Iteration 10900: Loss = -11059.267578125
Iteration 11000: Loss = -11059.2646484375
Iteration 11100: Loss = -11059.2587890625
Iteration 11200: Loss = -11059.25390625
Iteration 11300: Loss = -11059.251953125
Iteration 11400: Loss = -11059.248046875
Iteration 11500: Loss = -11059.2431640625
Iteration 11600: Loss = -11059.2412109375
Iteration 11700: Loss = -11059.23828125
Iteration 11800: Loss = -11059.234375
Iteration 11900: Loss = -11059.2314453125
Iteration 12000: Loss = -11059.228515625
Iteration 12100: Loss = -11059.224609375
Iteration 12200: Loss = -11059.2236328125
Iteration 12300: Loss = -11059.220703125
Iteration 12400: Loss = -11059.2197265625
Iteration 12500: Loss = -11059.2158203125
Iteration 12600: Loss = -11059.2138671875
Iteration 12700: Loss = -11059.2138671875
Iteration 12800: Loss = -11059.2119140625
Iteration 12900: Loss = -11059.208984375
Iteration 13000: Loss = -11059.20703125
Iteration 13100: Loss = -11059.205078125
Iteration 13200: Loss = -11059.205078125
Iteration 13300: Loss = -11059.203125
Iteration 13400: Loss = -11059.2021484375
Iteration 13500: Loss = -11059.201171875
Iteration 13600: Loss = -11059.2001953125
Iteration 13700: Loss = -11059.2001953125
Iteration 13800: Loss = -11059.19921875
Iteration 13900: Loss = -11059.1982421875
Iteration 14000: Loss = -11059.1953125
Iteration 14100: Loss = -11059.1953125
Iteration 14200: Loss = -11059.193359375
Iteration 14300: Loss = -11059.193359375
Iteration 14400: Loss = -11059.19140625
Iteration 14500: Loss = -11059.19140625
Iteration 14600: Loss = -11059.1904296875
Iteration 14700: Loss = -11059.1904296875
Iteration 14800: Loss = -11059.189453125
Iteration 14900: Loss = -11059.189453125
Iteration 15000: Loss = -11059.189453125
Iteration 15100: Loss = -11059.1884765625
Iteration 15200: Loss = -11059.1875
Iteration 15300: Loss = -11059.1875
Iteration 15400: Loss = -11059.1865234375
Iteration 15500: Loss = -11059.1865234375
Iteration 15600: Loss = -11059.1865234375
Iteration 15700: Loss = -11059.1865234375
Iteration 15800: Loss = -11059.1845703125
Iteration 15900: Loss = -11059.18359375
Iteration 16000: Loss = -11059.1845703125
1
Iteration 16100: Loss = -11059.18359375
Iteration 16200: Loss = -11059.185546875
1
Iteration 16300: Loss = -11059.18359375
Iteration 16400: Loss = -11059.181640625
Iteration 16500: Loss = -11059.1826171875
1
Iteration 16600: Loss = -11059.1826171875
2
Iteration 16700: Loss = -11059.181640625
Iteration 16800: Loss = -11059.1796875
Iteration 16900: Loss = -11059.1806640625
1
Iteration 17000: Loss = -11059.181640625
2
Iteration 17100: Loss = -11059.1806640625
3
Iteration 17200: Loss = -11059.1806640625
4
Iteration 17300: Loss = -11059.1796875
Iteration 17400: Loss = -11059.1787109375
Iteration 17500: Loss = -11059.1796875
1
Iteration 17600: Loss = -11059.1796875
2
Iteration 17700: Loss = -11059.1796875
3
Iteration 17800: Loss = -11059.177734375
Iteration 17900: Loss = -11059.1787109375
1
Iteration 18000: Loss = -11059.1767578125
Iteration 18100: Loss = -11059.1767578125
Iteration 18200: Loss = -11059.1767578125
Iteration 18300: Loss = -11059.17578125
Iteration 18400: Loss = -11059.17578125
Iteration 18500: Loss = -11059.1767578125
1
Iteration 18600: Loss = -11059.1748046875
Iteration 18700: Loss = -11059.1748046875
Iteration 18800: Loss = -11059.1787109375
1
Iteration 18900: Loss = -11059.17578125
2
Iteration 19000: Loss = -11059.1748046875
Iteration 19100: Loss = -11059.17578125
1
Iteration 19200: Loss = -11059.17578125
2
Iteration 19300: Loss = -11059.1748046875
Iteration 19400: Loss = -11059.173828125
Iteration 19500: Loss = -11059.173828125
Iteration 19600: Loss = -11059.1748046875
1
Iteration 19700: Loss = -11059.1728515625
Iteration 19800: Loss = -11059.1748046875
1
Iteration 19900: Loss = -11059.17578125
2
Iteration 20000: Loss = -11059.1728515625
Iteration 20100: Loss = -11059.1748046875
1
Iteration 20200: Loss = -11059.1748046875
2
Iteration 20300: Loss = -11059.171875
Iteration 20400: Loss = -11059.1708984375
Iteration 20500: Loss = -11059.1728515625
1
Iteration 20600: Loss = -11059.173828125
2
Iteration 20700: Loss = -11059.173828125
3
Iteration 20800: Loss = -11059.173828125
4
Iteration 20900: Loss = -11059.173828125
5
Iteration 21000: Loss = -11059.173828125
6
Iteration 21100: Loss = -11059.1728515625
7
Iteration 21200: Loss = -11059.173828125
8
Iteration 21300: Loss = -11059.1728515625
9
Iteration 21400: Loss = -11059.173828125
10
Iteration 21500: Loss = -11059.1748046875
11
Iteration 21600: Loss = -11059.1728515625
12
Iteration 21700: Loss = -11059.1728515625
13
Iteration 21800: Loss = -11059.1728515625
14
Iteration 21900: Loss = -11059.1728515625
15
Stopping early at iteration 21900 due to no improvement.
pi: tensor([[1.0000e+00, 2.6811e-06],
        [9.9219e-01, 7.8054e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9929e-01, 7.1010e-04], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1643, 0.0945],
         [0.1753, 0.8892]],

        [[0.0500, 0.1708],
         [0.1916, 0.9931]],

        [[0.0182, 0.3291],
         [0.0447, 0.0199]],

        [[0.9859, 0.1757],
         [0.4827, 0.9853]],

        [[0.0125, 0.2316],
         [0.0627, 0.1595]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39991.6484375
Iteration 100: Loss = -24428.6015625
Iteration 200: Loss = -15005.05078125
Iteration 300: Loss = -12477.806640625
Iteration 400: Loss = -11651.98828125
Iteration 500: Loss = -11367.685546875
Iteration 600: Loss = -11244.1455078125
Iteration 700: Loss = -11194.21875
Iteration 800: Loss = -11164.0390625
Iteration 900: Loss = -11142.11328125
Iteration 1000: Loss = -11125.59765625
Iteration 1100: Loss = -11110.5615234375
Iteration 1200: Loss = -11101.3935546875
Iteration 1300: Loss = -11095.1083984375
Iteration 1400: Loss = -11090.1376953125
Iteration 1500: Loss = -11086.12109375
Iteration 1600: Loss = -11082.8173828125
Iteration 1700: Loss = -11080.06640625
Iteration 1800: Loss = -11077.7568359375
Iteration 1900: Loss = -11075.798828125
Iteration 2000: Loss = -11074.1162109375
Iteration 2100: Loss = -11072.6669921875
Iteration 2200: Loss = -11071.4052734375
Iteration 2300: Loss = -11070.2998046875
Iteration 2400: Loss = -11069.3291015625
Iteration 2500: Loss = -11068.46484375
Iteration 2600: Loss = -11067.69921875
Iteration 2700: Loss = -11067.013671875
Iteration 2800: Loss = -11066.396484375
Iteration 2900: Loss = -11065.8427734375
Iteration 3000: Loss = -11065.341796875
Iteration 3100: Loss = -11064.8896484375
Iteration 3200: Loss = -11064.474609375
Iteration 3300: Loss = -11064.09765625
Iteration 3400: Loss = -11063.7529296875
Iteration 3500: Loss = -11063.4375
Iteration 3600: Loss = -11063.1484375
Iteration 3700: Loss = -11062.880859375
Iteration 3800: Loss = -11062.6357421875
Iteration 3900: Loss = -11062.408203125
Iteration 4000: Loss = -11062.1982421875
Iteration 4100: Loss = -11062.005859375
Iteration 4200: Loss = -11061.826171875
Iteration 4300: Loss = -11061.658203125
Iteration 4400: Loss = -11061.5029296875
Iteration 4500: Loss = -11061.357421875
Iteration 4600: Loss = -11061.2216796875
Iteration 4700: Loss = -11061.09765625
Iteration 4800: Loss = -11060.982421875
Iteration 4900: Loss = -11060.8720703125
Iteration 5000: Loss = -11060.7685546875
Iteration 5100: Loss = -11060.673828125
Iteration 5200: Loss = -11060.58203125
Iteration 5300: Loss = -11060.4970703125
Iteration 5400: Loss = -11060.419921875
Iteration 5500: Loss = -11060.345703125
Iteration 5600: Loss = -11060.275390625
Iteration 5700: Loss = -11060.2109375
Iteration 5800: Loss = -11060.1494140625
Iteration 5900: Loss = -11060.0908203125
Iteration 6000: Loss = -11060.037109375
Iteration 6100: Loss = -11059.9873046875
Iteration 6200: Loss = -11059.9375
Iteration 6300: Loss = -11059.8916015625
Iteration 6400: Loss = -11059.8486328125
Iteration 6500: Loss = -11059.80859375
Iteration 6600: Loss = -11059.7724609375
Iteration 6700: Loss = -11059.736328125
Iteration 6800: Loss = -11059.7021484375
Iteration 6900: Loss = -11059.6689453125
Iteration 7000: Loss = -11059.6396484375
Iteration 7100: Loss = -11059.611328125
Iteration 7200: Loss = -11059.5849609375
Iteration 7300: Loss = -11059.55859375
Iteration 7400: Loss = -11059.533203125
Iteration 7500: Loss = -11059.5126953125
Iteration 7600: Loss = -11059.490234375
Iteration 7700: Loss = -11059.4697265625
Iteration 7800: Loss = -11059.44921875
Iteration 7900: Loss = -11059.4296875
Iteration 8000: Loss = -11059.4130859375
Iteration 8100: Loss = -11059.3955078125
Iteration 8200: Loss = -11059.3818359375
Iteration 8300: Loss = -11059.3662109375
Iteration 8400: Loss = -11059.3515625
Iteration 8500: Loss = -11059.33984375
Iteration 8600: Loss = -11059.328125
Iteration 8700: Loss = -11059.31640625
Iteration 8800: Loss = -11059.3046875
Iteration 8900: Loss = -11059.2939453125
Iteration 9000: Loss = -11059.2861328125
Iteration 9100: Loss = -11059.2744140625
Iteration 9200: Loss = -11059.265625
Iteration 9300: Loss = -11059.2568359375
Iteration 9400: Loss = -11059.2490234375
Iteration 9500: Loss = -11059.2412109375
Iteration 9600: Loss = -11059.234375
Iteration 9700: Loss = -11059.2265625
Iteration 9800: Loss = -11059.21875
Iteration 9900: Loss = -11059.2138671875
Iteration 10000: Loss = -11059.208984375
Iteration 10100: Loss = -11059.201171875
Iteration 10200: Loss = -11059.1982421875
Iteration 10300: Loss = -11059.1923828125
Iteration 10400: Loss = -11059.1875
Iteration 10500: Loss = -11059.185546875
Iteration 10600: Loss = -11059.1796875
Iteration 10700: Loss = -11059.177734375
Iteration 10800: Loss = -11059.1728515625
Iteration 10900: Loss = -11059.1689453125
Iteration 11000: Loss = -11059.1650390625
Iteration 11100: Loss = -11059.1611328125
Iteration 11200: Loss = -11059.154296875
Iteration 11300: Loss = -11059.150390625
Iteration 11400: Loss = -11059.1435546875
Iteration 11500: Loss = -11059.1396484375
Iteration 11600: Loss = -11059.134765625
Iteration 11700: Loss = -11059.1298828125
Iteration 11800: Loss = -11059.1220703125
Iteration 11900: Loss = -11059.1171875
Iteration 12000: Loss = -11059.1103515625
Iteration 12100: Loss = -11059.103515625
Iteration 12200: Loss = -11059.099609375
Iteration 12300: Loss = -11059.0927734375
Iteration 12400: Loss = -11059.087890625
Iteration 12500: Loss = -11059.083984375
Iteration 12600: Loss = -11059.0810546875
Iteration 12700: Loss = -11059.0751953125
Iteration 12800: Loss = -11059.0693359375
Iteration 12900: Loss = -11059.0654296875
Iteration 13000: Loss = -11059.0615234375
Iteration 13100: Loss = -11059.0576171875
Iteration 13200: Loss = -11059.0517578125
Iteration 13300: Loss = -11059.0458984375
Iteration 13400: Loss = -11059.0400390625
Iteration 13500: Loss = -11059.0322265625
Iteration 13600: Loss = -11059.0244140625
Iteration 13700: Loss = -11059.015625
Iteration 13800: Loss = -11059.001953125
Iteration 13900: Loss = -11058.990234375
Iteration 14000: Loss = -11058.97265625
Iteration 14100: Loss = -11058.947265625
Iteration 14200: Loss = -11058.9130859375
Iteration 14300: Loss = -11058.8603515625
Iteration 14400: Loss = -11058.779296875
Iteration 14500: Loss = -11058.6416015625
Iteration 14600: Loss = -11058.43359375
Iteration 14700: Loss = -11058.23828125
Iteration 14800: Loss = -11058.14453125
Iteration 14900: Loss = -11058.1025390625
Iteration 15000: Loss = -11058.0791015625
Iteration 15100: Loss = -11057.994140625
Iteration 15200: Loss = -11057.974609375
Iteration 15300: Loss = -11057.962890625
Iteration 15400: Loss = -11057.9541015625
Iteration 15500: Loss = -11057.947265625
Iteration 15600: Loss = -11057.9423828125
Iteration 15700: Loss = -11057.9404296875
Iteration 15800: Loss = -11057.939453125
Iteration 15900: Loss = -11057.9375
Iteration 16000: Loss = -11057.9365234375
Iteration 16100: Loss = -11057.9345703125
Iteration 16200: Loss = -11057.93359375
Iteration 16300: Loss = -11057.9326171875
Iteration 16400: Loss = -11057.93359375
1
Iteration 16500: Loss = -11057.931640625
Iteration 16600: Loss = -11057.931640625
Iteration 16700: Loss = -11057.931640625
Iteration 16800: Loss = -11057.9287109375
Iteration 16900: Loss = -11057.9296875
1
Iteration 17000: Loss = -11057.927734375
Iteration 17100: Loss = -11057.9287109375
1
Iteration 17200: Loss = -11057.92578125
Iteration 17300: Loss = -11057.9306640625
1
Iteration 17400: Loss = -11057.92578125
Iteration 17500: Loss = -11057.9248046875
Iteration 17600: Loss = -11057.923828125
Iteration 17700: Loss = -11057.923828125
Iteration 17800: Loss = -11057.921875
Iteration 17900: Loss = -11057.9208984375
Iteration 18000: Loss = -11057.9208984375
Iteration 18100: Loss = -11057.9208984375
Iteration 18200: Loss = -11057.9208984375
Iteration 18300: Loss = -11057.921875
1
Iteration 18400: Loss = -11057.919921875
Iteration 18500: Loss = -11057.9208984375
1
Iteration 18600: Loss = -11057.9208984375
2
Iteration 18700: Loss = -11057.9208984375
3
Iteration 18800: Loss = -11057.9208984375
4
Iteration 18900: Loss = -11057.9228515625
5
Iteration 19000: Loss = -11057.9208984375
6
Iteration 19100: Loss = -11057.919921875
Iteration 19200: Loss = -11057.9208984375
1
Iteration 19300: Loss = -11057.9208984375
2
Iteration 19400: Loss = -11057.919921875
Iteration 19500: Loss = -11057.919921875
Iteration 19600: Loss = -11057.9208984375
1
Iteration 19700: Loss = -11057.919921875
Iteration 19800: Loss = -11057.921875
1
Iteration 19900: Loss = -11057.919921875
Iteration 20000: Loss = -11057.9189453125
Iteration 20100: Loss = -11057.9208984375
1
Iteration 20200: Loss = -11057.919921875
2
Iteration 20300: Loss = -11057.9189453125
Iteration 20400: Loss = -11057.91796875
Iteration 20500: Loss = -11057.9208984375
1
Iteration 20600: Loss = -11057.9189453125
2
Iteration 20700: Loss = -11057.91796875
Iteration 20800: Loss = -11057.9189453125
1
Iteration 20900: Loss = -11057.9189453125
2
Iteration 21000: Loss = -11057.9189453125
3
Iteration 21100: Loss = -11057.9208984375
4
Iteration 21200: Loss = -11057.91796875
Iteration 21300: Loss = -11057.919921875
1
Iteration 21400: Loss = -11057.919921875
2
Iteration 21500: Loss = -11057.9189453125
3
Iteration 21600: Loss = -11057.919921875
4
Iteration 21700: Loss = -11057.919921875
5
Iteration 21800: Loss = -11057.9189453125
6
Iteration 21900: Loss = -11057.9189453125
7
Iteration 22000: Loss = -11057.9189453125
8
Iteration 22100: Loss = -11057.919921875
9
Iteration 22200: Loss = -11057.919921875
10
Iteration 22300: Loss = -11057.9189453125
11
Iteration 22400: Loss = -11057.9189453125
12
Iteration 22500: Loss = -11057.9189453125
13
Iteration 22600: Loss = -11057.9189453125
14
Iteration 22700: Loss = -11057.9189453125
15
Stopping early at iteration 22700 due to no improvement.
pi: tensor([[9.6129e-01, 3.8707e-02],
        [9.9991e-01, 9.0548e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9916, 0.0084], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1632, 0.2398],
         [0.9809, 0.2161]],

        [[0.0118, 0.2079],
         [0.9736, 0.9843]],

        [[0.0262, 0.2009],
         [0.9416, 0.9465]],

        [[0.0703, 0.1893],
         [0.2609, 0.6316]],

        [[0.2102, 0.1065],
         [0.4450, 0.6952]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [11059.1728515625, 11057.9189453125]
-------------------------------------
This iteration is 79
True Objective function: Loss = -10796.319273118004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -42207.69921875
Iteration 100: Loss = -23697.076171875
Iteration 200: Loss = -13689.771484375
Iteration 300: Loss = -11670.40234375
Iteration 400: Loss = -11338.052734375
Iteration 500: Loss = -11190.8798828125
Iteration 600: Loss = -11107.5732421875
Iteration 700: Loss = -11046.8720703125
Iteration 800: Loss = -11013.5009765625
Iteration 900: Loss = -10992.177734375
Iteration 1000: Loss = -10977.2802734375
Iteration 1100: Loss = -10966.2158203125
Iteration 1200: Loss = -10957.681640625
Iteration 1300: Loss = -10950.91796875
Iteration 1400: Loss = -10945.447265625
Iteration 1500: Loss = -10940.9453125
Iteration 1600: Loss = -10937.1884765625
Iteration 1700: Loss = -10934.017578125
Iteration 1800: Loss = -10931.30859375
Iteration 1900: Loss = -10928.98046875
Iteration 2000: Loss = -10926.9580078125
Iteration 2100: Loss = -10925.19140625
Iteration 2200: Loss = -10923.63671875
Iteration 2300: Loss = -10922.2626953125
Iteration 2400: Loss = -10921.041015625
Iteration 2500: Loss = -10919.94921875
Iteration 2600: Loss = -10918.9677734375
Iteration 2700: Loss = -10918.0849609375
Iteration 2800: Loss = -10917.2880859375
Iteration 2900: Loss = -10916.5634765625
Iteration 3000: Loss = -10915.9033203125
Iteration 3100: Loss = -10915.302734375
Iteration 3200: Loss = -10914.748046875
Iteration 3300: Loss = -10914.236328125
Iteration 3400: Loss = -10913.7607421875
Iteration 3500: Loss = -10913.3193359375
Iteration 3600: Loss = -10912.9052734375
Iteration 3700: Loss = -10912.51171875
Iteration 3800: Loss = -10912.140625
Iteration 3900: Loss = -10911.787109375
Iteration 4000: Loss = -10911.455078125
Iteration 4100: Loss = -10911.158203125
Iteration 4200: Loss = -10910.8935546875
Iteration 4300: Loss = -10910.66796875
Iteration 4400: Loss = -10910.4765625
Iteration 4500: Loss = -10910.3046875
Iteration 4600: Loss = -10910.1533203125
Iteration 4700: Loss = -10910.0166015625
Iteration 4800: Loss = -10909.892578125
Iteration 4900: Loss = -10909.775390625
Iteration 5000: Loss = -10909.66796875
Iteration 5100: Loss = -10909.5654296875
Iteration 5200: Loss = -10909.4716796875
Iteration 5300: Loss = -10909.3818359375
Iteration 5400: Loss = -10909.298828125
Iteration 5500: Loss = -10909.220703125
Iteration 5600: Loss = -10909.1494140625
Iteration 5700: Loss = -10909.080078125
Iteration 5800: Loss = -10909.017578125
Iteration 5900: Loss = -10908.95703125
Iteration 6000: Loss = -10908.90234375
Iteration 6100: Loss = -10908.8515625
Iteration 6200: Loss = -10908.802734375
Iteration 6300: Loss = -10908.7578125
Iteration 6400: Loss = -10908.71484375
Iteration 6500: Loss = -10908.6748046875
Iteration 6600: Loss = -10908.63671875
Iteration 6700: Loss = -10908.6005859375
Iteration 6800: Loss = -10908.5654296875
Iteration 6900: Loss = -10908.533203125
Iteration 7000: Loss = -10908.501953125
Iteration 7100: Loss = -10908.4736328125
Iteration 7200: Loss = -10908.4462890625
Iteration 7300: Loss = -10908.419921875
Iteration 7400: Loss = -10908.396484375
Iteration 7500: Loss = -10908.375
Iteration 7600: Loss = -10908.3544921875
Iteration 7700: Loss = -10908.333984375
Iteration 7800: Loss = -10908.3154296875
Iteration 7900: Loss = -10908.296875
Iteration 8000: Loss = -10908.2802734375
Iteration 8100: Loss = -10908.2646484375
Iteration 8200: Loss = -10908.2509765625
Iteration 8300: Loss = -10908.2333984375
Iteration 8400: Loss = -10908.2197265625
Iteration 8500: Loss = -10908.205078125
Iteration 8600: Loss = -10908.1923828125
Iteration 8700: Loss = -10908.1796875
Iteration 8800: Loss = -10908.166015625
Iteration 8900: Loss = -10908.1552734375
Iteration 9000: Loss = -10908.1435546875
Iteration 9100: Loss = -10908.1318359375
Iteration 9200: Loss = -10908.1201171875
Iteration 9300: Loss = -10908.109375
Iteration 9400: Loss = -10908.099609375
Iteration 9500: Loss = -10908.0908203125
Iteration 9600: Loss = -10908.080078125
Iteration 9700: Loss = -10908.0732421875
Iteration 9800: Loss = -10908.064453125
Iteration 9900: Loss = -10908.0556640625
Iteration 10000: Loss = -10908.046875
Iteration 10100: Loss = -10908.0380859375
Iteration 10200: Loss = -10908.03125
Iteration 10300: Loss = -10908.0205078125
Iteration 10400: Loss = -10908.0107421875
Iteration 10500: Loss = -10908.0029296875
Iteration 10600: Loss = -10907.9931640625
Iteration 10700: Loss = -10907.9833984375
Iteration 10800: Loss = -10907.9716796875
Iteration 10900: Loss = -10907.9599609375
Iteration 11000: Loss = -10907.9453125
Iteration 11100: Loss = -10907.9306640625
Iteration 11200: Loss = -10907.916015625
Iteration 11300: Loss = -10907.8984375
Iteration 11400: Loss = -10907.8798828125
Iteration 11500: Loss = -10907.8642578125
Iteration 11600: Loss = -10907.84375
Iteration 11700: Loss = -10907.8212890625
Iteration 11800: Loss = -10907.798828125
Iteration 11900: Loss = -10907.7763671875
Iteration 12000: Loss = -10907.748046875
Iteration 12100: Loss = -10907.716796875
Iteration 12200: Loss = -10907.6845703125
Iteration 12300: Loss = -10907.646484375
Iteration 12400: Loss = -10907.5986328125
Iteration 12500: Loss = -10907.537109375
Iteration 12600: Loss = -10907.458984375
Iteration 12700: Loss = -10907.3525390625
Iteration 12800: Loss = -10907.197265625
Iteration 12900: Loss = -10906.9716796875
Iteration 13000: Loss = -10906.6650390625
Iteration 13100: Loss = -10906.357421875
Iteration 13200: Loss = -10906.1474609375
Iteration 13300: Loss = -10906.0
Iteration 13400: Loss = -10905.884765625
Iteration 13500: Loss = -10905.806640625
Iteration 13600: Loss = -10905.744140625
Iteration 13700: Loss = -10905.6923828125
Iteration 13800: Loss = -10905.6455078125
Iteration 13900: Loss = -10905.6005859375
Iteration 14000: Loss = -10905.5517578125
Iteration 14100: Loss = -10905.4931640625
Iteration 14200: Loss = -10905.38671875
Iteration 14300: Loss = -10905.333984375
Iteration 14400: Loss = -10905.2861328125
Iteration 14500: Loss = -10905.2373046875
Iteration 14600: Loss = -10905.1826171875
Iteration 14700: Loss = -10905.130859375
Iteration 14800: Loss = -10905.099609375
Iteration 14900: Loss = -10905.05859375
Iteration 15000: Loss = -10905.0458984375
Iteration 15100: Loss = -10905.0224609375
Iteration 15200: Loss = -10904.9619140625
Iteration 15300: Loss = -10904.9521484375
Iteration 15400: Loss = -10904.943359375
Iteration 15500: Loss = -10904.9404296875
Iteration 15600: Loss = -10904.9375
Iteration 15700: Loss = -10904.9326171875
Iteration 15800: Loss = -10904.931640625
Iteration 15900: Loss = -10904.9267578125
Iteration 16000: Loss = -10904.923828125
Iteration 16100: Loss = -10904.9228515625
Iteration 16200: Loss = -10904.923828125
1
Iteration 16300: Loss = -10904.9228515625
Iteration 16400: Loss = -10904.9228515625
Iteration 16500: Loss = -10904.921875
Iteration 16600: Loss = -10904.921875
Iteration 16700: Loss = -10904.921875
Iteration 16800: Loss = -10904.9228515625
1
Iteration 16900: Loss = -10904.921875
Iteration 17000: Loss = -10904.921875
Iteration 17100: Loss = -10904.921875
Iteration 17200: Loss = -10904.919921875
Iteration 17300: Loss = -10904.9208984375
1
Iteration 17400: Loss = -10904.9208984375
2
Iteration 17500: Loss = -10904.921875
3
Iteration 17600: Loss = -10904.919921875
Iteration 17700: Loss = -10904.9208984375
1
Iteration 17800: Loss = -10904.921875
2
Iteration 17900: Loss = -10904.9208984375
3
Iteration 18000: Loss = -10904.9208984375
4
Iteration 18100: Loss = -10904.919921875
Iteration 18200: Loss = -10904.919921875
Iteration 18300: Loss = -10904.9208984375
1
Iteration 18400: Loss = -10904.919921875
Iteration 18500: Loss = -10904.9208984375
1
Iteration 18600: Loss = -10904.9208984375
2
Iteration 18700: Loss = -10904.919921875
Iteration 18800: Loss = -10904.9208984375
1
Iteration 18900: Loss = -10904.919921875
Iteration 19000: Loss = -10904.919921875
Iteration 19100: Loss = -10904.919921875
Iteration 19200: Loss = -10904.919921875
Iteration 19300: Loss = -10904.919921875
Iteration 19400: Loss = -10904.9189453125
Iteration 19500: Loss = -10904.9189453125
Iteration 19600: Loss = -10904.9208984375
1
Iteration 19700: Loss = -10904.91796875
Iteration 19800: Loss = -10904.91796875
Iteration 19900: Loss = -10904.9208984375
1
Iteration 20000: Loss = -10904.919921875
2
Iteration 20100: Loss = -10904.9189453125
3
Iteration 20200: Loss = -10904.919921875
4
Iteration 20300: Loss = -10904.9208984375
5
Iteration 20400: Loss = -10904.919921875
6
Iteration 20500: Loss = -10904.9189453125
7
Iteration 20600: Loss = -10904.919921875
8
Iteration 20700: Loss = -10904.9208984375
9
Iteration 20800: Loss = -10904.9208984375
10
Iteration 20900: Loss = -10904.9189453125
11
Iteration 21000: Loss = -10904.919921875
12
Iteration 21100: Loss = -10904.9189453125
13
Iteration 21200: Loss = -10904.919921875
14
Iteration 21300: Loss = -10904.919921875
15
Stopping early at iteration 21300 due to no improvement.
pi: tensor([[0.0011, 0.9989],
        [0.0727, 0.9273]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1406, 0.8594], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2310, 0.1959],
         [0.4528, 0.1551]],

        [[0.9617, 0.1673],
         [0.8661, 0.3995]],

        [[0.8834, 0.1519],
         [0.9184, 0.9849]],

        [[0.9723, 0.2090],
         [0.0428, 0.0104]],

        [[0.2907, 0.2041],
         [0.1109, 0.9039]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00039967221681423806
Average Adjusted Rand Index: -0.0011554446491642023
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24967.55859375
Iteration 100: Loss = -18578.484375
Iteration 200: Loss = -12699.2392578125
Iteration 300: Loss = -11367.587890625
Iteration 400: Loss = -11154.7607421875
Iteration 500: Loss = -11064.1298828125
Iteration 600: Loss = -11015.8515625
Iteration 700: Loss = -10988.7744140625
Iteration 800: Loss = -10971.2890625
Iteration 900: Loss = -10959.0263671875
Iteration 1000: Loss = -10949.91796875
Iteration 1100: Loss = -10943.0029296875
Iteration 1200: Loss = -10937.6005859375
Iteration 1300: Loss = -10933.384765625
Iteration 1400: Loss = -10929.955078125
Iteration 1500: Loss = -10927.0986328125
Iteration 1600: Loss = -10924.66015625
Iteration 1700: Loss = -10922.6357421875
Iteration 1800: Loss = -10920.927734375
Iteration 1900: Loss = -10919.4619140625
Iteration 2000: Loss = -10918.1904296875
Iteration 2100: Loss = -10917.080078125
Iteration 2200: Loss = -10916.1025390625
Iteration 2300: Loss = -10915.2392578125
Iteration 2400: Loss = -10914.4716796875
Iteration 2500: Loss = -10913.7880859375
Iteration 2600: Loss = -10913.173828125
Iteration 2700: Loss = -10912.6201171875
Iteration 2800: Loss = -10912.1220703125
Iteration 2900: Loss = -10911.6708984375
Iteration 3000: Loss = -10911.2607421875
Iteration 3100: Loss = -10910.88671875
Iteration 3200: Loss = -10910.546875
Iteration 3300: Loss = -10910.2333984375
Iteration 3400: Loss = -10909.9482421875
Iteration 3500: Loss = -10909.685546875
Iteration 3600: Loss = -10909.4423828125
Iteration 3700: Loss = -10909.2197265625
Iteration 3800: Loss = -10909.0126953125
Iteration 3900: Loss = -10908.8203125
Iteration 4000: Loss = -10908.642578125
Iteration 4100: Loss = -10908.48046875
Iteration 4200: Loss = -10908.328125
Iteration 4300: Loss = -10908.1865234375
Iteration 4400: Loss = -10908.052734375
Iteration 4500: Loss = -10907.9287109375
Iteration 4600: Loss = -10907.814453125
Iteration 4700: Loss = -10907.705078125
Iteration 4800: Loss = -10907.60546875
Iteration 4900: Loss = -10907.51171875
Iteration 5000: Loss = -10907.4228515625
Iteration 5100: Loss = -10907.3408203125
Iteration 5200: Loss = -10907.2626953125
Iteration 5300: Loss = -10907.19140625
Iteration 5400: Loss = -10907.12109375
Iteration 5500: Loss = -10907.056640625
Iteration 5600: Loss = -10906.99609375
Iteration 5700: Loss = -10906.939453125
Iteration 5800: Loss = -10906.88671875
Iteration 5900: Loss = -10906.8349609375
Iteration 6000: Loss = -10906.787109375
Iteration 6100: Loss = -10906.7431640625
Iteration 6200: Loss = -10906.7001953125
Iteration 6300: Loss = -10906.66015625
Iteration 6400: Loss = -10906.623046875
Iteration 6500: Loss = -10906.5869140625
Iteration 6600: Loss = -10906.552734375
Iteration 6700: Loss = -10906.521484375
Iteration 6800: Loss = -10906.4912109375
Iteration 6900: Loss = -10906.4619140625
Iteration 7000: Loss = -10906.435546875
Iteration 7100: Loss = -10906.4091796875
Iteration 7200: Loss = -10906.3857421875
Iteration 7300: Loss = -10906.361328125
Iteration 7400: Loss = -10906.3388671875
Iteration 7500: Loss = -10906.318359375
Iteration 7600: Loss = -10906.298828125
Iteration 7700: Loss = -10906.279296875
Iteration 7800: Loss = -10906.2587890625
Iteration 7900: Loss = -10906.2333984375
Iteration 8000: Loss = -10906.1904296875
Iteration 8100: Loss = -10906.0419921875
Iteration 8200: Loss = -10905.978515625
Iteration 8300: Loss = -10905.9619140625
Iteration 8400: Loss = -10905.9501953125
Iteration 8500: Loss = -10905.9365234375
Iteration 8600: Loss = -10905.92578125
Iteration 8700: Loss = -10905.9140625
Iteration 8800: Loss = -10905.904296875
Iteration 8900: Loss = -10905.8955078125
Iteration 9000: Loss = -10905.884765625
Iteration 9100: Loss = -10905.8779296875
Iteration 9200: Loss = -10905.8701171875
Iteration 9300: Loss = -10905.861328125
Iteration 9400: Loss = -10905.8544921875
Iteration 9500: Loss = -10905.84765625
Iteration 9600: Loss = -10905.8408203125
Iteration 9700: Loss = -10905.8349609375
Iteration 9800: Loss = -10905.828125
Iteration 9900: Loss = -10905.8232421875
Iteration 10000: Loss = -10905.8173828125
Iteration 10100: Loss = -10905.8115234375
Iteration 10200: Loss = -10905.806640625
Iteration 10300: Loss = -10905.80078125
Iteration 10400: Loss = -10905.7958984375
Iteration 10500: Loss = -10905.7939453125
Iteration 10600: Loss = -10905.7890625
Iteration 10700: Loss = -10905.7861328125
Iteration 10800: Loss = -10905.78125
Iteration 10900: Loss = -10905.7802734375
Iteration 11000: Loss = -10905.775390625
Iteration 11100: Loss = -10905.7724609375
Iteration 11200: Loss = -10905.76953125
Iteration 11300: Loss = -10905.767578125
Iteration 11400: Loss = -10905.763671875
Iteration 11500: Loss = -10905.76171875
Iteration 11600: Loss = -10905.759765625
Iteration 11700: Loss = -10905.7568359375
Iteration 11800: Loss = -10905.755859375
Iteration 11900: Loss = -10905.7529296875
Iteration 12000: Loss = -10905.7509765625
Iteration 12100: Loss = -10905.75
Iteration 12200: Loss = -10905.7490234375
Iteration 12300: Loss = -10905.748046875
Iteration 12400: Loss = -10905.7451171875
Iteration 12500: Loss = -10905.7451171875
Iteration 12600: Loss = -10905.7431640625
Iteration 12700: Loss = -10905.7412109375
Iteration 12800: Loss = -10905.740234375
Iteration 12900: Loss = -10905.7392578125
Iteration 13000: Loss = -10905.7392578125
Iteration 13100: Loss = -10905.7373046875
Iteration 13200: Loss = -10905.7373046875
Iteration 13300: Loss = -10905.736328125
Iteration 13400: Loss = -10905.7353515625
Iteration 13500: Loss = -10905.734375
Iteration 13600: Loss = -10905.734375
Iteration 13700: Loss = -10905.732421875
Iteration 13800: Loss = -10905.732421875
Iteration 13900: Loss = -10905.7314453125
Iteration 14000: Loss = -10905.73046875
Iteration 14100: Loss = -10905.7294921875
Iteration 14200: Loss = -10905.73046875
1
Iteration 14300: Loss = -10905.728515625
Iteration 14400: Loss = -10905.728515625
Iteration 14500: Loss = -10905.7275390625
Iteration 14600: Loss = -10905.728515625
1
Iteration 14700: Loss = -10905.7275390625
Iteration 14800: Loss = -10905.7275390625
Iteration 14900: Loss = -10905.7265625
Iteration 15000: Loss = -10905.7255859375
Iteration 15100: Loss = -10905.7255859375
Iteration 15200: Loss = -10905.724609375
Iteration 15300: Loss = -10905.7255859375
1
Iteration 15400: Loss = -10905.724609375
Iteration 15500: Loss = -10905.724609375
Iteration 15600: Loss = -10905.7236328125
Iteration 15700: Loss = -10905.72265625
Iteration 15800: Loss = -10905.7236328125
1
Iteration 15900: Loss = -10905.7236328125
2
Iteration 16000: Loss = -10905.7236328125
3
Iteration 16100: Loss = -10905.7216796875
Iteration 16200: Loss = -10905.7236328125
1
Iteration 16300: Loss = -10905.7216796875
Iteration 16400: Loss = -10905.7216796875
Iteration 16500: Loss = -10905.72265625
1
Iteration 16600: Loss = -10905.7216796875
Iteration 16700: Loss = -10905.720703125
Iteration 16800: Loss = -10905.720703125
Iteration 16900: Loss = -10905.720703125
Iteration 17000: Loss = -10905.7216796875
1
Iteration 17100: Loss = -10905.7216796875
2
Iteration 17200: Loss = -10905.7216796875
3
Iteration 17300: Loss = -10905.7216796875
4
Iteration 17400: Loss = -10905.720703125
Iteration 17500: Loss = -10905.7197265625
Iteration 17600: Loss = -10905.720703125
1
Iteration 17700: Loss = -10905.7216796875
2
Iteration 17800: Loss = -10905.7197265625
Iteration 17900: Loss = -10905.720703125
1
Iteration 18000: Loss = -10905.7216796875
2
Iteration 18100: Loss = -10905.7216796875
3
Iteration 18200: Loss = -10905.7177734375
Iteration 18300: Loss = -10905.71875
1
Iteration 18400: Loss = -10905.71875
2
Iteration 18500: Loss = -10905.720703125
3
Iteration 18600: Loss = -10905.7197265625
4
Iteration 18700: Loss = -10905.7197265625
5
Iteration 18800: Loss = -10905.71875
6
Iteration 18900: Loss = -10905.71875
7
Iteration 19000: Loss = -10905.720703125
8
Iteration 19100: Loss = -10905.71875
9
Iteration 19200: Loss = -10905.7197265625
10
Iteration 19300: Loss = -10905.7177734375
Iteration 19400: Loss = -10905.71875
1
Iteration 19500: Loss = -10905.71875
2
Iteration 19600: Loss = -10905.7197265625
3
Iteration 19700: Loss = -10905.7197265625
4
Iteration 19800: Loss = -10905.7177734375
Iteration 19900: Loss = -10905.71875
1
Iteration 20000: Loss = -10905.7177734375
Iteration 20100: Loss = -10905.7177734375
Iteration 20200: Loss = -10905.7177734375
Iteration 20300: Loss = -10905.71875
1
Iteration 20400: Loss = -10905.7177734375
Iteration 20500: Loss = -10905.71875
1
Iteration 20600: Loss = -10905.7177734375
Iteration 20700: Loss = -10905.716796875
Iteration 20800: Loss = -10905.7197265625
1
Iteration 20900: Loss = -10905.716796875
Iteration 21000: Loss = -10905.7197265625
1
Iteration 21100: Loss = -10905.7177734375
2
Iteration 21200: Loss = -10905.71875
3
Iteration 21300: Loss = -10905.71875
4
Iteration 21400: Loss = -10905.71875
5
Iteration 21500: Loss = -10905.71875
6
Iteration 21600: Loss = -10905.7197265625
7
Iteration 21700: Loss = -10905.7197265625
8
Iteration 21800: Loss = -10905.7197265625
9
Iteration 21900: Loss = -10905.71875
10
Iteration 22000: Loss = -10905.71875
11
Iteration 22100: Loss = -10905.71875
12
Iteration 22200: Loss = -10905.7177734375
13
Iteration 22300: Loss = -10905.7197265625
14
Iteration 22400: Loss = -10905.71875
15
Stopping early at iteration 22400 due to no improvement.
pi: tensor([[7.2681e-06, 9.9999e-01],
        [4.2754e-02, 9.5725e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.7227e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1658, 0.4655],
         [0.0889, 0.1572]],

        [[0.9724, 0.1721],
         [0.9766, 0.8840]],

        [[0.2435, 0.1397],
         [0.1745, 0.0075]],

        [[0.5762, 0.2383],
         [0.0210, 0.4657]],

        [[0.0778, 0.2080],
         [0.3749, 0.9394]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014273976462850912
Average Adjusted Rand Index: -0.0015521208230856337
[-0.00039967221681423806, -0.0014273976462850912] [-0.0011554446491642023, -0.0015521208230856337] [10904.919921875, 10905.71875]
-------------------------------------
This iteration is 80
True Objective function: Loss = -10765.597244525363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -60924.6484375
Iteration 100: Loss = -39969.98828125
Iteration 200: Loss = -22169.015625
Iteration 300: Loss = -14737.5986328125
Iteration 400: Loss = -12281.671875
Iteration 500: Loss = -11402.1298828125
Iteration 600: Loss = -11114.818359375
Iteration 700: Loss = -11010.7666015625
Iteration 800: Loss = -10969.6552734375
Iteration 900: Loss = -10947.9541015625
Iteration 1000: Loss = -10931.53515625
Iteration 1100: Loss = -10914.06640625
Iteration 1200: Loss = -10905.0888671875
Iteration 1300: Loss = -10898.173828125
Iteration 1400: Loss = -10892.7197265625
Iteration 1500: Loss = -10888.326171875
Iteration 1600: Loss = -10884.7158203125
Iteration 1700: Loss = -10881.701171875
Iteration 1800: Loss = -10879.1474609375
Iteration 1900: Loss = -10876.9599609375
Iteration 2000: Loss = -10875.0693359375
Iteration 2100: Loss = -10873.421875
Iteration 2200: Loss = -10871.9775390625
Iteration 2300: Loss = -10870.7021484375
Iteration 2400: Loss = -10869.5712890625
Iteration 2500: Loss = -10868.5634765625
Iteration 2600: Loss = -10867.65625
Iteration 2700: Loss = -10866.845703125
Iteration 2800: Loss = -10866.1103515625
Iteration 2900: Loss = -10865.4443359375
Iteration 3000: Loss = -10864.8408203125
Iteration 3100: Loss = -10864.2880859375
Iteration 3200: Loss = -10863.7861328125
Iteration 3300: Loss = -10863.326171875
Iteration 3400: Loss = -10862.90234375
Iteration 3500: Loss = -10862.5126953125
Iteration 3600: Loss = -10862.154296875
Iteration 3700: Loss = -10861.8251953125
Iteration 3800: Loss = -10861.5185546875
Iteration 3900: Loss = -10861.2353515625
Iteration 4000: Loss = -10860.9736328125
Iteration 4100: Loss = -10860.7275390625
Iteration 4200: Loss = -10860.5
Iteration 4300: Loss = -10860.2890625
Iteration 4400: Loss = -10860.091796875
Iteration 4500: Loss = -10859.9072265625
Iteration 4600: Loss = -10859.736328125
Iteration 4700: Loss = -10859.572265625
Iteration 4800: Loss = -10859.423828125
Iteration 4900: Loss = -10859.283203125
Iteration 5000: Loss = -10859.1513671875
Iteration 5100: Loss = -10859.02734375
Iteration 5200: Loss = -10858.91015625
Iteration 5300: Loss = -10858.80078125
Iteration 5400: Loss = -10858.6982421875
Iteration 5500: Loss = -10858.6015625
Iteration 5600: Loss = -10858.5087890625
Iteration 5700: Loss = -10858.423828125
Iteration 5800: Loss = -10858.34375
Iteration 5900: Loss = -10858.267578125
Iteration 6000: Loss = -10858.1953125
Iteration 6100: Loss = -10858.126953125
Iteration 6200: Loss = -10858.0634765625
Iteration 6300: Loss = -10858.0029296875
Iteration 6400: Loss = -10857.9462890625
Iteration 6500: Loss = -10857.892578125
Iteration 6600: Loss = -10857.8408203125
Iteration 6700: Loss = -10857.7919921875
Iteration 6800: Loss = -10857.7470703125
Iteration 6900: Loss = -10857.7041015625
Iteration 7000: Loss = -10857.662109375
Iteration 7100: Loss = -10857.6240234375
Iteration 7200: Loss = -10857.5888671875
Iteration 7300: Loss = -10857.552734375
Iteration 7400: Loss = -10857.5205078125
Iteration 7500: Loss = -10857.490234375
Iteration 7600: Loss = -10857.458984375
Iteration 7700: Loss = -10857.4326171875
Iteration 7800: Loss = -10857.40625
Iteration 7900: Loss = -10857.3818359375
Iteration 8000: Loss = -10857.3564453125
Iteration 8100: Loss = -10857.3349609375
Iteration 8200: Loss = -10857.3134765625
Iteration 8300: Loss = -10857.2939453125
Iteration 8400: Loss = -10857.2724609375
Iteration 8500: Loss = -10857.255859375
Iteration 8600: Loss = -10857.2392578125
Iteration 8700: Loss = -10857.2216796875
Iteration 8800: Loss = -10857.2060546875
Iteration 8900: Loss = -10857.19140625
Iteration 9000: Loss = -10857.1787109375
Iteration 9100: Loss = -10857.166015625
Iteration 9200: Loss = -10857.15234375
Iteration 9300: Loss = -10857.140625
Iteration 9400: Loss = -10857.1298828125
Iteration 9500: Loss = -10857.1171875
Iteration 9600: Loss = -10857.109375
Iteration 9700: Loss = -10857.0966796875
Iteration 9800: Loss = -10857.08984375
Iteration 9900: Loss = -10857.078125
Iteration 10000: Loss = -10857.0712890625
Iteration 10100: Loss = -10857.0634765625
Iteration 10200: Loss = -10857.056640625
Iteration 10300: Loss = -10857.0478515625
Iteration 10400: Loss = -10857.041015625
Iteration 10500: Loss = -10857.033203125
Iteration 10600: Loss = -10857.029296875
Iteration 10700: Loss = -10857.0224609375
Iteration 10800: Loss = -10857.017578125
Iteration 10900: Loss = -10857.01171875
Iteration 11000: Loss = -10857.0068359375
Iteration 11100: Loss = -10857.001953125
Iteration 11200: Loss = -10856.9970703125
Iteration 11300: Loss = -10856.9931640625
Iteration 11400: Loss = -10856.9892578125
Iteration 11500: Loss = -10856.984375
Iteration 11600: Loss = -10856.98046875
Iteration 11700: Loss = -10856.9755859375
Iteration 11800: Loss = -10856.9736328125
Iteration 11900: Loss = -10856.970703125
Iteration 12000: Loss = -10856.96875
Iteration 12100: Loss = -10856.96484375
Iteration 12200: Loss = -10856.9638671875
Iteration 12300: Loss = -10856.9619140625
Iteration 12400: Loss = -10856.9599609375
Iteration 12500: Loss = -10856.95703125
Iteration 12600: Loss = -10856.9560546875
Iteration 12700: Loss = -10856.9541015625
Iteration 12800: Loss = -10856.951171875
Iteration 12900: Loss = -10856.9501953125
Iteration 13000: Loss = -10856.9482421875
Iteration 13100: Loss = -10856.947265625
Iteration 13200: Loss = -10856.9453125
Iteration 13300: Loss = -10856.9453125
Iteration 13400: Loss = -10856.943359375
Iteration 13500: Loss = -10856.943359375
Iteration 13600: Loss = -10856.94140625
Iteration 13700: Loss = -10856.94140625
Iteration 13800: Loss = -10856.9384765625
Iteration 13900: Loss = -10856.9384765625
Iteration 14000: Loss = -10856.9375
Iteration 14100: Loss = -10856.9375
Iteration 14200: Loss = -10856.935546875
Iteration 14300: Loss = -10856.9345703125
Iteration 14400: Loss = -10856.935546875
1
Iteration 14500: Loss = -10856.9345703125
Iteration 14600: Loss = -10856.93359375
Iteration 14700: Loss = -10856.93359375
Iteration 14800: Loss = -10856.9326171875
Iteration 14900: Loss = -10856.9306640625
Iteration 15000: Loss = -10856.9306640625
Iteration 15100: Loss = -10856.931640625
1
Iteration 15200: Loss = -10856.9296875
Iteration 15300: Loss = -10856.9296875
Iteration 15400: Loss = -10856.9296875
Iteration 15500: Loss = -10856.9287109375
Iteration 15600: Loss = -10856.9287109375
Iteration 15700: Loss = -10856.9287109375
Iteration 15800: Loss = -10856.9267578125
Iteration 15900: Loss = -10856.92578125
Iteration 16000: Loss = -10856.9267578125
1
Iteration 16100: Loss = -10856.92578125
Iteration 16200: Loss = -10856.92578125
Iteration 16300: Loss = -10856.92578125
Iteration 16400: Loss = -10856.92578125
Iteration 16500: Loss = -10856.92578125
Iteration 16600: Loss = -10856.9248046875
Iteration 16700: Loss = -10856.9267578125
1
Iteration 16800: Loss = -10856.9248046875
Iteration 16900: Loss = -10856.9248046875
Iteration 17000: Loss = -10856.9248046875
Iteration 17100: Loss = -10856.9228515625
Iteration 17200: Loss = -10856.9228515625
Iteration 17300: Loss = -10856.921875
Iteration 17400: Loss = -10856.9169921875
Iteration 17500: Loss = -10856.9150390625
Iteration 17600: Loss = -10856.9130859375
Iteration 17700: Loss = -10856.9130859375
Iteration 17800: Loss = -10856.9111328125
Iteration 17900: Loss = -10856.912109375
1
Iteration 18000: Loss = -10856.91015625
Iteration 18100: Loss = -10856.9091796875
Iteration 18200: Loss = -10856.91015625
1
Iteration 18300: Loss = -10856.9091796875
Iteration 18400: Loss = -10856.908203125
Iteration 18500: Loss = -10856.9091796875
1
Iteration 18600: Loss = -10856.90625
Iteration 18700: Loss = -10856.9072265625
1
Iteration 18800: Loss = -10856.9052734375
Iteration 18900: Loss = -10856.904296875
Iteration 19000: Loss = -10856.9052734375
1
Iteration 19100: Loss = -10856.9033203125
Iteration 19200: Loss = -10856.9013671875
Iteration 19300: Loss = -10856.900390625
Iteration 19400: Loss = -10856.8984375
Iteration 19500: Loss = -10856.8935546875
Iteration 19600: Loss = -10856.888671875
Iteration 19700: Loss = -10856.8818359375
Iteration 19800: Loss = -10856.859375
Iteration 19900: Loss = -10856.7802734375
Iteration 20000: Loss = -10856.732421875
Iteration 20100: Loss = -10856.7109375
Iteration 20200: Loss = -10856.708984375
Iteration 20300: Loss = -10856.5966796875
Iteration 20400: Loss = -10856.4521484375
Iteration 20500: Loss = -10856.279296875
Iteration 20600: Loss = -10855.38671875
Iteration 20700: Loss = -10854.5732421875
Iteration 20800: Loss = -10854.4716796875
Iteration 20900: Loss = -10854.4208984375
Iteration 21000: Loss = -10854.4140625
Iteration 21100: Loss = -10854.41015625
Iteration 21200: Loss = -10854.40625
Iteration 21300: Loss = -10854.2490234375
Iteration 21400: Loss = -10854.2080078125
Iteration 21500: Loss = -10854.171875
Iteration 21600: Loss = -10854.115234375
Iteration 21700: Loss = -10854.0419921875
Iteration 21800: Loss = -10854.0234375
Iteration 21900: Loss = -10854.009765625
Iteration 22000: Loss = -10853.9609375
Iteration 22100: Loss = -10853.287109375
Iteration 22200: Loss = -10852.5380859375
Iteration 22300: Loss = -10852.4443359375
Iteration 22400: Loss = -10852.419921875
Iteration 22500: Loss = -10852.412109375
Iteration 22600: Loss = -10852.4111328125
Iteration 22700: Loss = -10852.4091796875
Iteration 22800: Loss = -10852.408203125
Iteration 22900: Loss = -10852.408203125
Iteration 23000: Loss = -10852.4091796875
1
Iteration 23100: Loss = -10852.4072265625
Iteration 23200: Loss = -10852.404296875
Iteration 23300: Loss = -10852.4033203125
Iteration 23400: Loss = -10852.4033203125
Iteration 23500: Loss = -10852.404296875
1
Iteration 23600: Loss = -10852.4052734375
2
Iteration 23700: Loss = -10852.4033203125
Iteration 23800: Loss = -10852.40234375
Iteration 23900: Loss = -10852.4033203125
1
Iteration 24000: Loss = -10852.4033203125
2
Iteration 24100: Loss = -10852.4033203125
3
Iteration 24200: Loss = -10852.404296875
4
Iteration 24300: Loss = -10852.4033203125
5
Iteration 24400: Loss = -10852.40234375
Iteration 24500: Loss = -10852.40234375
Iteration 24600: Loss = -10852.40234375
Iteration 24700: Loss = -10852.4033203125
1
Iteration 24800: Loss = -10852.404296875
2
Iteration 24900: Loss = -10852.4033203125
3
Iteration 25000: Loss = -10852.40234375
Iteration 25100: Loss = -10852.40234375
Iteration 25200: Loss = -10852.4033203125
1
Iteration 25300: Loss = -10852.4033203125
2
Iteration 25400: Loss = -10852.4033203125
3
Iteration 25500: Loss = -10852.40234375
Iteration 25600: Loss = -10852.40234375
Iteration 25700: Loss = -10852.404296875
1
Iteration 25800: Loss = -10852.4033203125
2
Iteration 25900: Loss = -10852.40234375
Iteration 26000: Loss = -10852.40234375
Iteration 26100: Loss = -10852.40234375
Iteration 26200: Loss = -10852.400390625
Iteration 26300: Loss = -10852.400390625
Iteration 26400: Loss = -10852.3994140625
Iteration 26500: Loss = -10852.3984375
Iteration 26600: Loss = -10852.400390625
1
Iteration 26700: Loss = -10852.400390625
2
Iteration 26800: Loss = -10852.400390625
3
Iteration 26900: Loss = -10852.4013671875
4
Iteration 27000: Loss = -10852.400390625
5
Iteration 27100: Loss = -10852.400390625
6
Iteration 27200: Loss = -10852.400390625
7
Iteration 27300: Loss = -10852.400390625
8
Iteration 27400: Loss = -10852.400390625
9
Iteration 27500: Loss = -10852.3994140625
10
Iteration 27600: Loss = -10852.3994140625
11
Iteration 27700: Loss = -10852.3984375
Iteration 27800: Loss = -10852.3994140625
1
Iteration 27900: Loss = -10852.3984375
Iteration 28000: Loss = -10852.3994140625
1
Iteration 28100: Loss = -10852.3974609375
Iteration 28200: Loss = -10852.3994140625
1
Iteration 28300: Loss = -10852.3984375
2
Iteration 28400: Loss = -10852.3984375
3
Iteration 28500: Loss = -10852.3974609375
Iteration 28600: Loss = -10852.3974609375
Iteration 28700: Loss = -10852.3984375
1
Iteration 28800: Loss = -10852.3984375
2
Iteration 28900: Loss = -10852.3974609375
Iteration 29000: Loss = -10852.3974609375
Iteration 29100: Loss = -10852.3984375
1
Iteration 29200: Loss = -10852.3974609375
Iteration 29300: Loss = -10852.3974609375
Iteration 29400: Loss = -10852.3974609375
Iteration 29500: Loss = -10852.3974609375
Iteration 29600: Loss = -10852.3974609375
Iteration 29700: Loss = -10852.3974609375
Iteration 29800: Loss = -10852.400390625
1
Iteration 29900: Loss = -10852.3974609375
pi: tensor([[9.9999e-01, 1.0495e-05],
        [3.7748e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0377, 0.9623], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2433, 0.1482],
         [0.9925, 0.1584]],

        [[0.8679, 0.1146],
         [0.9204, 0.0344]],

        [[0.9824, 0.2373],
         [0.7559, 0.0374]],

        [[0.9797, 0.2185],
         [0.0161, 0.0633]],

        [[0.9907, 0.1314],
         [0.9700, 0.9907]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01204793216819519
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.002911726262720568
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.01717781179455718
Global Adjusted Rand Index: 0.002438488576449912
Average Adjusted Rand Index: -0.0023084006432731842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29236.666015625
Iteration 100: Loss = -16983.904296875
Iteration 200: Loss = -12328.005859375
Iteration 300: Loss = -11414.150390625
Iteration 400: Loss = -11156.07421875
Iteration 500: Loss = -11049.1767578125
Iteration 600: Loss = -10989.3046875
Iteration 700: Loss = -10959.46875
Iteration 800: Loss = -10932.15625
Iteration 900: Loss = -10913.1591796875
Iteration 1000: Loss = -10904.34375
Iteration 1100: Loss = -10897.3525390625
Iteration 1200: Loss = -10891.1884765625
Iteration 1300: Loss = -10885.8515625
Iteration 1400: Loss = -10882.458984375
Iteration 1500: Loss = -10879.0126953125
Iteration 1600: Loss = -10875.19921875
Iteration 1700: Loss = -10873.2265625
Iteration 1800: Loss = -10871.8037109375
Iteration 1900: Loss = -10870.689453125
Iteration 2000: Loss = -10869.759765625
Iteration 2100: Loss = -10868.966796875
Iteration 2200: Loss = -10868.2333984375
Iteration 2300: Loss = -10864.099609375
Iteration 2400: Loss = -10863.3203125
Iteration 2500: Loss = -10862.72265625
Iteration 2600: Loss = -10862.2265625
Iteration 2700: Loss = -10861.80078125
Iteration 2800: Loss = -10861.4287109375
Iteration 2900: Loss = -10861.09765625
Iteration 3000: Loss = -10860.80078125
Iteration 3100: Loss = -10860.533203125
Iteration 3200: Loss = -10860.2900390625
Iteration 3300: Loss = -10860.0693359375
Iteration 3400: Loss = -10859.865234375
Iteration 3500: Loss = -10859.6796875
Iteration 3600: Loss = -10859.5078125
Iteration 3700: Loss = -10859.349609375
Iteration 3800: Loss = -10859.201171875
Iteration 3900: Loss = -10859.0634765625
Iteration 4000: Loss = -10858.9365234375
Iteration 4100: Loss = -10858.8173828125
Iteration 4200: Loss = -10858.70703125
Iteration 4300: Loss = -10858.6025390625
Iteration 4400: Loss = -10858.5068359375
Iteration 4500: Loss = -10858.4169921875
Iteration 4600: Loss = -10858.3330078125
Iteration 4700: Loss = -10858.2548828125
Iteration 4800: Loss = -10858.1806640625
Iteration 4900: Loss = -10858.1103515625
Iteration 5000: Loss = -10858.0458984375
Iteration 5100: Loss = -10857.982421875
Iteration 5200: Loss = -10857.9248046875
Iteration 5300: Loss = -10857.869140625
Iteration 5400: Loss = -10857.81640625
Iteration 5500: Loss = -10857.767578125
Iteration 5600: Loss = -10857.7216796875
Iteration 5700: Loss = -10857.6748046875
Iteration 5800: Loss = -10857.6328125
Iteration 5900: Loss = -10857.5927734375
Iteration 6000: Loss = -10857.5517578125
Iteration 6100: Loss = -10857.51171875
Iteration 6200: Loss = -10857.47265625
Iteration 6300: Loss = -10857.427734375
Iteration 6400: Loss = -10857.376953125
Iteration 6500: Loss = -10857.3095703125
Iteration 6600: Loss = -10857.224609375
Iteration 6700: Loss = -10857.1396484375
Iteration 6800: Loss = -10857.0712890625
Iteration 6900: Loss = -10857.0224609375
Iteration 7000: Loss = -10856.9873046875
Iteration 7100: Loss = -10856.9560546875
Iteration 7200: Loss = -10856.9296875
Iteration 7300: Loss = -10856.90625
Iteration 7400: Loss = -10856.8857421875
Iteration 7500: Loss = -10856.865234375
Iteration 7600: Loss = -10856.84765625
Iteration 7700: Loss = -10856.830078125
Iteration 7800: Loss = -10856.814453125
Iteration 7900: Loss = -10856.7998046875
Iteration 8000: Loss = -10856.7861328125
Iteration 8100: Loss = -10856.7734375
Iteration 8200: Loss = -10856.7587890625
Iteration 8300: Loss = -10856.7490234375
Iteration 8400: Loss = -10856.7373046875
Iteration 8500: Loss = -10856.7275390625
Iteration 8600: Loss = -10856.71875
Iteration 8700: Loss = -10856.7080078125
Iteration 8800: Loss = -10856.7001953125
Iteration 8900: Loss = -10856.6923828125
Iteration 9000: Loss = -10856.6826171875
Iteration 9100: Loss = -10856.6748046875
Iteration 9200: Loss = -10856.66796875
Iteration 9300: Loss = -10856.6591796875
Iteration 9400: Loss = -10856.6533203125
Iteration 9500: Loss = -10856.6474609375
Iteration 9600: Loss = -10856.640625
Iteration 9700: Loss = -10856.6337890625
Iteration 9800: Loss = -10856.625
Iteration 9900: Loss = -10856.6181640625
Iteration 10000: Loss = -10856.611328125
Iteration 10100: Loss = -10856.6015625
Iteration 10200: Loss = -10856.5908203125
Iteration 10300: Loss = -10856.580078125
Iteration 10400: Loss = -10856.56640625
Iteration 10500: Loss = -10856.5537109375
Iteration 10600: Loss = -10856.5458984375
Iteration 10700: Loss = -10856.5390625
Iteration 10800: Loss = -10856.5361328125
Iteration 10900: Loss = -10856.5322265625
Iteration 11000: Loss = -10856.5302734375
Iteration 11100: Loss = -10856.52734375
Iteration 11200: Loss = -10856.5224609375
Iteration 11300: Loss = -10856.5146484375
Iteration 11400: Loss = -10856.5009765625
Iteration 11500: Loss = -10856.4814453125
Iteration 11600: Loss = -10856.4541015625
Iteration 11700: Loss = -10856.4130859375
Iteration 11800: Loss = -10856.3408203125
Iteration 11900: Loss = -10856.2197265625
Iteration 12000: Loss = -10856.091796875
Iteration 12100: Loss = -10855.951171875
Iteration 12200: Loss = -10855.6650390625
Iteration 12300: Loss = -10855.34375
Iteration 12400: Loss = -10855.0361328125
Iteration 12500: Loss = -10854.5400390625
Iteration 12600: Loss = -10854.3583984375
Iteration 12700: Loss = -10854.271484375
Iteration 12800: Loss = -10854.115234375
Iteration 12900: Loss = -10853.693359375
Iteration 13000: Loss = -10853.619140625
Iteration 13100: Loss = -10853.576171875
Iteration 13200: Loss = -10853.5546875
Iteration 13300: Loss = -10853.5361328125
Iteration 13400: Loss = -10853.4111328125
Iteration 13500: Loss = -10853.0205078125
Iteration 13600: Loss = -10852.3798828125
Iteration 13700: Loss = -10852.25
Iteration 13800: Loss = -10852.1982421875
Iteration 13900: Loss = -10852.1689453125
Iteration 14000: Loss = -10852.1572265625
Iteration 14100: Loss = -10852.1494140625
Iteration 14200: Loss = -10852.1396484375
Iteration 14300: Loss = -10852.134765625
Iteration 14400: Loss = -10852.1298828125
Iteration 14500: Loss = -10852.1259765625
Iteration 14600: Loss = -10852.1220703125
Iteration 14700: Loss = -10852.1181640625
Iteration 14800: Loss = -10852.1162109375
Iteration 14900: Loss = -10852.115234375
Iteration 15000: Loss = -10852.1103515625
Iteration 15100: Loss = -10852.1103515625
Iteration 15200: Loss = -10852.107421875
Iteration 15300: Loss = -10852.107421875
Iteration 15400: Loss = -10852.1044921875
Iteration 15500: Loss = -10852.1015625
Iteration 15600: Loss = -10852.1015625
Iteration 15700: Loss = -10852.1015625
Iteration 15800: Loss = -10852.099609375
Iteration 15900: Loss = -10852.0986328125
Iteration 16000: Loss = -10852.0966796875
Iteration 16100: Loss = -10852.09375
Iteration 16200: Loss = -10852.0947265625
1
Iteration 16300: Loss = -10852.09375
Iteration 16400: Loss = -10852.091796875
Iteration 16500: Loss = -10852.08984375
Iteration 16600: Loss = -10852.08984375
Iteration 16700: Loss = -10852.091796875
1
Iteration 16800: Loss = -10852.0908203125
2
Iteration 16900: Loss = -10852.0908203125
3
Iteration 17000: Loss = -10852.0888671875
Iteration 17100: Loss = -10852.08984375
1
Iteration 17200: Loss = -10852.0888671875
Iteration 17300: Loss = -10852.087890625
Iteration 17400: Loss = -10852.087890625
Iteration 17500: Loss = -10852.0869140625
Iteration 17600: Loss = -10852.08984375
1
Iteration 17700: Loss = -10852.0859375
Iteration 17800: Loss = -10852.0859375
Iteration 17900: Loss = -10852.0859375
Iteration 18000: Loss = -10852.0849609375
Iteration 18100: Loss = -10852.08203125
Iteration 18200: Loss = -10852.0830078125
1
Iteration 18300: Loss = -10852.08203125
Iteration 18400: Loss = -10852.080078125
Iteration 18500: Loss = -10852.0830078125
1
Iteration 18600: Loss = -10852.080078125
Iteration 18700: Loss = -10852.080078125
Iteration 18800: Loss = -10852.0810546875
1
Iteration 18900: Loss = -10852.0810546875
2
Iteration 19000: Loss = -10852.078125
Iteration 19100: Loss = -10852.0791015625
1
Iteration 19200: Loss = -10852.080078125
2
Iteration 19300: Loss = -10852.080078125
3
Iteration 19400: Loss = -10852.0791015625
4
Iteration 19500: Loss = -10852.078125
Iteration 19600: Loss = -10852.080078125
1
Iteration 19700: Loss = -10852.0771484375
Iteration 19800: Loss = -10852.078125
1
Iteration 19900: Loss = -10852.0751953125
Iteration 20000: Loss = -10852.076171875
1
Iteration 20100: Loss = -10852.0771484375
2
Iteration 20200: Loss = -10852.076171875
3
Iteration 20300: Loss = -10852.078125
4
Iteration 20400: Loss = -10852.078125
5
Iteration 20500: Loss = -10852.0771484375
6
Iteration 20600: Loss = -10852.0771484375
7
Iteration 20700: Loss = -10852.076171875
8
Iteration 20800: Loss = -10852.07421875
Iteration 20900: Loss = -10852.0751953125
1
Iteration 21000: Loss = -10852.0751953125
2
Iteration 21100: Loss = -10852.0751953125
3
Iteration 21200: Loss = -10852.076171875
4
Iteration 21300: Loss = -10852.0751953125
5
Iteration 21400: Loss = -10852.076171875
6
Iteration 21500: Loss = -10852.07421875
Iteration 21600: Loss = -10852.0751953125
1
Iteration 21700: Loss = -10852.072265625
Iteration 21800: Loss = -10852.0703125
Iteration 21900: Loss = -10852.0009765625
Iteration 22000: Loss = -10852.0
Iteration 22100: Loss = -10852.0009765625
1
Iteration 22200: Loss = -10852.001953125
2
Iteration 22300: Loss = -10852.0
Iteration 22400: Loss = -10852.0
Iteration 22500: Loss = -10851.9990234375
Iteration 22600: Loss = -10852.0
1
Iteration 22700: Loss = -10852.0
2
Iteration 22800: Loss = -10852.0
3
Iteration 22900: Loss = -10852.0009765625
4
Iteration 23000: Loss = -10852.0009765625
5
Iteration 23100: Loss = -10852.0
6
Iteration 23200: Loss = -10852.0
7
Iteration 23300: Loss = -10851.9990234375
Iteration 23400: Loss = -10851.986328125
Iteration 23500: Loss = -10851.986328125
Iteration 23600: Loss = -10851.9853515625
Iteration 23700: Loss = -10851.984375
Iteration 23800: Loss = -10851.986328125
1
Iteration 23900: Loss = -10851.986328125
2
Iteration 24000: Loss = -10851.9853515625
3
Iteration 24100: Loss = -10851.9794921875
Iteration 24200: Loss = -10851.9794921875
Iteration 24300: Loss = -10851.9794921875
Iteration 24400: Loss = -10851.978515625
Iteration 24500: Loss = -10851.978515625
Iteration 24600: Loss = -10851.978515625
Iteration 24700: Loss = -10851.9794921875
1
Iteration 24800: Loss = -10851.978515625
Iteration 24900: Loss = -10851.951171875
Iteration 25000: Loss = -10851.951171875
Iteration 25100: Loss = -10851.9521484375
1
Iteration 25200: Loss = -10851.94921875
Iteration 25300: Loss = -10851.94921875
Iteration 25400: Loss = -10851.947265625
Iteration 25500: Loss = -10851.947265625
Iteration 25600: Loss = -10851.9462890625
Iteration 25700: Loss = -10851.9462890625
Iteration 25800: Loss = -10851.9453125
Iteration 25900: Loss = -10851.94140625
Iteration 26000: Loss = -10851.9404296875
Iteration 26100: Loss = -10851.9423828125
1
Iteration 26200: Loss = -10851.9404296875
Iteration 26300: Loss = -10851.9404296875
Iteration 26400: Loss = -10851.9404296875
Iteration 26500: Loss = -10851.9404296875
Iteration 26600: Loss = -10851.9384765625
Iteration 26700: Loss = -10851.9384765625
Iteration 26800: Loss = -10851.9384765625
Iteration 26900: Loss = -10851.94140625
1
Iteration 27000: Loss = -10851.939453125
2
Iteration 27100: Loss = -10851.9375
Iteration 27200: Loss = -10851.9365234375
Iteration 27300: Loss = -10851.9375
1
Iteration 27400: Loss = -10851.9384765625
2
Iteration 27500: Loss = -10851.9375
3
Iteration 27600: Loss = -10851.9365234375
Iteration 27700: Loss = -10851.9375
1
Iteration 27800: Loss = -10851.9365234375
Iteration 27900: Loss = -10851.935546875
Iteration 28000: Loss = -10851.935546875
Iteration 28100: Loss = -10851.9345703125
Iteration 28200: Loss = -10851.9345703125
Iteration 28300: Loss = -10851.935546875
1
Iteration 28400: Loss = -10851.9345703125
Iteration 28500: Loss = -10851.935546875
1
Iteration 28600: Loss = -10851.9345703125
Iteration 28700: Loss = -10851.93359375
Iteration 28800: Loss = -10851.9345703125
1
Iteration 28900: Loss = -10851.93359375
Iteration 29000: Loss = -10851.9345703125
1
Iteration 29100: Loss = -10851.9345703125
2
Iteration 29200: Loss = -10851.9345703125
3
Iteration 29300: Loss = -10851.93359375
Iteration 29400: Loss = -10851.9345703125
1
Iteration 29500: Loss = -10851.9345703125
2
Iteration 29600: Loss = -10851.93359375
Iteration 29700: Loss = -10851.93359375
Iteration 29800: Loss = -10851.9345703125
1
Iteration 29900: Loss = -10851.93359375
pi: tensor([[9.9999e-01, 1.3737e-05],
        [3.5191e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0464, 0.9536], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2135, 0.1458],
         [0.9764, 0.1583]],

        [[0.3875, 0.1175],
         [0.0785, 0.6258]],

        [[0.5106, 0.2334],
         [0.9925, 0.7764]],

        [[0.9902, 0.2142],
         [0.0553, 0.0149]],

        [[0.9795, 0.1357],
         [0.9643, 0.8688]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01204793216819519
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.002911726262720568
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.01717781179455718
Global Adjusted Rand Index: 0.002438488576449912
Average Adjusted Rand Index: -0.0023084006432731842
[0.002438488576449912, 0.002438488576449912] [-0.0023084006432731842, -0.0023084006432731842] [10852.396484375, 10851.93359375]
-------------------------------------
This iteration is 81
True Objective function: Loss = -10895.394591573497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15463.263671875
Iteration 100: Loss = -12046.5986328125
Iteration 200: Loss = -11174.341796875
Iteration 300: Loss = -11050.4951171875
Iteration 400: Loss = -11022.279296875
Iteration 500: Loss = -11010.458984375
Iteration 600: Loss = -11004.9384765625
Iteration 700: Loss = -11001.853515625
Iteration 800: Loss = -10999.896484375
Iteration 900: Loss = -10998.6552734375
Iteration 1000: Loss = -10997.8193359375
Iteration 1100: Loss = -10997.205078125
Iteration 1200: Loss = -10996.7197265625
Iteration 1300: Loss = -10996.31640625
Iteration 1400: Loss = -10995.9443359375
Iteration 1500: Loss = -10995.5341796875
Iteration 1600: Loss = -10995.0029296875
Iteration 1700: Loss = -10994.2607421875
Iteration 1800: Loss = -10993.466796875
Iteration 1900: Loss = -10992.8720703125
Iteration 2000: Loss = -10992.4658203125
Iteration 2100: Loss = -10992.146484375
Iteration 2200: Loss = -10991.884765625
Iteration 2300: Loss = -10991.6669921875
Iteration 2400: Loss = -10991.4775390625
Iteration 2500: Loss = -10991.302734375
Iteration 2600: Loss = -10991.138671875
Iteration 2700: Loss = -10990.98828125
Iteration 2800: Loss = -10990.8544921875
Iteration 2900: Loss = -10990.7353515625
Iteration 3000: Loss = -10990.6259765625
Iteration 3100: Loss = -10990.52734375
Iteration 3200: Loss = -10990.4404296875
Iteration 3300: Loss = -10990.3642578125
Iteration 3400: Loss = -10990.30078125
Iteration 3500: Loss = -10990.2431640625
Iteration 3600: Loss = -10990.185546875
Iteration 3700: Loss = -10990.123046875
Iteration 3800: Loss = -10990.05859375
Iteration 3900: Loss = -10990.0205078125
Iteration 4000: Loss = -10989.9970703125
Iteration 4100: Loss = -10989.9716796875
Iteration 4200: Loss = -10989.9453125
Iteration 4300: Loss = -10989.9140625
Iteration 4400: Loss = -10989.8779296875
Iteration 4500: Loss = -10989.8486328125
Iteration 4600: Loss = -10989.8212890625
Iteration 4700: Loss = -10989.78515625
Iteration 4800: Loss = -10989.748046875
Iteration 4900: Loss = -10989.6962890625
Iteration 5000: Loss = -10989.484375
Iteration 5100: Loss = -10938.962890625
Iteration 5200: Loss = -10882.109375
Iteration 5300: Loss = -10858.6240234375
Iteration 5400: Loss = -10856.51953125
Iteration 5500: Loss = -10856.12109375
Iteration 5600: Loss = -10855.943359375
Iteration 5700: Loss = -10855.8466796875
Iteration 5800: Loss = -10855.783203125
Iteration 5900: Loss = -10855.7392578125
Iteration 6000: Loss = -10855.7060546875
Iteration 6100: Loss = -10855.6787109375
Iteration 6200: Loss = -10855.6591796875
Iteration 6300: Loss = -10855.6416015625
Iteration 6400: Loss = -10855.6259765625
Iteration 6500: Loss = -10855.61328125
Iteration 6600: Loss = -10855.6015625
Iteration 6700: Loss = -10855.5927734375
Iteration 6800: Loss = -10855.5849609375
Iteration 6900: Loss = -10855.578125
Iteration 7000: Loss = -10855.572265625
Iteration 7100: Loss = -10855.5673828125
Iteration 7200: Loss = -10855.5625
Iteration 7300: Loss = -10855.5595703125
Iteration 7400: Loss = -10855.5556640625
Iteration 7500: Loss = -10855.5517578125
Iteration 7600: Loss = -10855.5498046875
Iteration 7700: Loss = -10855.546875
Iteration 7800: Loss = -10855.544921875
Iteration 7900: Loss = -10855.5419921875
Iteration 8000: Loss = -10855.5400390625
Iteration 8100: Loss = -10855.5380859375
Iteration 8200: Loss = -10855.537109375
Iteration 8300: Loss = -10855.53515625
Iteration 8400: Loss = -10855.533203125
Iteration 8500: Loss = -10855.5322265625
Iteration 8600: Loss = -10855.5302734375
Iteration 8700: Loss = -10855.5302734375
Iteration 8800: Loss = -10855.529296875
Iteration 8900: Loss = -10855.52734375
Iteration 9000: Loss = -10855.52734375
Iteration 9100: Loss = -10855.52734375
Iteration 9200: Loss = -10855.525390625
Iteration 9300: Loss = -10855.5244140625
Iteration 9400: Loss = -10855.5244140625
Iteration 9500: Loss = -10855.5234375
Iteration 9600: Loss = -10855.5224609375
Iteration 9700: Loss = -10855.521484375
Iteration 9800: Loss = -10855.521484375
Iteration 9900: Loss = -10855.5224609375
1
Iteration 10000: Loss = -10855.5205078125
Iteration 10100: Loss = -10855.51953125
Iteration 10200: Loss = -10855.509765625
Iteration 10300: Loss = -10855.5087890625
Iteration 10400: Loss = -10855.5078125
Iteration 10500: Loss = -10855.5078125
Iteration 10600: Loss = -10855.5087890625
1
Iteration 10700: Loss = -10855.505859375
Iteration 10800: Loss = -10855.5068359375
1
Iteration 10900: Loss = -10855.505859375
Iteration 11000: Loss = -10855.5048828125
Iteration 11100: Loss = -10855.5068359375
1
Iteration 11200: Loss = -10855.505859375
2
Iteration 11300: Loss = -10855.505859375
3
Iteration 11400: Loss = -10855.5048828125
Iteration 11500: Loss = -10855.50390625
Iteration 11600: Loss = -10855.5029296875
Iteration 11700: Loss = -10855.5029296875
Iteration 11800: Loss = -10855.5029296875
Iteration 11900: Loss = -10855.5029296875
Iteration 12000: Loss = -10855.5029296875
Iteration 12100: Loss = -10855.501953125
Iteration 12200: Loss = -10855.5029296875
1
Iteration 12300: Loss = -10855.501953125
Iteration 12400: Loss = -10855.501953125
Iteration 12500: Loss = -10855.5009765625
Iteration 12600: Loss = -10855.5029296875
1
Iteration 12700: Loss = -10855.501953125
2
Iteration 12800: Loss = -10855.5009765625
Iteration 12900: Loss = -10855.501953125
1
Iteration 13000: Loss = -10855.5009765625
Iteration 13100: Loss = -10855.5009765625
Iteration 13200: Loss = -10855.5009765625
Iteration 13300: Loss = -10855.5009765625
Iteration 13400: Loss = -10855.5009765625
Iteration 13500: Loss = -10855.5009765625
Iteration 13600: Loss = -10855.5
Iteration 13700: Loss = -10855.5009765625
1
Iteration 13800: Loss = -10855.5
Iteration 13900: Loss = -10855.4990234375
Iteration 14000: Loss = -10855.4970703125
Iteration 14100: Loss = -10855.494140625
Iteration 14200: Loss = -10855.494140625
Iteration 14300: Loss = -10855.4921875
Iteration 14400: Loss = -10855.494140625
1
Iteration 14500: Loss = -10855.494140625
2
Iteration 14600: Loss = -10855.4931640625
3
Iteration 14700: Loss = -10855.4931640625
4
Iteration 14800: Loss = -10855.4931640625
5
Iteration 14900: Loss = -10855.494140625
6
Iteration 15000: Loss = -10855.494140625
7
Iteration 15100: Loss = -10855.4951171875
8
Iteration 15200: Loss = -10855.494140625
9
Iteration 15300: Loss = -10855.494140625
10
Iteration 15400: Loss = -10855.4931640625
11
Iteration 15500: Loss = -10855.494140625
12
Iteration 15600: Loss = -10855.4921875
Iteration 15700: Loss = -10855.4931640625
1
Iteration 15800: Loss = -10855.4931640625
2
Iteration 15900: Loss = -10855.494140625
3
Iteration 16000: Loss = -10855.494140625
4
Iteration 16100: Loss = -10855.4931640625
5
Iteration 16200: Loss = -10855.4921875
Iteration 16300: Loss = -10855.4921875
Iteration 16400: Loss = -10855.4931640625
1
Iteration 16500: Loss = -10855.4912109375
Iteration 16600: Loss = -10855.4931640625
1
Iteration 16700: Loss = -10855.4931640625
2
Iteration 16800: Loss = -10855.4912109375
Iteration 16900: Loss = -10855.4921875
1
Iteration 17000: Loss = -10855.4921875
2
Iteration 17100: Loss = -10855.4921875
3
Iteration 17200: Loss = -10855.4921875
4
Iteration 17300: Loss = -10855.4931640625
5
Iteration 17400: Loss = -10855.4921875
6
Iteration 17500: Loss = -10855.4921875
7
Iteration 17600: Loss = -10855.4931640625
8
Iteration 17700: Loss = -10855.4912109375
Iteration 17800: Loss = -10855.4931640625
1
Iteration 17900: Loss = -10855.4912109375
Iteration 18000: Loss = -10855.4931640625
1
Iteration 18100: Loss = -10855.4912109375
Iteration 18200: Loss = -10855.4931640625
1
Iteration 18300: Loss = -10855.4921875
2
Iteration 18400: Loss = -10855.4921875
3
Iteration 18500: Loss = -10855.4921875
4
Iteration 18600: Loss = -10855.4921875
5
Iteration 18700: Loss = -10855.4921875
6
Iteration 18800: Loss = -10855.4931640625
7
Iteration 18900: Loss = -10855.4912109375
Iteration 19000: Loss = -10855.4921875
1
Iteration 19100: Loss = -10855.4921875
2
Iteration 19200: Loss = -10855.4912109375
Iteration 19300: Loss = -10855.4921875
1
Iteration 19400: Loss = -10855.4912109375
Iteration 19500: Loss = -10855.4921875
1
Iteration 19600: Loss = -10855.4931640625
2
Iteration 19700: Loss = -10855.4921875
3
Iteration 19800: Loss = -10855.4921875
4
Iteration 19900: Loss = -10855.4912109375
Iteration 20000: Loss = -10855.4931640625
1
Iteration 20100: Loss = -10855.4921875
2
Iteration 20200: Loss = -10855.4912109375
Iteration 20300: Loss = -10855.4921875
1
Iteration 20400: Loss = -10855.4912109375
Iteration 20500: Loss = -10855.4912109375
Iteration 20600: Loss = -10855.4931640625
1
Iteration 20700: Loss = -10855.4921875
2
Iteration 20800: Loss = -10855.4912109375
Iteration 20900: Loss = -10855.4931640625
1
Iteration 21000: Loss = -10855.4912109375
Iteration 21100: Loss = -10855.4931640625
1
Iteration 21200: Loss = -10855.4931640625
2
Iteration 21300: Loss = -10855.4912109375
Iteration 21400: Loss = -10855.4921875
1
Iteration 21500: Loss = -10855.4921875
2
Iteration 21600: Loss = -10855.4921875
3
Iteration 21700: Loss = -10855.4921875
4
Iteration 21800: Loss = -10855.4921875
5
Iteration 21900: Loss = -10855.4921875
6
Iteration 22000: Loss = -10855.4921875
7
Iteration 22100: Loss = -10855.4921875
8
Iteration 22200: Loss = -10855.4912109375
Iteration 22300: Loss = -10855.4921875
1
Iteration 22400: Loss = -10855.4921875
2
Iteration 22500: Loss = -10855.4931640625
3
Iteration 22600: Loss = -10855.4921875
4
Iteration 22700: Loss = -10855.4921875
5
Iteration 22800: Loss = -10855.4921875
6
Iteration 22900: Loss = -10855.4931640625
7
Iteration 23000: Loss = -10855.494140625
8
Iteration 23100: Loss = -10855.4912109375
Iteration 23200: Loss = -10855.4921875
1
Iteration 23300: Loss = -10855.4921875
2
Iteration 23400: Loss = -10855.4912109375
Iteration 23500: Loss = -10855.4921875
1
Iteration 23600: Loss = -10855.4921875
2
Iteration 23700: Loss = -10855.4931640625
3
Iteration 23800: Loss = -10855.4931640625
4
Iteration 23900: Loss = -10855.4912109375
Iteration 24000: Loss = -10855.4912109375
Iteration 24100: Loss = -10855.4931640625
1
Iteration 24200: Loss = -10855.4912109375
Iteration 24300: Loss = -10855.4912109375
Iteration 24400: Loss = -10855.4912109375
Iteration 24500: Loss = -10855.4921875
1
Iteration 24600: Loss = -10855.4921875
2
Iteration 24700: Loss = -10855.4931640625
3
Iteration 24800: Loss = -10855.4921875
4
Iteration 24900: Loss = -10855.4921875
5
Iteration 25000: Loss = -10855.4921875
6
Iteration 25100: Loss = -10855.4921875
7
Iteration 25200: Loss = -10855.4921875
8
Iteration 25300: Loss = -10855.4921875
9
Iteration 25400: Loss = -10855.4921875
10
Iteration 25500: Loss = -10855.4921875
11
Iteration 25600: Loss = -10855.4921875
12
Iteration 25700: Loss = -10855.4921875
13
Iteration 25800: Loss = -10855.4921875
14
Iteration 25900: Loss = -10855.4921875
15
Stopping early at iteration 25900 due to no improvement.
pi: tensor([[0.7739, 0.2261],
        [0.2126, 0.7874]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5058, 0.4942], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.0994],
         [0.0163, 0.2548]],

        [[0.0134, 0.1079],
         [0.8092, 0.0614]],

        [[0.0147, 0.1068],
         [0.9902, 0.5262]],

        [[0.7685, 0.0977],
         [0.7584, 0.4642]],

        [[0.0161, 0.0985],
         [0.0134, 0.7526]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721342688903776
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6690727572201083
Global Adjusted Rand Index: 0.8387333467291663
Average Adjusted Rand Index: 0.8413680625533283
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -37474.7265625
Iteration 100: Loss = -22382.32421875
Iteration 200: Loss = -13346.5986328125
Iteration 300: Loss = -11835.9404296875
Iteration 400: Loss = -11494.1806640625
Iteration 500: Loss = -11360.25390625
Iteration 600: Loss = -11273.140625
Iteration 700: Loss = -11226.265625
Iteration 800: Loss = -11193.0185546875
Iteration 900: Loss = -11168.3349609375
Iteration 1000: Loss = -11152.169921875
Iteration 1100: Loss = -11134.109375
Iteration 1200: Loss = -11124.03125
Iteration 1300: Loss = -11112.81640625
Iteration 1400: Loss = -11103.4921875
Iteration 1500: Loss = -11098.0498046875
Iteration 1600: Loss = -11093.0224609375
Iteration 1700: Loss = -11086.900390625
Iteration 1800: Loss = -11081.1572265625
Iteration 1900: Loss = -11077.0595703125
Iteration 2000: Loss = -11073.6279296875
Iteration 2100: Loss = -11070.53125
Iteration 2200: Loss = -11067.6591796875
Iteration 2300: Loss = -11064.9462890625
Iteration 2400: Loss = -11061.873046875
Iteration 2500: Loss = -11059.90234375
Iteration 2600: Loss = -11058.2890625
Iteration 2700: Loss = -11055.8408203125
Iteration 2800: Loss = -11053.32421875
Iteration 2900: Loss = -11048.6982421875
Iteration 3000: Loss = -11046.3974609375
Iteration 3100: Loss = -11043.6572265625
Iteration 3200: Loss = -11042.5224609375
Iteration 3300: Loss = -11039.169921875
Iteration 3400: Loss = -11038.3896484375
Iteration 3500: Loss = -11033.244140625
Iteration 3600: Loss = -11028.64453125
Iteration 3700: Loss = -11026.9921875
Iteration 3800: Loss = -11025.8466796875
Iteration 3900: Loss = -11024.666015625
Iteration 4000: Loss = -11022.69140625
Iteration 4100: Loss = -11016.5703125
Iteration 4200: Loss = -11015.4931640625
Iteration 4300: Loss = -11015.07421875
Iteration 4400: Loss = -11014.7744140625
Iteration 4500: Loss = -11014.498046875
Iteration 4600: Loss = -11009.677734375
Iteration 4700: Loss = -11006.349609375
Iteration 4800: Loss = -11005.4111328125
Iteration 4900: Loss = -11005.134765625
Iteration 5000: Loss = -11004.921875
Iteration 5100: Loss = -11004.74609375
Iteration 5200: Loss = -11004.595703125
Iteration 5300: Loss = -11004.4638671875
Iteration 5400: Loss = -11004.3466796875
Iteration 5500: Loss = -11004.2412109375
Iteration 5600: Loss = -11004.146484375
Iteration 5700: Loss = -11004.05859375
Iteration 5800: Loss = -11003.9775390625
Iteration 5900: Loss = -11003.9033203125
Iteration 6000: Loss = -11003.833984375
Iteration 6100: Loss = -11003.7666015625
Iteration 6200: Loss = -11003.7021484375
Iteration 6300: Loss = -11003.6337890625
Iteration 6400: Loss = -11003.46875
Iteration 6500: Loss = -10997.4365234375
Iteration 6600: Loss = -10996.80078125
Iteration 6700: Loss = -10996.6015625
Iteration 6800: Loss = -10996.4755859375
Iteration 6900: Loss = -10996.37890625
Iteration 7000: Loss = -10995.9814453125
Iteration 7100: Loss = -10995.900390625
Iteration 7200: Loss = -10995.84375
Iteration 7300: Loss = -10995.794921875
Iteration 7400: Loss = -10995.751953125
Iteration 7500: Loss = -10995.71484375
Iteration 7600: Loss = -10995.6806640625
Iteration 7700: Loss = -10995.6494140625
Iteration 7800: Loss = -10995.6220703125
Iteration 7900: Loss = -10995.5966796875
Iteration 8000: Loss = -10995.572265625
Iteration 8100: Loss = -10995.5498046875
Iteration 8200: Loss = -10995.5322265625
Iteration 8300: Loss = -10995.5126953125
Iteration 8400: Loss = -10995.494140625
Iteration 8500: Loss = -10995.4794921875
Iteration 8600: Loss = -10995.4638671875
Iteration 8700: Loss = -10995.451171875
Iteration 8800: Loss = -10995.4365234375
Iteration 8900: Loss = -10995.4267578125
Iteration 9000: Loss = -10995.4140625
Iteration 9100: Loss = -10995.404296875
Iteration 9200: Loss = -10995.3935546875
Iteration 9300: Loss = -10995.3837890625
Iteration 9400: Loss = -10995.3740234375
Iteration 9500: Loss = -10995.3662109375
Iteration 9600: Loss = -10995.359375
Iteration 9700: Loss = -10995.3505859375
Iteration 9800: Loss = -10995.3466796875
Iteration 9900: Loss = -10995.337890625
Iteration 10000: Loss = -10995.3330078125
Iteration 10100: Loss = -10995.3271484375
Iteration 10200: Loss = -10995.3212890625
Iteration 10300: Loss = -10995.3154296875
Iteration 10400: Loss = -10995.310546875
Iteration 10500: Loss = -10995.306640625
Iteration 10600: Loss = -10995.3017578125
Iteration 10700: Loss = -10995.2978515625
Iteration 10800: Loss = -10995.2939453125
Iteration 10900: Loss = -10995.291015625
Iteration 11000: Loss = -10995.28515625
Iteration 11100: Loss = -10995.2822265625
Iteration 11200: Loss = -10995.2783203125
Iteration 11300: Loss = -10995.275390625
Iteration 11400: Loss = -10995.2734375
Iteration 11500: Loss = -10995.26953125
Iteration 11600: Loss = -10995.267578125
Iteration 11700: Loss = -10995.263671875
Iteration 11800: Loss = -10995.26171875
Iteration 11900: Loss = -10995.2587890625
Iteration 12000: Loss = -10995.2548828125
Iteration 12100: Loss = -10995.25
Iteration 12200: Loss = -10995.244140625
Iteration 12300: Loss = -10995.234375
Iteration 12400: Loss = -10995.1728515625
Iteration 12500: Loss = -10995.0673828125
Iteration 12600: Loss = -10995.01171875
Iteration 12700: Loss = -10994.9921875
Iteration 12800: Loss = -10994.9775390625
Iteration 12900: Loss = -10994.9560546875
Iteration 13000: Loss = -10994.9404296875
Iteration 13100: Loss = -10994.9384765625
Iteration 13200: Loss = -10994.931640625
Iteration 13300: Loss = -10994.931640625
Iteration 13400: Loss = -10994.919921875
Iteration 13500: Loss = -10994.908203125
Iteration 13600: Loss = -10994.9052734375
Iteration 13700: Loss = -10994.8974609375
Iteration 13800: Loss = -10994.8916015625
Iteration 13900: Loss = -10994.880859375
Iteration 14000: Loss = -10994.861328125
Iteration 14100: Loss = -10994.83984375
Iteration 14200: Loss = -10994.833984375
Iteration 14300: Loss = -10994.822265625
Iteration 14400: Loss = -10994.806640625
Iteration 14500: Loss = -10994.7861328125
Iteration 14600: Loss = -10994.775390625
Iteration 14700: Loss = -10994.7646484375
Iteration 14800: Loss = -10994.7431640625
Iteration 14900: Loss = -10994.7138671875
Iteration 15000: Loss = -10994.7041015625
Iteration 15100: Loss = -10994.6962890625
Iteration 15200: Loss = -10994.6865234375
Iteration 15300: Loss = -10994.6787109375
Iteration 15400: Loss = -10994.669921875
Iteration 15500: Loss = -10994.66015625
Iteration 15600: Loss = -10994.654296875
Iteration 15700: Loss = -10994.6455078125
Iteration 15800: Loss = -10994.6396484375
Iteration 15900: Loss = -10994.63671875
Iteration 16000: Loss = -10994.6337890625
Iteration 16100: Loss = -10994.6318359375
Iteration 16200: Loss = -10994.626953125
Iteration 16300: Loss = -10994.625
Iteration 16400: Loss = -10994.62109375
Iteration 16500: Loss = -10994.6201171875
Iteration 16600: Loss = -10994.6181640625
Iteration 16700: Loss = -10994.615234375
Iteration 16800: Loss = -10994.6142578125
Iteration 16900: Loss = -10994.611328125
Iteration 17000: Loss = -10994.6123046875
1
Iteration 17100: Loss = -10994.6103515625
Iteration 17200: Loss = -10994.609375
Iteration 17300: Loss = -10994.6083984375
Iteration 17400: Loss = -10994.609375
1
Iteration 17500: Loss = -10994.607421875
Iteration 17600: Loss = -10994.607421875
Iteration 17700: Loss = -10994.6083984375
1
Iteration 17800: Loss = -10994.6064453125
Iteration 17900: Loss = -10994.6083984375
1
Iteration 18000: Loss = -10994.6064453125
Iteration 18100: Loss = -10994.6064453125
Iteration 18200: Loss = -10994.6083984375
1
Iteration 18300: Loss = -10994.6083984375
2
Iteration 18400: Loss = -10994.60546875
Iteration 18500: Loss = -10994.6064453125
1
Iteration 18600: Loss = -10994.6064453125
2
Iteration 18700: Loss = -10994.6064453125
3
Iteration 18800: Loss = -10994.60546875
Iteration 18900: Loss = -10994.6064453125
1
Iteration 19000: Loss = -10994.6064453125
2
Iteration 19100: Loss = -10994.60546875
Iteration 19200: Loss = -10994.6064453125
1
Iteration 19300: Loss = -10994.6064453125
2
Iteration 19400: Loss = -10994.6064453125
3
Iteration 19500: Loss = -10994.599609375
Iteration 19600: Loss = -10994.59765625
Iteration 19700: Loss = -10994.6015625
1
Iteration 19800: Loss = -10994.5986328125
2
Iteration 19900: Loss = -10994.5986328125
3
Iteration 20000: Loss = -10994.59765625
Iteration 20100: Loss = -10994.59765625
Iteration 20200: Loss = -10994.599609375
1
Iteration 20300: Loss = -10994.5986328125
2
Iteration 20400: Loss = -10994.5986328125
3
Iteration 20500: Loss = -10994.5966796875
Iteration 20600: Loss = -10994.5986328125
1
Iteration 20700: Loss = -10994.5986328125
2
Iteration 20800: Loss = -10994.5986328125
3
Iteration 20900: Loss = -10994.548828125
Iteration 21000: Loss = -10994.548828125
Iteration 21100: Loss = -10994.5458984375
Iteration 21200: Loss = -10994.546875
1
Iteration 21300: Loss = -10994.5439453125
Iteration 21400: Loss = -10994.544921875
1
Iteration 21500: Loss = -10994.54296875
Iteration 21600: Loss = -10994.5439453125
1
Iteration 21700: Loss = -10994.5439453125
2
Iteration 21800: Loss = -10994.54296875
Iteration 21900: Loss = -10994.5439453125
1
Iteration 22000: Loss = -10994.5439453125
2
Iteration 22100: Loss = -10994.54296875
Iteration 22200: Loss = -10994.54296875
Iteration 22300: Loss = -10994.54296875
Iteration 22400: Loss = -10994.5439453125
1
Iteration 22500: Loss = -10994.5439453125
2
Iteration 22600: Loss = -10994.5439453125
3
Iteration 22700: Loss = -10994.5439453125
4
Iteration 22800: Loss = -10994.5419921875
Iteration 22900: Loss = -10994.54296875
1
Iteration 23000: Loss = -10994.54296875
2
Iteration 23100: Loss = -10994.5439453125
3
Iteration 23200: Loss = -10994.54296875
4
Iteration 23300: Loss = -10994.5439453125
5
Iteration 23400: Loss = -10994.5439453125
6
Iteration 23500: Loss = -10994.54296875
7
Iteration 23600: Loss = -10994.54296875
8
Iteration 23700: Loss = -10994.54296875
9
Iteration 23800: Loss = -10994.54296875
10
Iteration 23900: Loss = -10994.5439453125
11
Iteration 24000: Loss = -10994.5439453125
12
Iteration 24100: Loss = -10994.5439453125
13
Iteration 24200: Loss = -10994.5439453125
14
Iteration 24300: Loss = -10994.544921875
15
Stopping early at iteration 24300 due to no improvement.
pi: tensor([[1.1507e-04, 9.9988e-01],
        [1.1191e-02, 9.8881e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([8.6538e-05, 9.9991e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2845, 0.1544],
         [0.9516, 0.1613]],

        [[0.9922, 0.2375],
         [0.5906, 0.9662]],

        [[0.0243, 0.2751],
         [0.1860, 0.1659]],

        [[0.9924, 0.4860],
         [0.7026, 0.5113]],

        [[0.7705, 0.1993],
         [0.9832, 0.9932]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00030769329388876244
Average Adjusted Rand Index: -0.00015692302765368048
[0.8387333467291663, -0.00030769329388876244] [0.8413680625533283, -0.00015692302765368048] [10855.4921875, 10994.544921875]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11162.237221344172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -40645.78515625
Iteration 100: Loss = -24324.177734375
Iteration 200: Loss = -14316.109375
Iteration 300: Loss = -12014.013671875
Iteration 400: Loss = -11687.99609375
Iteration 500: Loss = -11560.5390625
Iteration 600: Loss = -11478.716796875
Iteration 700: Loss = -11426.2509765625
Iteration 800: Loss = -11394.1884765625
Iteration 900: Loss = -11360.5029296875
Iteration 1000: Loss = -11344.3544921875
Iteration 1100: Loss = -11332.65234375
Iteration 1200: Loss = -11302.3125
Iteration 1300: Loss = -11295.0927734375
Iteration 1400: Loss = -11286.58203125
Iteration 1500: Loss = -11282.751953125
Iteration 1600: Loss = -11276.37109375
Iteration 1700: Loss = -11273.9697265625
Iteration 1800: Loss = -11271.9951171875
Iteration 1900: Loss = -11270.2685546875
Iteration 2000: Loss = -11268.73046875
Iteration 2100: Loss = -11267.3427734375
Iteration 2200: Loss = -11266.08203125
Iteration 2300: Loss = -11264.9384765625
Iteration 2400: Loss = -11263.8974609375
Iteration 2500: Loss = -11262.9482421875
Iteration 2600: Loss = -11262.0888671875
Iteration 2700: Loss = -11261.310546875
Iteration 2800: Loss = -11260.603515625
Iteration 2900: Loss = -11259.9658203125
Iteration 3000: Loss = -11259.3828125
Iteration 3100: Loss = -11258.8515625
Iteration 3200: Loss = -11258.357421875
Iteration 3300: Loss = -11257.90625
Iteration 3400: Loss = -11257.4892578125
Iteration 3500: Loss = -11257.099609375
Iteration 3600: Loss = -11256.4833984375
Iteration 3700: Loss = -11250.931640625
Iteration 3800: Loss = -11250.537109375
Iteration 3900: Loss = -11250.236328125
Iteration 4000: Loss = -11249.97265625
Iteration 4100: Loss = -11249.7353515625
Iteration 4200: Loss = -11249.5185546875
Iteration 4300: Loss = -11249.318359375
Iteration 4400: Loss = -11249.130859375
Iteration 4500: Loss = -11248.955078125
Iteration 4600: Loss = -11248.7919921875
Iteration 4700: Loss = -11248.638671875
Iteration 4800: Loss = -11248.4931640625
Iteration 4900: Loss = -11248.3564453125
Iteration 5000: Loss = -11248.2265625
Iteration 5100: Loss = -11248.1064453125
Iteration 5200: Loss = -11247.9912109375
Iteration 5300: Loss = -11247.8818359375
Iteration 5400: Loss = -11247.7783203125
Iteration 5500: Loss = -11247.6806640625
Iteration 5600: Loss = -11247.58984375
Iteration 5700: Loss = -11247.5
Iteration 5800: Loss = -11247.416015625
Iteration 5900: Loss = -11247.3310546875
Iteration 6000: Loss = -11247.2529296875
Iteration 6100: Loss = -11247.1728515625
Iteration 6200: Loss = -11247.0947265625
Iteration 6300: Loss = -11247.015625
Iteration 6400: Loss = -11246.93359375
Iteration 6500: Loss = -11246.849609375
Iteration 6600: Loss = -11246.763671875
Iteration 6700: Loss = -11246.6787109375
Iteration 6800: Loss = -11246.6005859375
Iteration 6900: Loss = -11246.5234375
Iteration 7000: Loss = -11246.451171875
Iteration 7100: Loss = -11246.380859375
Iteration 7200: Loss = -11246.314453125
Iteration 7300: Loss = -11246.25
Iteration 7400: Loss = -11246.1865234375
Iteration 7500: Loss = -11246.1279296875
Iteration 7600: Loss = -11246.0693359375
Iteration 7700: Loss = -11246.013671875
Iteration 7800: Loss = -11245.958984375
Iteration 7900: Loss = -11245.90625
Iteration 8000: Loss = -11245.853515625
Iteration 8100: Loss = -11245.8046875
Iteration 8200: Loss = -11245.755859375
Iteration 8300: Loss = -11245.705078125
Iteration 8400: Loss = -11245.62890625
Iteration 8500: Loss = -11242.7919921875
Iteration 8600: Loss = -11242.6767578125
Iteration 8700: Loss = -11242.56640625
Iteration 8800: Loss = -11242.3837890625
Iteration 8900: Loss = -11242.3212890625
Iteration 9000: Loss = -11242.2607421875
Iteration 9100: Loss = -11242.203125
Iteration 9200: Loss = -11242.1455078125
Iteration 9300: Loss = -11242.0888671875
Iteration 9400: Loss = -11242.0341796875
Iteration 9500: Loss = -11241.9794921875
Iteration 9600: Loss = -11241.927734375
Iteration 9700: Loss = -11241.876953125
Iteration 9800: Loss = -11241.826171875
Iteration 9900: Loss = -11241.7783203125
Iteration 10000: Loss = -11241.73046875
Iteration 10100: Loss = -11241.6826171875
Iteration 10200: Loss = -11241.6318359375
Iteration 10300: Loss = -11241.58984375
Iteration 10400: Loss = -11241.5498046875
Iteration 10500: Loss = -11241.51953125
Iteration 10600: Loss = -11241.48828125
Iteration 10700: Loss = -11241.462890625
Iteration 10800: Loss = -11241.44140625
Iteration 10900: Loss = -11241.419921875
Iteration 11000: Loss = -11241.400390625
Iteration 11100: Loss = -11241.3828125
Iteration 11200: Loss = -11241.365234375
Iteration 11300: Loss = -11241.3525390625
Iteration 11400: Loss = -11241.341796875
Iteration 11500: Loss = -11241.330078125
Iteration 11600: Loss = -11241.322265625
Iteration 11700: Loss = -11241.3154296875
Iteration 11800: Loss = -11241.306640625
Iteration 11900: Loss = -11241.10546875
Iteration 12000: Loss = -11237.078125
Iteration 12100: Loss = -11236.7998046875
Iteration 12200: Loss = -11236.732421875
Iteration 12300: Loss = -11236.689453125
Iteration 12400: Loss = -11236.6572265625
Iteration 12500: Loss = -11236.6337890625
Iteration 12600: Loss = -11236.6123046875
Iteration 12700: Loss = -11236.59375
Iteration 12800: Loss = -11236.5771484375
Iteration 12900: Loss = -11236.5634765625
Iteration 13000: Loss = -11236.548828125
Iteration 13100: Loss = -11236.537109375
Iteration 13200: Loss = -11236.52734375
Iteration 13300: Loss = -11236.5146484375
Iteration 13400: Loss = -11236.505859375
Iteration 13500: Loss = -11236.4990234375
Iteration 13600: Loss = -11236.4912109375
Iteration 13700: Loss = -11236.4833984375
Iteration 13800: Loss = -11236.4765625
Iteration 13900: Loss = -11236.470703125
Iteration 14000: Loss = -11236.46484375
Iteration 14100: Loss = -11236.4609375
Iteration 14200: Loss = -11236.4541015625
Iteration 14300: Loss = -11236.451171875
Iteration 14400: Loss = -11236.4453125
Iteration 14500: Loss = -11236.44140625
Iteration 14600: Loss = -11236.439453125
Iteration 14700: Loss = -11236.4345703125
Iteration 14800: Loss = -11236.4326171875
Iteration 14900: Loss = -11236.4287109375
Iteration 15000: Loss = -11236.42578125
Iteration 15100: Loss = -11236.423828125
Iteration 15200: Loss = -11236.419921875
Iteration 15300: Loss = -11236.4189453125
Iteration 15400: Loss = -11236.4150390625
Iteration 15500: Loss = -11236.4140625
Iteration 15600: Loss = -11236.4111328125
Iteration 15700: Loss = -11236.41015625
Iteration 15800: Loss = -11236.4091796875
Iteration 15900: Loss = -11236.4072265625
Iteration 16000: Loss = -11236.4052734375
Iteration 16100: Loss = -11236.40234375
Iteration 16200: Loss = -11236.4033203125
1
Iteration 16300: Loss = -11236.4013671875
Iteration 16400: Loss = -11236.4013671875
Iteration 16500: Loss = -11236.3994140625
Iteration 16600: Loss = -11236.3984375
Iteration 16700: Loss = -11236.396484375
Iteration 16800: Loss = -11236.3955078125
Iteration 16900: Loss = -11236.39453125
Iteration 17000: Loss = -11236.39453125
Iteration 17100: Loss = -11236.3935546875
Iteration 17200: Loss = -11236.3935546875
Iteration 17300: Loss = -11236.3916015625
Iteration 17400: Loss = -11236.3916015625
Iteration 17500: Loss = -11236.390625
Iteration 17600: Loss = -11236.3896484375
Iteration 17700: Loss = -11236.388671875
Iteration 17800: Loss = -11236.3896484375
1
Iteration 17900: Loss = -11236.3876953125
Iteration 18000: Loss = -11236.388671875
1
Iteration 18100: Loss = -11236.3876953125
Iteration 18200: Loss = -11236.38671875
Iteration 18300: Loss = -11236.3857421875
Iteration 18400: Loss = -11236.3857421875
Iteration 18500: Loss = -11236.3876953125
1
Iteration 18600: Loss = -11236.38671875
2
Iteration 18700: Loss = -11236.384765625
Iteration 18800: Loss = -11236.3857421875
1
Iteration 18900: Loss = -11236.3857421875
2
Iteration 19000: Loss = -11236.384765625
Iteration 19100: Loss = -11236.384765625
Iteration 19200: Loss = -11236.384765625
Iteration 19300: Loss = -11236.384765625
Iteration 19400: Loss = -11236.3837890625
Iteration 19500: Loss = -11236.3837890625
Iteration 19600: Loss = -11236.384765625
1
Iteration 19700: Loss = -11236.3837890625
Iteration 19800: Loss = -11236.3837890625
Iteration 19900: Loss = -11236.3828125
Iteration 20000: Loss = -11236.3828125
Iteration 20100: Loss = -11236.3837890625
1
Iteration 20200: Loss = -11236.384765625
2
Iteration 20300: Loss = -11236.3837890625
3
Iteration 20400: Loss = -11236.3837890625
4
Iteration 20500: Loss = -11236.3837890625
5
Iteration 20600: Loss = -11236.3828125
Iteration 20700: Loss = -11236.3828125
Iteration 20800: Loss = -11236.3818359375
Iteration 20900: Loss = -11236.3828125
1
Iteration 21000: Loss = -11236.3837890625
2
Iteration 21100: Loss = -11236.3818359375
Iteration 21200: Loss = -11236.384765625
1
Iteration 21300: Loss = -11236.3828125
2
Iteration 21400: Loss = -11236.3837890625
3
Iteration 21500: Loss = -11236.3818359375
Iteration 21600: Loss = -11236.3818359375
Iteration 21700: Loss = -11236.3828125
1
Iteration 21800: Loss = -11236.3828125
2
Iteration 21900: Loss = -11236.3828125
3
Iteration 22000: Loss = -11236.3837890625
4
Iteration 22100: Loss = -11236.3828125
5
Iteration 22200: Loss = -11236.373046875
Iteration 22300: Loss = -11236.3701171875
Iteration 22400: Loss = -11236.37109375
1
Iteration 22500: Loss = -11236.32421875
Iteration 22600: Loss = -11235.505859375
Iteration 22700: Loss = -11234.9228515625
Iteration 22800: Loss = -11233.2880859375
Iteration 22900: Loss = -11232.826171875
Iteration 23000: Loss = -11232.67578125
Iteration 23100: Loss = -11232.6337890625
Iteration 23200: Loss = -11232.615234375
Iteration 23300: Loss = -11232.599609375
Iteration 23400: Loss = -11232.580078125
Iteration 23500: Loss = -11232.564453125
Iteration 23600: Loss = -11232.5576171875
Iteration 23700: Loss = -11232.5576171875
Iteration 23800: Loss = -11232.5478515625
Iteration 23900: Loss = -11232.52734375
Iteration 24000: Loss = -11232.4482421875
Iteration 24100: Loss = -11232.3935546875
Iteration 24200: Loss = -11232.390625
Iteration 24300: Loss = -11232.37890625
Iteration 24400: Loss = -11232.3662109375
Iteration 24500: Loss = -11232.3623046875
Iteration 24600: Loss = -11232.353515625
Iteration 24700: Loss = -11232.33984375
Iteration 24800: Loss = -11232.3369140625
Iteration 24900: Loss = -11232.337890625
1
Iteration 25000: Loss = -11232.3369140625
Iteration 25100: Loss = -11232.3388671875
1
Iteration 25200: Loss = -11232.3369140625
Iteration 25300: Loss = -11232.337890625
1
Iteration 25400: Loss = -11232.337890625
2
Iteration 25500: Loss = -11232.337890625
3
Iteration 25600: Loss = -11232.337890625
4
Iteration 25700: Loss = -11232.3369140625
Iteration 25800: Loss = -11232.3349609375
Iteration 25900: Loss = -11232.3369140625
1
Iteration 26000: Loss = -11232.3349609375
Iteration 26100: Loss = -11232.3369140625
1
Iteration 26200: Loss = -11232.3369140625
2
Iteration 26300: Loss = -11232.3369140625
3
Iteration 26400: Loss = -11232.3359375
4
Iteration 26500: Loss = -11232.337890625
5
Iteration 26600: Loss = -11232.3369140625
6
Iteration 26700: Loss = -11232.3359375
7
Iteration 26800: Loss = -11232.3359375
8
Iteration 26900: Loss = -11232.3359375
9
Iteration 27000: Loss = -11232.3369140625
10
Iteration 27100: Loss = -11232.3369140625
11
Iteration 27200: Loss = -11232.3359375
12
Iteration 27300: Loss = -11232.3349609375
Iteration 27400: Loss = -11232.3359375
1
Iteration 27500: Loss = -11232.3359375
2
Iteration 27600: Loss = -11232.3369140625
3
Iteration 27700: Loss = -11232.3359375
4
Iteration 27800: Loss = -11232.3388671875
5
Iteration 27900: Loss = -11232.3359375
6
Iteration 28000: Loss = -11232.3310546875
Iteration 28100: Loss = -11232.330078125
Iteration 28200: Loss = -11232.328125
Iteration 28300: Loss = -11232.3291015625
1
Iteration 28400: Loss = -11232.330078125
2
Iteration 28500: Loss = -11232.330078125
3
Iteration 28600: Loss = -11232.328125
Iteration 28700: Loss = -11232.3291015625
1
Iteration 28800: Loss = -11232.330078125
2
Iteration 28900: Loss = -11232.3291015625
3
Iteration 29000: Loss = -11232.3291015625
4
Iteration 29100: Loss = -11232.330078125
5
Iteration 29200: Loss = -11232.3271484375
Iteration 29300: Loss = -11232.3310546875
1
Iteration 29400: Loss = -11232.3291015625
2
Iteration 29500: Loss = -11232.328125
3
Iteration 29600: Loss = -11232.3291015625
4
Iteration 29700: Loss = -11232.328125
5
Iteration 29800: Loss = -11232.3271484375
Iteration 29900: Loss = -11232.328125
1
pi: tensor([[0.4307, 0.5693],
        [0.0305, 0.9695]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0245, 0.9755], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2463, 0.3065],
         [0.9784, 0.1630]],

        [[0.9772, 0.2330],
         [0.5450, 0.0457]],

        [[0.1517, 0.2566],
         [0.9088, 0.0068]],

        [[0.9849, 0.2233],
         [0.2665, 0.7422]],

        [[0.9166, 0.1933],
         [0.8971, 0.8264]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: -0.013574768645372375
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0010569298568351942
Average Adjusted Rand Index: -0.0018868615082868142
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22538.419921875
Iteration 100: Loss = -16163.3212890625
Iteration 200: Loss = -12127.052734375
Iteration 300: Loss = -11568.8837890625
Iteration 400: Loss = -11451.591796875
Iteration 500: Loss = -11390.40625
Iteration 600: Loss = -11352.5703125
Iteration 700: Loss = -11326.7744140625
Iteration 800: Loss = -11312.390625
Iteration 900: Loss = -11301.673828125
Iteration 1000: Loss = -11292.76171875
Iteration 1100: Loss = -11284.2421875
Iteration 1200: Loss = -11279.3876953125
Iteration 1300: Loss = -11275.37890625
Iteration 1400: Loss = -11271.5517578125
Iteration 1500: Loss = -11267.2763671875
Iteration 1600: Loss = -11263.5078125
Iteration 1700: Loss = -11261.2333984375
Iteration 1800: Loss = -11259.3857421875
Iteration 1900: Loss = -11257.814453125
Iteration 2000: Loss = -11256.1328125
Iteration 2100: Loss = -11254.4970703125
Iteration 2200: Loss = -11252.9658203125
Iteration 2300: Loss = -11251.5322265625
Iteration 2400: Loss = -11250.107421875
Iteration 2500: Loss = -11248.626953125
Iteration 2600: Loss = -11247.0869140625
Iteration 2700: Loss = -11245.677734375
Iteration 2800: Loss = -11244.66796875
Iteration 2900: Loss = -11243.9462890625
Iteration 3000: Loss = -11243.3896484375
Iteration 3100: Loss = -11242.9375
Iteration 3200: Loss = -11242.5615234375
Iteration 3300: Loss = -11242.240234375
Iteration 3400: Loss = -11241.962890625
Iteration 3500: Loss = -11241.720703125
Iteration 3600: Loss = -11241.50390625
Iteration 3700: Loss = -11241.3056640625
Iteration 3800: Loss = -11241.1201171875
Iteration 3900: Loss = -11240.923828125
Iteration 4000: Loss = -11240.669921875
Iteration 4100: Loss = -11240.298828125
Iteration 4200: Loss = -11239.9375
Iteration 4300: Loss = -11239.6943359375
Iteration 4400: Loss = -11239.52734375
Iteration 4500: Loss = -11239.3994140625
Iteration 4600: Loss = -11239.2939453125
Iteration 4700: Loss = -11239.203125
Iteration 4800: Loss = -11239.1201171875
Iteration 4900: Loss = -11239.0478515625
Iteration 5000: Loss = -11238.9814453125
Iteration 5100: Loss = -11238.919921875
Iteration 5200: Loss = -11238.8623046875
Iteration 5300: Loss = -11238.810546875
Iteration 5400: Loss = -11238.76171875
Iteration 5500: Loss = -11238.7158203125
Iteration 5600: Loss = -11238.671875
Iteration 5700: Loss = -11238.634765625
Iteration 5800: Loss = -11238.5966796875
Iteration 5900: Loss = -11238.56640625
Iteration 6000: Loss = -11238.533203125
Iteration 6100: Loss = -11238.505859375
Iteration 6200: Loss = -11238.478515625
Iteration 6300: Loss = -11238.4560546875
Iteration 6400: Loss = -11238.431640625
Iteration 6500: Loss = -11238.41015625
Iteration 6600: Loss = -11238.3896484375
Iteration 6700: Loss = -11238.373046875
Iteration 6800: Loss = -11238.353515625
Iteration 6900: Loss = -11238.337890625
Iteration 7000: Loss = -11238.3212890625
Iteration 7100: Loss = -11238.30859375
Iteration 7200: Loss = -11238.2939453125
Iteration 7300: Loss = -11238.2822265625
Iteration 7400: Loss = -11238.2705078125
Iteration 7500: Loss = -11238.259765625
Iteration 7600: Loss = -11238.248046875
Iteration 7700: Loss = -11238.2373046875
Iteration 7800: Loss = -11238.228515625
Iteration 7900: Loss = -11238.220703125
Iteration 8000: Loss = -11238.2119140625
Iteration 8100: Loss = -11238.203125
Iteration 8200: Loss = -11238.1962890625
Iteration 8300: Loss = -11238.189453125
Iteration 8400: Loss = -11238.1826171875
Iteration 8500: Loss = -11238.1748046875
Iteration 8600: Loss = -11238.1689453125
Iteration 8700: Loss = -11238.162109375
Iteration 8800: Loss = -11238.1572265625
Iteration 8900: Loss = -11238.15234375
Iteration 9000: Loss = -11238.1474609375
Iteration 9100: Loss = -11238.142578125
Iteration 9200: Loss = -11238.1376953125
Iteration 9300: Loss = -11238.1337890625
Iteration 9400: Loss = -11238.1298828125
Iteration 9500: Loss = -11238.126953125
Iteration 9600: Loss = -11238.12109375
Iteration 9700: Loss = -11238.119140625
Iteration 9800: Loss = -11238.1162109375
Iteration 9900: Loss = -11238.1123046875
Iteration 10000: Loss = -11238.1083984375
Iteration 10100: Loss = -11238.1064453125
Iteration 10200: Loss = -11238.103515625
Iteration 10300: Loss = -11238.1015625
Iteration 10400: Loss = -11238.099609375
Iteration 10500: Loss = -11238.0986328125
Iteration 10600: Loss = -11238.0947265625
Iteration 10700: Loss = -11238.091796875
Iteration 10800: Loss = -11238.091796875
Iteration 10900: Loss = -11238.0888671875
Iteration 11000: Loss = -11238.0859375
Iteration 11100: Loss = -11238.0859375
Iteration 11200: Loss = -11238.0830078125
Iteration 11300: Loss = -11238.08203125
Iteration 11400: Loss = -11238.080078125
Iteration 11500: Loss = -11238.080078125
Iteration 11600: Loss = -11238.078125
Iteration 11700: Loss = -11238.0771484375
Iteration 11800: Loss = -11238.076171875
Iteration 11900: Loss = -11238.0751953125
Iteration 12000: Loss = -11238.0732421875
Iteration 12100: Loss = -11238.072265625
Iteration 12200: Loss = -11238.072265625
Iteration 12300: Loss = -11238.0712890625
Iteration 12400: Loss = -11238.0693359375
Iteration 12500: Loss = -11238.0693359375
Iteration 12600: Loss = -11238.0693359375
Iteration 12700: Loss = -11238.0673828125
Iteration 12800: Loss = -11238.0673828125
Iteration 12900: Loss = -11238.0673828125
Iteration 13000: Loss = -11238.0654296875
Iteration 13100: Loss = -11238.0654296875
Iteration 13200: Loss = -11238.0634765625
Iteration 13300: Loss = -11238.064453125
1
Iteration 13400: Loss = -11238.0634765625
Iteration 13500: Loss = -11238.0625
Iteration 13600: Loss = -11238.0615234375
Iteration 13700: Loss = -11238.060546875
Iteration 13800: Loss = -11238.0634765625
1
Iteration 13900: Loss = -11238.0654296875
2
Iteration 14000: Loss = -11238.0615234375
3
Iteration 14100: Loss = -11238.0615234375
4
Iteration 14200: Loss = -11238.0595703125
Iteration 14300: Loss = -11238.0595703125
Iteration 14400: Loss = -11238.05859375
Iteration 14500: Loss = -11238.0595703125
1
Iteration 14600: Loss = -11238.0595703125
2
Iteration 14700: Loss = -11238.05859375
Iteration 14800: Loss = -11238.05859375
Iteration 14900: Loss = -11238.0576171875
Iteration 15000: Loss = -11238.056640625
Iteration 15100: Loss = -11238.0576171875
1
Iteration 15200: Loss = -11238.056640625
Iteration 15300: Loss = -11238.056640625
Iteration 15400: Loss = -11238.056640625
Iteration 15500: Loss = -11238.056640625
Iteration 15600: Loss = -11238.056640625
Iteration 15700: Loss = -11238.0576171875
1
Iteration 15800: Loss = -11238.056640625
Iteration 15900: Loss = -11238.0546875
Iteration 16000: Loss = -11238.0576171875
1
Iteration 16100: Loss = -11238.0556640625
2
Iteration 16200: Loss = -11238.0556640625
3
Iteration 16300: Loss = -11238.0556640625
4
Iteration 16400: Loss = -11238.0546875
Iteration 16500: Loss = -11238.0546875
Iteration 16600: Loss = -11238.0556640625
1
Iteration 16700: Loss = -11238.0556640625
2
Iteration 16800: Loss = -11238.0546875
Iteration 16900: Loss = -11238.0556640625
1
Iteration 17000: Loss = -11238.0546875
Iteration 17100: Loss = -11238.0546875
Iteration 17200: Loss = -11238.0546875
Iteration 17300: Loss = -11238.0546875
Iteration 17400: Loss = -11238.0537109375
Iteration 17500: Loss = -11238.0556640625
1
Iteration 17600: Loss = -11238.0556640625
2
Iteration 17700: Loss = -11238.0546875
3
Iteration 17800: Loss = -11238.0546875
4
Iteration 17900: Loss = -11238.0537109375
Iteration 18000: Loss = -11238.0546875
1
Iteration 18100: Loss = -11238.0537109375
Iteration 18200: Loss = -11238.0537109375
Iteration 18300: Loss = -11238.0546875
1
Iteration 18400: Loss = -11238.0556640625
2
Iteration 18500: Loss = -11238.052734375
Iteration 18600: Loss = -11238.0546875
1
Iteration 18700: Loss = -11238.0546875
2
Iteration 18800: Loss = -11238.0537109375
3
Iteration 18900: Loss = -11238.0537109375
4
Iteration 19000: Loss = -11238.0537109375
5
Iteration 19100: Loss = -11238.0537109375
6
Iteration 19200: Loss = -11238.052734375
Iteration 19300: Loss = -11237.9892578125
Iteration 19400: Loss = -11237.873046875
Iteration 19500: Loss = -11237.462890625
Iteration 19600: Loss = -11237.39453125
Iteration 19700: Loss = -11237.3720703125
Iteration 19800: Loss = -11237.3037109375
Iteration 19900: Loss = -11237.30078125
Iteration 20000: Loss = -11237.2333984375
Iteration 20100: Loss = -11237.23046875
Iteration 20200: Loss = -11237.22265625
Iteration 20300: Loss = -11237.0556640625
Iteration 20400: Loss = -11237.0556640625
Iteration 20500: Loss = -11237.0556640625
Iteration 20600: Loss = -11237.056640625
1
Iteration 20700: Loss = -11237.021484375
Iteration 20800: Loss = -11237.01953125
Iteration 20900: Loss = -11237.0146484375
Iteration 21000: Loss = -11236.984375
Iteration 21100: Loss = -11236.982421875
Iteration 21200: Loss = -11236.9658203125
Iteration 21300: Loss = -11236.9560546875
Iteration 21400: Loss = -11236.94921875
Iteration 21500: Loss = -11236.9384765625
Iteration 21600: Loss = -11236.939453125
1
Iteration 21700: Loss = -11236.939453125
2
Iteration 21800: Loss = -11236.9384765625
Iteration 21900: Loss = -11236.9296875
Iteration 22000: Loss = -11236.927734375
Iteration 22100: Loss = -11236.9267578125
Iteration 22200: Loss = -11236.927734375
1
Iteration 22300: Loss = -11236.927734375
2
Iteration 22400: Loss = -11236.927734375
3
Iteration 22500: Loss = -11236.9287109375
4
Iteration 22600: Loss = -11236.92578125
Iteration 22700: Loss = -11236.927734375
1
Iteration 22800: Loss = -11236.9267578125
2
Iteration 22900: Loss = -11236.9267578125
3
Iteration 23000: Loss = -11236.927734375
4
Iteration 23100: Loss = -11236.927734375
5
Iteration 23200: Loss = -11236.91796875
Iteration 23300: Loss = -11236.9189453125
1
Iteration 23400: Loss = -11236.9169921875
Iteration 23500: Loss = -11236.91796875
1
Iteration 23600: Loss = -11236.9130859375
Iteration 23700: Loss = -11236.9130859375
Iteration 23800: Loss = -11236.859375
Iteration 23900: Loss = -11236.8603515625
1
Iteration 24000: Loss = -11236.8583984375
Iteration 24100: Loss = -11236.859375
1
Iteration 24200: Loss = -11236.859375
2
Iteration 24300: Loss = -11236.7900390625
Iteration 24400: Loss = -11236.791015625
1
Iteration 24500: Loss = -11236.7890625
Iteration 24600: Loss = -11236.7900390625
1
Iteration 24700: Loss = -11236.7890625
Iteration 24800: Loss = -11236.7890625
Iteration 24900: Loss = -11236.791015625
1
Iteration 25000: Loss = -11236.7900390625
2
Iteration 25100: Loss = -11236.7900390625
3
Iteration 25200: Loss = -11236.7900390625
4
Iteration 25300: Loss = -11236.7900390625
5
Iteration 25400: Loss = -11236.7880859375
Iteration 25500: Loss = -11236.7900390625
1
Iteration 25600: Loss = -11236.7900390625
2
Iteration 25700: Loss = -11236.7900390625
3
Iteration 25800: Loss = -11236.7890625
4
Iteration 25900: Loss = -11236.791015625
5
Iteration 26000: Loss = -11236.7890625
6
Iteration 26100: Loss = -11236.7880859375
Iteration 26200: Loss = -11236.7890625
1
Iteration 26300: Loss = -11236.7900390625
2
Iteration 26400: Loss = -11236.7890625
3
Iteration 26500: Loss = -11236.7900390625
4
Iteration 26600: Loss = -11236.7890625
5
Iteration 26700: Loss = -11236.7890625
6
Iteration 26800: Loss = -11236.787109375
Iteration 26900: Loss = -11236.787109375
Iteration 27000: Loss = -11236.787109375
Iteration 27100: Loss = -11236.787109375
Iteration 27200: Loss = -11236.7861328125
Iteration 27300: Loss = -11236.787109375
1
Iteration 27400: Loss = -11236.7861328125
Iteration 27500: Loss = -11236.787109375
1
Iteration 27600: Loss = -11236.787109375
2
Iteration 27700: Loss = -11236.787109375
3
Iteration 27800: Loss = -11236.787109375
4
Iteration 27900: Loss = -11236.787109375
5
Iteration 28000: Loss = -11236.787109375
6
Iteration 28100: Loss = -11236.7861328125
Iteration 28200: Loss = -11236.78515625
Iteration 28300: Loss = -11236.78515625
Iteration 28400: Loss = -11236.783203125
Iteration 28500: Loss = -11236.783203125
Iteration 28600: Loss = -11236.7841796875
1
Iteration 28700: Loss = -11236.7841796875
2
Iteration 28800: Loss = -11236.7822265625
Iteration 28900: Loss = -11236.783203125
1
Iteration 29000: Loss = -11236.7841796875
2
Iteration 29100: Loss = -11236.783203125
3
Iteration 29200: Loss = -11236.7841796875
4
Iteration 29300: Loss = -11236.78515625
5
Iteration 29400: Loss = -11236.783203125
6
Iteration 29500: Loss = -11236.7841796875
7
Iteration 29600: Loss = -11236.7841796875
8
Iteration 29700: Loss = -11236.783203125
9
Iteration 29800: Loss = -11236.78515625
10
Iteration 29900: Loss = -11236.7841796875
11
pi: tensor([[9.6938e-01, 3.0617e-02],
        [1.0000e+00, 1.3712e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8907, 0.1093], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1735, 0.1181],
         [0.0324, 0.0920]],

        [[0.8862, 0.2404],
         [0.6370, 0.6591]],

        [[0.0257, 0.0942],
         [0.0194, 0.0147]],

        [[0.9173, 0.0942],
         [0.9599, 0.9931]],

        [[0.7012, 0.1440],
         [0.9921, 0.9827]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 33
Adjusted Rand Index: 0.07908627304272228
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00375584384439959
Average Adjusted Rand Index: 0.018914824068337697
[-0.0010569298568351942, 0.00375584384439959] [-0.0018868615082868142, 0.018914824068337697] [11232.3271484375, 11236.7822265625]
-------------------------------------
This iteration is 83
True Objective function: Loss = -10940.30836673985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39895.80078125
Iteration 100: Loss = -23722.39453125
Iteration 200: Loss = -13799.7822265625
Iteration 300: Loss = -11944.3974609375
Iteration 400: Loss = -11417.064453125
Iteration 500: Loss = -11273.5244140625
Iteration 600: Loss = -11174.5400390625
Iteration 700: Loss = -11125.1201171875
Iteration 800: Loss = -11098.5458984375
Iteration 900: Loss = -11080.1865234375
Iteration 1000: Loss = -11066.671875
Iteration 1100: Loss = -11056.3720703125
Iteration 1200: Loss = -11048.314453125
Iteration 1300: Loss = -11041.873046875
Iteration 1400: Loss = -11036.630859375
Iteration 1500: Loss = -11032.302734375
Iteration 1600: Loss = -11028.6787109375
Iteration 1700: Loss = -11025.615234375
Iteration 1800: Loss = -11022.998046875
Iteration 1900: Loss = -11020.7490234375
Iteration 2000: Loss = -11018.79296875
Iteration 2100: Loss = -11017.0869140625
Iteration 2200: Loss = -11015.5859375
Iteration 2300: Loss = -11014.2607421875
Iteration 2400: Loss = -11013.083984375
Iteration 2500: Loss = -11012.0341796875
Iteration 2600: Loss = -11011.0947265625
Iteration 2700: Loss = -11010.2509765625
Iteration 2800: Loss = -11009.490234375
Iteration 2900: Loss = -11008.802734375
Iteration 3000: Loss = -11008.1796875
Iteration 3100: Loss = -11007.6123046875
Iteration 3200: Loss = -11007.09375
Iteration 3300: Loss = -11006.6201171875
Iteration 3400: Loss = -11006.189453125
Iteration 3500: Loss = -11005.791015625
Iteration 3600: Loss = -11005.4287109375
Iteration 3700: Loss = -11005.0947265625
Iteration 3800: Loss = -11004.783203125
Iteration 3900: Loss = -11004.4970703125
Iteration 4000: Loss = -11004.2333984375
Iteration 4100: Loss = -11003.9892578125
Iteration 4200: Loss = -11003.763671875
Iteration 4300: Loss = -11003.552734375
Iteration 4400: Loss = -11003.359375
Iteration 4500: Loss = -11003.1767578125
Iteration 4600: Loss = -11003.009765625
Iteration 4700: Loss = -11002.8525390625
Iteration 4800: Loss = -11002.7080078125
Iteration 4900: Loss = -11002.5712890625
Iteration 5000: Loss = -11002.447265625
Iteration 5100: Loss = -11002.328125
Iteration 5200: Loss = -11002.21875
Iteration 5300: Loss = -11002.1171875
Iteration 5400: Loss = -11002.021484375
Iteration 5500: Loss = -11001.931640625
Iteration 5600: Loss = -11001.8505859375
Iteration 5700: Loss = -11001.771484375
Iteration 5800: Loss = -11001.69921875
Iteration 5900: Loss = -11001.630859375
Iteration 6000: Loss = -11001.568359375
Iteration 6100: Loss = -11001.5068359375
Iteration 6200: Loss = -11001.44921875
Iteration 6300: Loss = -11001.3955078125
Iteration 6400: Loss = -11001.3486328125
Iteration 6500: Loss = -11001.2998046875
Iteration 6600: Loss = -11001.255859375
Iteration 6700: Loss = -11001.2119140625
Iteration 6800: Loss = -11001.171875
Iteration 6900: Loss = -11001.1337890625
Iteration 7000: Loss = -11001.1005859375
Iteration 7100: Loss = -11001.06640625
Iteration 7200: Loss = -11001.0341796875
Iteration 7300: Loss = -11001.00390625
Iteration 7400: Loss = -11000.9755859375
Iteration 7500: Loss = -11000.947265625
Iteration 7600: Loss = -11000.9248046875
Iteration 7700: Loss = -11000.8994140625
Iteration 7800: Loss = -11000.8759765625
Iteration 7900: Loss = -11000.85546875
Iteration 8000: Loss = -11000.833984375
Iteration 8100: Loss = -11000.8134765625
Iteration 8200: Loss = -11000.794921875
Iteration 8300: Loss = -11000.77734375
Iteration 8400: Loss = -11000.76171875
Iteration 8500: Loss = -11000.7451171875
Iteration 8600: Loss = -11000.7294921875
Iteration 8700: Loss = -11000.7158203125
Iteration 8800: Loss = -11000.701171875
Iteration 8900: Loss = -11000.6884765625
Iteration 9000: Loss = -11000.67578125
Iteration 9100: Loss = -11000.6630859375
Iteration 9200: Loss = -11000.65234375
Iteration 9300: Loss = -11000.642578125
Iteration 9400: Loss = -11000.630859375
Iteration 9500: Loss = -11000.623046875
Iteration 9600: Loss = -11000.6162109375
Iteration 9700: Loss = -11000.6044921875
Iteration 9800: Loss = -11000.5947265625
Iteration 9900: Loss = -11000.5869140625
Iteration 10000: Loss = -11000.578125
Iteration 10100: Loss = -11000.5693359375
Iteration 10200: Loss = -11000.5625
Iteration 10300: Loss = -11000.5556640625
Iteration 10400: Loss = -11000.5498046875
Iteration 10500: Loss = -11000.54296875
Iteration 10600: Loss = -11000.5361328125
Iteration 10700: Loss = -11000.529296875
Iteration 10800: Loss = -11000.5244140625
Iteration 10900: Loss = -11000.51953125
Iteration 11000: Loss = -11000.513671875
Iteration 11100: Loss = -11000.5078125
Iteration 11200: Loss = -11000.50390625
Iteration 11300: Loss = -11000.4990234375
Iteration 11400: Loss = -11000.4951171875
Iteration 11500: Loss = -11000.4912109375
Iteration 11600: Loss = -11000.4892578125
Iteration 11700: Loss = -11000.486328125
Iteration 11800: Loss = -11000.48046875
Iteration 11900: Loss = -11000.4794921875
Iteration 12000: Loss = -11000.478515625
Iteration 12100: Loss = -11000.4736328125
Iteration 12200: Loss = -11000.47265625
Iteration 12300: Loss = -11000.46875
Iteration 12400: Loss = -11000.4677734375
Iteration 12500: Loss = -11000.4658203125
Iteration 12600: Loss = -11000.4638671875
Iteration 12700: Loss = -11000.462890625
Iteration 12800: Loss = -11000.4599609375
Iteration 12900: Loss = -11000.458984375
Iteration 13000: Loss = -11000.4580078125
Iteration 13100: Loss = -11000.4560546875
Iteration 13200: Loss = -11000.455078125
Iteration 13300: Loss = -11000.4541015625
Iteration 13400: Loss = -11000.4521484375
Iteration 13500: Loss = -11000.451171875
Iteration 13600: Loss = -11000.451171875
Iteration 13700: Loss = -11000.44921875
Iteration 13800: Loss = -11000.4482421875
Iteration 13900: Loss = -11000.447265625
Iteration 14000: Loss = -11000.4462890625
Iteration 14100: Loss = -11000.4443359375
Iteration 14200: Loss = -11000.4443359375
Iteration 14300: Loss = -11000.4423828125
Iteration 14400: Loss = -11000.443359375
1
Iteration 14500: Loss = -11000.4423828125
Iteration 14600: Loss = -11000.44140625
Iteration 14700: Loss = -11000.44140625
Iteration 14800: Loss = -11000.4384765625
Iteration 14900: Loss = -11000.4404296875
1
Iteration 15000: Loss = -11000.439453125
2
Iteration 15100: Loss = -11000.4375
Iteration 15200: Loss = -11000.4365234375
Iteration 15300: Loss = -11000.4365234375
Iteration 15400: Loss = -11000.3955078125
Iteration 15500: Loss = -11000.00390625
Iteration 15600: Loss = -10999.9013671875
Iteration 15700: Loss = -10999.73046875
Iteration 15800: Loss = -10999.4638671875
Iteration 15900: Loss = -10999.3193359375
Iteration 16000: Loss = -10999.2919921875
Iteration 16100: Loss = -10999.283203125
Iteration 16200: Loss = -10999.2783203125
Iteration 16300: Loss = -10999.275390625
Iteration 16400: Loss = -10999.271484375
Iteration 16500: Loss = -10999.2705078125
Iteration 16600: Loss = -10999.2255859375
Iteration 16700: Loss = -10999.2197265625
Iteration 16800: Loss = -10999.1171875
Iteration 16900: Loss = -10999.1123046875
Iteration 17000: Loss = -10999.03515625
Iteration 17100: Loss = -10998.9267578125
Iteration 17200: Loss = -10998.91015625
Iteration 17300: Loss = -10998.8037109375
Iteration 17400: Loss = -10998.80078125
Iteration 17500: Loss = -10998.78515625
Iteration 17600: Loss = -10998.7431640625
Iteration 17700: Loss = -10998.703125
Iteration 17800: Loss = -10998.669921875
Iteration 17900: Loss = -10998.6455078125
Iteration 18000: Loss = -10998.640625
Iteration 18100: Loss = -10998.6259765625
Iteration 18200: Loss = -10998.5791015625
Iteration 18300: Loss = -10998.548828125
Iteration 18400: Loss = -10998.517578125
Iteration 18500: Loss = -10998.51171875
Iteration 18600: Loss = -10998.498046875
Iteration 18700: Loss = -10998.4443359375
Iteration 18800: Loss = -10998.3798828125
Iteration 18900: Loss = -10998.3427734375
Iteration 19000: Loss = -10998.20703125
Iteration 19100: Loss = -10997.779296875
Iteration 19200: Loss = -10979.3408203125
Iteration 19300: Loss = -10959.8896484375
Iteration 19400: Loss = -10952.662109375
Iteration 19500: Loss = -10945.248046875
Iteration 19600: Loss = -10939.46875
Iteration 19700: Loss = -10931.240234375
Iteration 19800: Loss = -10930.8369140625
Iteration 19900: Loss = -10930.771484375
Iteration 20000: Loss = -10930.271484375
Iteration 20100: Loss = -10930.248046875
Iteration 20200: Loss = -10930.1708984375
Iteration 20300: Loss = -10930.166015625
Iteration 20400: Loss = -10930.1044921875
Iteration 20500: Loss = -10930.0986328125
Iteration 20600: Loss = -10930.0986328125
Iteration 20700: Loss = -10930.095703125
Iteration 20800: Loss = -10930.09375
Iteration 20900: Loss = -10930.09375
Iteration 21000: Loss = -10930.0927734375
Iteration 21100: Loss = -10930.091796875
Iteration 21200: Loss = -10930.0908203125
Iteration 21300: Loss = -10930.0908203125
Iteration 21400: Loss = -10930.08984375
Iteration 21500: Loss = -10930.08984375
Iteration 21600: Loss = -10930.0888671875
Iteration 21700: Loss = -10930.0888671875
Iteration 21800: Loss = -10930.08984375
1
Iteration 21900: Loss = -10930.087890625
Iteration 22000: Loss = -10930.087890625
Iteration 22100: Loss = -10930.087890625
Iteration 22200: Loss = -10930.087890625
Iteration 22300: Loss = -10930.087890625
Iteration 22400: Loss = -10930.0869140625
Iteration 22500: Loss = -10930.0859375
Iteration 22600: Loss = -10930.0869140625
1
Iteration 22700: Loss = -10930.087890625
2
Iteration 22800: Loss = -10930.0869140625
3
Iteration 22900: Loss = -10930.087890625
4
Iteration 23000: Loss = -10930.0869140625
5
Iteration 23100: Loss = -10930.087890625
6
Iteration 23200: Loss = -10930.0869140625
7
Iteration 23300: Loss = -10930.0869140625
8
Iteration 23400: Loss = -10930.0859375
Iteration 23500: Loss = -10930.0859375
Iteration 23600: Loss = -10930.0859375
Iteration 23700: Loss = -10930.0869140625
1
Iteration 23800: Loss = -10930.0869140625
2
Iteration 23900: Loss = -10930.0859375
Iteration 24000: Loss = -10930.087890625
1
Iteration 24100: Loss = -10930.0869140625
2
Iteration 24200: Loss = -10930.0859375
Iteration 24300: Loss = -10930.0859375
Iteration 24400: Loss = -10930.0859375
Iteration 24500: Loss = -10930.0859375
Iteration 24600: Loss = -10930.0859375
Iteration 24700: Loss = -10930.0869140625
1
Iteration 24800: Loss = -10930.087890625
2
Iteration 24900: Loss = -10930.0859375
Iteration 25000: Loss = -10930.0849609375
Iteration 25100: Loss = -10930.0859375
1
Iteration 25200: Loss = -10930.0849609375
Iteration 25300: Loss = -10930.083984375
Iteration 25400: Loss = -10930.078125
Iteration 25500: Loss = -10929.9013671875
Iteration 25600: Loss = -10926.6982421875
Iteration 25700: Loss = -10925.6572265625
Iteration 25800: Loss = -10923.6162109375
Iteration 25900: Loss = -10921.388671875
Iteration 26000: Loss = -10915.1943359375
Iteration 26100: Loss = -10914.2236328125
Iteration 26200: Loss = -10914.2158203125
Iteration 26300: Loss = -10914.1982421875
Iteration 26400: Loss = -10909.298828125
Iteration 26500: Loss = -10909.068359375
Iteration 26600: Loss = -10907.9951171875
Iteration 26700: Loss = -10907.9873046875
Iteration 26800: Loss = -10907.8896484375
Iteration 26900: Loss = -10902.8447265625
Iteration 27000: Loss = -10902.71875
Iteration 27100: Loss = -10902.7060546875
Iteration 27200: Loss = -10902.7001953125
Iteration 27300: Loss = -10902.498046875
Iteration 27400: Loss = -10902.3564453125
Iteration 27500: Loss = -10902.3515625
Iteration 27600: Loss = -10902.3505859375
Iteration 27700: Loss = -10902.349609375
Iteration 27800: Loss = -10902.3505859375
1
Iteration 27900: Loss = -10902.349609375
Iteration 28000: Loss = -10902.34765625
Iteration 28100: Loss = -10902.34765625
Iteration 28200: Loss = -10902.34765625
Iteration 28300: Loss = -10902.3466796875
Iteration 28400: Loss = -10901.85546875
Iteration 28500: Loss = -10901.8076171875
Iteration 28600: Loss = -10899.1953125
Iteration 28700: Loss = -10899.1904296875
Iteration 28800: Loss = -10898.4091796875
Iteration 28900: Loss = -10893.86328125
Iteration 29000: Loss = -10893.78515625
Iteration 29100: Loss = -10893.7744140625
Iteration 29200: Loss = -10893.7666015625
Iteration 29300: Loss = -10893.6875
Iteration 29400: Loss = -10893.685546875
Iteration 29500: Loss = -10893.68359375
Iteration 29600: Loss = -10893.68359375
Iteration 29700: Loss = -10893.68359375
Iteration 29800: Loss = -10893.6826171875
Iteration 29900: Loss = -10893.681640625
pi: tensor([[0.6969, 0.3031],
        [0.1770, 0.8230]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5823, 0.4177], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.1082],
         [0.5724, 0.2364]],

        [[0.0137, 0.1012],
         [0.1196, 0.6126]],

        [[0.7331, 0.0966],
         [0.1705, 0.1071]],

        [[0.3092, 0.0980],
         [0.0162, 0.0068]],

        [[0.2191, 0.1065],
         [0.9730, 0.0462]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 81
Adjusted Rand Index: 0.37832390107760294
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369552685595733
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 92
Adjusted Rand Index: 0.7027008103753108
time is 3
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733903713149096
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8447122004349823
Global Adjusted Rand Index: 0.6392777626361078
Average Adjusted Rand Index: 0.6472165103524758
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23875.509765625
Iteration 100: Loss = -16136.6923828125
Iteration 200: Loss = -11999.05078125
Iteration 300: Loss = -11393.0400390625
Iteration 400: Loss = -11249.2978515625
Iteration 500: Loss = -11196.5771484375
Iteration 600: Loss = -11165.916015625
Iteration 700: Loss = -11144.8115234375
Iteration 800: Loss = -11129.5703125
Iteration 900: Loss = -11117.80859375
Iteration 1000: Loss = -11106.9111328125
Iteration 1100: Loss = -11098.6123046875
Iteration 1200: Loss = -11088.9677734375
Iteration 1300: Loss = -11080.3505859375
Iteration 1400: Loss = -11074.4404296875
Iteration 1500: Loss = -11067.5595703125
Iteration 1600: Loss = -11061.6435546875
Iteration 1700: Loss = -11056.86328125
Iteration 1800: Loss = -11051.5859375
Iteration 1900: Loss = -11044.7568359375
Iteration 2000: Loss = -11040.6767578125
Iteration 2100: Loss = -11037.61328125
Iteration 2200: Loss = -11034.8642578125
Iteration 2300: Loss = -11032.154296875
Iteration 2400: Loss = -11030.099609375
Iteration 2500: Loss = -11028.1025390625
Iteration 2600: Loss = -11025.4931640625
Iteration 2700: Loss = -11022.4716796875
Iteration 2800: Loss = -11021.138671875
Iteration 2900: Loss = -11020.3095703125
Iteration 3000: Loss = -11018.615234375
Iteration 3100: Loss = -11016.6845703125
Iteration 3200: Loss = -11016.00390625
Iteration 3300: Loss = -11015.5283203125
Iteration 3400: Loss = -11015.1513671875
Iteration 3500: Loss = -11014.8447265625
Iteration 3600: Loss = -11014.5869140625
Iteration 3700: Loss = -11014.36328125
Iteration 3800: Loss = -11014.1728515625
Iteration 3900: Loss = -11014.00390625
Iteration 4000: Loss = -11013.8466796875
Iteration 4100: Loss = -11013.681640625
Iteration 4200: Loss = -11013.4560546875
Iteration 4300: Loss = -11013.283203125
Iteration 4400: Loss = -11013.1376953125
Iteration 4500: Loss = -11009.0009765625
Iteration 4600: Loss = -11004.873046875
Iteration 4700: Loss = -11004.3544921875
Iteration 4800: Loss = -11002.8447265625
Iteration 4900: Loss = -11002.400390625
Iteration 5000: Loss = -11002.2373046875
Iteration 5100: Loss = -11002.1201171875
Iteration 5200: Loss = -11002.021484375
Iteration 5300: Loss = -11001.939453125
Iteration 5400: Loss = -11001.8642578125
Iteration 5500: Loss = -11001.796875
Iteration 5600: Loss = -11001.7353515625
Iteration 5700: Loss = -11001.677734375
Iteration 5800: Loss = -11001.625
Iteration 5900: Loss = -11001.57421875
Iteration 6000: Loss = -11001.5302734375
Iteration 6100: Loss = -11001.48828125
Iteration 6200: Loss = -11001.451171875
Iteration 6300: Loss = -11001.4169921875
Iteration 6400: Loss = -11001.38671875
Iteration 6500: Loss = -11001.3583984375
Iteration 6600: Loss = -11001.33203125
Iteration 6700: Loss = -11001.3056640625
Iteration 6800: Loss = -11001.283203125
Iteration 6900: Loss = -11001.263671875
Iteration 7000: Loss = -11001.244140625
Iteration 7100: Loss = -11001.2255859375
Iteration 7200: Loss = -11001.2099609375
Iteration 7300: Loss = -11001.19140625
Iteration 7400: Loss = -11001.1787109375
Iteration 7500: Loss = -11001.1640625
Iteration 7600: Loss = -11001.1513671875
Iteration 7700: Loss = -11001.1376953125
Iteration 7800: Loss = -11001.126953125
Iteration 7900: Loss = -11001.1162109375
Iteration 8000: Loss = -11001.10546875
Iteration 8100: Loss = -11001.095703125
Iteration 8200: Loss = -11001.0869140625
Iteration 8300: Loss = -11001.0751953125
Iteration 8400: Loss = -11001.0625
Iteration 8500: Loss = -11000.982421875
Iteration 8600: Loss = -11000.8828125
Iteration 8700: Loss = -11000.8662109375
Iteration 8800: Loss = -11000.857421875
Iteration 8900: Loss = -11000.8486328125
Iteration 9000: Loss = -11000.8427734375
Iteration 9100: Loss = -11000.8359375
Iteration 9200: Loss = -11000.828125
Iteration 9300: Loss = -11000.8232421875
Iteration 9400: Loss = -11000.81640625
Iteration 9500: Loss = -11000.8115234375
Iteration 9600: Loss = -11000.8056640625
Iteration 9700: Loss = -11000.798828125
Iteration 9800: Loss = -11000.7451171875
Iteration 9900: Loss = -11000.283203125
Iteration 10000: Loss = -11000.2734375
Iteration 10100: Loss = -11000.267578125
Iteration 10200: Loss = -11000.263671875
Iteration 10300: Loss = -11000.2607421875
Iteration 10400: Loss = -11000.2587890625
Iteration 10500: Loss = -11000.255859375
Iteration 10600: Loss = -11000.2529296875
Iteration 10700: Loss = -11000.2529296875
Iteration 10800: Loss = -11000.25
Iteration 10900: Loss = -11000.248046875
Iteration 11000: Loss = -11000.2451171875
Iteration 11100: Loss = -11000.2431640625
Iteration 11200: Loss = -11000.2421875
Iteration 11300: Loss = -11000.240234375
Iteration 11400: Loss = -11000.2392578125
Iteration 11500: Loss = -11000.23828125
Iteration 11600: Loss = -11000.236328125
Iteration 11700: Loss = -11000.2353515625
Iteration 11800: Loss = -11000.2333984375
Iteration 11900: Loss = -11000.232421875
Iteration 12000: Loss = -11000.2294921875
Iteration 12100: Loss = -11000.228515625
Iteration 12200: Loss = -11000.228515625
Iteration 12300: Loss = -11000.2265625
Iteration 12400: Loss = -11000.2255859375
Iteration 12500: Loss = -11000.2255859375
Iteration 12600: Loss = -11000.224609375
Iteration 12700: Loss = -11000.22265625
Iteration 12800: Loss = -11000.2216796875
Iteration 12900: Loss = -11000.2216796875
Iteration 13000: Loss = -11000.2197265625
Iteration 13100: Loss = -11000.2197265625
Iteration 13200: Loss = -11000.2197265625
Iteration 13300: Loss = -11000.21875
Iteration 13400: Loss = -11000.2197265625
1
Iteration 13500: Loss = -11000.21875
Iteration 13600: Loss = -11000.2177734375
Iteration 13700: Loss = -11000.216796875
Iteration 13800: Loss = -11000.216796875
Iteration 13900: Loss = -11000.2158203125
Iteration 14000: Loss = -11000.2158203125
Iteration 14100: Loss = -11000.216796875
1
Iteration 14200: Loss = -11000.2138671875
Iteration 14300: Loss = -11000.2138671875
Iteration 14400: Loss = -11000.21484375
1
Iteration 14500: Loss = -11000.2158203125
2
Iteration 14600: Loss = -11000.2138671875
Iteration 14700: Loss = -11000.2158203125
1
Iteration 14800: Loss = -11000.2138671875
Iteration 14900: Loss = -11000.212890625
Iteration 15000: Loss = -11000.212890625
Iteration 15100: Loss = -11000.2138671875
1
Iteration 15200: Loss = -11000.2138671875
2
Iteration 15300: Loss = -11000.212890625
Iteration 15400: Loss = -11000.2119140625
Iteration 15500: Loss = -11000.2119140625
Iteration 15600: Loss = -11000.2109375
Iteration 15700: Loss = -11000.2099609375
Iteration 15800: Loss = -11000.2109375
1
Iteration 15900: Loss = -11000.2119140625
2
Iteration 16000: Loss = -11000.2119140625
3
Iteration 16100: Loss = -11000.2109375
4
Iteration 16200: Loss = -11000.2109375
5
Iteration 16300: Loss = -11000.2109375
6
Iteration 16400: Loss = -11000.2109375
7
Iteration 16500: Loss = -11000.2109375
8
Iteration 16600: Loss = -11000.2109375
9
Iteration 16700: Loss = -11000.2109375
10
Iteration 16800: Loss = -11000.2099609375
Iteration 16900: Loss = -11000.2109375
1
Iteration 17000: Loss = -11000.208984375
Iteration 17100: Loss = -11000.208984375
Iteration 17200: Loss = -11000.2099609375
1
Iteration 17300: Loss = -11000.208984375
Iteration 17400: Loss = -11000.208984375
Iteration 17500: Loss = -11000.2099609375
1
Iteration 17600: Loss = -11000.2119140625
2
Iteration 17700: Loss = -11000.2099609375
3
Iteration 17800: Loss = -11000.208984375
Iteration 17900: Loss = -11000.208984375
Iteration 18000: Loss = -11000.2099609375
1
Iteration 18100: Loss = -11000.2080078125
Iteration 18200: Loss = -11000.208984375
1
Iteration 18300: Loss = -11000.2099609375
2
Iteration 18400: Loss = -11000.208984375
3
Iteration 18500: Loss = -11000.208984375
4
Iteration 18600: Loss = -11000.2099609375
5
Iteration 18700: Loss = -11000.2099609375
6
Iteration 18800: Loss = -11000.2109375
7
Iteration 18900: Loss = -11000.2099609375
8
Iteration 19000: Loss = -11000.20703125
Iteration 19100: Loss = -11000.2080078125
1
Iteration 19200: Loss = -11000.208984375
2
Iteration 19300: Loss = -11000.208984375
3
Iteration 19400: Loss = -11000.208984375
4
Iteration 19500: Loss = -11000.208984375
5
Iteration 19600: Loss = -11000.20703125
Iteration 19700: Loss = -11000.2080078125
1
Iteration 19800: Loss = -11000.20703125
Iteration 19900: Loss = -11000.208984375
1
Iteration 20000: Loss = -11000.20703125
Iteration 20100: Loss = -11000.2080078125
1
Iteration 20200: Loss = -11000.208984375
2
Iteration 20300: Loss = -11000.208984375
3
Iteration 20400: Loss = -11000.208984375
4
Iteration 20500: Loss = -11000.20703125
Iteration 20600: Loss = -11000.2080078125
1
Iteration 20700: Loss = -11000.208984375
2
Iteration 20800: Loss = -11000.2080078125
3
Iteration 20900: Loss = -11000.2080078125
4
Iteration 21000: Loss = -11000.2080078125
5
Iteration 21100: Loss = -11000.2080078125
6
Iteration 21200: Loss = -11000.2080078125
7
Iteration 21300: Loss = -11000.2080078125
8
Iteration 21400: Loss = -11000.208984375
9
Iteration 21500: Loss = -11000.2080078125
10
Iteration 21600: Loss = -11000.208984375
11
Iteration 21700: Loss = -11000.208984375
12
Iteration 21800: Loss = -11000.208984375
13
Iteration 21900: Loss = -11000.2080078125
14
Iteration 22000: Loss = -11000.20703125
Iteration 22100: Loss = -11000.2080078125
1
Iteration 22200: Loss = -11000.20703125
Iteration 22300: Loss = -11000.2080078125
1
Iteration 22400: Loss = -11000.20703125
Iteration 22500: Loss = -11000.20703125
Iteration 22600: Loss = -11000.2060546875
Iteration 22700: Loss = -11000.1884765625
Iteration 22800: Loss = -11000.1884765625
Iteration 22900: Loss = -11000.189453125
1
Iteration 23000: Loss = -11000.1875
Iteration 23100: Loss = -11000.1875
Iteration 23200: Loss = -11000.1884765625
1
Iteration 23300: Loss = -11000.1884765625
2
Iteration 23400: Loss = -11000.1884765625
3
Iteration 23500: Loss = -10999.4794921875
Iteration 23600: Loss = -10999.2314453125
Iteration 23700: Loss = -10999.2099609375
Iteration 23800: Loss = -10999.2109375
1
Iteration 23900: Loss = -10999.1640625
Iteration 24000: Loss = -10999.1640625
Iteration 24100: Loss = -10999.0380859375
Iteration 24200: Loss = -10999.03125
Iteration 24300: Loss = -10999.03125
Iteration 24400: Loss = -10999.025390625
Iteration 24500: Loss = -10999.0107421875
Iteration 24600: Loss = -10999.0087890625
Iteration 24700: Loss = -10999.0029296875
Iteration 24800: Loss = -10999.001953125
Iteration 24900: Loss = -10999.0029296875
1
Iteration 25000: Loss = -10999.00390625
2
Iteration 25100: Loss = -10999.0029296875
3
Iteration 25200: Loss = -10999.00390625
4
Iteration 25300: Loss = -10999.00390625
5
Iteration 25400: Loss = -10999.00390625
6
Iteration 25500: Loss = -10999.001953125
Iteration 25600: Loss = -10999.0029296875
1
Iteration 25700: Loss = -10999.00390625
2
Iteration 25800: Loss = -10999.0009765625
Iteration 25900: Loss = -10998.998046875
Iteration 26000: Loss = -10998.998046875
Iteration 26100: Loss = -10998.9990234375
1
Iteration 26200: Loss = -10999.0
2
Iteration 26300: Loss = -10998.9990234375
3
Iteration 26400: Loss = -10998.9990234375
4
Iteration 26500: Loss = -10998.994140625
Iteration 26600: Loss = -10998.994140625
Iteration 26700: Loss = -10998.994140625
Iteration 26800: Loss = -10998.9921875
Iteration 26900: Loss = -10998.9853515625
Iteration 27000: Loss = -10998.9853515625
Iteration 27100: Loss = -10998.986328125
1
Iteration 27200: Loss = -10998.9833984375
Iteration 27300: Loss = -10998.984375
1
Iteration 27400: Loss = -10998.984375
2
Iteration 27500: Loss = -10998.9853515625
3
Iteration 27600: Loss = -10998.9853515625
4
Iteration 27700: Loss = -10998.9853515625
5
Iteration 27800: Loss = -10998.9853515625
6
Iteration 27900: Loss = -10998.9853515625
7
Iteration 28000: Loss = -10998.986328125
8
Iteration 28100: Loss = -10998.986328125
9
Iteration 28200: Loss = -10998.984375
10
Iteration 28300: Loss = -10998.984375
11
Iteration 28400: Loss = -10998.984375
12
Iteration 28500: Loss = -10998.984375
13
Iteration 28600: Loss = -10998.984375
14
Iteration 28700: Loss = -10998.9853515625
15
Stopping early at iteration 28700 due to no improvement.
pi: tensor([[1.8676e-06, 1.0000e+00],
        [2.7624e-02, 9.7238e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.3424e-06, 9.9999e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3264, 0.1588],
         [0.8198, 0.1599]],

        [[0.0236, 0.2278],
         [0.9164, 0.9915]],

        [[0.9897, 0.2322],
         [0.2975, 0.0158]],

        [[0.9810, 0.2686],
         [0.0298, 0.3957]],

        [[0.5516, 0.2186],
         [0.8712, 0.0120]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0010030369507932977
Average Adjusted Rand Index: -0.0004272969042054721
[0.6392777626361078, -0.0010030369507932977] [0.6472165103524758, -0.0004272969042054721] [10893.681640625, 10998.9853515625]
-------------------------------------
This iteration is 84
True Objective function: Loss = -10858.516436123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35062.23828125
Iteration 100: Loss = -22786.841796875
Iteration 200: Loss = -13565.0986328125
Iteration 300: Loss = -11952.072265625
Iteration 400: Loss = -11539.80859375
Iteration 500: Loss = -11383.8388671875
Iteration 600: Loss = -11313.7099609375
Iteration 700: Loss = -11277.2333984375
Iteration 800: Loss = -11256.341796875
Iteration 900: Loss = -11241.6962890625
Iteration 1000: Loss = -11228.232421875
Iteration 1100: Loss = -11217.5498046875
Iteration 1200: Loss = -11211.01953125
Iteration 1300: Loss = -11205.056640625
Iteration 1400: Loss = -11199.744140625
Iteration 1500: Loss = -11193.087890625
Iteration 1600: Loss = -11188.5224609375
Iteration 1700: Loss = -11183.92578125
Iteration 1800: Loss = -11176.5986328125
Iteration 1900: Loss = -11168.216796875
Iteration 2000: Loss = -11162.1884765625
Iteration 2100: Loss = -11155.1953125
Iteration 2200: Loss = -11149.74609375
Iteration 2300: Loss = -11144.734375
Iteration 2400: Loss = -11139.689453125
Iteration 2500: Loss = -11133.666015625
Iteration 2600: Loss = -11126.94921875
Iteration 2700: Loss = -11119.591796875
Iteration 2800: Loss = -11108.0048828125
Iteration 2900: Loss = -11100.587890625
Iteration 3000: Loss = -11095.294921875
Iteration 3100: Loss = -11090.6875
Iteration 3200: Loss = -11084.791015625
Iteration 3300: Loss = -11081.3193359375
Iteration 3400: Loss = -11079.0595703125
Iteration 3500: Loss = -11077.123046875
Iteration 3600: Loss = -11075.7109375
Iteration 3700: Loss = -11074.4052734375
Iteration 3800: Loss = -11073.1787109375
Iteration 3900: Loss = -11071.9267578125
Iteration 4000: Loss = -11071.037109375
Iteration 4100: Loss = -11070.328125
Iteration 4200: Loss = -11069.5751953125
Iteration 4300: Loss = -11068.7373046875
Iteration 4400: Loss = -11066.1845703125
Iteration 4500: Loss = -11065.4775390625
Iteration 4600: Loss = -11065.017578125
Iteration 4700: Loss = -11064.6748046875
Iteration 4800: Loss = -11064.3935546875
Iteration 4900: Loss = -11064.146484375
Iteration 5000: Loss = -11063.546875
Iteration 5100: Loss = -11063.306640625
Iteration 5200: Loss = -11063.1103515625
Iteration 5300: Loss = -11062.896484375
Iteration 5400: Loss = -11062.646484375
Iteration 5500: Loss = -11062.412109375
Iteration 5600: Loss = -11062.1884765625
Iteration 5700: Loss = -11061.775390625
Iteration 5800: Loss = -11061.5888671875
Iteration 5900: Loss = -11061.462890625
Iteration 6000: Loss = -11061.3583984375
Iteration 6100: Loss = -11061.2646484375
Iteration 6200: Loss = -11061.173828125
Iteration 6300: Loss = -11060.6669921875
Iteration 6400: Loss = -11060.5927734375
Iteration 6500: Loss = -11060.52734375
Iteration 6600: Loss = -11060.46484375
Iteration 6700: Loss = -11060.408203125
Iteration 6800: Loss = -11060.3544921875
Iteration 6900: Loss = -11060.306640625
Iteration 7000: Loss = -11060.259765625
Iteration 7100: Loss = -11060.2158203125
Iteration 7200: Loss = -11060.17578125
Iteration 7300: Loss = -11060.13671875
Iteration 7400: Loss = -11060.1015625
Iteration 7500: Loss = -11060.0673828125
Iteration 7600: Loss = -11060.0361328125
Iteration 7700: Loss = -11059.9482421875
Iteration 7800: Loss = -11059.560546875
Iteration 7900: Loss = -11059.53125
Iteration 8000: Loss = -11059.505859375
Iteration 8100: Loss = -11059.482421875
Iteration 8200: Loss = -11059.4599609375
Iteration 8300: Loss = -11059.435546875
Iteration 8400: Loss = -11058.9990234375
Iteration 8500: Loss = -11058.9658203125
Iteration 8600: Loss = -11058.947265625
Iteration 8700: Loss = -11058.9296875
Iteration 8800: Loss = -11058.9130859375
Iteration 8900: Loss = -11058.896484375
Iteration 9000: Loss = -11058.8818359375
Iteration 9100: Loss = -11058.8681640625
Iteration 9200: Loss = -11058.8564453125
Iteration 9300: Loss = -11058.8427734375
Iteration 9400: Loss = -11058.8310546875
Iteration 9500: Loss = -11058.8212890625
Iteration 9600: Loss = -11058.810546875
Iteration 9700: Loss = -11058.7998046875
Iteration 9800: Loss = -11058.791015625
Iteration 9900: Loss = -11058.783203125
Iteration 10000: Loss = -11058.7734375
Iteration 10100: Loss = -11058.765625
Iteration 10200: Loss = -11058.7587890625
Iteration 10300: Loss = -11058.7509765625
Iteration 10400: Loss = -11058.7451171875
Iteration 10500: Loss = -11058.73828125
Iteration 10600: Loss = -11058.732421875
Iteration 10700: Loss = -11058.7265625
Iteration 10800: Loss = -11058.7197265625
Iteration 10900: Loss = -11058.716796875
Iteration 11000: Loss = -11058.7109375
Iteration 11100: Loss = -11058.7060546875
Iteration 11200: Loss = -11058.7001953125
Iteration 11300: Loss = -11058.697265625
Iteration 11400: Loss = -11058.693359375
Iteration 11500: Loss = -11058.6904296875
Iteration 11600: Loss = -11058.685546875
Iteration 11700: Loss = -11058.6826171875
Iteration 11800: Loss = -11058.6787109375
Iteration 11900: Loss = -11058.67578125
Iteration 12000: Loss = -11058.673828125
Iteration 12100: Loss = -11058.669921875
Iteration 12200: Loss = -11058.66796875
Iteration 12300: Loss = -11058.6650390625
Iteration 12400: Loss = -11058.6630859375
Iteration 12500: Loss = -11058.66015625
Iteration 12600: Loss = -11058.658203125
Iteration 12700: Loss = -11058.65625
Iteration 12800: Loss = -11058.654296875
Iteration 12900: Loss = -11058.65234375
Iteration 13000: Loss = -11058.6513671875
Iteration 13100: Loss = -11058.6494140625
Iteration 13200: Loss = -11058.6474609375
Iteration 13300: Loss = -11058.64453125
Iteration 13400: Loss = -11058.6435546875
Iteration 13500: Loss = -11058.642578125
Iteration 13600: Loss = -11058.6416015625
Iteration 13700: Loss = -11058.6396484375
Iteration 13800: Loss = -11058.638671875
Iteration 13900: Loss = -11058.6376953125
Iteration 14000: Loss = -11058.63671875
Iteration 14100: Loss = -11058.63671875
Iteration 14200: Loss = -11058.634765625
Iteration 14300: Loss = -11058.6328125
Iteration 14400: Loss = -11058.6328125
Iteration 14500: Loss = -11058.6318359375
Iteration 14600: Loss = -11058.630859375
Iteration 14700: Loss = -11058.630859375
Iteration 14800: Loss = -11058.6318359375
1
Iteration 14900: Loss = -11058.62890625
Iteration 15000: Loss = -11058.6298828125
1
Iteration 15100: Loss = -11058.62890625
Iteration 15200: Loss = -11058.6279296875
Iteration 15300: Loss = -11058.626953125
Iteration 15400: Loss = -11058.626953125
Iteration 15500: Loss = -11058.6259765625
Iteration 15600: Loss = -11058.625
Iteration 15700: Loss = -11058.625
Iteration 15800: Loss = -11058.625
Iteration 15900: Loss = -11058.6240234375
Iteration 16000: Loss = -11058.623046875
Iteration 16100: Loss = -11058.6240234375
1
Iteration 16200: Loss = -11058.6240234375
2
Iteration 16300: Loss = -11058.6220703125
Iteration 16400: Loss = -11058.6240234375
1
Iteration 16500: Loss = -11058.623046875
2
Iteration 16600: Loss = -11058.6220703125
Iteration 16700: Loss = -11058.6220703125
Iteration 16800: Loss = -11058.6220703125
Iteration 16900: Loss = -11058.62109375
Iteration 17000: Loss = -11058.62109375
Iteration 17100: Loss = -11058.6220703125
1
Iteration 17200: Loss = -11058.6201171875
Iteration 17300: Loss = -11058.62109375
1
Iteration 17400: Loss = -11058.62109375
2
Iteration 17500: Loss = -11058.6201171875
Iteration 17600: Loss = -11058.62109375
1
Iteration 17700: Loss = -11058.6201171875
Iteration 17800: Loss = -11058.6171875
Iteration 17900: Loss = -11058.619140625
1
Iteration 18000: Loss = -11058.6181640625
2
Iteration 18100: Loss = -11058.619140625
3
Iteration 18200: Loss = -11058.6201171875
4
Iteration 18300: Loss = -11058.619140625
5
Iteration 18400: Loss = -11058.6181640625
6
Iteration 18500: Loss = -11058.6171875
Iteration 18600: Loss = -11058.6171875
Iteration 18700: Loss = -11058.619140625
1
Iteration 18800: Loss = -11058.6171875
Iteration 18900: Loss = -11058.6171875
Iteration 19000: Loss = -11058.6181640625
1
Iteration 19100: Loss = -11058.619140625
2
Iteration 19200: Loss = -11058.619140625
3
Iteration 19300: Loss = -11058.6171875
Iteration 19400: Loss = -11058.6171875
Iteration 19500: Loss = -11058.6181640625
1
Iteration 19600: Loss = -11058.6171875
Iteration 19700: Loss = -11058.6171875
Iteration 19800: Loss = -11058.6171875
Iteration 19900: Loss = -11058.6171875
Iteration 20000: Loss = -11058.6171875
Iteration 20100: Loss = -11058.6162109375
Iteration 20200: Loss = -11058.6162109375
Iteration 20300: Loss = -11058.6162109375
Iteration 20400: Loss = -11058.6162109375
Iteration 20500: Loss = -11058.6162109375
Iteration 20600: Loss = -11058.6162109375
Iteration 20700: Loss = -11058.6162109375
Iteration 20800: Loss = -11058.6162109375
Iteration 20900: Loss = -11058.6162109375
Iteration 21000: Loss = -11058.6162109375
Iteration 21100: Loss = -11058.6162109375
Iteration 21200: Loss = -11058.6181640625
1
Iteration 21300: Loss = -11058.615234375
Iteration 21400: Loss = -11058.6162109375
1
Iteration 21500: Loss = -11058.6162109375
2
Iteration 21600: Loss = -11058.6162109375
3
Iteration 21700: Loss = -11058.6171875
4
Iteration 21800: Loss = -11058.6162109375
5
Iteration 21900: Loss = -11058.6162109375
6
Iteration 22000: Loss = -11058.6162109375
7
Iteration 22100: Loss = -11058.6162109375
8
Iteration 22200: Loss = -11058.6162109375
9
Iteration 22300: Loss = -11058.6162109375
10
Iteration 22400: Loss = -11058.6162109375
11
Iteration 22500: Loss = -11058.6171875
12
Iteration 22600: Loss = -11058.6171875
13
Iteration 22700: Loss = -11058.6181640625
14
Iteration 22800: Loss = -11058.6162109375
15
Stopping early at iteration 22800 due to no improvement.
pi: tensor([[1.0000e+00, 3.0237e-06],
        [4.2262e-01, 5.7738e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.9385e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1612, 0.1533],
         [0.2831, 0.1604]],

        [[0.0697, 0.7369],
         [0.3691, 0.6302]],

        [[0.0699, 0.1738],
         [0.9128, 0.1318]],

        [[0.9366, 0.5946],
         [0.0401, 0.1290]],

        [[0.9025, 0.1398],
         [0.8290, 0.9872]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017887818129207296
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39760.80078125
Iteration 100: Loss = -22954.48046875
Iteration 200: Loss = -13744.59375
Iteration 300: Loss = -11773.8408203125
Iteration 400: Loss = -11366.3662109375
Iteration 500: Loss = -11238.8857421875
Iteration 600: Loss = -11162.0615234375
Iteration 700: Loss = -11119.5810546875
Iteration 800: Loss = -11089.951171875
Iteration 900: Loss = -11060.4052734375
Iteration 1000: Loss = -11037.4814453125
Iteration 1100: Loss = -11023.0126953125
Iteration 1200: Loss = -11014.8369140625
Iteration 1300: Loss = -11008.138671875
Iteration 1400: Loss = -11002.1279296875
Iteration 1500: Loss = -10994.3623046875
Iteration 1600: Loss = -10989.458984375
Iteration 1700: Loss = -10986.5087890625
Iteration 1800: Loss = -10984.1552734375
Iteration 1900: Loss = -10982.193359375
Iteration 2000: Loss = -10980.5166015625
Iteration 2100: Loss = -10979.068359375
Iteration 2200: Loss = -10977.80078125
Iteration 2300: Loss = -10976.6826171875
Iteration 2400: Loss = -10975.693359375
Iteration 2500: Loss = -10974.80859375
Iteration 2600: Loss = -10974.0126953125
Iteration 2700: Loss = -10973.298828125
Iteration 2800: Loss = -10972.6533203125
Iteration 2900: Loss = -10972.06640625
Iteration 3000: Loss = -10971.533203125
Iteration 3100: Loss = -10971.0439453125
Iteration 3200: Loss = -10970.5966796875
Iteration 3300: Loss = -10970.1865234375
Iteration 3400: Loss = -10969.80859375
Iteration 3500: Loss = -10969.4599609375
Iteration 3600: Loss = -10969.140625
Iteration 3700: Loss = -10968.8427734375
Iteration 3800: Loss = -10968.568359375
Iteration 3900: Loss = -10968.3134765625
Iteration 4000: Loss = -10968.07421875
Iteration 4100: Loss = -10967.8544921875
Iteration 4200: Loss = -10967.6494140625
Iteration 4300: Loss = -10967.4580078125
Iteration 4400: Loss = -10967.2783203125
Iteration 4500: Loss = -10967.111328125
Iteration 4600: Loss = -10966.955078125
Iteration 4700: Loss = -10966.80859375
Iteration 4800: Loss = -10966.669921875
Iteration 4900: Loss = -10966.5400390625
Iteration 5000: Loss = -10966.4208984375
Iteration 5100: Loss = -10966.306640625
Iteration 5200: Loss = -10966.2001953125
Iteration 5300: Loss = -10966.1005859375
Iteration 5400: Loss = -10966.0048828125
Iteration 5500: Loss = -10965.9150390625
Iteration 5600: Loss = -10965.83203125
Iteration 5700: Loss = -10965.7509765625
Iteration 5800: Loss = -10965.6767578125
Iteration 5900: Loss = -10965.6064453125
Iteration 6000: Loss = -10965.541015625
Iteration 6100: Loss = -10965.4775390625
Iteration 6200: Loss = -10965.4169921875
Iteration 6300: Loss = -10965.3623046875
Iteration 6400: Loss = -10965.306640625
Iteration 6500: Loss = -10965.2568359375
Iteration 6600: Loss = -10965.2099609375
Iteration 6700: Loss = -10965.166015625
Iteration 6800: Loss = -10965.12109375
Iteration 6900: Loss = -10965.08203125
Iteration 7000: Loss = -10965.04296875
Iteration 7100: Loss = -10965.0068359375
Iteration 7200: Loss = -10964.9716796875
Iteration 7300: Loss = -10964.9384765625
Iteration 7400: Loss = -10964.9072265625
Iteration 7500: Loss = -10964.8779296875
Iteration 7600: Loss = -10964.8505859375
Iteration 7700: Loss = -10964.82421875
Iteration 7800: Loss = -10964.7998046875
Iteration 7900: Loss = -10964.7763671875
Iteration 8000: Loss = -10964.7529296875
Iteration 8100: Loss = -10964.732421875
Iteration 8200: Loss = -10964.7109375
Iteration 8300: Loss = -10964.6923828125
Iteration 8400: Loss = -10964.6748046875
Iteration 8500: Loss = -10964.65625
Iteration 8600: Loss = -10964.6396484375
Iteration 8700: Loss = -10964.625
Iteration 8800: Loss = -10964.6103515625
Iteration 8900: Loss = -10964.595703125
Iteration 9000: Loss = -10964.5810546875
Iteration 9100: Loss = -10964.5693359375
Iteration 9200: Loss = -10964.5576171875
Iteration 9300: Loss = -10964.5458984375
Iteration 9400: Loss = -10964.5341796875
Iteration 9500: Loss = -10964.5244140625
Iteration 9600: Loss = -10964.5126953125
Iteration 9700: Loss = -10964.5029296875
Iteration 9800: Loss = -10964.494140625
Iteration 9900: Loss = -10964.484375
Iteration 10000: Loss = -10964.4775390625
Iteration 10100: Loss = -10964.470703125
Iteration 10200: Loss = -10964.462890625
Iteration 10300: Loss = -10964.4541015625
Iteration 10400: Loss = -10964.44921875
Iteration 10500: Loss = -10964.443359375
Iteration 10600: Loss = -10964.4365234375
Iteration 10700: Loss = -10964.4296875
Iteration 10800: Loss = -10964.4169921875
Iteration 10900: Loss = -10964.40234375
Iteration 11000: Loss = -10964.3662109375
Iteration 11100: Loss = -10964.2373046875
Iteration 11200: Loss = -10964.1357421875
Iteration 11300: Loss = -10964.099609375
Iteration 11400: Loss = -10964.056640625
Iteration 11500: Loss = -10963.9912109375
Iteration 11600: Loss = -10963.9013671875
Iteration 11700: Loss = -10963.1884765625
Iteration 11800: Loss = -10961.5458984375
Iteration 11900: Loss = -10960.310546875
Iteration 12000: Loss = -10960.1923828125
Iteration 12100: Loss = -10960.125
Iteration 12200: Loss = -10960.08203125
Iteration 12300: Loss = -10960.0517578125
Iteration 12400: Loss = -10960.0263671875
Iteration 12500: Loss = -10960.0068359375
Iteration 12600: Loss = -10959.9921875
Iteration 12700: Loss = -10959.978515625
Iteration 12800: Loss = -10959.9658203125
Iteration 12900: Loss = -10959.9580078125
Iteration 13000: Loss = -10959.94921875
Iteration 13100: Loss = -10959.943359375
Iteration 13200: Loss = -10959.935546875
Iteration 13300: Loss = -10959.9287109375
Iteration 13400: Loss = -10959.9228515625
Iteration 13500: Loss = -10959.91796875
Iteration 13600: Loss = -10959.9140625
Iteration 13700: Loss = -10959.912109375
Iteration 13800: Loss = -10959.9072265625
Iteration 13900: Loss = -10959.904296875
Iteration 14000: Loss = -10959.8994140625
Iteration 14100: Loss = -10959.8974609375
Iteration 14200: Loss = -10959.896484375
Iteration 14300: Loss = -10959.8916015625
Iteration 14400: Loss = -10959.8896484375
Iteration 14500: Loss = -10959.8876953125
Iteration 14600: Loss = -10959.8837890625
Iteration 14700: Loss = -10959.8837890625
Iteration 14800: Loss = -10959.8828125
Iteration 14900: Loss = -10959.8798828125
Iteration 15000: Loss = -10959.876953125
Iteration 15100: Loss = -10959.876953125
Iteration 15200: Loss = -10959.875
Iteration 15300: Loss = -10959.8740234375
Iteration 15400: Loss = -10959.87109375
Iteration 15500: Loss = -10959.8720703125
1
Iteration 15600: Loss = -10959.87109375
Iteration 15700: Loss = -10959.869140625
Iteration 15800: Loss = -10959.869140625
Iteration 15900: Loss = -10959.8671875
Iteration 16000: Loss = -10959.8671875
Iteration 16100: Loss = -10959.8662109375
Iteration 16200: Loss = -10959.8642578125
Iteration 16300: Loss = -10959.8623046875
Iteration 16400: Loss = -10959.8642578125
1
Iteration 16500: Loss = -10959.8623046875
Iteration 16600: Loss = -10959.86328125
1
Iteration 16700: Loss = -10959.861328125
Iteration 16800: Loss = -10959.859375
Iteration 16900: Loss = -10959.8603515625
1
Iteration 17000: Loss = -10959.8583984375
Iteration 17100: Loss = -10959.859375
1
Iteration 17200: Loss = -10959.8583984375
Iteration 17300: Loss = -10959.8583984375
Iteration 17400: Loss = -10959.857421875
Iteration 17500: Loss = -10959.857421875
Iteration 17600: Loss = -10959.859375
1
Iteration 17700: Loss = -10959.8564453125
Iteration 17800: Loss = -10959.85546875
Iteration 17900: Loss = -10959.8564453125
1
Iteration 18000: Loss = -10959.8564453125
2
Iteration 18100: Loss = -10959.85546875
Iteration 18200: Loss = -10959.8544921875
Iteration 18300: Loss = -10959.85546875
1
Iteration 18400: Loss = -10959.853515625
Iteration 18500: Loss = -10959.8544921875
1
Iteration 18600: Loss = -10959.8544921875
2
Iteration 18700: Loss = -10959.8544921875
3
Iteration 18800: Loss = -10959.8544921875
4
Iteration 18900: Loss = -10959.8544921875
5
Iteration 19000: Loss = -10959.853515625
Iteration 19100: Loss = -10959.853515625
Iteration 19200: Loss = -10959.853515625
Iteration 19300: Loss = -10959.8525390625
Iteration 19400: Loss = -10959.8515625
Iteration 19500: Loss = -10959.8515625
Iteration 19600: Loss = -10959.8525390625
1
Iteration 19700: Loss = -10959.8515625
Iteration 19800: Loss = -10959.853515625
1
Iteration 19900: Loss = -10959.853515625
2
Iteration 20000: Loss = -10959.8505859375
Iteration 20100: Loss = -10959.8515625
1
Iteration 20200: Loss = -10959.8515625
2
Iteration 20300: Loss = -10959.8505859375
Iteration 20400: Loss = -10959.8701171875
1
Iteration 20500: Loss = -10959.8505859375
Iteration 20600: Loss = -10959.8525390625
1
Iteration 20700: Loss = -10959.8505859375
Iteration 20800: Loss = -10959.8505859375
Iteration 20900: Loss = -10959.8505859375
Iteration 21000: Loss = -10959.8505859375
Iteration 21100: Loss = -10959.8515625
1
Iteration 21200: Loss = -10959.8515625
2
Iteration 21300: Loss = -10959.853515625
3
Iteration 21400: Loss = -10959.8505859375
Iteration 21500: Loss = -10959.8525390625
1
Iteration 21600: Loss = -10959.8525390625
2
Iteration 21700: Loss = -10959.8525390625
3
Iteration 21800: Loss = -10959.8525390625
4
Iteration 21900: Loss = -10959.8515625
5
Iteration 22000: Loss = -10959.849609375
Iteration 22100: Loss = -10959.849609375
Iteration 22200: Loss = -10959.8662109375
1
Iteration 22300: Loss = -10959.8515625
2
Iteration 22400: Loss = -10959.8505859375
3
Iteration 22500: Loss = -10959.8505859375
4
Iteration 22600: Loss = -10959.8505859375
5
Iteration 22700: Loss = -10959.8505859375
6
Iteration 22800: Loss = -10959.8505859375
7
Iteration 22900: Loss = -10959.8486328125
Iteration 23000: Loss = -10959.8505859375
1
Iteration 23100: Loss = -10959.849609375
2
Iteration 23200: Loss = -10959.8486328125
Iteration 23300: Loss = -10959.84375
Iteration 23400: Loss = -10959.84375
Iteration 23500: Loss = -10959.84375
Iteration 23600: Loss = -10959.84375
Iteration 23700: Loss = -10959.8330078125
Iteration 23800: Loss = -10959.83203125
Iteration 23900: Loss = -10959.8310546875
Iteration 24000: Loss = -10959.8310546875
Iteration 24100: Loss = -10959.8310546875
Iteration 24200: Loss = -10959.83203125
1
Iteration 24300: Loss = -10959.8310546875
Iteration 24400: Loss = -10959.8310546875
Iteration 24500: Loss = -10959.83203125
1
Iteration 24600: Loss = -10959.83203125
2
Iteration 24700: Loss = -10959.83203125
3
Iteration 24800: Loss = -10959.8310546875
Iteration 24900: Loss = -10959.8310546875
Iteration 25000: Loss = -10959.8310546875
Iteration 25100: Loss = -10959.8291015625
Iteration 25200: Loss = -10959.828125
Iteration 25300: Loss = -10959.828125
Iteration 25400: Loss = -10959.8291015625
1
Iteration 25500: Loss = -10959.8291015625
2
Iteration 25600: Loss = -10959.8232421875
Iteration 25700: Loss = -10959.8232421875
Iteration 25800: Loss = -10959.8232421875
Iteration 25900: Loss = -10959.8193359375
Iteration 26000: Loss = -10959.8203125
1
Iteration 26100: Loss = -10959.8203125
2
Iteration 26200: Loss = -10959.8203125
3
Iteration 26300: Loss = -10959.822265625
4
Iteration 26400: Loss = -10959.8203125
5
Iteration 26500: Loss = -10959.8193359375
Iteration 26600: Loss = -10959.8193359375
Iteration 26700: Loss = -10959.818359375
Iteration 26800: Loss = -10959.818359375
Iteration 26900: Loss = -10959.818359375
Iteration 27000: Loss = -10959.818359375
Iteration 27100: Loss = -10959.8203125
1
Iteration 27200: Loss = -10959.818359375
Iteration 27300: Loss = -10959.8193359375
1
Iteration 27400: Loss = -10959.8173828125
Iteration 27500: Loss = -10959.818359375
1
Iteration 27600: Loss = -10959.818359375
2
Iteration 27700: Loss = -10959.818359375
3
Iteration 27800: Loss = -10959.8173828125
Iteration 27900: Loss = -10959.8173828125
Iteration 28000: Loss = -10959.818359375
1
Iteration 28100: Loss = -10959.818359375
2
Iteration 28200: Loss = -10959.8193359375
3
Iteration 28300: Loss = -10959.8173828125
Iteration 28400: Loss = -10959.8193359375
1
Iteration 28500: Loss = -10959.8203125
2
Iteration 28600: Loss = -10959.8193359375
3
Iteration 28700: Loss = -10959.818359375
4
Iteration 28800: Loss = -10959.818359375
5
Iteration 28900: Loss = -10959.8173828125
Iteration 29000: Loss = -10959.818359375
1
Iteration 29100: Loss = -10959.826171875
2
Iteration 29200: Loss = -10959.818359375
3
Iteration 29300: Loss = -10959.818359375
4
Iteration 29400: Loss = -10959.8193359375
5
Iteration 29500: Loss = -10959.818359375
6
Iteration 29600: Loss = -10959.8193359375
7
Iteration 29700: Loss = -10959.8193359375
8
Iteration 29800: Loss = -10959.8173828125
Iteration 29900: Loss = -10959.8173828125
pi: tensor([[9.9997e-01, 3.3275e-05],
        [1.2309e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0103, 0.9897], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4910, 0.1215],
         [0.0106, 0.1610]],

        [[0.2630, 0.2994],
         [0.0735, 0.4833]],

        [[0.2818, 0.1731],
         [0.0524, 0.2745]],

        [[0.0092, 0.2302],
         [0.0215, 0.2150]],

        [[0.0356, 0.2311],
         [0.3102, 0.0087]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00035843312135763213
Average Adjusted Rand Index: -0.00011881818592060981
[-0.0017887818129207296, 0.00035843312135763213] [0.0, -0.00011881818592060981] [11058.6162109375, 10959.818359375]
-------------------------------------
This iteration is 85
True Objective function: Loss = -10794.634743853168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -62536.76171875
Iteration 100: Loss = -45807.4609375
Iteration 200: Loss = -31174.220703125
Iteration 300: Loss = -19870.587890625
Iteration 400: Loss = -14033.048828125
Iteration 500: Loss = -11909.490234375
Iteration 600: Loss = -11235.0322265625
Iteration 700: Loss = -11033.1044921875
Iteration 800: Loss = -10958.9755859375
Iteration 900: Loss = -10927.5283203125
Iteration 1000: Loss = -10916.2734375
Iteration 1100: Loss = -10910.3681640625
Iteration 1200: Loss = -10904.263671875
Iteration 1300: Loss = -10899.1103515625
Iteration 1400: Loss = -10891.958984375
Iteration 1500: Loss = -10885.1142578125
Iteration 1600: Loss = -10883.572265625
Iteration 1700: Loss = -10882.2666015625
Iteration 1800: Loss = -10880.994140625
Iteration 1900: Loss = -10880.0595703125
Iteration 2000: Loss = -10878.7939453125
Iteration 2100: Loss = -10876.837890625
Iteration 2200: Loss = -10876.095703125
Iteration 2300: Loss = -10875.5712890625
Iteration 2400: Loss = -10874.51171875
Iteration 2500: Loss = -10872.8134765625
Iteration 2600: Loss = -10867.828125
Iteration 2700: Loss = -10867.0966796875
Iteration 2800: Loss = -10866.6806640625
Iteration 2900: Loss = -10866.3740234375
Iteration 3000: Loss = -10866.1298828125
Iteration 3100: Loss = -10865.9208984375
Iteration 3200: Loss = -10865.7421875
Iteration 3300: Loss = -10865.583984375
Iteration 3400: Loss = -10865.44140625
Iteration 3500: Loss = -10865.31640625
Iteration 3600: Loss = -10865.1982421875
Iteration 3700: Loss = -10865.0966796875
Iteration 3800: Loss = -10864.9990234375
Iteration 3900: Loss = -10864.9091796875
Iteration 4000: Loss = -10864.8291015625
Iteration 4100: Loss = -10864.7548828125
Iteration 4200: Loss = -10864.6845703125
Iteration 4300: Loss = -10864.62109375
Iteration 4400: Loss = -10864.5595703125
Iteration 4500: Loss = -10864.50390625
Iteration 4600: Loss = -10864.4501953125
Iteration 4700: Loss = -10864.4033203125
Iteration 4800: Loss = -10864.3583984375
Iteration 4900: Loss = -10864.314453125
Iteration 5000: Loss = -10864.275390625
Iteration 5100: Loss = -10864.23828125
Iteration 5200: Loss = -10864.203125
Iteration 5300: Loss = -10864.1708984375
Iteration 5400: Loss = -10864.140625
Iteration 5500: Loss = -10864.111328125
Iteration 5600: Loss = -10864.0830078125
Iteration 5700: Loss = -10864.056640625
Iteration 5800: Loss = -10864.03125
Iteration 5900: Loss = -10864.0107421875
Iteration 6000: Loss = -10863.9873046875
Iteration 6100: Loss = -10863.966796875
Iteration 6200: Loss = -10863.9443359375
Iteration 6300: Loss = -10863.927734375
Iteration 6400: Loss = -10863.9111328125
Iteration 6500: Loss = -10863.892578125
Iteration 6600: Loss = -10863.87890625
Iteration 6700: Loss = -10863.861328125
Iteration 6800: Loss = -10863.8486328125
Iteration 6900: Loss = -10863.833984375
Iteration 7000: Loss = -10863.8232421875
Iteration 7100: Loss = -10863.8095703125
Iteration 7200: Loss = -10863.7978515625
Iteration 7300: Loss = -10863.7861328125
Iteration 7400: Loss = -10863.7763671875
Iteration 7500: Loss = -10863.7666015625
Iteration 7600: Loss = -10863.755859375
Iteration 7700: Loss = -10863.748046875
Iteration 7800: Loss = -10863.7392578125
Iteration 7900: Loss = -10863.7294921875
Iteration 8000: Loss = -10863.72265625
Iteration 8100: Loss = -10863.7177734375
Iteration 8200: Loss = -10863.7080078125
Iteration 8300: Loss = -10863.703125
Iteration 8400: Loss = -10863.6962890625
Iteration 8500: Loss = -10863.6904296875
Iteration 8600: Loss = -10863.68359375
Iteration 8700: Loss = -10863.6796875
Iteration 8800: Loss = -10863.673828125
Iteration 8900: Loss = -10863.6689453125
Iteration 9000: Loss = -10863.6640625
Iteration 9100: Loss = -10863.6591796875
Iteration 9200: Loss = -10863.65625
Iteration 9300: Loss = -10863.650390625
Iteration 9400: Loss = -10863.6474609375
Iteration 9500: Loss = -10863.6435546875
Iteration 9600: Loss = -10863.6416015625
Iteration 9700: Loss = -10863.63671875
Iteration 9800: Loss = -10863.634765625
Iteration 9900: Loss = -10863.6298828125
Iteration 10000: Loss = -10863.6279296875
Iteration 10100: Loss = -10863.6259765625
Iteration 10200: Loss = -10863.623046875
Iteration 10300: Loss = -10863.62109375
Iteration 10400: Loss = -10863.6171875
Iteration 10500: Loss = -10863.615234375
Iteration 10600: Loss = -10863.61328125
Iteration 10700: Loss = -10863.611328125
Iteration 10800: Loss = -10863.6083984375
Iteration 10900: Loss = -10863.607421875
Iteration 11000: Loss = -10863.60546875
Iteration 11100: Loss = -10863.6044921875
Iteration 11200: Loss = -10863.6025390625
Iteration 11300: Loss = -10863.599609375
Iteration 11400: Loss = -10863.5986328125
Iteration 11500: Loss = -10863.5966796875
Iteration 11600: Loss = -10863.5966796875
Iteration 11700: Loss = -10863.5966796875
Iteration 11800: Loss = -10863.59375
Iteration 11900: Loss = -10863.59375
Iteration 12000: Loss = -10863.5908203125
Iteration 12100: Loss = -10863.5908203125
Iteration 12200: Loss = -10863.5908203125
Iteration 12300: Loss = -10863.587890625
Iteration 12400: Loss = -10863.587890625
Iteration 12500: Loss = -10863.587890625
Iteration 12600: Loss = -10863.5869140625
Iteration 12700: Loss = -10863.5849609375
Iteration 12800: Loss = -10863.5859375
1
Iteration 12900: Loss = -10863.5849609375
Iteration 13000: Loss = -10863.58203125
Iteration 13100: Loss = -10863.5830078125
1
Iteration 13200: Loss = -10863.58203125
Iteration 13300: Loss = -10863.58203125
Iteration 13400: Loss = -10863.5830078125
1
Iteration 13500: Loss = -10863.580078125
Iteration 13600: Loss = -10863.580078125
Iteration 13700: Loss = -10863.580078125
Iteration 13800: Loss = -10863.5791015625
Iteration 13900: Loss = -10863.580078125
1
Iteration 14000: Loss = -10863.5771484375
Iteration 14100: Loss = -10863.5791015625
1
Iteration 14200: Loss = -10863.5791015625
2
Iteration 14300: Loss = -10863.5791015625
3
Iteration 14400: Loss = -10863.5771484375
Iteration 14500: Loss = -10863.576171875
Iteration 14600: Loss = -10863.576171875
Iteration 14700: Loss = -10863.576171875
Iteration 14800: Loss = -10863.576171875
Iteration 14900: Loss = -10863.576171875
Iteration 15000: Loss = -10863.576171875
Iteration 15100: Loss = -10863.5751953125
Iteration 15200: Loss = -10863.5751953125
Iteration 15300: Loss = -10863.5732421875
Iteration 15400: Loss = -10863.57421875
1
Iteration 15500: Loss = -10863.5732421875
Iteration 15600: Loss = -10863.5732421875
Iteration 15700: Loss = -10863.5732421875
Iteration 15800: Loss = -10863.5732421875
Iteration 15900: Loss = -10863.5732421875
Iteration 16000: Loss = -10863.5732421875
Iteration 16100: Loss = -10863.5732421875
Iteration 16200: Loss = -10863.5732421875
Iteration 16300: Loss = -10863.5712890625
Iteration 16400: Loss = -10863.5732421875
1
Iteration 16500: Loss = -10863.572265625
2
Iteration 16600: Loss = -10863.5732421875
3
Iteration 16700: Loss = -10863.572265625
4
Iteration 16800: Loss = -10863.5732421875
5
Iteration 16900: Loss = -10863.5703125
Iteration 17000: Loss = -10863.5703125
Iteration 17100: Loss = -10863.572265625
1
Iteration 17200: Loss = -10863.5693359375
Iteration 17300: Loss = -10863.568359375
Iteration 17400: Loss = -10862.1923828125
Iteration 17500: Loss = -10862.1279296875
Iteration 17600: Loss = -10862.10546875
Iteration 17700: Loss = -10862.0732421875
Iteration 17800: Loss = -10862.009765625
Iteration 17900: Loss = -10861.98828125
Iteration 18000: Loss = -10861.9833984375
Iteration 18100: Loss = -10861.9716796875
Iteration 18200: Loss = -10861.796875
Iteration 18300: Loss = -10861.775390625
Iteration 18400: Loss = -10861.73828125
Iteration 18500: Loss = -10861.6689453125
Iteration 18600: Loss = -10861.619140625
Iteration 18700: Loss = -10861.609375
Iteration 18800: Loss = -10861.609375
Iteration 18900: Loss = -10861.6025390625
Iteration 19000: Loss = -10861.58984375
Iteration 19100: Loss = -10861.583984375
Iteration 19200: Loss = -10861.5849609375
1
Iteration 19300: Loss = -10861.5830078125
Iteration 19400: Loss = -10861.576171875
Iteration 19500: Loss = -10861.5732421875
Iteration 19600: Loss = -10861.568359375
Iteration 19700: Loss = -10861.56640625
Iteration 19800: Loss = -10861.568359375
1
Iteration 19900: Loss = -10861.564453125
Iteration 20000: Loss = -10861.56640625
1
Iteration 20100: Loss = -10861.564453125
Iteration 20200: Loss = -10861.564453125
Iteration 20300: Loss = -10861.564453125
Iteration 20400: Loss = -10861.5595703125
Iteration 20500: Loss = -10861.5439453125
Iteration 20600: Loss = -10861.5419921875
Iteration 20700: Loss = -10861.5419921875
Iteration 20800: Loss = -10861.537109375
Iteration 20900: Loss = -10861.52734375
Iteration 21000: Loss = -10861.525390625
Iteration 21100: Loss = -10861.5244140625
Iteration 21200: Loss = -10861.5244140625
Iteration 21300: Loss = -10861.5205078125
Iteration 21400: Loss = -10861.521484375
1
Iteration 21500: Loss = -10861.521484375
2
Iteration 21600: Loss = -10861.5224609375
3
Iteration 21700: Loss = -10861.521484375
4
Iteration 21800: Loss = -10861.5224609375
5
Iteration 21900: Loss = -10861.521484375
6
Iteration 22000: Loss = -10861.5224609375
7
Iteration 22100: Loss = -10861.521484375
8
Iteration 22200: Loss = -10861.5234375
9
Iteration 22300: Loss = -10861.5126953125
Iteration 22400: Loss = -10861.5107421875
Iteration 22500: Loss = -10861.5107421875
Iteration 22600: Loss = -10861.51171875
1
Iteration 22700: Loss = -10861.51171875
2
Iteration 22800: Loss = -10861.5126953125
3
Iteration 22900: Loss = -10861.51171875
4
Iteration 23000: Loss = -10861.51171875
5
Iteration 23100: Loss = -10861.5107421875
Iteration 23200: Loss = -10861.51171875
1
Iteration 23300: Loss = -10861.5126953125
2
Iteration 23400: Loss = -10861.51171875
3
Iteration 23500: Loss = -10861.5126953125
4
Iteration 23600: Loss = -10861.5126953125
5
Iteration 23700: Loss = -10861.5107421875
Iteration 23800: Loss = -10861.4853515625
Iteration 23900: Loss = -10861.484375
Iteration 24000: Loss = -10861.4853515625
1
Iteration 24100: Loss = -10861.4833984375
Iteration 24200: Loss = -10861.47265625
Iteration 24300: Loss = -10861.4716796875
Iteration 24400: Loss = -10861.4716796875
Iteration 24500: Loss = -10861.470703125
Iteration 24600: Loss = -10861.4716796875
1
Iteration 24700: Loss = -10861.4697265625
Iteration 24800: Loss = -10861.4697265625
Iteration 24900: Loss = -10861.4697265625
Iteration 25000: Loss = -10861.4658203125
Iteration 25100: Loss = -10861.46484375
Iteration 25200: Loss = -10861.46484375
Iteration 25300: Loss = -10861.4658203125
1
Iteration 25400: Loss = -10861.46484375
Iteration 25500: Loss = -10861.46484375
Iteration 25600: Loss = -10861.46484375
Iteration 25700: Loss = -10861.4658203125
1
Iteration 25800: Loss = -10861.46484375
Iteration 25900: Loss = -10861.4658203125
1
Iteration 26000: Loss = -10861.4658203125
2
Iteration 26100: Loss = -10861.46484375
Iteration 26200: Loss = -10861.4658203125
1
Iteration 26300: Loss = -10861.46484375
Iteration 26400: Loss = -10861.4658203125
1
Iteration 26500: Loss = -10861.4638671875
Iteration 26600: Loss = -10861.46484375
1
Iteration 26700: Loss = -10861.4658203125
2
Iteration 26800: Loss = -10861.4677734375
3
Iteration 26900: Loss = -10861.46484375
4
Iteration 27000: Loss = -10861.46484375
5
Iteration 27100: Loss = -10861.4658203125
6
Iteration 27200: Loss = -10861.46484375
7
Iteration 27300: Loss = -10861.46484375
8
Iteration 27400: Loss = -10861.4658203125
9
Iteration 27500: Loss = -10861.4658203125
10
Iteration 27600: Loss = -10861.46484375
11
Iteration 27700: Loss = -10861.46484375
12
Iteration 27800: Loss = -10861.46484375
13
Iteration 27900: Loss = -10861.4658203125
14
Iteration 28000: Loss = -10861.4658203125
15
Stopping early at iteration 28000 due to no improvement.
pi: tensor([[1.0000e+00, 2.3262e-06],
        [1.0000e+00, 8.2213e-08]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9692, 0.0308], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1583, 0.2579],
         [0.5474, 0.4987]],

        [[0.8982, 0.6423],
         [0.2101, 0.2196]],

        [[0.8481, 0.1542],
         [0.1054, 0.0102]],

        [[0.9108, 0.1647],
         [0.9916, 0.8213]],

        [[0.8440, 0.1433],
         [0.1783, 0.0368]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00026829885392748833
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27323.732421875
Iteration 100: Loss = -18582.39453125
Iteration 200: Loss = -13587.6103515625
Iteration 300: Loss = -11999.8564453125
Iteration 400: Loss = -11418.6943359375
Iteration 500: Loss = -11165.623046875
Iteration 600: Loss = -11050.9326171875
Iteration 700: Loss = -10973.1865234375
Iteration 800: Loss = -10928.8779296875
Iteration 900: Loss = -10910.869140625
Iteration 1000: Loss = -10895.505859375
Iteration 1100: Loss = -10889.3017578125
Iteration 1200: Loss = -10884.955078125
Iteration 1300: Loss = -10881.59765625
Iteration 1400: Loss = -10876.3056640625
Iteration 1500: Loss = -10873.4580078125
Iteration 1600: Loss = -10871.400390625
Iteration 1700: Loss = -10870.2353515625
Iteration 1800: Loss = -10869.2470703125
Iteration 1900: Loss = -10868.3974609375
Iteration 2000: Loss = -10867.6318359375
Iteration 2100: Loss = -10866.8408203125
Iteration 2200: Loss = -10866.2080078125
Iteration 2300: Loss = -10865.8037109375
Iteration 2400: Loss = -10865.4619140625
Iteration 2500: Loss = -10865.1611328125
Iteration 2600: Loss = -10864.896484375
Iteration 2700: Loss = -10864.6552734375
Iteration 2800: Loss = -10864.439453125
Iteration 2900: Loss = -10864.24609375
Iteration 3000: Loss = -10864.0703125
Iteration 3100: Loss = -10863.9111328125
Iteration 3200: Loss = -10863.7646484375
Iteration 3300: Loss = -10863.62890625
Iteration 3400: Loss = -10863.505859375
Iteration 3500: Loss = -10863.390625
Iteration 3600: Loss = -10863.2861328125
Iteration 3700: Loss = -10863.1865234375
Iteration 3800: Loss = -10863.0927734375
Iteration 3900: Loss = -10863.0087890625
Iteration 4000: Loss = -10862.9306640625
Iteration 4100: Loss = -10862.857421875
Iteration 4200: Loss = -10862.79296875
Iteration 4300: Loss = -10862.732421875
Iteration 4400: Loss = -10862.6767578125
Iteration 4500: Loss = -10862.6220703125
Iteration 4600: Loss = -10862.5751953125
Iteration 4700: Loss = -10862.529296875
Iteration 4800: Loss = -10862.486328125
Iteration 4900: Loss = -10862.447265625
Iteration 5000: Loss = -10862.4111328125
Iteration 5100: Loss = -10862.3740234375
Iteration 5200: Loss = -10862.3427734375
Iteration 5300: Loss = -10862.3134765625
Iteration 5400: Loss = -10862.2841796875
Iteration 5500: Loss = -10862.2568359375
Iteration 5600: Loss = -10862.2333984375
Iteration 5700: Loss = -10862.20703125
Iteration 5800: Loss = -10862.185546875
Iteration 5900: Loss = -10862.1640625
Iteration 6000: Loss = -10862.1435546875
Iteration 6100: Loss = -10862.1259765625
Iteration 6200: Loss = -10862.107421875
Iteration 6300: Loss = -10862.08984375
Iteration 6400: Loss = -10862.07421875
Iteration 6500: Loss = -10862.05859375
Iteration 6600: Loss = -10862.0439453125
Iteration 6700: Loss = -10862.029296875
Iteration 6800: Loss = -10862.015625
Iteration 6900: Loss = -10862.0029296875
Iteration 7000: Loss = -10861.990234375
Iteration 7100: Loss = -10861.978515625
Iteration 7200: Loss = -10861.966796875
Iteration 7300: Loss = -10861.9541015625
Iteration 7400: Loss = -10861.9404296875
Iteration 7500: Loss = -10861.9296875
Iteration 7600: Loss = -10861.916015625
Iteration 7700: Loss = -10861.904296875
Iteration 7800: Loss = -10861.8916015625
Iteration 7900: Loss = -10861.8818359375
Iteration 8000: Loss = -10861.8720703125
Iteration 8100: Loss = -10861.8642578125
Iteration 8200: Loss = -10861.8564453125
Iteration 8300: Loss = -10861.8515625
Iteration 8400: Loss = -10861.845703125
Iteration 8500: Loss = -10861.83984375
Iteration 8600: Loss = -10861.833984375
Iteration 8700: Loss = -10861.8310546875
Iteration 8800: Loss = -10861.8251953125
Iteration 8900: Loss = -10861.8212890625
Iteration 9000: Loss = -10861.81640625
Iteration 9100: Loss = -10861.8134765625
Iteration 9200: Loss = -10861.810546875
Iteration 9300: Loss = -10861.806640625
Iteration 9400: Loss = -10861.802734375
Iteration 9500: Loss = -10861.80078125
Iteration 9600: Loss = -10861.798828125
Iteration 9700: Loss = -10861.794921875
Iteration 9800: Loss = -10861.79296875
Iteration 9900: Loss = -10861.7900390625
Iteration 10000: Loss = -10861.7880859375
Iteration 10100: Loss = -10861.787109375
Iteration 10200: Loss = -10861.7841796875
Iteration 10300: Loss = -10861.78125
Iteration 10400: Loss = -10861.78125
Iteration 10500: Loss = -10861.7763671875
Iteration 10600: Loss = -10861.775390625
Iteration 10700: Loss = -10861.7734375
Iteration 10800: Loss = -10861.7734375
Iteration 10900: Loss = -10861.7724609375
Iteration 11000: Loss = -10861.7705078125
Iteration 11100: Loss = -10861.7685546875
Iteration 11200: Loss = -10861.767578125
Iteration 11300: Loss = -10861.765625
Iteration 11400: Loss = -10861.7646484375
Iteration 11500: Loss = -10861.7626953125
Iteration 11600: Loss = -10861.7626953125
Iteration 11700: Loss = -10861.76171875
Iteration 11800: Loss = -10861.759765625
Iteration 11900: Loss = -10861.759765625
Iteration 12000: Loss = -10861.7587890625
Iteration 12100: Loss = -10861.7568359375
Iteration 12200: Loss = -10861.755859375
Iteration 12300: Loss = -10861.7548828125
Iteration 12400: Loss = -10861.75390625
Iteration 12500: Loss = -10861.75390625
Iteration 12600: Loss = -10861.7529296875
Iteration 12700: Loss = -10861.751953125
Iteration 12800: Loss = -10861.751953125
Iteration 12900: Loss = -10861.751953125
Iteration 13000: Loss = -10861.75
Iteration 13100: Loss = -10861.7490234375
Iteration 13200: Loss = -10861.7490234375
Iteration 13300: Loss = -10861.75
1
Iteration 13400: Loss = -10861.7490234375
Iteration 13500: Loss = -10861.748046875
Iteration 13600: Loss = -10861.748046875
Iteration 13700: Loss = -10861.748046875
Iteration 13800: Loss = -10861.7451171875
Iteration 13900: Loss = -10861.748046875
1
Iteration 14000: Loss = -10861.74609375
2
Iteration 14100: Loss = -10861.7451171875
Iteration 14200: Loss = -10861.744140625
Iteration 14300: Loss = -10861.7451171875
1
Iteration 14400: Loss = -10861.74609375
2
Iteration 14500: Loss = -10861.7451171875
3
Iteration 14600: Loss = -10861.744140625
Iteration 14700: Loss = -10861.744140625
Iteration 14800: Loss = -10861.744140625
Iteration 14900: Loss = -10861.7421875
Iteration 15000: Loss = -10861.7421875
Iteration 15100: Loss = -10861.744140625
1
Iteration 15200: Loss = -10861.7431640625
2
Iteration 15300: Loss = -10861.7431640625
3
Iteration 15400: Loss = -10861.7431640625
4
Iteration 15500: Loss = -10861.7421875
Iteration 15600: Loss = -10861.7431640625
1
Iteration 15700: Loss = -10861.7421875
Iteration 15800: Loss = -10861.744140625
1
Iteration 15900: Loss = -10861.7421875
Iteration 16000: Loss = -10861.7431640625
1
Iteration 16100: Loss = -10861.7421875
Iteration 16200: Loss = -10861.7412109375
Iteration 16300: Loss = -10861.7412109375
Iteration 16400: Loss = -10861.740234375
Iteration 16500: Loss = -10861.740234375
Iteration 16600: Loss = -10861.7431640625
1
Iteration 16700: Loss = -10861.7421875
2
Iteration 16800: Loss = -10861.740234375
Iteration 16900: Loss = -10861.7412109375
1
Iteration 17000: Loss = -10861.7412109375
2
Iteration 17100: Loss = -10861.7412109375
3
Iteration 17200: Loss = -10861.7421875
4
Iteration 17300: Loss = -10861.7392578125
Iteration 17400: Loss = -10861.740234375
1
Iteration 17500: Loss = -10861.7412109375
2
Iteration 17600: Loss = -10861.740234375
3
Iteration 17700: Loss = -10861.740234375
4
Iteration 17800: Loss = -10861.740234375
5
Iteration 17900: Loss = -10861.740234375
6
Iteration 18000: Loss = -10861.7392578125
Iteration 18100: Loss = -10861.7421875
1
Iteration 18200: Loss = -10861.740234375
2
Iteration 18300: Loss = -10861.7392578125
Iteration 18400: Loss = -10861.7412109375
1
Iteration 18500: Loss = -10861.740234375
2
Iteration 18600: Loss = -10861.740234375
3
Iteration 18700: Loss = -10861.7392578125
Iteration 18800: Loss = -10861.740234375
1
Iteration 18900: Loss = -10861.7412109375
2
Iteration 19000: Loss = -10861.740234375
3
Iteration 19100: Loss = -10861.7412109375
4
Iteration 19200: Loss = -10861.7412109375
5
Iteration 19300: Loss = -10861.7392578125
Iteration 19400: Loss = -10861.7412109375
1
Iteration 19500: Loss = -10861.7412109375
2
Iteration 19600: Loss = -10861.7412109375
3
Iteration 19700: Loss = -10861.740234375
4
Iteration 19800: Loss = -10861.740234375
5
Iteration 19900: Loss = -10861.7392578125
Iteration 20000: Loss = -10861.7392578125
Iteration 20100: Loss = -10861.7392578125
Iteration 20200: Loss = -10861.7392578125
Iteration 20300: Loss = -10861.740234375
1
Iteration 20400: Loss = -10861.7392578125
Iteration 20500: Loss = -10861.7392578125
Iteration 20600: Loss = -10861.7392578125
Iteration 20700: Loss = -10861.7392578125
Iteration 20800: Loss = -10861.7392578125
Iteration 20900: Loss = -10861.7392578125
Iteration 21000: Loss = -10861.7392578125
Iteration 21100: Loss = -10861.740234375
1
Iteration 21200: Loss = -10861.740234375
2
Iteration 21300: Loss = -10861.7392578125
Iteration 21400: Loss = -10861.7392578125
Iteration 21500: Loss = -10861.7392578125
Iteration 21600: Loss = -10861.7392578125
Iteration 21700: Loss = -10861.7392578125
Iteration 21800: Loss = -10861.7392578125
Iteration 21900: Loss = -10861.7421875
1
Iteration 22000: Loss = -10861.740234375
2
Iteration 22100: Loss = -10861.7412109375
3
Iteration 22200: Loss = -10861.7392578125
Iteration 22300: Loss = -10861.7392578125
Iteration 22400: Loss = -10861.7392578125
Iteration 22500: Loss = -10861.740234375
1
Iteration 22600: Loss = -10861.740234375
2
Iteration 22700: Loss = -10861.7392578125
Iteration 22800: Loss = -10861.73828125
Iteration 22900: Loss = -10861.740234375
1
Iteration 23000: Loss = -10861.740234375
2
Iteration 23100: Loss = -10861.740234375
3
Iteration 23200: Loss = -10861.7392578125
4
Iteration 23300: Loss = -10861.740234375
5
Iteration 23400: Loss = -10861.73828125
Iteration 23500: Loss = -10861.744140625
1
Iteration 23600: Loss = -10861.7392578125
2
Iteration 23700: Loss = -10861.7392578125
3
Iteration 23800: Loss = -10861.7392578125
4
Iteration 23900: Loss = -10861.7392578125
5
Iteration 24000: Loss = -10861.7421875
6
Iteration 24100: Loss = -10861.7392578125
7
Iteration 24200: Loss = -10861.740234375
8
Iteration 24300: Loss = -10861.7392578125
9
Iteration 24400: Loss = -10861.7412109375
10
Iteration 24500: Loss = -10861.740234375
11
Iteration 24600: Loss = -10861.740234375
12
Iteration 24700: Loss = -10861.740234375
13
Iteration 24800: Loss = -10861.7412109375
14
Iteration 24900: Loss = -10861.73828125
Iteration 25000: Loss = -10861.7392578125
1
Iteration 25100: Loss = -10861.7392578125
2
Iteration 25200: Loss = -10861.7392578125
3
Iteration 25300: Loss = -10861.7392578125
4
Iteration 25400: Loss = -10861.7392578125
5
Iteration 25500: Loss = -10861.7392578125
6
Iteration 25600: Loss = -10861.7392578125
7
Iteration 25700: Loss = -10861.7392578125
8
Iteration 25800: Loss = -10861.7392578125
9
Iteration 25900: Loss = -10861.728515625
Iteration 26000: Loss = -10861.7275390625
Iteration 26100: Loss = -10861.7275390625
Iteration 26200: Loss = -10861.7275390625
Iteration 26300: Loss = -10861.7275390625
Iteration 26400: Loss = -10861.7275390625
Iteration 26500: Loss = -10861.7255859375
Iteration 26600: Loss = -10861.728515625
1
Iteration 26700: Loss = -10861.7265625
2
Iteration 26800: Loss = -10861.7265625
3
Iteration 26900: Loss = -10861.7265625
4
Iteration 27000: Loss = -10861.7265625
5
Iteration 27100: Loss = -10861.7265625
6
Iteration 27200: Loss = -10861.7255859375
Iteration 27300: Loss = -10861.7265625
1
Iteration 27400: Loss = -10861.7255859375
Iteration 27500: Loss = -10861.7265625
1
Iteration 27600: Loss = -10861.7255859375
Iteration 27700: Loss = -10861.7265625
1
Iteration 27800: Loss = -10861.7265625
2
Iteration 27900: Loss = -10861.7255859375
Iteration 28000: Loss = -10861.7265625
1
Iteration 28100: Loss = -10861.7275390625
2
Iteration 28200: Loss = -10861.7265625
3
Iteration 28300: Loss = -10861.7265625
4
Iteration 28400: Loss = -10861.7265625
5
Iteration 28500: Loss = -10861.724609375
Iteration 28600: Loss = -10861.7255859375
1
Iteration 28700: Loss = -10861.7265625
2
Iteration 28800: Loss = -10861.7265625
3
Iteration 28900: Loss = -10861.7265625
4
Iteration 29000: Loss = -10861.7255859375
5
Iteration 29100: Loss = -10861.7265625
6
Iteration 29200: Loss = -10861.724609375
Iteration 29300: Loss = -10861.7265625
1
Iteration 29400: Loss = -10861.7265625
2
Iteration 29500: Loss = -10861.7265625
3
Iteration 29600: Loss = -10861.7265625
4
Iteration 29700: Loss = -10861.7265625
5
Iteration 29800: Loss = -10861.7255859375
6
Iteration 29900: Loss = -10861.7265625
7
pi: tensor([[5.9320e-08, 1.0000e+00],
        [2.2064e-02, 9.7794e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9745, 0.0255], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.2627],
         [0.8702, 0.1592]],

        [[0.9924, 0.2440],
         [0.3453, 0.9866]],

        [[0.2129, 0.1091],
         [0.6749, 0.0088]],

        [[0.9773, 0.1621],
         [0.0363, 0.3770]],

        [[0.8682, 0.1321],
         [0.0403, 0.9665]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0003087170253360732
Average Adjusted Rand Index: 0.0008888888888888889
[0.00026829885392748833, 0.0003087170253360732] [0.0, 0.0008888888888888889] [10861.4658203125, 10861.7265625]
-------------------------------------
This iteration is 86
True Objective function: Loss = -10965.367998623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16459.685546875
Iteration 100: Loss = -12610.5107421875
Iteration 200: Loss = -11404.8994140625
Iteration 300: Loss = -11253.9482421875
Iteration 400: Loss = -11212.5791015625
Iteration 500: Loss = -11182.71484375
Iteration 600: Loss = -11160.6689453125
Iteration 700: Loss = -11145.26171875
Iteration 800: Loss = -11126.8779296875
Iteration 900: Loss = -11116.5107421875
Iteration 1000: Loss = -11107.658203125
Iteration 1100: Loss = -11101.9775390625
Iteration 1200: Loss = -11098.103515625
Iteration 1300: Loss = -11095.375
Iteration 1400: Loss = -11093.3486328125
Iteration 1500: Loss = -11091.7421875
Iteration 1600: Loss = -11090.3515625
Iteration 1700: Loss = -11088.9609375
Iteration 1800: Loss = -11087.484375
Iteration 1900: Loss = -11085.265625
Iteration 2000: Loss = -11082.6875
Iteration 2100: Loss = -11079.9345703125
Iteration 2200: Loss = -11077.884765625
Iteration 2300: Loss = -11075.79296875
Iteration 2400: Loss = -11073.7578125
Iteration 2500: Loss = -11071.7041015625
Iteration 2600: Loss = -11070.5673828125
Iteration 2700: Loss = -11069.861328125
Iteration 2800: Loss = -11069.3447265625
Iteration 2900: Loss = -11068.912109375
Iteration 3000: Loss = -11068.5859375
Iteration 3100: Loss = -11068.341796875
Iteration 3200: Loss = -11068.1474609375
Iteration 3300: Loss = -11067.984375
Iteration 3400: Loss = -11067.849609375
Iteration 3500: Loss = -11067.7353515625
Iteration 3600: Loss = -11067.634765625
Iteration 3700: Loss = -11067.548828125
Iteration 3800: Loss = -11067.47265625
Iteration 3900: Loss = -11067.4052734375
Iteration 4000: Loss = -11067.34375
Iteration 4100: Loss = -11067.2890625
Iteration 4200: Loss = -11067.2421875
Iteration 4300: Loss = -11067.1982421875
Iteration 4400: Loss = -11067.158203125
Iteration 4500: Loss = -11067.1220703125
Iteration 4600: Loss = -11067.087890625
Iteration 4700: Loss = -11067.0576171875
Iteration 4800: Loss = -11067.0302734375
Iteration 4900: Loss = -11066.916015625
Iteration 5000: Loss = -11066.310546875
Iteration 5100: Loss = -11066.2861328125
Iteration 5200: Loss = -11066.265625
Iteration 5300: Loss = -11066.24609375
Iteration 5400: Loss = -11066.228515625
Iteration 5500: Loss = -11066.2109375
Iteration 5600: Loss = -11066.197265625
Iteration 5700: Loss = -11066.1826171875
Iteration 5800: Loss = -11066.1689453125
Iteration 5900: Loss = -11066.1572265625
Iteration 6000: Loss = -11066.142578125
Iteration 6100: Loss = -11066.12890625
Iteration 6200: Loss = -11066.1142578125
Iteration 6300: Loss = -11066.103515625
Iteration 6400: Loss = -11066.0947265625
Iteration 6500: Loss = -11066.0859375
Iteration 6600: Loss = -11066.0771484375
Iteration 6700: Loss = -11066.0693359375
Iteration 6800: Loss = -11066.0615234375
Iteration 6900: Loss = -11066.0537109375
Iteration 7000: Loss = -11066.037109375
Iteration 7100: Loss = -11065.10546875
Iteration 7200: Loss = -11065.0947265625
Iteration 7300: Loss = -11065.087890625
Iteration 7400: Loss = -11065.08203125
Iteration 7500: Loss = -11065.0771484375
Iteration 7600: Loss = -11065.0732421875
Iteration 7700: Loss = -11065.0673828125
Iteration 7800: Loss = -11065.064453125
Iteration 7900: Loss = -11065.0625
Iteration 8000: Loss = -11065.056640625
Iteration 8100: Loss = -11065.0546875
Iteration 8200: Loss = -11065.0498046875
Iteration 8300: Loss = -11065.0458984375
Iteration 8400: Loss = -11065.04296875
Iteration 8500: Loss = -11065.03515625
Iteration 8600: Loss = -11065.017578125
Iteration 8700: Loss = -11064.9716796875
Iteration 8800: Loss = -11064.943359375
Iteration 8900: Loss = -11064.80078125
Iteration 9000: Loss = -11059.599609375
Iteration 9100: Loss = -11058.578125
Iteration 9200: Loss = -11058.5595703125
Iteration 9300: Loss = -11057.990234375
Iteration 9400: Loss = -11056.4638671875
Iteration 9500: Loss = -11054.591796875
Iteration 9600: Loss = -11053.2265625
Iteration 9700: Loss = -11052.15625
Iteration 9800: Loss = -11051.359375
Iteration 9900: Loss = -11050.3916015625
Iteration 10000: Loss = -11047.7333984375
Iteration 10100: Loss = -11045.9453125
Iteration 10200: Loss = -11043.7412109375
Iteration 10300: Loss = -11042.091796875
Iteration 10400: Loss = -11040.9365234375
Iteration 10500: Loss = -11040.1416015625
Iteration 10600: Loss = -11039.716796875
Iteration 10700: Loss = -11038.5830078125
Iteration 10800: Loss = -11034.802734375
Iteration 10900: Loss = -11033.537109375
Iteration 11000: Loss = -11021.26953125
Iteration 11100: Loss = -11017.109375
Iteration 11200: Loss = -11016.9169921875
Iteration 11300: Loss = -11016.8515625
Iteration 11400: Loss = -11014.3349609375
Iteration 11500: Loss = -11013.623046875
Iteration 11600: Loss = -11010.4453125
Iteration 11700: Loss = -11009.93359375
Iteration 11800: Loss = -11008.6015625
Iteration 11900: Loss = -11006.82421875
Iteration 12000: Loss = -11005.474609375
Iteration 12100: Loss = -11000.7763671875
Iteration 12200: Loss = -10999.515625
Iteration 12300: Loss = -10998.9599609375
Iteration 12400: Loss = -10998.6669921875
Iteration 12500: Loss = -10998.486328125
Iteration 12600: Loss = -10998.3486328125
Iteration 12700: Loss = -10998.3349609375
Iteration 12800: Loss = -10998.3291015625
Iteration 12900: Loss = -10998.3115234375
Iteration 13000: Loss = -10998.30859375
Iteration 13100: Loss = -10998.30859375
Iteration 13200: Loss = -10998.3076171875
Iteration 13300: Loss = -10998.3046875
Iteration 13400: Loss = -10998.3037109375
Iteration 13500: Loss = -10996.755859375
Iteration 13600: Loss = -10996.7490234375
Iteration 13700: Loss = -10996.748046875
Iteration 13800: Loss = -10996.744140625
Iteration 13900: Loss = -10996.7353515625
Iteration 14000: Loss = -10996.7333984375
Iteration 14100: Loss = -10996.7314453125
Iteration 14200: Loss = -10996.73046875
Iteration 14300: Loss = -10996.7294921875
Iteration 14400: Loss = -10996.728515625
Iteration 14500: Loss = -10996.7080078125
Iteration 14600: Loss = -10996.7060546875
Iteration 14700: Loss = -10996.7060546875
Iteration 14800: Loss = -10996.705078125
Iteration 14900: Loss = -10996.7041015625
Iteration 15000: Loss = -10996.623046875
Iteration 15100: Loss = -10996.3349609375
Iteration 15200: Loss = -10996.3330078125
Iteration 15300: Loss = -10996.333984375
1
Iteration 15400: Loss = -10996.3359375
2
Iteration 15500: Loss = -10996.333984375
3
Iteration 15600: Loss = -10996.333984375
4
Iteration 15700: Loss = -10996.3330078125
Iteration 15800: Loss = -10996.333984375
1
Iteration 15900: Loss = -10996.33203125
Iteration 16000: Loss = -10996.33203125
Iteration 16100: Loss = -10996.33203125
Iteration 16200: Loss = -10996.3310546875
Iteration 16300: Loss = -10996.3173828125
Iteration 16400: Loss = -10996.3154296875
Iteration 16500: Loss = -10996.31640625
1
Iteration 16600: Loss = -10996.314453125
Iteration 16700: Loss = -10996.314453125
Iteration 16800: Loss = -10996.31640625
1
Iteration 16900: Loss = -10996.3154296875
2
Iteration 17000: Loss = -10996.3154296875
3
Iteration 17100: Loss = -10996.314453125
Iteration 17200: Loss = -10996.3154296875
1
Iteration 17300: Loss = -10996.3154296875
2
Iteration 17400: Loss = -10996.314453125
Iteration 17500: Loss = -10996.314453125
Iteration 17600: Loss = -10996.30859375
Iteration 17700: Loss = -10992.271484375
Iteration 17800: Loss = -10964.8828125
Iteration 17900: Loss = -10952.4375
Iteration 18000: Loss = -10951.341796875
Iteration 18100: Loss = -10951.26953125
Iteration 18200: Loss = -10951.2451171875
Iteration 18300: Loss = -10951.23046875
Iteration 18400: Loss = -10944.9208984375
Iteration 18500: Loss = -10944.3359375
Iteration 18600: Loss = -10944.3056640625
Iteration 18700: Loss = -10944.291015625
Iteration 18800: Loss = -10944.275390625
Iteration 18900: Loss = -10944.251953125
Iteration 19000: Loss = -10944.24609375
Iteration 19100: Loss = -10944.244140625
Iteration 19200: Loss = -10944.2421875
Iteration 19300: Loss = -10944.2412109375
Iteration 19400: Loss = -10944.23828125
Iteration 19500: Loss = -10944.2373046875
Iteration 19600: Loss = -10944.236328125
Iteration 19700: Loss = -10944.236328125
Iteration 19800: Loss = -10944.234375
Iteration 19900: Loss = -10944.146484375
Iteration 20000: Loss = -10941.1552734375
Iteration 20100: Loss = -10934.3251953125
Iteration 20200: Loss = -10934.1435546875
Iteration 20300: Loss = -10934.1318359375
Iteration 20400: Loss = -10934.125
Iteration 20500: Loss = -10934.119140625
Iteration 20600: Loss = -10934.1181640625
Iteration 20700: Loss = -10934.07421875
Iteration 20800: Loss = -10934.0009765625
Iteration 20900: Loss = -10934.0009765625
Iteration 21000: Loss = -10934.0
Iteration 21100: Loss = -10934.0
Iteration 21200: Loss = -10933.9990234375
Iteration 21300: Loss = -10933.9990234375
Iteration 21400: Loss = -10933.998046875
Iteration 21500: Loss = -10933.998046875
Iteration 21600: Loss = -10933.9970703125
Iteration 21700: Loss = -10933.9970703125
Iteration 21800: Loss = -10933.99609375
Iteration 21900: Loss = -10933.9951171875
Iteration 22000: Loss = -10933.9951171875
Iteration 22100: Loss = -10933.99609375
1
Iteration 22200: Loss = -10933.99609375
2
Iteration 22300: Loss = -10933.9755859375
Iteration 22400: Loss = -10933.974609375
Iteration 22500: Loss = -10933.974609375
Iteration 22600: Loss = -10933.974609375
Iteration 22700: Loss = -10933.958984375
Iteration 22800: Loss = -10933.9580078125
Iteration 22900: Loss = -10933.9580078125
Iteration 23000: Loss = -10933.9560546875
Iteration 23100: Loss = -10933.9541015625
Iteration 23200: Loss = -10933.9541015625
Iteration 23300: Loss = -10933.955078125
1
Iteration 23400: Loss = -10933.955078125
2
Iteration 23500: Loss = -10933.955078125
3
Iteration 23600: Loss = -10933.953125
Iteration 23700: Loss = -10933.955078125
1
Iteration 23800: Loss = -10933.955078125
2
Iteration 23900: Loss = -10933.9541015625
3
Iteration 24000: Loss = -10933.955078125
4
Iteration 24100: Loss = -10933.955078125
5
Iteration 24200: Loss = -10933.955078125
6
Iteration 24300: Loss = -10933.9560546875
7
Iteration 24400: Loss = -10933.9541015625
8
Iteration 24500: Loss = -10933.953125
Iteration 24600: Loss = -10933.9541015625
1
Iteration 24700: Loss = -10933.9541015625
2
Iteration 24800: Loss = -10933.9541015625
3
Iteration 24900: Loss = -10933.9541015625
4
Iteration 25000: Loss = -10933.955078125
5
Iteration 25100: Loss = -10933.9541015625
6
Iteration 25200: Loss = -10933.955078125
7
Iteration 25300: Loss = -10933.953125
Iteration 25400: Loss = -10933.953125
Iteration 25500: Loss = -10933.955078125
1
Iteration 25600: Loss = -10933.955078125
2
Iteration 25700: Loss = -10933.9541015625
3
Iteration 25800: Loss = -10933.9541015625
4
Iteration 25900: Loss = -10933.9541015625
5
Iteration 26000: Loss = -10933.955078125
6
Iteration 26100: Loss = -10933.9541015625
7
Iteration 26200: Loss = -10933.9541015625
8
Iteration 26300: Loss = -10933.9521484375
Iteration 26400: Loss = -10933.951171875
Iteration 26500: Loss = -10933.953125
1
Iteration 26600: Loss = -10933.9521484375
2
Iteration 26700: Loss = -10933.9521484375
3
Iteration 26800: Loss = -10933.9521484375
4
Iteration 26900: Loss = -10933.9521484375
5
Iteration 27000: Loss = -10933.9521484375
6
Iteration 27100: Loss = -10933.951171875
Iteration 27200: Loss = -10933.9521484375
1
Iteration 27300: Loss = -10933.9462890625
Iteration 27400: Loss = -10933.9453125
Iteration 27500: Loss = -10933.9453125
Iteration 27600: Loss = -10933.9462890625
1
Iteration 27700: Loss = -10933.9443359375
Iteration 27800: Loss = -10933.9453125
1
Iteration 27900: Loss = -10933.9443359375
Iteration 28000: Loss = -10933.9443359375
Iteration 28100: Loss = -10933.9453125
1
Iteration 28200: Loss = -10933.9443359375
Iteration 28300: Loss = -10933.9453125
1
Iteration 28400: Loss = -10933.9443359375
Iteration 28500: Loss = -10933.9453125
1
Iteration 28600: Loss = -10933.9462890625
2
Iteration 28700: Loss = -10933.943359375
Iteration 28800: Loss = -10933.9443359375
1
Iteration 28900: Loss = -10933.9443359375
2
Iteration 29000: Loss = -10933.9453125
3
Iteration 29100: Loss = -10933.9453125
4
Iteration 29200: Loss = -10933.9443359375
5
Iteration 29300: Loss = -10933.9443359375
6
Iteration 29400: Loss = -10933.9443359375
7
Iteration 29500: Loss = -10933.9443359375
8
Iteration 29600: Loss = -10933.9443359375
9
Iteration 29700: Loss = -10933.943359375
Iteration 29800: Loss = -10933.9423828125
Iteration 29900: Loss = -10933.94140625
pi: tensor([[0.2727, 0.7273],
        [0.7903, 0.2097]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4604, 0.5396], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2330, 0.0955],
         [0.5256, 0.2241]],

        [[0.7537, 0.1092],
         [0.9839, 0.9849]],

        [[0.6376, 0.0973],
         [0.9861, 0.0544]],

        [[0.9901, 0.1001],
         [0.9715, 0.0271]],

        [[0.9262, 0.0932],
         [0.2234, 0.9835]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7719210443888802
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.03493924811746628
Average Adjusted Rand Index: 0.8313918835261767
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19667.595703125
Iteration 100: Loss = -13590.4150390625
Iteration 200: Loss = -11540.734375
Iteration 300: Loss = -11237.5048828125
Iteration 400: Loss = -11161.8076171875
Iteration 500: Loss = -11127.6357421875
Iteration 600: Loss = -11108.4541015625
Iteration 700: Loss = -11095.6962890625
Iteration 800: Loss = -11090.0
Iteration 900: Loss = -11085.5400390625
Iteration 1000: Loss = -11081.81640625
Iteration 1100: Loss = -11078.705078125
Iteration 1200: Loss = -11076.0458984375
Iteration 1300: Loss = -11074.158203125
Iteration 1400: Loss = -11072.7705078125
Iteration 1500: Loss = -11071.6708984375
Iteration 1600: Loss = -11070.7490234375
Iteration 1700: Loss = -11069.93359375
Iteration 1800: Loss = -11069.197265625
Iteration 1900: Loss = -11068.5244140625
Iteration 2000: Loss = -11067.9052734375
Iteration 2100: Loss = -11067.3427734375
Iteration 2200: Loss = -11066.8291015625
Iteration 2300: Loss = -11066.3662109375
Iteration 2400: Loss = -11065.951171875
Iteration 2500: Loss = -11065.576171875
Iteration 2600: Loss = -11065.189453125
Iteration 2700: Loss = -11064.833984375
Iteration 2800: Loss = -11064.5361328125
Iteration 2900: Loss = -11064.2509765625
Iteration 3000: Loss = -11064.001953125
Iteration 3100: Loss = -11063.7646484375
Iteration 3200: Loss = -11063.546875
Iteration 3300: Loss = -11063.357421875
Iteration 3400: Loss = -11063.1806640625
Iteration 3500: Loss = -11063.013671875
Iteration 3600: Loss = -11062.853515625
Iteration 3700: Loss = -11062.681640625
Iteration 3800: Loss = -11062.513671875
Iteration 3900: Loss = -11062.3515625
Iteration 4000: Loss = -11062.1904296875
Iteration 4100: Loss = -11062.0546875
Iteration 4200: Loss = -11061.9267578125
Iteration 4300: Loss = -11061.7802734375
Iteration 4400: Loss = -11061.654296875
Iteration 4500: Loss = -11061.578125
Iteration 4600: Loss = -11061.5087890625
Iteration 4700: Loss = -11061.4462890625
Iteration 4800: Loss = -11061.3916015625
Iteration 4900: Loss = -11061.34375
Iteration 5000: Loss = -11061.3017578125
Iteration 5100: Loss = -11061.2626953125
Iteration 5200: Loss = -11061.224609375
Iteration 5300: Loss = -11061.1923828125
Iteration 5400: Loss = -11061.162109375
Iteration 5500: Loss = -11061.134765625
Iteration 5600: Loss = -11061.107421875
Iteration 5700: Loss = -11061.0849609375
Iteration 5800: Loss = -11061.0615234375
Iteration 5900: Loss = -11061.04296875
Iteration 6000: Loss = -11061.0224609375
Iteration 6100: Loss = -11061.0029296875
Iteration 6200: Loss = -11060.984375
Iteration 6300: Loss = -11060.966796875
Iteration 6400: Loss = -11060.951171875
Iteration 6500: Loss = -11060.93359375
Iteration 6600: Loss = -11060.9169921875
Iteration 6700: Loss = -11060.9033203125
Iteration 6800: Loss = -11060.884765625
Iteration 6900: Loss = -11060.87109375
Iteration 7000: Loss = -11060.8564453125
Iteration 7100: Loss = -11060.84375
Iteration 7200: Loss = -11060.8291015625
Iteration 7300: Loss = -11060.8193359375
Iteration 7400: Loss = -11060.8076171875
Iteration 7500: Loss = -11060.7958984375
Iteration 7600: Loss = -11060.7880859375
Iteration 7700: Loss = -11060.779296875
Iteration 7800: Loss = -11060.771484375
Iteration 7900: Loss = -11060.763671875
Iteration 8000: Loss = -11060.7578125
Iteration 8100: Loss = -11060.7529296875
Iteration 8200: Loss = -11060.74609375
Iteration 8300: Loss = -11060.7421875
Iteration 8400: Loss = -11060.736328125
Iteration 8500: Loss = -11060.7333984375
Iteration 8600: Loss = -11060.73046875
Iteration 8700: Loss = -11060.728515625
Iteration 8800: Loss = -11060.7236328125
Iteration 8900: Loss = -11060.720703125
Iteration 9000: Loss = -11060.7197265625
Iteration 9100: Loss = -11060.716796875
Iteration 9200: Loss = -11060.7158203125
Iteration 9300: Loss = -11060.712890625
Iteration 9400: Loss = -11060.7119140625
Iteration 9500: Loss = -11060.708984375
Iteration 9600: Loss = -11060.7080078125
Iteration 9700: Loss = -11060.7060546875
Iteration 9800: Loss = -11060.7041015625
Iteration 9900: Loss = -11060.7021484375
Iteration 10000: Loss = -11060.7021484375
Iteration 10100: Loss = -11060.7001953125
Iteration 10200: Loss = -11060.7001953125
Iteration 10300: Loss = -11060.6982421875
Iteration 10400: Loss = -11060.6962890625
Iteration 10500: Loss = -11060.6953125
Iteration 10600: Loss = -11060.6953125
Iteration 10700: Loss = -11060.6943359375
Iteration 10800: Loss = -11060.693359375
Iteration 10900: Loss = -11060.693359375
Iteration 11000: Loss = -11060.693359375
Iteration 11100: Loss = -11060.69140625
Iteration 11200: Loss = -11060.6884765625
Iteration 11300: Loss = -11060.689453125
1
Iteration 11400: Loss = -11060.6875
Iteration 11500: Loss = -11060.6884765625
1
Iteration 11600: Loss = -11060.6884765625
2
Iteration 11700: Loss = -11060.6884765625
3
Iteration 11800: Loss = -11060.6865234375
Iteration 11900: Loss = -11060.6865234375
Iteration 12000: Loss = -11060.6875
1
Iteration 12100: Loss = -11060.6865234375
Iteration 12200: Loss = -11060.6845703125
Iteration 12300: Loss = -11060.685546875
1
Iteration 12400: Loss = -11060.68359375
Iteration 12500: Loss = -11060.6845703125
1
Iteration 12600: Loss = -11060.6845703125
2
Iteration 12700: Loss = -11060.68359375
Iteration 12800: Loss = -11060.68359375
Iteration 12900: Loss = -11060.68359375
Iteration 13000: Loss = -11060.681640625
Iteration 13100: Loss = -11060.6826171875
1
Iteration 13200: Loss = -11060.681640625
Iteration 13300: Loss = -11060.6806640625
Iteration 13400: Loss = -11060.6826171875
1
Iteration 13500: Loss = -11060.681640625
2
Iteration 13600: Loss = -11060.6826171875
3
Iteration 13700: Loss = -11060.681640625
4
Iteration 13800: Loss = -11060.681640625
5
Iteration 13900: Loss = -11060.6806640625
Iteration 14000: Loss = -11060.681640625
1
Iteration 14100: Loss = -11060.6806640625
Iteration 14200: Loss = -11060.681640625
1
Iteration 14300: Loss = -11060.6796875
Iteration 14400: Loss = -11060.6796875
Iteration 14500: Loss = -11060.6796875
Iteration 14600: Loss = -11060.6806640625
1
Iteration 14700: Loss = -11060.6796875
Iteration 14800: Loss = -11060.6796875
Iteration 14900: Loss = -11060.6787109375
Iteration 15000: Loss = -11060.6806640625
1
Iteration 15100: Loss = -11060.6806640625
2
Iteration 15200: Loss = -11060.6787109375
Iteration 15300: Loss = -11060.677734375
Iteration 15400: Loss = -11060.677734375
Iteration 15500: Loss = -11060.6787109375
1
Iteration 15600: Loss = -11060.677734375
Iteration 15700: Loss = -11060.6787109375
1
Iteration 15800: Loss = -11060.6787109375
2
Iteration 15900: Loss = -11060.677734375
Iteration 16000: Loss = -11060.6787109375
1
Iteration 16100: Loss = -11060.6767578125
Iteration 16200: Loss = -11060.6787109375
1
Iteration 16300: Loss = -11060.6767578125
Iteration 16400: Loss = -11060.677734375
1
Iteration 16500: Loss = -11060.67578125
Iteration 16600: Loss = -11060.6767578125
1
Iteration 16700: Loss = -11060.6748046875
Iteration 16800: Loss = -11060.6748046875
Iteration 16900: Loss = -11060.6748046875
Iteration 17000: Loss = -11060.67578125
1
Iteration 17100: Loss = -11060.67578125
2
Iteration 17200: Loss = -11060.67578125
3
Iteration 17300: Loss = -11060.673828125
Iteration 17400: Loss = -11060.6748046875
1
Iteration 17500: Loss = -11060.6748046875
2
Iteration 17600: Loss = -11060.67578125
3
Iteration 17700: Loss = -11060.67578125
4
Iteration 17800: Loss = -11060.673828125
Iteration 17900: Loss = -11060.6748046875
1
Iteration 18000: Loss = -11060.6748046875
2
Iteration 18100: Loss = -11060.67578125
3
Iteration 18200: Loss = -11060.67578125
4
Iteration 18300: Loss = -11060.6748046875
5
Iteration 18400: Loss = -11060.67578125
6
Iteration 18500: Loss = -11060.673828125
Iteration 18600: Loss = -11060.6767578125
1
Iteration 18700: Loss = -11060.67578125
2
Iteration 18800: Loss = -11060.67578125
3
Iteration 18900: Loss = -11060.67578125
4
Iteration 19000: Loss = -11060.6748046875
5
Iteration 19100: Loss = -11060.67578125
6
Iteration 19200: Loss = -11060.67578125
7
Iteration 19300: Loss = -11060.6748046875
8
Iteration 19400: Loss = -11060.67578125
9
Iteration 19500: Loss = -11060.6748046875
10
Iteration 19600: Loss = -11060.67578125
11
Iteration 19700: Loss = -11060.67578125
12
Iteration 19800: Loss = -11060.673828125
Iteration 19900: Loss = -11060.67578125
1
Iteration 20000: Loss = -11060.67578125
2
Iteration 20100: Loss = -11060.67578125
3
Iteration 20200: Loss = -11060.67578125
4
Iteration 20300: Loss = -11060.6748046875
5
Iteration 20400: Loss = -11060.673828125
Iteration 20500: Loss = -11060.67578125
1
Iteration 20600: Loss = -11060.6748046875
2
Iteration 20700: Loss = -11060.6767578125
3
Iteration 20800: Loss = -11060.6748046875
4
Iteration 20900: Loss = -11060.6748046875
5
Iteration 21000: Loss = -11060.6748046875
6
Iteration 21100: Loss = -11060.673828125
Iteration 21200: Loss = -11060.6748046875
1
Iteration 21300: Loss = -11060.6748046875
2
Iteration 21400: Loss = -11060.67578125
3
Iteration 21500: Loss = -11060.67578125
4
Iteration 21600: Loss = -11060.6767578125
5
Iteration 21700: Loss = -11060.6748046875
6
Iteration 21800: Loss = -11060.6748046875
7
Iteration 21900: Loss = -11060.6748046875
8
Iteration 22000: Loss = -11060.67578125
9
Iteration 22100: Loss = -11060.6748046875
10
Iteration 22200: Loss = -11060.67578125
11
Iteration 22300: Loss = -11060.677734375
12
Iteration 22400: Loss = -11060.6748046875
13
Iteration 22500: Loss = -11060.6787109375
14
Iteration 22600: Loss = -11060.6748046875
15
Stopping early at iteration 22600 due to no improvement.
pi: tensor([[9.5492e-01, 4.5077e-02],
        [9.9996e-01, 4.0867e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9818, 0.0182], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1626, 0.2851],
         [0.0077, 0.2205]],

        [[0.9866, 0.2239],
         [0.0083, 0.9895]],

        [[0.7375, 0.1318],
         [0.1199, 0.8252]],

        [[0.6714, 0.2532],
         [0.8501, 0.3269]],

        [[0.0501, 0.0704],
         [0.0238, 0.7816]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.015837733814333767
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: -0.0008915251244807935
Average Adjusted Rand Index: -0.0019730564143693244
[0.03493924811746628, -0.0008915251244807935] [0.8313918835261767, -0.0019730564143693244] [10933.9423828125, 11060.6748046875]
-------------------------------------
This iteration is 87
True Objective function: Loss = -10750.727681481323
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -56974.06640625
Iteration 100: Loss = -32252.25
Iteration 200: Loss = -16124.5771484375
Iteration 300: Loss = -12250.638671875
Iteration 400: Loss = -11582.6708984375
Iteration 500: Loss = -11337.25390625
Iteration 600: Loss = -11209.7666015625
Iteration 700: Loss = -11125.5068359375
Iteration 800: Loss = -11076.990234375
Iteration 900: Loss = -11041.283203125
Iteration 1000: Loss = -11016.7158203125
Iteration 1100: Loss = -10997.884765625
Iteration 1200: Loss = -10984.462890625
Iteration 1300: Loss = -10974.5634765625
Iteration 1400: Loss = -10966.7001953125
Iteration 1500: Loss = -10960.2412109375
Iteration 1600: Loss = -10954.7626953125
Iteration 1700: Loss = -10940.1806640625
Iteration 1800: Loss = -10931.685546875
Iteration 1900: Loss = -10928.0615234375
Iteration 2000: Loss = -10925.23046875
Iteration 2100: Loss = -10922.8173828125
Iteration 2200: Loss = -10920.7158203125
Iteration 2300: Loss = -10918.865234375
Iteration 2400: Loss = -10917.2255859375
Iteration 2500: Loss = -10915.7646484375
Iteration 2600: Loss = -10914.45703125
Iteration 2700: Loss = -10913.2841796875
Iteration 2800: Loss = -10912.2216796875
Iteration 2900: Loss = -10911.2578125
Iteration 3000: Loss = -10910.3818359375
Iteration 3100: Loss = -10909.5849609375
Iteration 3200: Loss = -10908.8623046875
Iteration 3300: Loss = -10908.19921875
Iteration 3400: Loss = -10907.5927734375
Iteration 3500: Loss = -10907.03515625
Iteration 3600: Loss = -10906.517578125
Iteration 3700: Loss = -10906.0283203125
Iteration 3800: Loss = -10902.65234375
Iteration 3900: Loss = -10899.486328125
Iteration 4000: Loss = -10898.990234375
Iteration 4100: Loss = -10898.59375
Iteration 4200: Loss = -10898.2412109375
Iteration 4300: Loss = -10897.91796875
Iteration 4400: Loss = -10897.6240234375
Iteration 4500: Loss = -10897.3505859375
Iteration 4600: Loss = -10897.095703125
Iteration 4700: Loss = -10896.859375
Iteration 4800: Loss = -10896.63671875
Iteration 4900: Loss = -10896.431640625
Iteration 5000: Loss = -10896.236328125
Iteration 5100: Loss = -10896.0537109375
Iteration 5200: Loss = -10895.8798828125
Iteration 5300: Loss = -10895.71484375
Iteration 5400: Loss = -10895.55859375
Iteration 5500: Loss = -10895.4052734375
Iteration 5600: Loss = -10895.259765625
Iteration 5700: Loss = -10895.126953125
Iteration 5800: Loss = -10895.009765625
Iteration 5900: Loss = -10894.9033203125
Iteration 6000: Loss = -10894.806640625
Iteration 6100: Loss = -10894.720703125
Iteration 6200: Loss = -10894.642578125
Iteration 6300: Loss = -10894.568359375
Iteration 6400: Loss = -10894.5029296875
Iteration 6500: Loss = -10894.4404296875
Iteration 6600: Loss = -10894.3828125
Iteration 6700: Loss = -10894.3271484375
Iteration 6800: Loss = -10894.2763671875
Iteration 6900: Loss = -10894.2265625
Iteration 7000: Loss = -10894.18359375
Iteration 7100: Loss = -10894.140625
Iteration 7200: Loss = -10894.099609375
Iteration 7300: Loss = -10894.0595703125
Iteration 7400: Loss = -10894.0224609375
Iteration 7500: Loss = -10893.98828125
Iteration 7600: Loss = -10893.953125
Iteration 7700: Loss = -10893.9208984375
Iteration 7800: Loss = -10893.890625
Iteration 7900: Loss = -10893.8603515625
Iteration 8000: Loss = -10893.833984375
Iteration 8100: Loss = -10893.8056640625
Iteration 8200: Loss = -10893.78125
Iteration 8300: Loss = -10893.7587890625
Iteration 8400: Loss = -10893.7353515625
Iteration 8500: Loss = -10893.712890625
Iteration 8600: Loss = -10893.693359375
Iteration 8700: Loss = -10893.673828125
Iteration 8800: Loss = -10893.654296875
Iteration 8900: Loss = -10893.634765625
Iteration 9000: Loss = -10893.619140625
Iteration 9100: Loss = -10893.6025390625
Iteration 9200: Loss = -10893.5859375
Iteration 9300: Loss = -10893.5693359375
Iteration 9400: Loss = -10893.5576171875
Iteration 9500: Loss = -10893.54296875
Iteration 9600: Loss = -10893.5302734375
Iteration 9700: Loss = -10893.515625
Iteration 9800: Loss = -10893.5029296875
Iteration 9900: Loss = -10893.4892578125
Iteration 10000: Loss = -10893.4775390625
Iteration 10100: Loss = -10893.462890625
Iteration 10200: Loss = -10893.443359375
Iteration 10300: Loss = -10893.3603515625
Iteration 10400: Loss = -10893.2529296875
Iteration 10500: Loss = -10893.21875
Iteration 10600: Loss = -10893.1904296875
Iteration 10700: Loss = -10893.16796875
Iteration 10800: Loss = -10893.146484375
Iteration 10900: Loss = -10893.1279296875
Iteration 11000: Loss = -10893.11328125
Iteration 11100: Loss = -10893.0966796875
Iteration 11200: Loss = -10893.0849609375
Iteration 11300: Loss = -10893.072265625
Iteration 11400: Loss = -10893.0625
Iteration 11500: Loss = -10893.052734375
Iteration 11600: Loss = -10893.041015625
Iteration 11700: Loss = -10893.0341796875
Iteration 11800: Loss = -10893.0263671875
Iteration 11900: Loss = -10893.0166015625
Iteration 12000: Loss = -10893.01171875
Iteration 12100: Loss = -10893.0048828125
Iteration 12200: Loss = -10893.0
Iteration 12300: Loss = -10892.994140625
Iteration 12400: Loss = -10892.9892578125
Iteration 12500: Loss = -10892.9814453125
Iteration 12600: Loss = -10892.9765625
Iteration 12700: Loss = -10892.97265625
Iteration 12800: Loss = -10892.96875
Iteration 12900: Loss = -10892.9619140625
Iteration 13000: Loss = -10892.9580078125
Iteration 13100: Loss = -10892.9521484375
Iteration 13200: Loss = -10892.94921875
Iteration 13300: Loss = -10892.943359375
Iteration 13400: Loss = -10892.9384765625
Iteration 13500: Loss = -10892.9345703125
Iteration 13600: Loss = -10892.9287109375
Iteration 13700: Loss = -10892.9228515625
Iteration 13800: Loss = -10892.916015625
Iteration 13900: Loss = -10892.9111328125
Iteration 14000: Loss = -10892.904296875
Iteration 14100: Loss = -10892.8984375
Iteration 14200: Loss = -10892.890625
Iteration 14300: Loss = -10892.880859375
Iteration 14400: Loss = -10892.8642578125
Iteration 14500: Loss = -10892.796875
Iteration 14600: Loss = -10891.2060546875
Iteration 14700: Loss = -10891.1650390625
Iteration 14800: Loss = -10891.1513671875
Iteration 14900: Loss = -10891.1455078125
Iteration 15000: Loss = -10891.1396484375
Iteration 15100: Loss = -10891.13671875
Iteration 15200: Loss = -10891.130859375
Iteration 15300: Loss = -10891.12890625
Iteration 15400: Loss = -10891.1259765625
Iteration 15500: Loss = -10891.125
Iteration 15600: Loss = -10891.123046875
Iteration 15700: Loss = -10891.1201171875
Iteration 15800: Loss = -10891.1181640625
Iteration 15900: Loss = -10891.1171875
Iteration 16000: Loss = -10891.11328125
Iteration 16100: Loss = -10891.1123046875
Iteration 16200: Loss = -10891.109375
Iteration 16300: Loss = -10891.1083984375
Iteration 16400: Loss = -10891.107421875
Iteration 16500: Loss = -10891.1044921875
Iteration 16600: Loss = -10891.10546875
1
Iteration 16700: Loss = -10891.1044921875
Iteration 16800: Loss = -10891.103515625
Iteration 16900: Loss = -10891.1005859375
Iteration 17000: Loss = -10891.1005859375
Iteration 17100: Loss = -10891.09765625
Iteration 17200: Loss = -10891.09765625
Iteration 17300: Loss = -10891.0947265625
Iteration 17400: Loss = -10891.091796875
Iteration 17500: Loss = -10891.0927734375
1
Iteration 17600: Loss = -10891.08984375
Iteration 17700: Loss = -10891.0869140625
Iteration 17800: Loss = -10891.087890625
1
Iteration 17900: Loss = -10891.0830078125
Iteration 18000: Loss = -10891.0810546875
Iteration 18100: Loss = -10891.0791015625
Iteration 18200: Loss = -10891.0771484375
Iteration 18300: Loss = -10891.078125
1
Iteration 18400: Loss = -10891.07421875
Iteration 18500: Loss = -10891.076171875
1
Iteration 18600: Loss = -10891.07421875
Iteration 18700: Loss = -10891.07421875
Iteration 18800: Loss = -10891.07421875
Iteration 18900: Loss = -10891.07421875
Iteration 19000: Loss = -10891.0732421875
Iteration 19100: Loss = -10891.0732421875
Iteration 19200: Loss = -10891.07421875
1
Iteration 19300: Loss = -10891.072265625
Iteration 19400: Loss = -10891.072265625
Iteration 19500: Loss = -10891.0732421875
1
Iteration 19600: Loss = -10891.0732421875
2
Iteration 19700: Loss = -10891.0732421875
3
Iteration 19800: Loss = -10891.0712890625
Iteration 19900: Loss = -10891.072265625
1
Iteration 20000: Loss = -10891.0712890625
Iteration 20100: Loss = -10891.0712890625
Iteration 20200: Loss = -10891.0712890625
Iteration 20300: Loss = -10891.0703125
Iteration 20400: Loss = -10891.0703125
Iteration 20500: Loss = -10891.072265625
1
Iteration 20600: Loss = -10891.0712890625
2
Iteration 20700: Loss = -10891.072265625
3
Iteration 20800: Loss = -10891.0712890625
4
Iteration 20900: Loss = -10891.0712890625
5
Iteration 21000: Loss = -10891.0703125
Iteration 21100: Loss = -10891.0712890625
1
Iteration 21200: Loss = -10891.0712890625
2
Iteration 21300: Loss = -10891.0712890625
3
Iteration 21400: Loss = -10891.072265625
4
Iteration 21500: Loss = -10891.0390625
Iteration 21600: Loss = -10889.7685546875
Iteration 21700: Loss = -10889.7158203125
Iteration 21800: Loss = -10889.7021484375
Iteration 21900: Loss = -10889.6953125
Iteration 22000: Loss = -10889.6923828125
Iteration 22100: Loss = -10889.6884765625
Iteration 22200: Loss = -10889.685546875
Iteration 22300: Loss = -10889.6865234375
1
Iteration 22400: Loss = -10889.68359375
Iteration 22500: Loss = -10889.68359375
Iteration 22600: Loss = -10889.681640625
Iteration 22700: Loss = -10889.6806640625
Iteration 22800: Loss = -10889.6796875
Iteration 22900: Loss = -10889.6796875
Iteration 23000: Loss = -10889.6796875
Iteration 23100: Loss = -10889.6796875
Iteration 23200: Loss = -10889.6796875
Iteration 23300: Loss = -10889.6787109375
Iteration 23400: Loss = -10889.6787109375
Iteration 23500: Loss = -10889.677734375
Iteration 23600: Loss = -10889.6748046875
Iteration 23700: Loss = -10889.5966796875
Iteration 23800: Loss = -10889.59375
Iteration 23900: Loss = -10889.59375
Iteration 24000: Loss = -10889.5927734375
Iteration 24100: Loss = -10889.59375
1
Iteration 24200: Loss = -10889.599609375
2
Iteration 24300: Loss = -10889.435546875
Iteration 24400: Loss = -10889.4345703125
Iteration 24500: Loss = -10889.431640625
Iteration 24600: Loss = -10889.43359375
1
Iteration 24700: Loss = -10889.4345703125
2
Iteration 24800: Loss = -10889.43359375
3
Iteration 24900: Loss = -10889.4326171875
4
Iteration 25000: Loss = -10889.4326171875
5
Iteration 25100: Loss = -10889.4306640625
Iteration 25200: Loss = -10889.431640625
1
Iteration 25300: Loss = -10889.4345703125
2
Iteration 25400: Loss = -10889.42578125
Iteration 25500: Loss = -10889.4248046875
Iteration 25600: Loss = -10889.42578125
1
Iteration 25700: Loss = -10889.423828125
Iteration 25800: Loss = -10889.4267578125
1
Iteration 25900: Loss = -10889.423828125
Iteration 26000: Loss = -10889.4248046875
1
Iteration 26100: Loss = -10889.423828125
Iteration 26200: Loss = -10889.42578125
1
Iteration 26300: Loss = -10889.4248046875
2
Iteration 26400: Loss = -10889.4228515625
Iteration 26500: Loss = -10889.4228515625
Iteration 26600: Loss = -10889.4248046875
1
Iteration 26700: Loss = -10889.41796875
Iteration 26800: Loss = -10889.40234375
Iteration 26900: Loss = -10888.3759765625
Iteration 27000: Loss = -10887.80859375
Iteration 27100: Loss = -10887.802734375
Iteration 27200: Loss = -10887.7978515625
Iteration 27300: Loss = -10887.7958984375
Iteration 27400: Loss = -10887.794921875
Iteration 27500: Loss = -10887.794921875
Iteration 27600: Loss = -10887.7900390625
Iteration 27700: Loss = -10887.7900390625
Iteration 27800: Loss = -10887.763671875
Iteration 27900: Loss = -10887.7607421875
Iteration 28000: Loss = -10887.734375
Iteration 28100: Loss = -10887.7353515625
1
Iteration 28200: Loss = -10887.5498046875
Iteration 28300: Loss = -10887.1376953125
Iteration 28400: Loss = -10886.9990234375
Iteration 28500: Loss = -10886.6708984375
Iteration 28600: Loss = -10886.654296875
Iteration 28700: Loss = -10886.5576171875
Iteration 28800: Loss = -10886.5244140625
Iteration 28900: Loss = -10886.5205078125
Iteration 29000: Loss = -10886.5205078125
Iteration 29100: Loss = -10886.51953125
Iteration 29200: Loss = -10886.5205078125
1
Iteration 29300: Loss = -10886.51953125
Iteration 29400: Loss = -10886.4404296875
Iteration 29500: Loss = -10886.2021484375
Iteration 29600: Loss = -10886.1923828125
Iteration 29700: Loss = -10886.1748046875
Iteration 29800: Loss = -10886.169921875
Iteration 29900: Loss = -10886.1689453125
pi: tensor([[1.0000e+00, 1.6131e-06],
        [8.8938e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9109, 0.0891], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1540, 0.1844],
         [0.4013, 0.2853]],

        [[0.0577, 0.2124],
         [0.0075, 0.1876]],

        [[0.0102, 0.1467],
         [0.1715, 0.8081]],

        [[0.7675, 0.1990],
         [0.9736, 0.9036]],

        [[0.2056, 0.1912],
         [0.5700, 0.9830]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.03604979046254031
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0023115331287292215
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.03219388124931273
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005914731726761669
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.005131431169739117
Global Adjusted Rand Index: 0.011961076835712292
Average Adjusted Rand Index: 0.011901808388816293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39490.25390625
Iteration 100: Loss = -22949.458984375
Iteration 200: Loss = -13756.482421875
Iteration 300: Loss = -11672.43359375
Iteration 400: Loss = -11331.1259765625
Iteration 500: Loss = -11146.8662109375
Iteration 600: Loss = -11070.7314453125
Iteration 700: Loss = -11027.697265625
Iteration 800: Loss = -10996.7080078125
Iteration 900: Loss = -10971.28515625
Iteration 1000: Loss = -10956.927734375
Iteration 1100: Loss = -10948.3974609375
Iteration 1200: Loss = -10941.7724609375
Iteration 1300: Loss = -10936.11328125
Iteration 1400: Loss = -10929.21875
Iteration 1500: Loss = -10921.9384765625
Iteration 1600: Loss = -10917.9375
Iteration 1700: Loss = -10914.0908203125
Iteration 1800: Loss = -10911.0615234375
Iteration 1900: Loss = -10908.8974609375
Iteration 2000: Loss = -10907.2138671875
Iteration 2100: Loss = -10905.814453125
Iteration 2200: Loss = -10904.6318359375
Iteration 2300: Loss = -10903.6083984375
Iteration 2400: Loss = -10902.7119140625
Iteration 2500: Loss = -10901.919921875
Iteration 2600: Loss = -10901.212890625
Iteration 2700: Loss = -10900.5791015625
Iteration 2800: Loss = -10900.0107421875
Iteration 2900: Loss = -10899.4970703125
Iteration 3000: Loss = -10899.0302734375
Iteration 3100: Loss = -10898.6064453125
Iteration 3200: Loss = -10898.22265625
Iteration 3300: Loss = -10897.87109375
Iteration 3400: Loss = -10897.548828125
Iteration 3500: Loss = -10897.25390625
Iteration 3600: Loss = -10896.9814453125
Iteration 3700: Loss = -10896.7294921875
Iteration 3800: Loss = -10896.498046875
Iteration 3900: Loss = -10896.2802734375
Iteration 4000: Loss = -10896.080078125
Iteration 4100: Loss = -10895.8935546875
Iteration 4200: Loss = -10895.71875
Iteration 4300: Loss = -10895.5537109375
Iteration 4400: Loss = -10895.400390625
Iteration 4500: Loss = -10895.2548828125
Iteration 4600: Loss = -10895.1201171875
Iteration 4700: Loss = -10894.99609375
Iteration 4800: Loss = -10894.8818359375
Iteration 4900: Loss = -10894.7763671875
Iteration 5000: Loss = -10894.6796875
Iteration 5100: Loss = -10894.591796875
Iteration 5200: Loss = -10894.509765625
Iteration 5300: Loss = -10894.4365234375
Iteration 5400: Loss = -10894.3701171875
Iteration 5500: Loss = -10894.3095703125
Iteration 5600: Loss = -10894.2529296875
Iteration 5700: Loss = -10894.201171875
Iteration 5800: Loss = -10894.1533203125
Iteration 5900: Loss = -10894.111328125
Iteration 6000: Loss = -10894.0732421875
Iteration 6100: Loss = -10894.03515625
Iteration 6200: Loss = -10894.001953125
Iteration 6300: Loss = -10893.9697265625
Iteration 6400: Loss = -10893.939453125
Iteration 6500: Loss = -10893.912109375
Iteration 6600: Loss = -10893.8857421875
Iteration 6700: Loss = -10893.861328125
Iteration 6800: Loss = -10893.83984375
Iteration 6900: Loss = -10893.818359375
Iteration 7000: Loss = -10893.798828125
Iteration 7100: Loss = -10893.7802734375
Iteration 7200: Loss = -10893.7626953125
Iteration 7300: Loss = -10893.7470703125
Iteration 7400: Loss = -10893.7294921875
Iteration 7500: Loss = -10893.71484375
Iteration 7600: Loss = -10893.69921875
Iteration 7700: Loss = -10893.685546875
Iteration 7800: Loss = -10893.6728515625
Iteration 7900: Loss = -10893.66015625
Iteration 8000: Loss = -10893.6484375
Iteration 8100: Loss = -10893.63671875
Iteration 8200: Loss = -10893.625
Iteration 8300: Loss = -10893.61328125
Iteration 8400: Loss = -10893.6015625
Iteration 8500: Loss = -10893.5908203125
Iteration 8600: Loss = -10893.578125
Iteration 8700: Loss = -10893.56640625
Iteration 8800: Loss = -10893.552734375
Iteration 8900: Loss = -10893.5419921875
Iteration 9000: Loss = -10893.5302734375
Iteration 9100: Loss = -10893.51953125
Iteration 9200: Loss = -10893.5087890625
Iteration 9300: Loss = -10893.4970703125
Iteration 9400: Loss = -10893.486328125
Iteration 9500: Loss = -10893.4775390625
Iteration 9600: Loss = -10893.470703125
Iteration 9700: Loss = -10893.462890625
Iteration 9800: Loss = -10893.4580078125
Iteration 9900: Loss = -10893.4521484375
Iteration 10000: Loss = -10893.4462890625
Iteration 10100: Loss = -10893.4423828125
Iteration 10200: Loss = -10893.4365234375
Iteration 10300: Loss = -10893.431640625
Iteration 10400: Loss = -10893.4267578125
Iteration 10500: Loss = -10893.4228515625
Iteration 10600: Loss = -10893.4169921875
Iteration 10700: Loss = -10893.412109375
Iteration 10800: Loss = -10893.40625
Iteration 10900: Loss = -10893.40234375
Iteration 11000: Loss = -10893.3994140625
Iteration 11100: Loss = -10893.392578125
Iteration 11200: Loss = -10893.3876953125
Iteration 11300: Loss = -10893.3828125
Iteration 11400: Loss = -10893.3779296875
Iteration 11500: Loss = -10893.37109375
Iteration 11600: Loss = -10893.3642578125
Iteration 11700: Loss = -10893.3515625
Iteration 11800: Loss = -10893.345703125
Iteration 11900: Loss = -10893.33984375
Iteration 12000: Loss = -10893.3359375
Iteration 12100: Loss = -10893.328125
Iteration 12200: Loss = -10893.32421875
Iteration 12300: Loss = -10893.318359375
Iteration 12400: Loss = -10893.3134765625
Iteration 12500: Loss = -10893.3056640625
Iteration 12600: Loss = -10893.2998046875
Iteration 12700: Loss = -10893.2939453125
Iteration 12800: Loss = -10893.2841796875
Iteration 12900: Loss = -10893.2763671875
Iteration 13000: Loss = -10893.2666015625
Iteration 13100: Loss = -10893.255859375
Iteration 13200: Loss = -10893.2373046875
Iteration 13300: Loss = -10893.2158203125
Iteration 13400: Loss = -10893.193359375
Iteration 13500: Loss = -10893.1591796875
Iteration 13600: Loss = -10893.1162109375
Iteration 13700: Loss = -10893.0517578125
Iteration 13800: Loss = -10892.9599609375
Iteration 13900: Loss = -10892.83984375
Iteration 14000: Loss = -10892.7275390625
Iteration 14100: Loss = -10892.6513671875
Iteration 14200: Loss = -10892.5947265625
Iteration 14300: Loss = -10892.5576171875
Iteration 14400: Loss = -10892.53125
Iteration 14500: Loss = -10892.5087890625
Iteration 14600: Loss = -10892.4736328125
Iteration 14700: Loss = -10892.4501953125
Iteration 14800: Loss = -10892.4248046875
Iteration 14900: Loss = -10892.3505859375
Iteration 15000: Loss = -10892.3037109375
Iteration 15100: Loss = -10892.255859375
Iteration 15200: Loss = -10892.2255859375
Iteration 15300: Loss = -10892.2041015625
Iteration 15400: Loss = -10892.18359375
Iteration 15500: Loss = -10892.173828125
Iteration 15600: Loss = -10892.1669921875
Iteration 15700: Loss = -10892.1650390625
Iteration 15800: Loss = -10892.1591796875
Iteration 15900: Loss = -10892.1591796875
Iteration 16000: Loss = -10892.1591796875
Iteration 16100: Loss = -10892.15625
Iteration 16200: Loss = -10892.1533203125
Iteration 16300: Loss = -10892.1484375
Iteration 16400: Loss = -10892.1279296875
Iteration 16500: Loss = -10889.087890625
Iteration 16600: Loss = -10864.564453125
Iteration 16700: Loss = -10812.8955078125
Iteration 16800: Loss = -10803.810546875
Iteration 16900: Loss = -10801.8076171875
Iteration 17000: Loss = -10794.166015625
Iteration 17100: Loss = -10789.533203125
Iteration 17200: Loss = -10788.828125
Iteration 17300: Loss = -10788.7529296875
Iteration 17400: Loss = -10788.7216796875
Iteration 17500: Loss = -10788.6484375
Iteration 17600: Loss = -10788.63671875
Iteration 17700: Loss = -10788.6142578125
Iteration 17800: Loss = -10788.421875
Iteration 17900: Loss = -10788.3828125
Iteration 18000: Loss = -10788.3828125
Iteration 18100: Loss = -10788.375
Iteration 18200: Loss = -10788.375
Iteration 18300: Loss = -10788.35546875
Iteration 18400: Loss = -10788.353515625
Iteration 18500: Loss = -10788.3525390625
Iteration 18600: Loss = -10788.3515625
Iteration 18700: Loss = -10788.3525390625
1
Iteration 18800: Loss = -10788.3515625
Iteration 18900: Loss = -10785.99609375
Iteration 19000: Loss = -10785.935546875
Iteration 19100: Loss = -10785.93359375
Iteration 19200: Loss = -10785.9326171875
Iteration 19300: Loss = -10785.931640625
Iteration 19400: Loss = -10785.931640625
Iteration 19500: Loss = -10785.9150390625
Iteration 19600: Loss = -10779.8046875
Iteration 19700: Loss = -10779.30859375
Iteration 19800: Loss = -10779.255859375
Iteration 19900: Loss = -10779.224609375
Iteration 20000: Loss = -10779.1328125
Iteration 20100: Loss = -10772.1650390625
Iteration 20200: Loss = -10765.904296875
Iteration 20300: Loss = -10751.90234375
Iteration 20400: Loss = -10751.76953125
Iteration 20500: Loss = -10751.7294921875
Iteration 20600: Loss = -10751.7099609375
Iteration 20700: Loss = -10751.697265625
Iteration 20800: Loss = -10751.6875
Iteration 20900: Loss = -10751.681640625
Iteration 21000: Loss = -10751.671875
Iteration 21100: Loss = -10751.6689453125
Iteration 21200: Loss = -10751.666015625
Iteration 21300: Loss = -10751.6630859375
Iteration 21400: Loss = -10751.662109375
Iteration 21500: Loss = -10751.6611328125
Iteration 21600: Loss = -10751.66015625
Iteration 21700: Loss = -10751.6591796875
Iteration 21800: Loss = -10751.658203125
Iteration 21900: Loss = -10751.658203125
Iteration 22000: Loss = -10751.65625
Iteration 22100: Loss = -10751.65625
Iteration 22200: Loss = -10751.6552734375
Iteration 22300: Loss = -10751.654296875
Iteration 22400: Loss = -10751.654296875
Iteration 22500: Loss = -10751.654296875
Iteration 22600: Loss = -10751.654296875
Iteration 22700: Loss = -10751.6552734375
1
Iteration 22800: Loss = -10751.654296875
Iteration 22900: Loss = -10751.6533203125
Iteration 23000: Loss = -10751.6357421875
Iteration 23100: Loss = -10751.634765625
Iteration 23200: Loss = -10751.6337890625
Iteration 23300: Loss = -10745.29296875
Iteration 23400: Loss = -10745.1240234375
Iteration 23500: Loss = -10745.10546875
Iteration 23600: Loss = -10745.09765625
Iteration 23700: Loss = -10745.091796875
Iteration 23800: Loss = -10745.0888671875
Iteration 23900: Loss = -10745.0888671875
Iteration 24000: Loss = -10745.0869140625
Iteration 24100: Loss = -10745.0859375
Iteration 24200: Loss = -10744.8974609375
Iteration 24300: Loss = -10738.2138671875
Iteration 24400: Loss = -10737.5205078125
Iteration 24500: Loss = -10734.6904296875
Iteration 24600: Loss = -10734.671875
Iteration 24700: Loss = -10734.6640625
Iteration 24800: Loss = -10734.66015625
Iteration 24900: Loss = -10734.65625
Iteration 25000: Loss = -10734.654296875
Iteration 25100: Loss = -10734.6533203125
Iteration 25200: Loss = -10734.650390625
Iteration 25300: Loss = -10734.650390625
Iteration 25400: Loss = -10734.6494140625
Iteration 25500: Loss = -10734.650390625
1
Iteration 25600: Loss = -10734.6474609375
Iteration 25700: Loss = -10734.6474609375
Iteration 25800: Loss = -10734.6484375
1
Iteration 25900: Loss = -10734.6474609375
Iteration 26000: Loss = -10734.646484375
Iteration 26100: Loss = -10734.646484375
Iteration 26200: Loss = -10734.646484375
Iteration 26300: Loss = -10734.6455078125
Iteration 26400: Loss = -10734.646484375
1
Iteration 26500: Loss = -10734.6455078125
Iteration 26600: Loss = -10734.646484375
1
Iteration 26700: Loss = -10734.6455078125
Iteration 26800: Loss = -10734.646484375
1
Iteration 26900: Loss = -10734.6416015625
Iteration 27000: Loss = -10734.6416015625
Iteration 27100: Loss = -10734.6416015625
Iteration 27200: Loss = -10734.6416015625
Iteration 27300: Loss = -10734.6435546875
1
Iteration 27400: Loss = -10734.6416015625
Iteration 27500: Loss = -10734.6416015625
Iteration 27600: Loss = -10734.6416015625
Iteration 27700: Loss = -10734.642578125
1
Iteration 27800: Loss = -10734.640625
Iteration 27900: Loss = -10734.6416015625
1
Iteration 28000: Loss = -10734.6416015625
2
Iteration 28100: Loss = -10734.640625
Iteration 28200: Loss = -10734.6416015625
1
Iteration 28300: Loss = -10729.1767578125
Iteration 28400: Loss = -10727.5263671875
Iteration 28500: Loss = -10727.49609375
Iteration 28600: Loss = -10727.484375
Iteration 28700: Loss = -10727.48046875
Iteration 28800: Loss = -10727.474609375
Iteration 28900: Loss = -10727.4716796875
Iteration 29000: Loss = -10727.470703125
Iteration 29100: Loss = -10727.4697265625
Iteration 29200: Loss = -10727.46875
Iteration 29300: Loss = -10727.4677734375
Iteration 29400: Loss = -10727.46484375
Iteration 29500: Loss = -10727.4638671875
Iteration 29600: Loss = -10727.462890625
Iteration 29700: Loss = -10727.4638671875
1
Iteration 29800: Loss = -10727.462890625
Iteration 29900: Loss = -10727.4638671875
1
pi: tensor([[0.8020, 0.1980],
        [0.2064, 0.7936]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4781, 0.5219], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2496, 0.0980],
         [0.9931, 0.2003]],

        [[0.1684, 0.0969],
         [0.9783, 0.0204]],

        [[0.0100, 0.0916],
         [0.0757, 0.7486]],

        [[0.8328, 0.0994],
         [0.9848, 0.5179]],

        [[0.0129, 0.0979],
         [0.0954, 0.4856]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.9446731404961201
Average Adjusted Rand Index: 0.9449664588706608
[0.011961076835712292, 0.9446731404961201] [0.011901808388816293, 0.9449664588706608] [10886.1279296875, 10727.462890625]
-------------------------------------
This iteration is 88
True Objective function: Loss = -10926.515562180504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21972.015625
Iteration 100: Loss = -14638.12109375
Iteration 200: Loss = -11817.1005859375
Iteration 300: Loss = -11343.3564453125
Iteration 400: Loss = -11207.7998046875
Iteration 500: Loss = -11147.865234375
Iteration 600: Loss = -11118.953125
Iteration 700: Loss = -11103.0947265625
Iteration 800: Loss = -11091.9140625
Iteration 900: Loss = -11079.1611328125
Iteration 1000: Loss = -11070.3916015625
Iteration 1100: Loss = -11059.7421875
Iteration 1200: Loss = -11050.8701171875
Iteration 1300: Loss = -11041.6337890625
Iteration 1400: Loss = -11033.5029296875
Iteration 1500: Loss = -11029.1611328125
Iteration 1600: Loss = -11024.8740234375
Iteration 1700: Loss = -11019.9052734375
Iteration 1800: Loss = -11016.3701171875
Iteration 1900: Loss = -11009.9326171875
Iteration 2000: Loss = -11006.560546875
Iteration 2100: Loss = -11005.0146484375
Iteration 2200: Loss = -11003.98046875
Iteration 2300: Loss = -11003.1806640625
Iteration 2400: Loss = -11002.4013671875
Iteration 2500: Loss = -11000.015625
Iteration 2600: Loss = -10998.34765625
Iteration 2700: Loss = -10995.7333984375
Iteration 2800: Loss = -10994.82421875
Iteration 2900: Loss = -10994.3154296875
Iteration 3000: Loss = -10993.93359375
Iteration 3100: Loss = -10993.6142578125
Iteration 3200: Loss = -10993.3291015625
Iteration 3300: Loss = -10993.0595703125
Iteration 3400: Loss = -10992.7724609375
Iteration 3500: Loss = -10992.40625
Iteration 3600: Loss = -10989.140625
Iteration 3700: Loss = -10988.37109375
Iteration 3800: Loss = -10987.5712890625
Iteration 3900: Loss = -10986.8876953125
Iteration 4000: Loss = -10986.37890625
Iteration 4100: Loss = -10986.00390625
Iteration 4200: Loss = -10985.724609375
Iteration 4300: Loss = -10985.509765625
Iteration 4400: Loss = -10985.341796875
Iteration 4500: Loss = -10985.2021484375
Iteration 4600: Loss = -10985.0908203125
Iteration 4700: Loss = -10984.99609375
Iteration 4800: Loss = -10984.9150390625
Iteration 4900: Loss = -10984.8447265625
Iteration 5000: Loss = -10984.783203125
Iteration 5100: Loss = -10984.7275390625
Iteration 5200: Loss = -10984.67578125
Iteration 5300: Loss = -10984.6279296875
Iteration 5400: Loss = -10984.583984375
Iteration 5500: Loss = -10984.546875
Iteration 5600: Loss = -10984.5087890625
Iteration 5700: Loss = -10984.470703125
Iteration 5800: Loss = -10984.4345703125
Iteration 5900: Loss = -10984.3984375
Iteration 6000: Loss = -10984.3642578125
Iteration 6100: Loss = -10984.33984375
Iteration 6200: Loss = -10984.31640625
Iteration 6300: Loss = -10984.296875
Iteration 6400: Loss = -10984.2802734375
Iteration 6500: Loss = -10984.2607421875
Iteration 6600: Loss = -10984.2451171875
Iteration 6700: Loss = -10984.2294921875
Iteration 6800: Loss = -10984.2158203125
Iteration 6900: Loss = -10984.2041015625
Iteration 7000: Loss = -10984.189453125
Iteration 7100: Loss = -10984.1787109375
Iteration 7200: Loss = -10984.166015625
Iteration 7300: Loss = -10984.1572265625
Iteration 7400: Loss = -10984.1474609375
Iteration 7500: Loss = -10984.1396484375
Iteration 7600: Loss = -10984.1298828125
Iteration 7700: Loss = -10984.1240234375
Iteration 7800: Loss = -10984.115234375
Iteration 7900: Loss = -10984.109375
Iteration 8000: Loss = -10984.1015625
Iteration 8100: Loss = -10984.0966796875
Iteration 8200: Loss = -10984.08984375
Iteration 8300: Loss = -10984.083984375
Iteration 8400: Loss = -10984.0771484375
Iteration 8500: Loss = -10984.07421875
Iteration 8600: Loss = -10984.068359375
Iteration 8700: Loss = -10984.0654296875
Iteration 8800: Loss = -10984.0595703125
Iteration 8900: Loss = -10984.0576171875
Iteration 9000: Loss = -10984.0517578125
Iteration 9100: Loss = -10984.0498046875
Iteration 9200: Loss = -10984.0439453125
Iteration 9300: Loss = -10984.04296875
Iteration 9400: Loss = -10984.0400390625
Iteration 9500: Loss = -10984.0361328125
Iteration 9600: Loss = -10984.0322265625
Iteration 9700: Loss = -10984.02734375
Iteration 9800: Loss = -10984.017578125
Iteration 9900: Loss = -10984.013671875
Iteration 10000: Loss = -10984.0107421875
Iteration 10100: Loss = -10984.009765625
Iteration 10200: Loss = -10984.005859375
Iteration 10300: Loss = -10984.0029296875
Iteration 10400: Loss = -10984.001953125
Iteration 10500: Loss = -10984.001953125
Iteration 10600: Loss = -10983.9990234375
Iteration 10700: Loss = -10983.9970703125
Iteration 10800: Loss = -10983.994140625
Iteration 10900: Loss = -10983.994140625
Iteration 11000: Loss = -10983.9931640625
Iteration 11100: Loss = -10983.9912109375
Iteration 11200: Loss = -10983.9912109375
Iteration 11300: Loss = -10983.9873046875
Iteration 11400: Loss = -10983.986328125
Iteration 11500: Loss = -10983.982421875
Iteration 11600: Loss = -10983.98046875
Iteration 11700: Loss = -10983.9794921875
Iteration 11800: Loss = -10983.978515625
Iteration 11900: Loss = -10983.9775390625
Iteration 12000: Loss = -10983.9765625
Iteration 12100: Loss = -10983.9755859375
Iteration 12200: Loss = -10983.9755859375
Iteration 12300: Loss = -10983.97265625
Iteration 12400: Loss = -10983.974609375
1
Iteration 12500: Loss = -10983.97265625
Iteration 12600: Loss = -10983.974609375
1
Iteration 12700: Loss = -10979.150390625
Iteration 12800: Loss = -10978.9228515625
Iteration 12900: Loss = -10978.8759765625
Iteration 13000: Loss = -10978.85546875
Iteration 13100: Loss = -10978.8427734375
Iteration 13200: Loss = -10978.8349609375
Iteration 13300: Loss = -10978.8291015625
Iteration 13400: Loss = -10978.826171875
Iteration 13500: Loss = -10978.8212890625
Iteration 13600: Loss = -10978.8193359375
Iteration 13700: Loss = -10978.81640625
Iteration 13800: Loss = -10978.814453125
Iteration 13900: Loss = -10978.8134765625
Iteration 14000: Loss = -10978.8115234375
Iteration 14100: Loss = -10978.8095703125
Iteration 14200: Loss = -10978.8095703125
Iteration 14300: Loss = -10978.80859375
Iteration 14400: Loss = -10978.80859375
Iteration 14500: Loss = -10978.806640625
Iteration 14600: Loss = -10978.806640625
Iteration 14700: Loss = -10978.8056640625
Iteration 14800: Loss = -10978.8056640625
Iteration 14900: Loss = -10978.8056640625
Iteration 15000: Loss = -10978.802734375
Iteration 15100: Loss = -10978.8037109375
1
Iteration 15200: Loss = -10978.8017578125
Iteration 15300: Loss = -10978.802734375
1
Iteration 15400: Loss = -10978.80078125
Iteration 15500: Loss = -10978.7978515625
Iteration 15600: Loss = -10978.7978515625
Iteration 15700: Loss = -10978.79296875
Iteration 15800: Loss = -10978.7783203125
Iteration 15900: Loss = -10978.716796875
Iteration 16000: Loss = -10978.5693359375
Iteration 16100: Loss = -10978.546875
Iteration 16200: Loss = -10978.46484375
Iteration 16300: Loss = -10978.359375
Iteration 16400: Loss = -10978.3505859375
Iteration 16500: Loss = -10978.3466796875
Iteration 16600: Loss = -10978.3408203125
Iteration 16700: Loss = -10978.2314453125
Iteration 16800: Loss = -10978.2216796875
Iteration 16900: Loss = -10978.0771484375
Iteration 17000: Loss = -10978.0751953125
Iteration 17100: Loss = -10978.072265625
Iteration 17200: Loss = -10978.0693359375
Iteration 17300: Loss = -10978.064453125
Iteration 17400: Loss = -10978.064453125
Iteration 17500: Loss = -10978.0654296875
1
Iteration 17600: Loss = -10978.0537109375
Iteration 17700: Loss = -10978.029296875
Iteration 17800: Loss = -10978.015625
Iteration 17900: Loss = -10978.0126953125
Iteration 18000: Loss = -10978.0078125
Iteration 18100: Loss = -10977.7646484375
Iteration 18200: Loss = -10977.7490234375
Iteration 18300: Loss = -10977.744140625
Iteration 18400: Loss = -10977.7353515625
Iteration 18500: Loss = -10977.7333984375
Iteration 18600: Loss = -10977.732421875
Iteration 18700: Loss = -10977.7275390625
Iteration 18800: Loss = -10977.724609375
Iteration 18900: Loss = -10977.724609375
Iteration 19000: Loss = -10977.7216796875
Iteration 19100: Loss = -10977.7177734375
Iteration 19200: Loss = -10977.7158203125
Iteration 19300: Loss = -10977.705078125
Iteration 19400: Loss = -10977.701171875
Iteration 19500: Loss = -10977.7001953125
Iteration 19600: Loss = -10977.7001953125
Iteration 19700: Loss = -10977.69921875
Iteration 19800: Loss = -10977.6982421875
Iteration 19900: Loss = -10977.6982421875
Iteration 20000: Loss = -10977.69921875
1
Iteration 20100: Loss = -10977.6982421875
Iteration 20200: Loss = -10977.697265625
Iteration 20300: Loss = -10977.697265625
Iteration 20400: Loss = -10977.6962890625
Iteration 20500: Loss = -10977.6982421875
1
Iteration 20600: Loss = -10977.6953125
Iteration 20700: Loss = -10977.634765625
Iteration 20800: Loss = -10977.6337890625
Iteration 20900: Loss = -10977.6357421875
1
Iteration 21000: Loss = -10977.6337890625
Iteration 21100: Loss = -10977.6337890625
Iteration 21200: Loss = -10977.634765625
1
Iteration 21300: Loss = -10977.634765625
2
Iteration 21400: Loss = -10977.634765625
3
Iteration 21500: Loss = -10977.634765625
4
Iteration 21600: Loss = -10977.634765625
5
Iteration 21700: Loss = -10977.6337890625
Iteration 21800: Loss = -10977.6357421875
1
Iteration 21900: Loss = -10977.6337890625
Iteration 22000: Loss = -10977.6357421875
1
Iteration 22100: Loss = -10977.634765625
2
Iteration 22200: Loss = -10977.634765625
3
Iteration 22300: Loss = -10977.634765625
4
Iteration 22400: Loss = -10977.6337890625
Iteration 22500: Loss = -10977.634765625
1
Iteration 22600: Loss = -10977.6337890625
Iteration 22700: Loss = -10977.6337890625
Iteration 22800: Loss = -10977.6337890625
Iteration 22900: Loss = -10977.6123046875
Iteration 23000: Loss = -10977.611328125
Iteration 23100: Loss = -10977.6025390625
Iteration 23200: Loss = -10977.5576171875
Iteration 23300: Loss = -10977.5498046875
Iteration 23400: Loss = -10977.5517578125
1
Iteration 23500: Loss = -10977.55078125
2
Iteration 23600: Loss = -10977.55078125
3
Iteration 23700: Loss = -10977.529296875
Iteration 23800: Loss = -10977.529296875
Iteration 23900: Loss = -10977.529296875
Iteration 24000: Loss = -10977.529296875
Iteration 24100: Loss = -10977.5283203125
Iteration 24200: Loss = -10977.5166015625
Iteration 24300: Loss = -10977.515625
Iteration 24400: Loss = -10977.517578125
1
Iteration 24500: Loss = -10977.5166015625
2
Iteration 24600: Loss = -10977.5166015625
3
Iteration 24700: Loss = -10977.5166015625
4
Iteration 24800: Loss = -10977.515625
Iteration 24900: Loss = -10977.517578125
1
Iteration 25000: Loss = -10977.515625
Iteration 25100: Loss = -10977.513671875
Iteration 25200: Loss = -10977.513671875
Iteration 25300: Loss = -10977.5146484375
1
Iteration 25400: Loss = -10977.5146484375
2
Iteration 25500: Loss = -10977.5126953125
Iteration 25600: Loss = -10977.5126953125
Iteration 25700: Loss = -10977.513671875
1
Iteration 25800: Loss = -10977.5126953125
Iteration 25900: Loss = -10977.513671875
1
Iteration 26000: Loss = -10977.51171875
Iteration 26100: Loss = -10977.5126953125
1
Iteration 26200: Loss = -10977.4775390625
Iteration 26300: Loss = -10977.4765625
Iteration 26400: Loss = -10977.4765625
Iteration 26500: Loss = -10977.4775390625
1
Iteration 26600: Loss = -10977.478515625
2
Iteration 26700: Loss = -10977.4765625
Iteration 26800: Loss = -10977.4765625
Iteration 26900: Loss = -10977.4755859375
Iteration 27000: Loss = -10977.4794921875
1
Iteration 27100: Loss = -10977.478515625
2
Iteration 27200: Loss = -10977.4765625
3
Iteration 27300: Loss = -10977.4765625
4
Iteration 27400: Loss = -10977.4775390625
5
Iteration 27500: Loss = -10977.4765625
6
Iteration 27600: Loss = -10977.4765625
7
Iteration 27700: Loss = -10977.4775390625
8
Iteration 27800: Loss = -10977.4765625
9
Iteration 27900: Loss = -10977.4765625
10
Iteration 28000: Loss = -10977.4755859375
Iteration 28100: Loss = -10977.4765625
1
Iteration 28200: Loss = -10977.4775390625
2
Iteration 28300: Loss = -10977.4765625
3
Iteration 28400: Loss = -10977.4765625
4
Iteration 28500: Loss = -10977.4765625
5
Iteration 28600: Loss = -10977.4775390625
6
Iteration 28700: Loss = -10977.4765625
7
Iteration 28800: Loss = -10977.4765625
8
Iteration 28900: Loss = -10977.478515625
9
Iteration 29000: Loss = -10977.4765625
10
Iteration 29100: Loss = -10977.4775390625
11
Iteration 29200: Loss = -10977.4775390625
12
Iteration 29300: Loss = -10977.4775390625
13
Iteration 29400: Loss = -10977.4765625
14
Iteration 29500: Loss = -10977.4775390625
15
Stopping early at iteration 29500 due to no improvement.
pi: tensor([[3.3167e-06, 1.0000e+00],
        [2.8380e-02, 9.7162e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0462, 0.9538], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1601, 0.0692],
         [0.9695, 0.1640]],

        [[0.1915, 0.1193],
         [0.0475, 0.3326]],

        [[0.0419, 0.1335],
         [0.9480, 0.0133]],

        [[0.9871, 0.2534],
         [0.0594, 0.0304]],

        [[0.7001, 0.1666],
         [0.3806, 0.8507]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011670747769946018
Average Adjusted Rand Index: -0.0006519855017482344
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -51405.57421875
Iteration 100: Loss = -29520.091796875
Iteration 200: Loss = -15313.884765625
Iteration 300: Loss = -12346.8759765625
Iteration 400: Loss = -11756.732421875
Iteration 500: Loss = -11506.5517578125
Iteration 600: Loss = -11311.509765625
Iteration 700: Loss = -11225.66015625
Iteration 800: Loss = -11147.7548828125
Iteration 900: Loss = -11110.505859375
Iteration 1000: Loss = -11084.322265625
Iteration 1100: Loss = -11068.9873046875
Iteration 1200: Loss = -11057.5947265625
Iteration 1300: Loss = -11048.4853515625
Iteration 1400: Loss = -11039.3837890625
Iteration 1500: Loss = -11029.359375
Iteration 1600: Loss = -11022.9609375
Iteration 1700: Loss = -11017.8701171875
Iteration 1800: Loss = -11013.84765625
Iteration 1900: Loss = -11010.5478515625
Iteration 2000: Loss = -11007.7626953125
Iteration 2100: Loss = -11005.3759765625
Iteration 2200: Loss = -11003.298828125
Iteration 2300: Loss = -11001.4794921875
Iteration 2400: Loss = -10999.8720703125
Iteration 2500: Loss = -10998.4453125
Iteration 2600: Loss = -10997.1728515625
Iteration 2700: Loss = -10996.03125
Iteration 2800: Loss = -10995.00390625
Iteration 2900: Loss = -10994.076171875
Iteration 3000: Loss = -10993.2333984375
Iteration 3100: Loss = -10992.4677734375
Iteration 3200: Loss = -10991.767578125
Iteration 3300: Loss = -10991.119140625
Iteration 3400: Loss = -10990.521484375
Iteration 3500: Loss = -10989.9814453125
Iteration 3600: Loss = -10989.4833984375
Iteration 3700: Loss = -10989.0244140625
Iteration 3800: Loss = -10988.6015625
Iteration 3900: Loss = -10988.2099609375
Iteration 4000: Loss = -10987.8466796875
Iteration 4100: Loss = -10987.5107421875
Iteration 4200: Loss = -10987.197265625
Iteration 4300: Loss = -10986.90625
Iteration 4400: Loss = -10986.63671875
Iteration 4500: Loss = -10986.3837890625
Iteration 4600: Loss = -10986.150390625
Iteration 4700: Loss = -10985.931640625
Iteration 4800: Loss = -10985.7275390625
Iteration 4900: Loss = -10985.5380859375
Iteration 5000: Loss = -10985.3583984375
Iteration 5100: Loss = -10985.19140625
Iteration 5200: Loss = -10985.037109375
Iteration 5300: Loss = -10984.888671875
Iteration 5400: Loss = -10984.7529296875
Iteration 5500: Loss = -10984.623046875
Iteration 5600: Loss = -10984.5029296875
Iteration 5700: Loss = -10984.388671875
Iteration 5800: Loss = -10984.2802734375
Iteration 5900: Loss = -10984.1826171875
Iteration 6000: Loss = -10984.0859375
Iteration 6100: Loss = -10983.998046875
Iteration 6200: Loss = -10983.9130859375
Iteration 6300: Loss = -10983.8330078125
Iteration 6400: Loss = -10983.759765625
Iteration 6500: Loss = -10983.689453125
Iteration 6600: Loss = -10983.6240234375
Iteration 6700: Loss = -10983.560546875
Iteration 6800: Loss = -10983.5029296875
Iteration 6900: Loss = -10983.4462890625
Iteration 7000: Loss = -10983.39453125
Iteration 7100: Loss = -10983.345703125
Iteration 7200: Loss = -10983.2978515625
Iteration 7300: Loss = -10983.2529296875
Iteration 7400: Loss = -10983.2119140625
Iteration 7500: Loss = -10983.169921875
Iteration 7600: Loss = -10983.1337890625
Iteration 7700: Loss = -10983.09765625
Iteration 7800: Loss = -10983.064453125
Iteration 7900: Loss = -10983.0322265625
Iteration 8000: Loss = -10983.0029296875
Iteration 8100: Loss = -10982.9736328125
Iteration 8200: Loss = -10982.9453125
Iteration 8300: Loss = -10982.9208984375
Iteration 8400: Loss = -10982.8955078125
Iteration 8500: Loss = -10982.87109375
Iteration 8600: Loss = -10982.8505859375
Iteration 8700: Loss = -10982.828125
Iteration 8800: Loss = -10982.8076171875
Iteration 8900: Loss = -10982.7900390625
Iteration 9000: Loss = -10982.7705078125
Iteration 9100: Loss = -10982.7548828125
Iteration 9200: Loss = -10982.7373046875
Iteration 9300: Loss = -10982.7216796875
Iteration 9400: Loss = -10982.70703125
Iteration 9500: Loss = -10982.6943359375
Iteration 9600: Loss = -10982.6806640625
Iteration 9700: Loss = -10982.66796875
Iteration 9800: Loss = -10982.6552734375
Iteration 9900: Loss = -10982.6435546875
Iteration 10000: Loss = -10982.6328125
Iteration 10100: Loss = -10982.6220703125
Iteration 10200: Loss = -10982.611328125
Iteration 10300: Loss = -10982.6025390625
Iteration 10400: Loss = -10982.5927734375
Iteration 10500: Loss = -10982.5859375
Iteration 10600: Loss = -10982.5771484375
Iteration 10700: Loss = -10982.5693359375
Iteration 10800: Loss = -10982.5625
Iteration 10900: Loss = -10982.5556640625
Iteration 11000: Loss = -10982.5498046875
Iteration 11100: Loss = -10982.541015625
Iteration 11200: Loss = -10982.5380859375
Iteration 11300: Loss = -10982.5302734375
Iteration 11400: Loss = -10982.525390625
Iteration 11500: Loss = -10982.51953125
Iteration 11600: Loss = -10982.515625
Iteration 11700: Loss = -10982.5087890625
Iteration 11800: Loss = -10982.5048828125
Iteration 11900: Loss = -10982.5009765625
Iteration 12000: Loss = -10982.4970703125
Iteration 12100: Loss = -10982.4931640625
Iteration 12200: Loss = -10982.490234375
Iteration 12300: Loss = -10982.4873046875
Iteration 12400: Loss = -10982.482421875
Iteration 12500: Loss = -10982.4794921875
Iteration 12600: Loss = -10982.4755859375
Iteration 12700: Loss = -10982.4736328125
Iteration 12800: Loss = -10982.470703125
Iteration 12900: Loss = -10982.4677734375
Iteration 13000: Loss = -10982.4638671875
Iteration 13100: Loss = -10982.462890625
Iteration 13200: Loss = -10982.4619140625
Iteration 13300: Loss = -10982.4580078125
Iteration 13400: Loss = -10982.4560546875
Iteration 13500: Loss = -10982.451171875
Iteration 13600: Loss = -10982.447265625
Iteration 13700: Loss = -10982.4248046875
Iteration 13800: Loss = -10981.7099609375
Iteration 13900: Loss = -10981.5908203125
Iteration 14000: Loss = -10981.5234375
Iteration 14100: Loss = -10981.451171875
Iteration 14200: Loss = -10981.341796875
Iteration 14300: Loss = -10980.23046875
Iteration 14400: Loss = -10979.4013671875
Iteration 14500: Loss = -10978.9482421875
Iteration 14600: Loss = -10978.8544921875
Iteration 14700: Loss = -10978.833984375
Iteration 14800: Loss = -10978.8193359375
Iteration 14900: Loss = -10978.814453125
Iteration 15000: Loss = -10978.8125
Iteration 15100: Loss = -10978.80859375
Iteration 15200: Loss = -10978.806640625
Iteration 15300: Loss = -10978.8046875
Iteration 15400: Loss = -10978.8046875
Iteration 15500: Loss = -10978.8037109375
Iteration 15600: Loss = -10978.802734375
Iteration 15700: Loss = -10978.8017578125
Iteration 15800: Loss = -10978.802734375
1
Iteration 15900: Loss = -10978.7998046875
Iteration 16000: Loss = -10978.8017578125
1
Iteration 16100: Loss = -10978.80078125
2
Iteration 16200: Loss = -10978.80078125
3
Iteration 16300: Loss = -10978.7998046875
Iteration 16400: Loss = -10978.7998046875
Iteration 16500: Loss = -10978.798828125
Iteration 16600: Loss = -10978.7998046875
1
Iteration 16700: Loss = -10978.794921875
Iteration 16800: Loss = -10978.794921875
Iteration 16900: Loss = -10978.7919921875
Iteration 17000: Loss = -10978.7919921875
Iteration 17100: Loss = -10978.79296875
1
Iteration 17200: Loss = -10978.791015625
Iteration 17300: Loss = -10978.791015625
Iteration 17400: Loss = -10978.7900390625
Iteration 17500: Loss = -10978.7880859375
Iteration 17600: Loss = -10978.7890625
1
Iteration 17700: Loss = -10978.7890625
2
Iteration 17800: Loss = -10978.7880859375
Iteration 17900: Loss = -10978.7861328125
Iteration 18000: Loss = -10978.7841796875
Iteration 18100: Loss = -10978.783203125
Iteration 18200: Loss = -10978.77734375
Iteration 18300: Loss = -10978.765625
Iteration 18400: Loss = -10978.6962890625
Iteration 18500: Loss = -10978.4892578125
Iteration 18600: Loss = -10978.2939453125
Iteration 18700: Loss = -10978.2080078125
Iteration 18800: Loss = -10978.16796875
Iteration 18900: Loss = -10978.134765625
Iteration 19000: Loss = -10978.0498046875
Iteration 19100: Loss = -10977.9921875
Iteration 19200: Loss = -10977.9169921875
Iteration 19300: Loss = -10977.8828125
Iteration 19400: Loss = -10977.8115234375
Iteration 19500: Loss = -10977.759765625
Iteration 19600: Loss = -10977.7197265625
Iteration 19700: Loss = -10977.6884765625
Iteration 19800: Loss = -10977.662109375
Iteration 19900: Loss = -10977.650390625
Iteration 20000: Loss = -10977.6435546875
Iteration 20100: Loss = -10977.6357421875
Iteration 20200: Loss = -10977.626953125
Iteration 20300: Loss = -10977.623046875
Iteration 20400: Loss = -10977.619140625
Iteration 20500: Loss = -10977.6162109375
Iteration 20600: Loss = -10977.611328125
Iteration 20700: Loss = -10977.6103515625
Iteration 20800: Loss = -10977.6083984375
Iteration 20900: Loss = -10977.6083984375
Iteration 21000: Loss = -10977.607421875
Iteration 21100: Loss = -10977.6064453125
Iteration 21200: Loss = -10977.607421875
1
Iteration 21300: Loss = -10977.60546875
Iteration 21400: Loss = -10977.6044921875
Iteration 21500: Loss = -10977.6044921875
Iteration 21600: Loss = -10977.6044921875
Iteration 21700: Loss = -10977.6044921875
Iteration 21800: Loss = -10977.60546875
1
Iteration 21900: Loss = -10977.6044921875
Iteration 22000: Loss = -10977.603515625
Iteration 22100: Loss = -10977.603515625
Iteration 22200: Loss = -10977.6044921875
1
Iteration 22300: Loss = -10977.60546875
2
Iteration 22400: Loss = -10977.603515625
Iteration 22500: Loss = -10977.6044921875
1
Iteration 22600: Loss = -10977.603515625
Iteration 22700: Loss = -10977.603515625
Iteration 22800: Loss = -10977.603515625
Iteration 22900: Loss = -10977.603515625
Iteration 23000: Loss = -10977.6044921875
1
Iteration 23100: Loss = -10977.603515625
Iteration 23200: Loss = -10977.6044921875
1
Iteration 23300: Loss = -10977.6044921875
2
Iteration 23400: Loss = -10977.603515625
Iteration 23500: Loss = -10977.6044921875
1
Iteration 23600: Loss = -10977.6025390625
Iteration 23700: Loss = -10977.6044921875
1
Iteration 23800: Loss = -10977.6025390625
Iteration 23900: Loss = -10977.6044921875
1
Iteration 24000: Loss = -10977.6025390625
Iteration 24100: Loss = -10977.6025390625
Iteration 24200: Loss = -10977.603515625
1
Iteration 24300: Loss = -10977.603515625
2
Iteration 24400: Loss = -10977.6025390625
Iteration 24500: Loss = -10977.6044921875
1
Iteration 24600: Loss = -10977.6044921875
2
Iteration 24700: Loss = -10977.6044921875
3
Iteration 24800: Loss = -10977.603515625
4
Iteration 24900: Loss = -10977.603515625
5
Iteration 25000: Loss = -10977.6044921875
6
Iteration 25100: Loss = -10977.60546875
7
Iteration 25200: Loss = -10977.6044921875
8
Iteration 25300: Loss = -10977.6025390625
Iteration 25400: Loss = -10977.607421875
1
Iteration 25500: Loss = -10977.603515625
2
Iteration 25600: Loss = -10977.60546875
3
Iteration 25700: Loss = -10977.6044921875
4
Iteration 25800: Loss = -10977.6044921875
5
Iteration 25900: Loss = -10977.603515625
6
Iteration 26000: Loss = -10977.6025390625
Iteration 26100: Loss = -10977.6044921875
1
Iteration 26200: Loss = -10977.6044921875
2
Iteration 26300: Loss = -10977.603515625
3
Iteration 26400: Loss = -10977.6044921875
4
Iteration 26500: Loss = -10977.6015625
Iteration 26600: Loss = -10977.603515625
1
Iteration 26700: Loss = -10977.6044921875
2
Iteration 26800: Loss = -10977.603515625
3
Iteration 26900: Loss = -10977.6044921875
4
Iteration 27000: Loss = -10977.603515625
5
Iteration 27100: Loss = -10977.603515625
6
Iteration 27200: Loss = -10977.603515625
7
Iteration 27300: Loss = -10977.6015625
Iteration 27400: Loss = -10977.603515625
1
Iteration 27500: Loss = -10977.6025390625
2
Iteration 27600: Loss = -10977.6025390625
3
Iteration 27700: Loss = -10977.6025390625
4
Iteration 27800: Loss = -10977.603515625
5
Iteration 27900: Loss = -10977.6044921875
6
Iteration 28000: Loss = -10977.6025390625
7
Iteration 28100: Loss = -10977.6025390625
8
Iteration 28200: Loss = -10977.603515625
9
Iteration 28300: Loss = -10977.6044921875
10
Iteration 28400: Loss = -10977.6044921875
11
Iteration 28500: Loss = -10977.603515625
12
Iteration 28600: Loss = -10977.603515625
13
Iteration 28700: Loss = -10977.603515625
14
Iteration 28800: Loss = -10977.6044921875
15
Stopping early at iteration 28800 due to no improvement.
pi: tensor([[5.9113e-06, 9.9999e-01],
        [2.4425e-02, 9.7557e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0462, 0.9538], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1618, 0.0692],
         [0.7878, 0.1640]],

        [[0.0119, 0.1163],
         [0.0139, 0.0812]],

        [[0.8356, 0.1316],
         [0.9699, 0.8059]],

        [[0.0183, 0.2577],
         [0.2819, 0.9618]],

        [[0.9657, 0.1667],
         [0.1961, 0.9821]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011670747769946018
Average Adjusted Rand Index: -0.0006519855017482344
[-0.0011670747769946018, -0.0011670747769946018] [-0.0006519855017482344, -0.0006519855017482344] [10977.4775390625, 10977.6044921875]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11011.627641105853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34757.5625
Iteration 100: Loss = -22653.294921875
Iteration 200: Loss = -13971.7275390625
Iteration 300: Loss = -11909.2314453125
Iteration 400: Loss = -11553.830078125
Iteration 500: Loss = -11427.359375
Iteration 600: Loss = -11345.837890625
Iteration 700: Loss = -11301.298828125
Iteration 800: Loss = -11267.0703125
Iteration 900: Loss = -11236.8359375
Iteration 1000: Loss = -11218.8916015625
Iteration 1100: Loss = -11204.7099609375
Iteration 1200: Loss = -11192.4384765625
Iteration 1300: Loss = -11179.5966796875
Iteration 1400: Loss = -11173.587890625
Iteration 1500: Loss = -11168.6826171875
Iteration 1600: Loss = -11164.5791015625
Iteration 1700: Loss = -11161.1669921875
Iteration 1800: Loss = -11158.384765625
Iteration 1900: Loss = -11156.177734375
Iteration 2000: Loss = -11154.42578125
Iteration 2100: Loss = -11152.986328125
Iteration 2200: Loss = -11151.6865234375
Iteration 2300: Loss = -11149.3916015625
Iteration 2400: Loss = -11147.830078125
Iteration 2500: Loss = -11147.048828125
Iteration 2600: Loss = -11146.41015625
Iteration 2700: Loss = -11145.8671875
Iteration 2800: Loss = -11145.392578125
Iteration 2900: Loss = -11144.966796875
Iteration 3000: Loss = -11144.5703125
Iteration 3100: Loss = -11139.6611328125
Iteration 3200: Loss = -11139.17578125
Iteration 3300: Loss = -11138.6552734375
Iteration 3400: Loss = -11134.287109375
Iteration 3500: Loss = -11133.505859375
Iteration 3600: Loss = -11133.1494140625
Iteration 3700: Loss = -11132.8720703125
Iteration 3800: Loss = -11132.6337890625
Iteration 3900: Loss = -11132.423828125
Iteration 4000: Loss = -11132.234375
Iteration 4100: Loss = -11132.0634765625
Iteration 4200: Loss = -11131.9052734375
Iteration 4300: Loss = -11131.7587890625
Iteration 4400: Loss = -11131.625
Iteration 4500: Loss = -11131.4990234375
Iteration 4600: Loss = -11131.3818359375
Iteration 4700: Loss = -11131.2685546875
Iteration 4800: Loss = -11131.1611328125
Iteration 4900: Loss = -11131.056640625
Iteration 5000: Loss = -11130.9521484375
Iteration 5100: Loss = -11130.8427734375
Iteration 5200: Loss = -11130.7255859375
Iteration 5300: Loss = -11130.5869140625
Iteration 5400: Loss = -11130.41796875
Iteration 5500: Loss = -11130.2216796875
Iteration 5600: Loss = -11130.033203125
Iteration 5700: Loss = -11129.875
Iteration 5800: Loss = -11129.7470703125
Iteration 5900: Loss = -11129.640625
Iteration 6000: Loss = -11129.544921875
Iteration 6100: Loss = -11129.4619140625
Iteration 6200: Loss = -11129.388671875
Iteration 6300: Loss = -11129.3212890625
Iteration 6400: Loss = -11129.255859375
Iteration 6500: Loss = -11129.197265625
Iteration 6600: Loss = -11129.140625
Iteration 6700: Loss = -11129.0869140625
Iteration 6800: Loss = -11129.0390625
Iteration 6900: Loss = -11128.9833984375
Iteration 7000: Loss = -11123.8642578125
Iteration 7100: Loss = -11123.59375
Iteration 7200: Loss = -11123.451171875
Iteration 7300: Loss = -11123.34765625
Iteration 7400: Loss = -11123.2607421875
Iteration 7500: Loss = -11123.1865234375
Iteration 7600: Loss = -11123.119140625
Iteration 7700: Loss = -11123.0576171875
Iteration 7800: Loss = -11123.0
Iteration 7900: Loss = -11122.943359375
Iteration 8000: Loss = -11122.890625
Iteration 8100: Loss = -11122.8359375
Iteration 8200: Loss = -11122.7841796875
Iteration 8300: Loss = -11122.7294921875
Iteration 8400: Loss = -11122.677734375
Iteration 8500: Loss = -11122.6220703125
Iteration 8600: Loss = -11122.5654296875
Iteration 8700: Loss = -11122.5078125
Iteration 8800: Loss = -11122.4482421875
Iteration 8900: Loss = -11122.3876953125
Iteration 9000: Loss = -11122.3271484375
Iteration 9100: Loss = -11122.2646484375
Iteration 9200: Loss = -11122.20703125
Iteration 9300: Loss = -11122.1494140625
Iteration 9400: Loss = -11122.09375
Iteration 9500: Loss = -11122.046875
Iteration 9600: Loss = -11122.005859375
Iteration 9700: Loss = -11121.9677734375
Iteration 9800: Loss = -11121.9375
Iteration 9900: Loss = -11121.9111328125
Iteration 10000: Loss = -11121.892578125
Iteration 10100: Loss = -11121.875
Iteration 10200: Loss = -11121.86328125
Iteration 10300: Loss = -11121.853515625
Iteration 10400: Loss = -11121.841796875
Iteration 10500: Loss = -11121.8349609375
Iteration 10600: Loss = -11121.826171875
Iteration 10700: Loss = -11121.8212890625
Iteration 10800: Loss = -11121.8154296875
Iteration 10900: Loss = -11121.810546875
Iteration 11000: Loss = -11121.8076171875
Iteration 11100: Loss = -11121.8017578125
Iteration 11200: Loss = -11121.7978515625
Iteration 11300: Loss = -11121.7939453125
Iteration 11400: Loss = -11121.7900390625
Iteration 11500: Loss = -11121.787109375
Iteration 11600: Loss = -11121.7841796875
Iteration 11700: Loss = -11121.78125
Iteration 11800: Loss = -11121.7783203125
Iteration 11900: Loss = -11121.7763671875
Iteration 12000: Loss = -11121.7734375
Iteration 12100: Loss = -11121.7705078125
Iteration 12200: Loss = -11121.7685546875
Iteration 12300: Loss = -11121.765625
Iteration 12400: Loss = -11121.7646484375
Iteration 12500: Loss = -11121.7626953125
Iteration 12600: Loss = -11121.7607421875
Iteration 12700: Loss = -11121.7587890625
Iteration 12800: Loss = -11121.7587890625
Iteration 12900: Loss = -11121.7578125
Iteration 13000: Loss = -11121.7548828125
Iteration 13100: Loss = -11121.75390625
Iteration 13200: Loss = -11121.751953125
Iteration 13300: Loss = -11121.7509765625
Iteration 13400: Loss = -11121.7509765625
Iteration 13500: Loss = -11121.748046875
Iteration 13600: Loss = -11121.74609375
Iteration 13700: Loss = -11121.7451171875
Iteration 13800: Loss = -11121.7451171875
Iteration 13900: Loss = -11121.7451171875
Iteration 14000: Loss = -11121.744140625
Iteration 14100: Loss = -11121.7431640625
Iteration 14200: Loss = -11121.7421875
Iteration 14300: Loss = -11121.7431640625
1
Iteration 14400: Loss = -11121.7412109375
Iteration 14500: Loss = -11121.740234375
Iteration 14600: Loss = -11121.740234375
Iteration 14700: Loss = -11121.7392578125
Iteration 14800: Loss = -11121.7373046875
Iteration 14900: Loss = -11121.73828125
1
Iteration 15000: Loss = -11121.73828125
2
Iteration 15100: Loss = -11121.73828125
3
Iteration 15200: Loss = -11121.7373046875
Iteration 15300: Loss = -11121.736328125
Iteration 15400: Loss = -11121.736328125
Iteration 15500: Loss = -11121.7353515625
Iteration 15600: Loss = -11121.734375
Iteration 15700: Loss = -11121.734375
Iteration 15800: Loss = -11121.734375
Iteration 15900: Loss = -11121.734375
Iteration 16000: Loss = -11121.7353515625
1
Iteration 16100: Loss = -11121.734375
Iteration 16200: Loss = -11121.734375
Iteration 16300: Loss = -11121.7353515625
1
Iteration 16400: Loss = -11121.7333984375
Iteration 16500: Loss = -11121.7314453125
Iteration 16600: Loss = -11121.732421875
1
Iteration 16700: Loss = -11121.7314453125
Iteration 16800: Loss = -11121.7314453125
Iteration 16900: Loss = -11121.73046875
Iteration 17000: Loss = -11121.7314453125
1
Iteration 17100: Loss = -11121.7314453125
2
Iteration 17200: Loss = -11121.7314453125
3
Iteration 17300: Loss = -11121.7314453125
4
Iteration 17400: Loss = -11121.73046875
Iteration 17500: Loss = -11121.73046875
Iteration 17600: Loss = -11121.7294921875
Iteration 17700: Loss = -11121.73046875
1
Iteration 17800: Loss = -11121.728515625
Iteration 17900: Loss = -11121.728515625
Iteration 18000: Loss = -11121.7294921875
1
Iteration 18100: Loss = -11121.73046875
2
Iteration 18200: Loss = -11121.73046875
3
Iteration 18300: Loss = -11121.7294921875
4
Iteration 18400: Loss = -11121.7294921875
5
Iteration 18500: Loss = -11121.7294921875
6
Iteration 18600: Loss = -11121.73046875
7
Iteration 18700: Loss = -11121.728515625
Iteration 18800: Loss = -11121.7314453125
1
Iteration 18900: Loss = -11121.7294921875
2
Iteration 19000: Loss = -11121.7275390625
Iteration 19100: Loss = -11121.7294921875
1
Iteration 19200: Loss = -11121.73046875
2
Iteration 19300: Loss = -11121.728515625
3
Iteration 19400: Loss = -11121.728515625
4
Iteration 19500: Loss = -11121.7294921875
5
Iteration 19600: Loss = -11121.7294921875
6
Iteration 19700: Loss = -11121.728515625
7
Iteration 19800: Loss = -11121.7275390625
Iteration 19900: Loss = -11121.7275390625
Iteration 20000: Loss = -11121.728515625
1
Iteration 20100: Loss = -11121.728515625
2
Iteration 20200: Loss = -11121.7275390625
Iteration 20300: Loss = -11121.7294921875
1
Iteration 20400: Loss = -11121.728515625
2
Iteration 20500: Loss = -11121.728515625
3
Iteration 20600: Loss = -11121.728515625
4
Iteration 20700: Loss = -11121.728515625
5
Iteration 20800: Loss = -11121.728515625
6
Iteration 20900: Loss = -11121.728515625
7
Iteration 21000: Loss = -11121.7275390625
Iteration 21100: Loss = -11121.728515625
1
Iteration 21200: Loss = -11121.728515625
2
Iteration 21300: Loss = -11121.7275390625
Iteration 21400: Loss = -11121.7275390625
Iteration 21500: Loss = -11121.728515625
1
Iteration 21600: Loss = -11121.728515625
2
Iteration 21700: Loss = -11121.7265625
Iteration 21800: Loss = -11121.73046875
1
Iteration 21900: Loss = -11121.728515625
2
Iteration 22000: Loss = -11121.7294921875
3
Iteration 22100: Loss = -11121.7294921875
4
Iteration 22200: Loss = -11121.728515625
5
Iteration 22300: Loss = -11121.7275390625
6
Iteration 22400: Loss = -11121.7294921875
7
Iteration 22500: Loss = -11121.7265625
Iteration 22600: Loss = -11121.7265625
Iteration 22700: Loss = -11121.728515625
1
Iteration 22800: Loss = -11121.7275390625
2
Iteration 22900: Loss = -11121.728515625
3
Iteration 23000: Loss = -11121.7275390625
4
Iteration 23100: Loss = -11121.7275390625
5
Iteration 23200: Loss = -11121.728515625
6
Iteration 23300: Loss = -11121.728515625
7
Iteration 23400: Loss = -11121.7275390625
8
Iteration 23500: Loss = -11121.7265625
Iteration 23600: Loss = -11121.7294921875
1
Iteration 23700: Loss = -11121.728515625
2
Iteration 23800: Loss = -11121.7275390625
3
Iteration 23900: Loss = -11121.7265625
Iteration 24000: Loss = -11121.7275390625
1
Iteration 24100: Loss = -11121.728515625
2
Iteration 24200: Loss = -11121.7294921875
3
Iteration 24300: Loss = -11121.728515625
4
Iteration 24400: Loss = -11121.732421875
5
Iteration 24500: Loss = -11121.7294921875
6
Iteration 24600: Loss = -11121.7294921875
7
Iteration 24700: Loss = -11121.7294921875
8
Iteration 24800: Loss = -11121.728515625
9
Iteration 24900: Loss = -11121.7275390625
10
Iteration 25000: Loss = -11121.728515625
11
Iteration 25100: Loss = -11121.728515625
12
Iteration 25200: Loss = -11121.7294921875
13
Iteration 25300: Loss = -11121.728515625
14
Iteration 25400: Loss = -11121.7275390625
15
Stopping early at iteration 25400 due to no improvement.
pi: tensor([[8.3913e-07, 1.0000e+00],
        [3.8257e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0695, 0.9305], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0527, 0.1048],
         [0.9804, 0.1677]],

        [[0.0617, 0.1863],
         [0.9381, 0.7353]],

        [[0.8866, 0.1705],
         [0.9416, 0.9408]],

        [[0.0342, 0.2401],
         [0.3046, 0.3490]],

        [[0.1555, 0.2606],
         [0.0077, 0.0632]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: -0.002274891251268548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17274.291015625
Iteration 100: Loss = -12926.791015625
Iteration 200: Loss = -11503.5703125
Iteration 300: Loss = -11210.5390625
Iteration 400: Loss = -11158.0859375
Iteration 500: Loss = -11141.392578125
Iteration 600: Loss = -11135.2255859375
Iteration 700: Loss = -11131.9091796875
Iteration 800: Loss = -11129.822265625
Iteration 900: Loss = -11128.3662109375
Iteration 1000: Loss = -11127.2900390625
Iteration 1100: Loss = -11126.4638671875
Iteration 1200: Loss = -11125.7880859375
Iteration 1300: Loss = -11125.2431640625
Iteration 1400: Loss = -11124.787109375
Iteration 1500: Loss = -11124.4072265625
Iteration 1600: Loss = -11124.0859375
Iteration 1700: Loss = -11123.82421875
Iteration 1800: Loss = -11123.6044921875
Iteration 1900: Loss = -11123.4140625
Iteration 2000: Loss = -11123.248046875
Iteration 2100: Loss = -11123.103515625
Iteration 2200: Loss = -11122.9541015625
Iteration 2300: Loss = -11122.8349609375
Iteration 2400: Loss = -11122.7373046875
Iteration 2500: Loss = -11122.65234375
Iteration 2600: Loss = -11122.57421875
Iteration 2700: Loss = -11122.5068359375
Iteration 2800: Loss = -11122.4423828125
Iteration 2900: Loss = -11122.3857421875
Iteration 3000: Loss = -11122.33203125
Iteration 3100: Loss = -11122.2822265625
Iteration 3200: Loss = -11122.232421875
Iteration 3300: Loss = -11122.1884765625
Iteration 3400: Loss = -11122.1435546875
Iteration 3500: Loss = -11122.099609375
Iteration 3600: Loss = -11122.0556640625
Iteration 3700: Loss = -11122.009765625
Iteration 3800: Loss = -11121.9609375
Iteration 3900: Loss = -11121.9091796875
Iteration 4000: Loss = -11121.845703125
Iteration 4100: Loss = -11121.765625
Iteration 4200: Loss = -11121.63671875
Iteration 4300: Loss = -11121.3935546875
Iteration 4400: Loss = -11120.8876953125
Iteration 4500: Loss = -11120.298828125
Iteration 4600: Loss = -11119.744140625
Iteration 4700: Loss = -11119.1220703125
Iteration 4800: Loss = -11118.32421875
Iteration 4900: Loss = -11117.3623046875
Iteration 5000: Loss = -11116.1064453125
Iteration 5100: Loss = -11109.6298828125
Iteration 5200: Loss = -11058.4658203125
Iteration 5300: Loss = -11024.908203125
Iteration 5400: Loss = -11016.3369140625
Iteration 5500: Loss = -11012.7939453125
Iteration 5600: Loss = -11009.09375
Iteration 5700: Loss = -11008.9296875
Iteration 5800: Loss = -11008.830078125
Iteration 5900: Loss = -11008.7548828125
Iteration 6000: Loss = -11008.6904296875
Iteration 6100: Loss = -11008.5859375
Iteration 6200: Loss = -11008.3583984375
Iteration 6300: Loss = -11008.259765625
Iteration 6400: Loss = -11008.15234375
Iteration 6500: Loss = -11007.9716796875
Iteration 6600: Loss = -11007.3818359375
Iteration 6700: Loss = -11005.46484375
Iteration 6800: Loss = -10994.193359375
Iteration 6900: Loss = -10975.9521484375
Iteration 7000: Loss = -10969.1162109375
Iteration 7100: Loss = -10968.8759765625
Iteration 7200: Loss = -10968.8095703125
Iteration 7300: Loss = -10968.7744140625
Iteration 7400: Loss = -10968.7548828125
Iteration 7500: Loss = -10968.7412109375
Iteration 7600: Loss = -10968.7314453125
Iteration 7700: Loss = -10968.72265625
Iteration 7800: Loss = -10968.71875
Iteration 7900: Loss = -10968.7119140625
Iteration 8000: Loss = -10968.708984375
Iteration 8100: Loss = -10968.705078125
Iteration 8200: Loss = -10968.701171875
Iteration 8300: Loss = -10968.6982421875
Iteration 8400: Loss = -10968.6953125
Iteration 8500: Loss = -10968.6953125
Iteration 8600: Loss = -10968.69140625
Iteration 8700: Loss = -10968.6884765625
Iteration 8800: Loss = -10968.68359375
Iteration 8900: Loss = -10968.6767578125
Iteration 9000: Loss = -10968.6728515625
Iteration 9100: Loss = -10968.6708984375
Iteration 9200: Loss = -10968.669921875
Iteration 9300: Loss = -10968.66796875
Iteration 9400: Loss = -10968.6669921875
Iteration 9500: Loss = -10968.666015625
Iteration 9600: Loss = -10968.6650390625
Iteration 9700: Loss = -10968.6640625
Iteration 9800: Loss = -10968.6630859375
Iteration 9900: Loss = -10968.6640625
1
Iteration 10000: Loss = -10968.6630859375
Iteration 10100: Loss = -10968.662109375
Iteration 10200: Loss = -10968.662109375
Iteration 10300: Loss = -10968.6611328125
Iteration 10400: Loss = -10968.6611328125
Iteration 10500: Loss = -10968.6611328125
Iteration 10600: Loss = -10968.66015625
Iteration 10700: Loss = -10968.66015625
Iteration 10800: Loss = -10968.66015625
Iteration 10900: Loss = -10968.66015625
Iteration 11000: Loss = -10968.66015625
Iteration 11100: Loss = -10968.66015625
Iteration 11200: Loss = -10968.66015625
Iteration 11300: Loss = -10968.658203125
Iteration 11400: Loss = -10968.6591796875
1
Iteration 11500: Loss = -10968.658203125
Iteration 11600: Loss = -10968.6484375
Iteration 11700: Loss = -10968.6240234375
Iteration 11800: Loss = -10968.623046875
Iteration 11900: Loss = -10968.623046875
Iteration 12000: Loss = -10968.6220703125
Iteration 12100: Loss = -10968.6220703125
Iteration 12200: Loss = -10968.623046875
1
Iteration 12300: Loss = -10968.6220703125
Iteration 12400: Loss = -10968.6220703125
Iteration 12500: Loss = -10968.62109375
Iteration 12600: Loss = -10968.6220703125
1
Iteration 12700: Loss = -10968.62109375
Iteration 12800: Loss = -10968.62109375
Iteration 12900: Loss = -10968.62109375
Iteration 13000: Loss = -10968.62109375
Iteration 13100: Loss = -10968.62109375
Iteration 13200: Loss = -10968.6220703125
1
Iteration 13300: Loss = -10968.62109375
Iteration 13400: Loss = -10968.62109375
Iteration 13500: Loss = -10968.6201171875
Iteration 13600: Loss = -10968.62109375
1
Iteration 13700: Loss = -10968.6201171875
Iteration 13800: Loss = -10968.6201171875
Iteration 13900: Loss = -10968.62109375
1
Iteration 14000: Loss = -10968.6181640625
Iteration 14100: Loss = -10968.615234375
Iteration 14200: Loss = -10968.6142578125
Iteration 14300: Loss = -10968.615234375
1
Iteration 14400: Loss = -10968.615234375
2
Iteration 14500: Loss = -10968.6142578125
Iteration 14600: Loss = -10968.6142578125
Iteration 14700: Loss = -10968.6142578125
Iteration 14800: Loss = -10968.6142578125
Iteration 14900: Loss = -10968.6142578125
Iteration 15000: Loss = -10968.61328125
Iteration 15100: Loss = -10968.6142578125
1
Iteration 15200: Loss = -10968.615234375
2
Iteration 15300: Loss = -10968.61328125
Iteration 15400: Loss = -10968.615234375
1
Iteration 15500: Loss = -10968.6142578125
2
Iteration 15600: Loss = -10968.6142578125
3
Iteration 15700: Loss = -10968.615234375
4
Iteration 15800: Loss = -10968.61328125
Iteration 15900: Loss = -10968.6142578125
1
Iteration 16000: Loss = -10968.6142578125
2
Iteration 16100: Loss = -10968.61328125
Iteration 16200: Loss = -10968.6142578125
1
Iteration 16300: Loss = -10968.6142578125
2
Iteration 16400: Loss = -10968.6142578125
3
Iteration 16500: Loss = -10968.6142578125
4
Iteration 16600: Loss = -10968.6142578125
5
Iteration 16700: Loss = -10968.6142578125
6
Iteration 16800: Loss = -10968.61328125
Iteration 16900: Loss = -10968.61328125
Iteration 17000: Loss = -10968.6142578125
1
Iteration 17100: Loss = -10968.61328125
Iteration 17200: Loss = -10968.61328125
Iteration 17300: Loss = -10968.615234375
1
Iteration 17400: Loss = -10968.6142578125
2
Iteration 17500: Loss = -10968.6142578125
3
Iteration 17600: Loss = -10968.615234375
4
Iteration 17700: Loss = -10968.611328125
Iteration 17800: Loss = -10968.611328125
Iteration 17900: Loss = -10968.6123046875
1
Iteration 18000: Loss = -10968.609375
Iteration 18100: Loss = -10968.6103515625
1
Iteration 18200: Loss = -10968.6103515625
2
Iteration 18300: Loss = -10968.611328125
3
Iteration 18400: Loss = -10968.61328125
4
Iteration 18500: Loss = -10968.6103515625
5
Iteration 18600: Loss = -10968.611328125
6
Iteration 18700: Loss = -10968.611328125
7
Iteration 18800: Loss = -10968.6103515625
8
Iteration 18900: Loss = -10968.6103515625
9
Iteration 19000: Loss = -10968.6103515625
10
Iteration 19100: Loss = -10968.611328125
11
Iteration 19200: Loss = -10968.6103515625
12
Iteration 19300: Loss = -10968.6103515625
13
Iteration 19400: Loss = -10968.611328125
14
Iteration 19500: Loss = -10968.611328125
15
Stopping early at iteration 19500 due to no improvement.
pi: tensor([[0.7970, 0.2030],
        [0.2587, 0.7413]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4032, 0.5968], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2605, 0.1008],
         [0.9834, 0.1988]],

        [[0.5004, 0.0954],
         [0.9905, 0.9299]],

        [[0.9912, 0.1108],
         [0.8108, 0.4903]],

        [[0.7045, 0.1138],
         [0.0689, 0.0594]],

        [[0.3672, 0.0908],
         [0.9765, 0.9288]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448509923071951
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.8683602308355959
Average Adjusted Rand Index: 0.8680238690987674
[3.206515228510384e-05, 0.8683602308355959] [-0.002274891251268548, 0.8680238690987674] [11121.7275390625, 10968.611328125]
-------------------------------------
This iteration is 90
True Objective function: Loss = -10881.51246830235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27062.48046875
Iteration 100: Loss = -17696.701171875
Iteration 200: Loss = -12718.80078125
Iteration 300: Loss = -11707.4033203125
Iteration 400: Loss = -11373.98046875
Iteration 500: Loss = -11231.4921875
Iteration 600: Loss = -11154.896484375
Iteration 700: Loss = -11111.49609375
Iteration 800: Loss = -11084.4013671875
Iteration 900: Loss = -11072.779296875
Iteration 1000: Loss = -11064.1748046875
Iteration 1100: Loss = -11055.0107421875
Iteration 1200: Loss = -11047.9326171875
Iteration 1300: Loss = -11042.3203125
Iteration 1400: Loss = -11037.8125
Iteration 1500: Loss = -11034.4140625
Iteration 1600: Loss = -11031.9306640625
Iteration 1700: Loss = -11030.05859375
Iteration 1800: Loss = -11028.45703125
Iteration 1900: Loss = -11026.919921875
Iteration 2000: Loss = -11025.302734375
Iteration 2100: Loss = -11023.46875
Iteration 2200: Loss = -11018.13671875
Iteration 2300: Loss = -11014.90234375
Iteration 2400: Loss = -11013.2509765625
Iteration 2500: Loss = -11011.9228515625
Iteration 2600: Loss = -11010.8193359375
Iteration 2700: Loss = -11009.783203125
Iteration 2800: Loss = -11008.74609375
Iteration 2900: Loss = -11007.259765625
Iteration 3000: Loss = -11005.095703125
Iteration 3100: Loss = -11003.5
Iteration 3200: Loss = -11002.5634765625
Iteration 3300: Loss = -11001.412109375
Iteration 3400: Loss = -10999.5712890625
Iteration 3500: Loss = -10998.498046875
Iteration 3600: Loss = -10997.6748046875
Iteration 3700: Loss = -10995.04296875
Iteration 3800: Loss = -10994.193359375
Iteration 3900: Loss = -10993.7724609375
Iteration 4000: Loss = -10993.4404296875
Iteration 4100: Loss = -10993.166015625
Iteration 4200: Loss = -10992.931640625
Iteration 4300: Loss = -10992.7275390625
Iteration 4400: Loss = -10992.5439453125
Iteration 4500: Loss = -10990.6533203125
Iteration 4600: Loss = -10989.0927734375
Iteration 4700: Loss = -10988.833984375
Iteration 4800: Loss = -10988.6123046875
Iteration 4900: Loss = -10988.4150390625
Iteration 5000: Loss = -10988.2392578125
Iteration 5100: Loss = -10988.08203125
Iteration 5200: Loss = -10987.939453125
Iteration 5300: Loss = -10987.798828125
Iteration 5400: Loss = -10987.08984375
Iteration 5500: Loss = -10985.361328125
Iteration 5600: Loss = -10985.01171875
Iteration 5700: Loss = -10984.8271484375
Iteration 5800: Loss = -10984.7021484375
Iteration 5900: Loss = -10984.6025390625
Iteration 6000: Loss = -10984.521484375
Iteration 6100: Loss = -10984.455078125
Iteration 6200: Loss = -10984.392578125
Iteration 6300: Loss = -10984.33984375
Iteration 6400: Loss = -10984.2919921875
Iteration 6500: Loss = -10984.2470703125
Iteration 6600: Loss = -10984.20703125
Iteration 6700: Loss = -10984.1708984375
Iteration 6800: Loss = -10984.1376953125
Iteration 6900: Loss = -10984.107421875
Iteration 7000: Loss = -10984.0771484375
Iteration 7100: Loss = -10984.05078125
Iteration 7200: Loss = -10984.0283203125
Iteration 7300: Loss = -10984.0048828125
Iteration 7400: Loss = -10983.9814453125
Iteration 7500: Loss = -10983.9638671875
Iteration 7600: Loss = -10983.943359375
Iteration 7700: Loss = -10983.9267578125
Iteration 7800: Loss = -10983.91015625
Iteration 7900: Loss = -10983.8935546875
Iteration 8000: Loss = -10983.8798828125
Iteration 8100: Loss = -10983.8671875
Iteration 8200: Loss = -10983.8525390625
Iteration 8300: Loss = -10983.8408203125
Iteration 8400: Loss = -10983.830078125
Iteration 8500: Loss = -10983.8193359375
Iteration 8600: Loss = -10983.810546875
Iteration 8700: Loss = -10983.80078125
Iteration 8800: Loss = -10983.7919921875
Iteration 8900: Loss = -10983.7822265625
Iteration 9000: Loss = -10983.7744140625
Iteration 9100: Loss = -10983.765625
Iteration 9200: Loss = -10983.759765625
Iteration 9300: Loss = -10983.751953125
Iteration 9400: Loss = -10983.74609375
Iteration 9500: Loss = -10983.7392578125
Iteration 9600: Loss = -10983.734375
Iteration 9700: Loss = -10983.7294921875
Iteration 9800: Loss = -10983.724609375
Iteration 9900: Loss = -10983.7197265625
Iteration 10000: Loss = -10983.7138671875
Iteration 10100: Loss = -10983.7099609375
Iteration 10200: Loss = -10983.705078125
Iteration 10300: Loss = -10983.7021484375
Iteration 10400: Loss = -10983.6982421875
Iteration 10500: Loss = -10983.6943359375
Iteration 10600: Loss = -10983.693359375
Iteration 10700: Loss = -10983.6875
Iteration 10800: Loss = -10983.685546875
Iteration 10900: Loss = -10983.6826171875
Iteration 11000: Loss = -10983.6787109375
Iteration 11100: Loss = -10983.67578125
Iteration 11200: Loss = -10983.67578125
Iteration 11300: Loss = -10983.6728515625
Iteration 11400: Loss = -10983.6708984375
Iteration 11500: Loss = -10983.6689453125
Iteration 11600: Loss = -10983.666015625
Iteration 11700: Loss = -10983.6640625
Iteration 11800: Loss = -10983.6630859375
Iteration 11900: Loss = -10983.662109375
Iteration 12000: Loss = -10983.66015625
Iteration 12100: Loss = -10983.65625
Iteration 12200: Loss = -10983.6552734375
Iteration 12300: Loss = -10983.65625
1
Iteration 12400: Loss = -10983.6533203125
Iteration 12500: Loss = -10983.6533203125
Iteration 12600: Loss = -10983.6513671875
Iteration 12700: Loss = -10983.6494140625
Iteration 12800: Loss = -10983.6474609375
Iteration 12900: Loss = -10983.646484375
Iteration 13000: Loss = -10983.64453125
Iteration 13100: Loss = -10983.640625
Iteration 13200: Loss = -10983.6376953125
Iteration 13300: Loss = -10983.5771484375
Iteration 13400: Loss = -10982.1181640625
Iteration 13500: Loss = -10982.0537109375
Iteration 13600: Loss = -10981.978515625
Iteration 13700: Loss = -10979.1865234375
Iteration 13800: Loss = -10979.171875
Iteration 13900: Loss = -10979.1640625
Iteration 14000: Loss = -10979.16015625
Iteration 14100: Loss = -10979.162109375
1
Iteration 14200: Loss = -10979.158203125
Iteration 14300: Loss = -10979.1572265625
Iteration 14400: Loss = -10979.15625
Iteration 14500: Loss = -10979.154296875
Iteration 14600: Loss = -10979.1533203125
Iteration 14700: Loss = -10979.15234375
Iteration 14800: Loss = -10979.150390625
Iteration 14900: Loss = -10979.150390625
Iteration 15000: Loss = -10979.150390625
Iteration 15100: Loss = -10979.1494140625
Iteration 15200: Loss = -10979.1484375
Iteration 15300: Loss = -10979.1474609375
Iteration 15400: Loss = -10979.1484375
1
Iteration 15500: Loss = -10979.1484375
2
Iteration 15600: Loss = -10979.1474609375
Iteration 15700: Loss = -10979.1474609375
Iteration 15800: Loss = -10979.1455078125
Iteration 15900: Loss = -10979.1455078125
Iteration 16000: Loss = -10979.14453125
Iteration 16100: Loss = -10979.14453125
Iteration 16200: Loss = -10979.14453125
Iteration 16300: Loss = -10979.14453125
Iteration 16400: Loss = -10979.142578125
Iteration 16500: Loss = -10979.138671875
Iteration 16600: Loss = -10979.140625
1
Iteration 16700: Loss = -10979.1396484375
2
Iteration 16800: Loss = -10979.138671875
Iteration 16900: Loss = -10979.1396484375
1
Iteration 17000: Loss = -10979.138671875
Iteration 17100: Loss = -10979.138671875
Iteration 17200: Loss = -10979.1396484375
1
Iteration 17300: Loss = -10979.138671875
Iteration 17400: Loss = -10979.1396484375
1
Iteration 17500: Loss = -10979.138671875
Iteration 17600: Loss = -10979.1376953125
Iteration 17700: Loss = -10979.130859375
Iteration 17800: Loss = -10979.1298828125
Iteration 17900: Loss = -10979.1298828125
Iteration 18000: Loss = -10979.12890625
Iteration 18100: Loss = -10979.1298828125
1
Iteration 18200: Loss = -10979.1298828125
2
Iteration 18300: Loss = -10979.12890625
Iteration 18400: Loss = -10979.12890625
Iteration 18500: Loss = -10979.1279296875
Iteration 18600: Loss = -10979.12890625
1
Iteration 18700: Loss = -10979.12890625
2
Iteration 18800: Loss = -10979.1279296875
Iteration 18900: Loss = -10979.1279296875
Iteration 19000: Loss = -10979.1279296875
Iteration 19100: Loss = -10979.1279296875
Iteration 19200: Loss = -10979.1279296875
Iteration 19300: Loss = -10979.125
Iteration 19400: Loss = -10979.126953125
1
Iteration 19500: Loss = -10979.126953125
2
Iteration 19600: Loss = -10979.126953125
3
Iteration 19700: Loss = -10979.1259765625
4
Iteration 19800: Loss = -10979.126953125
5
Iteration 19900: Loss = -10979.1259765625
6
Iteration 20000: Loss = -10979.1259765625
7
Iteration 20100: Loss = -10979.1259765625
8
Iteration 20200: Loss = -10979.125
Iteration 20300: Loss = -10979.1259765625
1
Iteration 20400: Loss = -10979.125
Iteration 20500: Loss = -10979.125
Iteration 20600: Loss = -10979.126953125
1
Iteration 20700: Loss = -10979.125
Iteration 20800: Loss = -10979.1259765625
1
Iteration 20900: Loss = -10979.126953125
2
Iteration 21000: Loss = -10979.125
Iteration 21100: Loss = -10979.1259765625
1
Iteration 21200: Loss = -10979.125
Iteration 21300: Loss = -10979.1162109375
Iteration 21400: Loss = -10979.1064453125
Iteration 21500: Loss = -10979.1064453125
Iteration 21600: Loss = -10979.107421875
1
Iteration 21700: Loss = -10979.10546875
Iteration 21800: Loss = -10979.10546875
Iteration 21900: Loss = -10979.10546875
Iteration 22000: Loss = -10979.103515625
Iteration 22100: Loss = -10979.1044921875
1
Iteration 22200: Loss = -10979.10546875
2
Iteration 22300: Loss = -10979.1044921875
3
Iteration 22400: Loss = -10979.1044921875
4
Iteration 22500: Loss = -10979.1064453125
5
Iteration 22600: Loss = -10979.1044921875
6
Iteration 22700: Loss = -10979.10546875
7
Iteration 22800: Loss = -10979.1015625
Iteration 22900: Loss = -10979.103515625
1
Iteration 23000: Loss = -10979.103515625
2
Iteration 23100: Loss = -10979.1015625
Iteration 23200: Loss = -10979.1025390625
1
Iteration 23300: Loss = -10979.1015625
Iteration 23400: Loss = -10979.103515625
1
Iteration 23500: Loss = -10979.1025390625
2
Iteration 23600: Loss = -10979.0888671875
Iteration 23700: Loss = -10979.08984375
1
Iteration 23800: Loss = -10979.0888671875
Iteration 23900: Loss = -10979.08984375
1
Iteration 24000: Loss = -10979.08984375
2
Iteration 24100: Loss = -10979.0859375
Iteration 24200: Loss = -10979.0859375
Iteration 24300: Loss = -10979.0849609375
Iteration 24400: Loss = -10979.0869140625
1
Iteration 24500: Loss = -10979.0859375
2
Iteration 24600: Loss = -10979.0859375
3
Iteration 24700: Loss = -10979.0859375
4
Iteration 24800: Loss = -10979.0859375
5
Iteration 24900: Loss = -10979.0859375
6
Iteration 25000: Loss = -10979.076171875
Iteration 25100: Loss = -10979.0703125
Iteration 25200: Loss = -10979.0693359375
Iteration 25300: Loss = -10979.0693359375
Iteration 25400: Loss = -10979.068359375
Iteration 25500: Loss = -10979.068359375
Iteration 25600: Loss = -10979.068359375
Iteration 25700: Loss = -10979.05859375
Iteration 25800: Loss = -10979.05859375
Iteration 25900: Loss = -10979.0595703125
1
Iteration 26000: Loss = -10979.05859375
Iteration 26100: Loss = -10979.0595703125
1
Iteration 26200: Loss = -10979.0595703125
2
Iteration 26300: Loss = -10979.0595703125
3
Iteration 26400: Loss = -10979.056640625
Iteration 26500: Loss = -10979.0576171875
1
Iteration 26600: Loss = -10979.0478515625
Iteration 26700: Loss = -10979.0498046875
1
Iteration 26800: Loss = -10979.0478515625
Iteration 26900: Loss = -10979.048828125
1
Iteration 27000: Loss = -10979.0458984375
Iteration 27100: Loss = -10979.046875
1
Iteration 27200: Loss = -10979.046875
2
Iteration 27300: Loss = -10979.044921875
Iteration 27400: Loss = -10979.046875
1
Iteration 27500: Loss = -10979.0458984375
2
Iteration 27600: Loss = -10979.0458984375
3
Iteration 27700: Loss = -10979.046875
4
Iteration 27800: Loss = -10979.0458984375
5
Iteration 27900: Loss = -10979.044921875
Iteration 28000: Loss = -10979.046875
1
Iteration 28100: Loss = -10979.044921875
Iteration 28200: Loss = -10979.046875
1
Iteration 28300: Loss = -10979.0458984375
2
Iteration 28400: Loss = -10979.046875
3
Iteration 28500: Loss = -10979.0458984375
4
Iteration 28600: Loss = -10979.046875
5
Iteration 28700: Loss = -10979.0302734375
Iteration 28800: Loss = -10979.029296875
Iteration 28900: Loss = -10979.0302734375
1
Iteration 29000: Loss = -10979.0283203125
Iteration 29100: Loss = -10979.02734375
Iteration 29200: Loss = -10979.0283203125
1
Iteration 29300: Loss = -10979.0244140625
Iteration 29400: Loss = -10979.0263671875
1
Iteration 29500: Loss = -10979.025390625
2
Iteration 29600: Loss = -10979.0244140625
Iteration 29700: Loss = -10979.0244140625
Iteration 29800: Loss = -10979.025390625
1
Iteration 29900: Loss = -10979.025390625
2
pi: tensor([[3.8277e-02, 9.6172e-01],
        [8.9299e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9768, 0.0232], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1557, 0.2137],
         [0.5375, 0.1614]],

        [[0.9604, 0.2780],
         [0.0460, 0.9467]],

        [[0.8365, 0.2786],
         [0.0405, 0.8768]],

        [[0.8825, 0.2021],
         [0.6723, 0.8590]],

        [[0.4023, 0.1710],
         [0.9410, 0.7581]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011062378951710496
Average Adjusted Rand Index: -0.0016969696969696972
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26004.7578125
Iteration 100: Loss = -18160.29296875
Iteration 200: Loss = -12861.408203125
Iteration 300: Loss = -11456.70703125
Iteration 400: Loss = -11217.341796875
Iteration 500: Loss = -11128.1650390625
Iteration 600: Loss = -11078.779296875
Iteration 700: Loss = -11051.4130859375
Iteration 800: Loss = -11034.9814453125
Iteration 900: Loss = -11023.185546875
Iteration 1000: Loss = -11014.09375
Iteration 1100: Loss = -11008.0869140625
Iteration 1200: Loss = -11003.6796875
Iteration 1300: Loss = -11000.3642578125
Iteration 1400: Loss = -10997.94921875
Iteration 1500: Loss = -10996.0068359375
Iteration 1600: Loss = -10994.32421875
Iteration 1700: Loss = -10992.78125
Iteration 1800: Loss = -10991.3994140625
Iteration 1900: Loss = -10990.34375
Iteration 2000: Loss = -10989.4384765625
Iteration 2100: Loss = -10988.6279296875
Iteration 2200: Loss = -10987.8876953125
Iteration 2300: Loss = -10987.189453125
Iteration 2400: Loss = -10986.486328125
Iteration 2500: Loss = -10985.6962890625
Iteration 2600: Loss = -10984.6826171875
Iteration 2700: Loss = -10983.419921875
Iteration 2800: Loss = -10982.181640625
Iteration 2900: Loss = -10981.1806640625
Iteration 3000: Loss = -10980.4248046875
Iteration 3100: Loss = -10979.8515625
Iteration 3200: Loss = -10979.4091796875
Iteration 3300: Loss = -10979.0556640625
Iteration 3400: Loss = -10978.763671875
Iteration 3500: Loss = -10978.521484375
Iteration 3600: Loss = -10978.30859375
Iteration 3700: Loss = -10978.1240234375
Iteration 3800: Loss = -10977.9599609375
Iteration 3900: Loss = -10977.8134765625
Iteration 4000: Loss = -10977.681640625
Iteration 4100: Loss = -10977.564453125
Iteration 4200: Loss = -10977.4560546875
Iteration 4300: Loss = -10977.3583984375
Iteration 4400: Loss = -10977.267578125
Iteration 4500: Loss = -10977.185546875
Iteration 4600: Loss = -10977.107421875
Iteration 4700: Loss = -10977.03515625
Iteration 4800: Loss = -10976.970703125
Iteration 4900: Loss = -10976.91015625
Iteration 5000: Loss = -10976.8515625
Iteration 5100: Loss = -10976.798828125
Iteration 5200: Loss = -10976.75
Iteration 5300: Loss = -10976.7041015625
Iteration 5400: Loss = -10976.6611328125
Iteration 5500: Loss = -10976.62109375
Iteration 5600: Loss = -10976.58203125
Iteration 5700: Loss = -10976.546875
Iteration 5800: Loss = -10976.513671875
Iteration 5900: Loss = -10976.482421875
Iteration 6000: Loss = -10976.4521484375
Iteration 6100: Loss = -10976.42578125
Iteration 6200: Loss = -10976.3994140625
Iteration 6300: Loss = -10976.375
Iteration 6400: Loss = -10976.3515625
Iteration 6500: Loss = -10976.33203125
Iteration 6600: Loss = -10976.3095703125
Iteration 6700: Loss = -10976.29296875
Iteration 6800: Loss = -10976.2744140625
Iteration 6900: Loss = -10976.255859375
Iteration 7000: Loss = -10976.2412109375
Iteration 7100: Loss = -10976.2255859375
Iteration 7200: Loss = -10976.208984375
Iteration 7300: Loss = -10976.1962890625
Iteration 7400: Loss = -10976.18359375
Iteration 7500: Loss = -10976.171875
Iteration 7600: Loss = -10976.1611328125
Iteration 7700: Loss = -10976.1494140625
Iteration 7800: Loss = -10976.138671875
Iteration 7900: Loss = -10976.12890625
Iteration 8000: Loss = -10976.119140625
Iteration 8100: Loss = -10976.1103515625
Iteration 8200: Loss = -10976.1025390625
Iteration 8300: Loss = -10976.0947265625
Iteration 8400: Loss = -10976.087890625
Iteration 8500: Loss = -10976.0810546875
Iteration 8600: Loss = -10976.072265625
Iteration 8700: Loss = -10976.06640625
Iteration 8800: Loss = -10976.060546875
Iteration 8900: Loss = -10976.0556640625
Iteration 9000: Loss = -10976.0498046875
Iteration 9100: Loss = -10976.0458984375
Iteration 9200: Loss = -10976.041015625
Iteration 9300: Loss = -10976.0361328125
Iteration 9400: Loss = -10976.03125
Iteration 9500: Loss = -10976.02734375
Iteration 9600: Loss = -10976.021484375
Iteration 9700: Loss = -10976.01953125
Iteration 9800: Loss = -10976.015625
Iteration 9900: Loss = -10976.0107421875
Iteration 10000: Loss = -10976.009765625
Iteration 10100: Loss = -10976.005859375
Iteration 10200: Loss = -10976.0029296875
Iteration 10300: Loss = -10976.0009765625
Iteration 10400: Loss = -10975.9970703125
Iteration 10500: Loss = -10975.9951171875
Iteration 10600: Loss = -10975.9921875
Iteration 10700: Loss = -10975.990234375
Iteration 10800: Loss = -10975.9853515625
Iteration 10900: Loss = -10975.984375
Iteration 11000: Loss = -10975.982421875
Iteration 11100: Loss = -10975.98046875
Iteration 11200: Loss = -10975.978515625
Iteration 11300: Loss = -10975.9765625
Iteration 11400: Loss = -10975.9755859375
Iteration 11500: Loss = -10975.9736328125
Iteration 11600: Loss = -10975.970703125
Iteration 11700: Loss = -10975.966796875
Iteration 11800: Loss = -10975.9658203125
Iteration 11900: Loss = -10975.96484375
Iteration 12000: Loss = -10975.9619140625
Iteration 12100: Loss = -10975.9609375
Iteration 12200: Loss = -10975.958984375
Iteration 12300: Loss = -10975.958984375
Iteration 12400: Loss = -10975.95703125
Iteration 12500: Loss = -10975.9560546875
Iteration 12600: Loss = -10975.9560546875
Iteration 12700: Loss = -10975.9560546875
Iteration 12800: Loss = -10975.955078125
Iteration 12900: Loss = -10975.955078125
Iteration 13000: Loss = -10975.9521484375
Iteration 13100: Loss = -10975.9521484375
Iteration 13200: Loss = -10975.9501953125
Iteration 13300: Loss = -10975.9501953125
Iteration 13400: Loss = -10975.951171875
1
Iteration 13500: Loss = -10975.9501953125
Iteration 13600: Loss = -10975.951171875
1
Iteration 13700: Loss = -10975.9501953125
Iteration 13800: Loss = -10975.9482421875
Iteration 13900: Loss = -10975.9482421875
Iteration 14000: Loss = -10975.947265625
Iteration 14100: Loss = -10975.947265625
Iteration 14200: Loss = -10975.947265625
Iteration 14300: Loss = -10975.9453125
Iteration 14400: Loss = -10975.9453125
Iteration 14500: Loss = -10975.9453125
Iteration 14600: Loss = -10975.9462890625
1
Iteration 14700: Loss = -10975.9453125
Iteration 14800: Loss = -10975.9443359375
Iteration 14900: Loss = -10975.9443359375
Iteration 15000: Loss = -10975.9453125
1
Iteration 15100: Loss = -10975.9443359375
Iteration 15200: Loss = -10975.9443359375
Iteration 15300: Loss = -10975.943359375
Iteration 15400: Loss = -10975.9443359375
1
Iteration 15500: Loss = -10975.943359375
Iteration 15600: Loss = -10975.943359375
Iteration 15700: Loss = -10975.9423828125
Iteration 15800: Loss = -10975.9423828125
Iteration 15900: Loss = -10975.9423828125
Iteration 16000: Loss = -10975.943359375
1
Iteration 16100: Loss = -10975.9443359375
2
Iteration 16200: Loss = -10975.9423828125
Iteration 16300: Loss = -10975.94140625
Iteration 16400: Loss = -10975.9423828125
1
Iteration 16500: Loss = -10975.94140625
Iteration 16600: Loss = -10975.9423828125
1
Iteration 16700: Loss = -10975.94140625
Iteration 16800: Loss = -10975.9404296875
Iteration 16900: Loss = -10975.94140625
1
Iteration 17000: Loss = -10975.94140625
2
Iteration 17100: Loss = -10975.94140625
3
Iteration 17200: Loss = -10975.94140625
4
Iteration 17300: Loss = -10975.9404296875
Iteration 17400: Loss = -10975.9404296875
Iteration 17500: Loss = -10975.9404296875
Iteration 17600: Loss = -10975.9404296875
Iteration 17700: Loss = -10975.9404296875
Iteration 17800: Loss = -10975.94140625
1
Iteration 17900: Loss = -10975.9404296875
Iteration 18000: Loss = -10975.9404296875
Iteration 18100: Loss = -10975.94140625
1
Iteration 18200: Loss = -10975.94140625
2
Iteration 18300: Loss = -10975.94140625
3
Iteration 18400: Loss = -10975.9423828125
4
Iteration 18500: Loss = -10975.94140625
5
Iteration 18600: Loss = -10975.939453125
Iteration 18700: Loss = -10975.9404296875
1
Iteration 18800: Loss = -10975.939453125
Iteration 18900: Loss = -10975.9404296875
1
Iteration 19000: Loss = -10975.939453125
Iteration 19100: Loss = -10975.939453125
Iteration 19200: Loss = -10975.9404296875
1
Iteration 19300: Loss = -10975.939453125
Iteration 19400: Loss = -10975.939453125
Iteration 19500: Loss = -10975.939453125
Iteration 19600: Loss = -10975.9384765625
Iteration 19700: Loss = -10975.9404296875
1
Iteration 19800: Loss = -10975.9404296875
2
Iteration 19900: Loss = -10975.94140625
3
Iteration 20000: Loss = -10975.939453125
4
Iteration 20100: Loss = -10975.9404296875
5
Iteration 20200: Loss = -10975.9404296875
6
Iteration 20300: Loss = -10975.9404296875
7
Iteration 20400: Loss = -10975.939453125
8
Iteration 20500: Loss = -10975.9423828125
9
Iteration 20600: Loss = -10975.9404296875
10
Iteration 20700: Loss = -10975.9384765625
Iteration 20800: Loss = -10975.9404296875
1
Iteration 20900: Loss = -10975.939453125
2
Iteration 21000: Loss = -10975.9404296875
3
Iteration 21100: Loss = -10975.9404296875
4
Iteration 21200: Loss = -10975.9404296875
5
Iteration 21300: Loss = -10975.9404296875
6
Iteration 21400: Loss = -10975.939453125
7
Iteration 21500: Loss = -10975.94140625
8
Iteration 21600: Loss = -10975.9404296875
9
Iteration 21700: Loss = -10975.9404296875
10
Iteration 21800: Loss = -10975.9384765625
Iteration 21900: Loss = -10975.939453125
1
Iteration 22000: Loss = -10975.9404296875
2
Iteration 22100: Loss = -10975.939453125
3
Iteration 22200: Loss = -10975.9404296875
4
Iteration 22300: Loss = -10975.94140625
5
Iteration 22400: Loss = -10975.9404296875
6
Iteration 22500: Loss = -10975.939453125
7
Iteration 22600: Loss = -10975.9423828125
8
Iteration 22700: Loss = -10975.9404296875
9
Iteration 22800: Loss = -10975.9404296875
10
Iteration 22900: Loss = -10975.9404296875
11
Iteration 23000: Loss = -10975.9404296875
12
Iteration 23100: Loss = -10975.9404296875
13
Iteration 23200: Loss = -10975.939453125
14
Iteration 23300: Loss = -10975.94140625
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[0.9549, 0.0451],
        [0.9560, 0.0440]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.3271e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1564, 0.2135],
         [0.9743, 0.1589]],

        [[0.0525, 0.2735],
         [0.8410, 0.0122]],

        [[0.9766, 0.2497],
         [0.8867, 0.9853]],

        [[0.6867, 0.2345],
         [0.0290, 0.8200]],

        [[0.6831, 0.1915],
         [0.0361, 0.0083]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015539260913902783
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014065093854941256
Average Adjusted Rand Index: -0.0017616608428465998
[-0.0011062378951710496, -0.0014065093854941256] [-0.0016969696969696972, -0.0017616608428465998] [10979.025390625, 10975.94140625]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11057.813146933191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19906.203125
Iteration 100: Loss = -14099.4365234375
Iteration 200: Loss = -11754.337890625
Iteration 300: Loss = -11401.583984375
Iteration 400: Loss = -11325.2333984375
Iteration 500: Loss = -11282.3349609375
Iteration 600: Loss = -11258.3740234375
Iteration 700: Loss = -11247.3466796875
Iteration 800: Loss = -11232.216796875
Iteration 900: Loss = -11223.037109375
Iteration 1000: Loss = -11215.4765625
Iteration 1100: Loss = -11211.6865234375
Iteration 1200: Loss = -11209.1962890625
Iteration 1300: Loss = -11207.44140625
Iteration 1400: Loss = -11206.1015625
Iteration 1500: Loss = -11205.060546875
Iteration 1600: Loss = -11204.205078125
Iteration 1700: Loss = -11203.4794921875
Iteration 1800: Loss = -11202.853515625
Iteration 1900: Loss = -11202.3076171875
Iteration 2000: Loss = -11201.8232421875
Iteration 2100: Loss = -11201.3994140625
Iteration 2200: Loss = -11201.0205078125
Iteration 2300: Loss = -11200.68359375
Iteration 2400: Loss = -11200.3818359375
Iteration 2500: Loss = -11200.1123046875
Iteration 2600: Loss = -11199.8701171875
Iteration 2700: Loss = -11199.65234375
Iteration 2800: Loss = -11199.45703125
Iteration 2900: Loss = -11199.28125
Iteration 3000: Loss = -11199.1220703125
Iteration 3100: Loss = -11198.9755859375
Iteration 3200: Loss = -11198.8447265625
Iteration 3300: Loss = -11198.7236328125
Iteration 3400: Loss = -11198.61328125
Iteration 3500: Loss = -11198.5107421875
Iteration 3600: Loss = -11198.4130859375
Iteration 3700: Loss = -11198.322265625
Iteration 3800: Loss = -11198.2333984375
Iteration 3900: Loss = -11198.150390625
Iteration 4000: Loss = -11198.0703125
Iteration 4100: Loss = -11197.9912109375
Iteration 4200: Loss = -11197.9130859375
Iteration 4300: Loss = -11197.8349609375
Iteration 4400: Loss = -11197.76171875
Iteration 4500: Loss = -11197.6884765625
Iteration 4600: Loss = -11197.6201171875
Iteration 4700: Loss = -11197.5556640625
Iteration 4800: Loss = -11197.5
Iteration 4900: Loss = -11197.4482421875
Iteration 5000: Loss = -11197.4033203125
Iteration 5100: Loss = -11197.3603515625
Iteration 5200: Loss = -11197.3212890625
Iteration 5300: Loss = -11197.28515625
Iteration 5400: Loss = -11197.25
Iteration 5500: Loss = -11197.2177734375
Iteration 5600: Loss = -11197.1826171875
Iteration 5700: Loss = -11197.1513671875
Iteration 5800: Loss = -11197.119140625
Iteration 5900: Loss = -11197.0859375
Iteration 6000: Loss = -11197.0458984375
Iteration 6100: Loss = -11196.9970703125
Iteration 6200: Loss = -11196.9287109375
Iteration 6300: Loss = -11196.8232421875
Iteration 6400: Loss = -11196.6533203125
Iteration 6500: Loss = -11196.423828125
Iteration 6600: Loss = -11196.2060546875
Iteration 6700: Loss = -11195.7685546875
Iteration 6800: Loss = -11195.654296875
Iteration 6900: Loss = -11195.6015625
Iteration 7000: Loss = -11195.537109375
Iteration 7100: Loss = -11195.5283203125
Iteration 7200: Loss = -11195.5166015625
Iteration 7300: Loss = -11195.509765625
Iteration 7400: Loss = -11195.4990234375
Iteration 7500: Loss = -11195.484375
Iteration 7600: Loss = -11195.470703125
Iteration 7700: Loss = -11195.4599609375
Iteration 7800: Loss = -11195.451171875
Iteration 7900: Loss = -11195.443359375
Iteration 8000: Loss = -11195.435546875
Iteration 8100: Loss = -11195.4296875
Iteration 8200: Loss = -11195.4189453125
Iteration 8300: Loss = -11195.412109375
Iteration 8400: Loss = -11195.404296875
Iteration 8500: Loss = -11195.3974609375
Iteration 8600: Loss = -11195.3935546875
Iteration 8700: Loss = -11195.3857421875
Iteration 8800: Loss = -11195.37890625
Iteration 8900: Loss = -11195.375
Iteration 9000: Loss = -11195.3671875
Iteration 9100: Loss = -11195.3623046875
Iteration 9200: Loss = -11195.353515625
Iteration 9300: Loss = -11195.3505859375
Iteration 9400: Loss = -11195.341796875
Iteration 9500: Loss = -11195.3330078125
Iteration 9600: Loss = -11195.3291015625
Iteration 9700: Loss = -11195.3251953125
Iteration 9800: Loss = -11195.32421875
Iteration 9900: Loss = -11195.3212890625
Iteration 10000: Loss = -11195.3193359375
Iteration 10100: Loss = -11195.318359375
Iteration 10200: Loss = -11195.318359375
Iteration 10300: Loss = -11195.314453125
Iteration 10400: Loss = -11195.314453125
Iteration 10500: Loss = -11195.314453125
Iteration 10600: Loss = -11195.3115234375
Iteration 10700: Loss = -11195.310546875
Iteration 10800: Loss = -11195.310546875
Iteration 10900: Loss = -11195.30859375
Iteration 11000: Loss = -11195.30859375
Iteration 11100: Loss = -11195.30859375
Iteration 11200: Loss = -11195.306640625
Iteration 11300: Loss = -11195.3056640625
Iteration 11400: Loss = -11195.3056640625
Iteration 11500: Loss = -11195.306640625
1
Iteration 11600: Loss = -11195.3046875
Iteration 11700: Loss = -11195.3056640625
1
Iteration 11800: Loss = -11195.3046875
Iteration 11900: Loss = -11195.3037109375
Iteration 12000: Loss = -11195.3046875
1
Iteration 12100: Loss = -11195.3046875
2
Iteration 12200: Loss = -11195.302734375
Iteration 12300: Loss = -11195.3037109375
1
Iteration 12400: Loss = -11195.302734375
Iteration 12500: Loss = -11195.3037109375
1
Iteration 12600: Loss = -11195.3037109375
2
Iteration 12700: Loss = -11195.3017578125
Iteration 12800: Loss = -11195.302734375
1
Iteration 12900: Loss = -11195.3017578125
Iteration 13000: Loss = -11195.3017578125
Iteration 13100: Loss = -11195.30078125
Iteration 13200: Loss = -11195.302734375
1
Iteration 13300: Loss = -11195.30078125
Iteration 13400: Loss = -11195.3037109375
1
Iteration 13500: Loss = -11195.30078125
Iteration 13600: Loss = -11195.2998046875
Iteration 13700: Loss = -11195.30078125
1
Iteration 13800: Loss = -11195.30078125
2
Iteration 13900: Loss = -11195.3017578125
3
Iteration 14000: Loss = -11195.2998046875
Iteration 14100: Loss = -11195.2998046875
Iteration 14200: Loss = -11195.30078125
1
Iteration 14300: Loss = -11195.30078125
2
Iteration 14400: Loss = -11195.298828125
Iteration 14500: Loss = -11195.2978515625
Iteration 14600: Loss = -11195.296875
Iteration 14700: Loss = -11195.2978515625
1
Iteration 14800: Loss = -11195.2978515625
2
Iteration 14900: Loss = -11195.2978515625
3
Iteration 15000: Loss = -11195.2978515625
4
Iteration 15100: Loss = -11195.296875
Iteration 15200: Loss = -11195.2978515625
1
Iteration 15300: Loss = -11195.2978515625
2
Iteration 15400: Loss = -11195.296875
Iteration 15500: Loss = -11195.30078125
1
Iteration 15600: Loss = -11195.2978515625
2
Iteration 15700: Loss = -11195.2978515625
3
Iteration 15800: Loss = -11195.296875
Iteration 15900: Loss = -11195.296875
Iteration 16000: Loss = -11195.2978515625
1
Iteration 16100: Loss = -11195.2978515625
2
Iteration 16200: Loss = -11195.2978515625
3
Iteration 16300: Loss = -11195.30078125
4
Iteration 16400: Loss = -11195.298828125
5
Iteration 16500: Loss = -11195.2978515625
6
Iteration 16600: Loss = -11195.2978515625
7
Iteration 16700: Loss = -11195.298828125
8
Iteration 16800: Loss = -11195.298828125
9
Iteration 16900: Loss = -11195.296875
Iteration 17000: Loss = -11195.296875
Iteration 17100: Loss = -11195.296875
Iteration 17200: Loss = -11195.2978515625
1
Iteration 17300: Loss = -11195.296875
Iteration 17400: Loss = -11195.298828125
1
Iteration 17500: Loss = -11195.296875
Iteration 17600: Loss = -11195.296875
Iteration 17700: Loss = -11195.296875
Iteration 17800: Loss = -11195.2978515625
1
Iteration 17900: Loss = -11195.2958984375
Iteration 18000: Loss = -11195.296875
1
Iteration 18100: Loss = -11195.296875
2
Iteration 18200: Loss = -11195.296875
3
Iteration 18300: Loss = -11195.2978515625
4
Iteration 18400: Loss = -11195.2978515625
5
Iteration 18500: Loss = -11195.2958984375
Iteration 18600: Loss = -11195.2978515625
1
Iteration 18700: Loss = -11195.296875
2
Iteration 18800: Loss = -11195.296875
3
Iteration 18900: Loss = -11195.2958984375
Iteration 19000: Loss = -11195.2978515625
1
Iteration 19100: Loss = -11195.2958984375
Iteration 19200: Loss = -11195.296875
1
Iteration 19300: Loss = -11195.2958984375
Iteration 19400: Loss = -11195.2978515625
1
Iteration 19500: Loss = -11195.296875
2
Iteration 19600: Loss = -11195.2978515625
3
Iteration 19700: Loss = -11195.2978515625
4
Iteration 19800: Loss = -11195.2958984375
Iteration 19900: Loss = -11195.2958984375
Iteration 20000: Loss = -11195.2978515625
1
Iteration 20100: Loss = -11195.2978515625
2
Iteration 20200: Loss = -11195.296875
3
Iteration 20300: Loss = -11195.296875
4
Iteration 20400: Loss = -11195.2958984375
Iteration 20500: Loss = -11195.296875
1
Iteration 20600: Loss = -11195.296875
2
Iteration 20700: Loss = -11195.296875
3
Iteration 20800: Loss = -11195.296875
4
Iteration 20900: Loss = -11195.2958984375
Iteration 21000: Loss = -11195.2958984375
Iteration 21100: Loss = -11195.296875
1
Iteration 21200: Loss = -11195.2978515625
2
Iteration 21300: Loss = -11195.2958984375
Iteration 21400: Loss = -11195.298828125
1
Iteration 21500: Loss = -11195.298828125
2
Iteration 21600: Loss = -11195.2978515625
3
Iteration 21700: Loss = -11195.2958984375
Iteration 21800: Loss = -11195.2978515625
1
Iteration 21900: Loss = -11195.2978515625
2
Iteration 22000: Loss = -11195.296875
3
Iteration 22100: Loss = -11195.2958984375
Iteration 22200: Loss = -11195.2958984375
Iteration 22300: Loss = -11195.2978515625
1
Iteration 22400: Loss = -11195.296875
2
Iteration 22500: Loss = -11195.296875
3
Iteration 22600: Loss = -11195.2978515625
4
Iteration 22700: Loss = -11195.2978515625
5
Iteration 22800: Loss = -11195.296875
6
Iteration 22900: Loss = -11195.296875
7
Iteration 23000: Loss = -11195.2958984375
Iteration 23100: Loss = -11195.2978515625
1
Iteration 23200: Loss = -11195.2958984375
Iteration 23300: Loss = -11195.296875
1
Iteration 23400: Loss = -11195.2978515625
2
Iteration 23500: Loss = -11195.296875
3
Iteration 23600: Loss = -11195.2958984375
Iteration 23700: Loss = -11195.2978515625
1
Iteration 23800: Loss = -11195.296875
2
Iteration 23900: Loss = -11195.298828125
3
Iteration 24000: Loss = -11195.296875
4
Iteration 24100: Loss = -11195.296875
5
Iteration 24200: Loss = -11195.296875
6
Iteration 24300: Loss = -11195.296875
7
Iteration 24400: Loss = -11195.296875
8
Iteration 24500: Loss = -11195.2978515625
9
Iteration 24600: Loss = -11195.296875
10
Iteration 24700: Loss = -11195.296875
11
Iteration 24800: Loss = -11195.2958984375
Iteration 24900: Loss = -11195.296875
1
Iteration 25000: Loss = -11195.296875
2
Iteration 25100: Loss = -11195.296875
3
Iteration 25200: Loss = -11195.296875
4
Iteration 25300: Loss = -11195.2958984375
Iteration 25400: Loss = -11195.296875
1
Iteration 25500: Loss = -11195.296875
2
Iteration 25600: Loss = -11195.294921875
Iteration 25700: Loss = -11195.296875
1
Iteration 25800: Loss = -11195.2978515625
2
Iteration 25900: Loss = -11195.296875
3
Iteration 26000: Loss = -11195.2958984375
4
Iteration 26100: Loss = -11195.294921875
Iteration 26200: Loss = -11195.2958984375
1
Iteration 26300: Loss = -11195.296875
2
Iteration 26400: Loss = -11195.2958984375
3
Iteration 26500: Loss = -11195.294921875
Iteration 26600: Loss = -11195.2958984375
1
Iteration 26700: Loss = -11195.296875
2
Iteration 26800: Loss = -11195.2939453125
Iteration 26900: Loss = -11195.2958984375
1
Iteration 27000: Loss = -11195.2958984375
2
Iteration 27100: Loss = -11195.2958984375
3
Iteration 27200: Loss = -11195.2958984375
4
Iteration 27300: Loss = -11195.2958984375
5
Iteration 27400: Loss = -11195.2958984375
6
Iteration 27500: Loss = -11195.294921875
7
Iteration 27600: Loss = -11195.294921875
8
Iteration 27700: Loss = -11195.2978515625
9
Iteration 27800: Loss = -11195.296875
10
Iteration 27900: Loss = -11195.2958984375
11
Iteration 28000: Loss = -11195.2958984375
12
Iteration 28100: Loss = -11195.294921875
13
Iteration 28200: Loss = -11195.2958984375
14
Iteration 28300: Loss = -11195.2958984375
15
Stopping early at iteration 28300 due to no improvement.
pi: tensor([[0.9785, 0.0215],
        [0.9538, 0.0462]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9906, 0.0094], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1700, 0.1220],
         [0.8294, 0.0525]],

        [[0.0277, 0.0961],
         [0.9759, 0.3267]],

        [[0.0072, 0.2619],
         [0.8547, 0.5851]],

        [[0.0069, 0.1080],
         [0.0411, 0.4600]],

        [[0.0262, 0.0597],
         [0.0242, 0.0423]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
Global Adjusted Rand Index: 0.0012007967358244876
Average Adjusted Rand Index: 0.0031515151515151517
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33896.6875
Iteration 100: Loss = -19629.068359375
Iteration 200: Loss = -12992.466796875
Iteration 300: Loss = -11764.6748046875
Iteration 400: Loss = -11484.228515625
Iteration 500: Loss = -11359.3515625
Iteration 600: Loss = -11310.59375
Iteration 700: Loss = -11267.3916015625
Iteration 800: Loss = -11250.2978515625
Iteration 900: Loss = -11238.166015625
Iteration 1000: Loss = -11231.330078125
Iteration 1100: Loss = -11225.4599609375
Iteration 1200: Loss = -11220.6572265625
Iteration 1300: Loss = -11217.7646484375
Iteration 1400: Loss = -11215.505859375
Iteration 1500: Loss = -11213.6474609375
Iteration 1600: Loss = -11212.091796875
Iteration 1700: Loss = -11210.7763671875
Iteration 1800: Loss = -11209.65234375
Iteration 1900: Loss = -11208.6806640625
Iteration 2000: Loss = -11207.8369140625
Iteration 2100: Loss = -11207.09765625
Iteration 2200: Loss = -11206.447265625
Iteration 2300: Loss = -11205.8720703125
Iteration 2400: Loss = -11205.359375
Iteration 2500: Loss = -11204.904296875
Iteration 2600: Loss = -11204.4951171875
Iteration 2700: Loss = -11204.126953125
Iteration 2800: Loss = -11203.7939453125
Iteration 2900: Loss = -11203.4912109375
Iteration 3000: Loss = -11203.216796875
Iteration 3100: Loss = -11202.9658203125
Iteration 3200: Loss = -11202.73828125
Iteration 3300: Loss = -11202.52734375
Iteration 3400: Loss = -11202.3349609375
Iteration 3500: Loss = -11202.15625
Iteration 3600: Loss = -11201.9951171875
Iteration 3700: Loss = -11201.8427734375
Iteration 3800: Loss = -11201.7001953125
Iteration 3900: Loss = -11201.5693359375
Iteration 4000: Loss = -11201.447265625
Iteration 4100: Loss = -11201.3349609375
Iteration 4200: Loss = -11201.2294921875
Iteration 4300: Loss = -11201.1279296875
Iteration 4400: Loss = -11201.0361328125
Iteration 4500: Loss = -11200.9482421875
Iteration 4600: Loss = -11200.8662109375
Iteration 4700: Loss = -11200.7880859375
Iteration 4800: Loss = -11200.716796875
Iteration 4900: Loss = -11200.6484375
Iteration 5000: Loss = -11200.5849609375
Iteration 5100: Loss = -11200.525390625
Iteration 5200: Loss = -11200.4677734375
Iteration 5300: Loss = -11200.4140625
Iteration 5400: Loss = -11200.3662109375
Iteration 5500: Loss = -11200.3173828125
Iteration 5600: Loss = -11200.2744140625
Iteration 5700: Loss = -11200.232421875
Iteration 5800: Loss = -11200.1953125
Iteration 5900: Loss = -11200.158203125
Iteration 6000: Loss = -11200.123046875
Iteration 6100: Loss = -11200.0908203125
Iteration 6200: Loss = -11200.0615234375
Iteration 6300: Loss = -11200.0322265625
Iteration 6400: Loss = -11200.00390625
Iteration 6500: Loss = -11199.978515625
Iteration 6600: Loss = -11199.953125
Iteration 6700: Loss = -11199.9306640625
Iteration 6800: Loss = -11199.9091796875
Iteration 6900: Loss = -11199.8876953125
Iteration 7000: Loss = -11199.8681640625
Iteration 7100: Loss = -11199.8486328125
Iteration 7200: Loss = -11199.8310546875
Iteration 7300: Loss = -11199.8115234375
Iteration 7400: Loss = -11199.7978515625
Iteration 7500: Loss = -11199.7822265625
Iteration 7600: Loss = -11199.7666015625
Iteration 7700: Loss = -11199.7509765625
Iteration 7800: Loss = -11199.736328125
Iteration 7900: Loss = -11199.724609375
Iteration 8000: Loss = -11199.7119140625
Iteration 8100: Loss = -11199.6982421875
Iteration 8200: Loss = -11199.685546875
Iteration 8300: Loss = -11199.673828125
Iteration 8400: Loss = -11199.6630859375
Iteration 8500: Loss = -11199.6513671875
Iteration 8600: Loss = -11199.640625
Iteration 8700: Loss = -11199.62890625
Iteration 8800: Loss = -11199.6181640625
Iteration 8900: Loss = -11199.6083984375
Iteration 9000: Loss = -11199.6005859375
Iteration 9100: Loss = -11199.5927734375
Iteration 9200: Loss = -11199.583984375
Iteration 9300: Loss = -11199.576171875
Iteration 9400: Loss = -11199.568359375
Iteration 9500: Loss = -11199.55859375
Iteration 9600: Loss = -11199.55078125
Iteration 9700: Loss = -11199.541015625
Iteration 9800: Loss = -11199.5283203125
Iteration 9900: Loss = -11199.515625
Iteration 10000: Loss = -11199.5029296875
Iteration 10100: Loss = -11199.4892578125
Iteration 10200: Loss = -11199.4775390625
Iteration 10300: Loss = -11199.46484375
Iteration 10400: Loss = -11199.451171875
Iteration 10500: Loss = -11199.439453125
Iteration 10600: Loss = -11199.427734375
Iteration 10700: Loss = -11199.4150390625
Iteration 10800: Loss = -11199.4033203125
Iteration 10900: Loss = -11199.3916015625
Iteration 11000: Loss = -11199.376953125
Iteration 11100: Loss = -11199.3642578125
Iteration 11200: Loss = -11199.34765625
Iteration 11300: Loss = -11199.3291015625
Iteration 11400: Loss = -11199.30859375
Iteration 11500: Loss = -11199.2861328125
Iteration 11600: Loss = -11199.255859375
Iteration 11700: Loss = -11199.2197265625
Iteration 11800: Loss = -11199.171875
Iteration 11900: Loss = -11199.111328125
Iteration 12000: Loss = -11199.037109375
Iteration 12100: Loss = -11198.96875
Iteration 12200: Loss = -11198.919921875
Iteration 12300: Loss = -11198.8828125
Iteration 12400: Loss = -11198.59375
Iteration 12500: Loss = -11196.9404296875
Iteration 12600: Loss = -11196.1904296875
Iteration 12700: Loss = -11195.7275390625
Iteration 12800: Loss = -11195.1640625
Iteration 12900: Loss = -11195.0517578125
Iteration 13000: Loss = -11194.7041015625
Iteration 13100: Loss = -11186.0341796875
Iteration 13200: Loss = -11145.31640625
Iteration 13300: Loss = -11088.8095703125
Iteration 13400: Loss = -11071.408203125
Iteration 13500: Loss = -11068.037109375
Iteration 13600: Loss = -11065.9208984375
Iteration 13700: Loss = -11064.3857421875
Iteration 13800: Loss = -11056.333984375
Iteration 13900: Loss = -11056.162109375
Iteration 14000: Loss = -11056.103515625
Iteration 14100: Loss = -11056.0595703125
Iteration 14200: Loss = -11055.90625
Iteration 14300: Loss = -11051.4140625
Iteration 14400: Loss = -11051.0966796875
Iteration 14500: Loss = -11050.93359375
Iteration 14600: Loss = -11050.8798828125
Iteration 14700: Loss = -11050.8671875
Iteration 14800: Loss = -11050.8330078125
Iteration 14900: Loss = -11046.7939453125
Iteration 15000: Loss = -11043.908203125
Iteration 15100: Loss = -11043.75
Iteration 15200: Loss = -11037.3203125
Iteration 15300: Loss = -11031.138671875
Iteration 15400: Loss = -11031.013671875
Iteration 15500: Loss = -11030.9853515625
Iteration 15600: Loss = -11030.8310546875
Iteration 15700: Loss = -11030.6123046875
Iteration 15800: Loss = -11030.5859375
Iteration 15900: Loss = -11030.572265625
Iteration 16000: Loss = -11030.5673828125
Iteration 16100: Loss = -11030.564453125
Iteration 16200: Loss = -11030.5625
Iteration 16300: Loss = -11030.5595703125
Iteration 16400: Loss = -11030.5556640625
Iteration 16500: Loss = -11030.552734375
Iteration 16600: Loss = -11030.55078125
Iteration 16700: Loss = -11030.5498046875
Iteration 16800: Loss = -11030.548828125
Iteration 16900: Loss = -11030.548828125
Iteration 17000: Loss = -11030.5458984375
Iteration 17100: Loss = -11030.5458984375
Iteration 17200: Loss = -11030.544921875
Iteration 17300: Loss = -11030.5390625
Iteration 17400: Loss = -11030.5390625
Iteration 17500: Loss = -11030.4453125
Iteration 17600: Loss = -11030.443359375
Iteration 17700: Loss = -11030.4423828125
Iteration 17800: Loss = -11030.4423828125
Iteration 17900: Loss = -11030.4423828125
Iteration 18000: Loss = -11030.4423828125
Iteration 18100: Loss = -11030.4423828125
Iteration 18200: Loss = -11030.4404296875
Iteration 18300: Loss = -11030.4404296875
Iteration 18400: Loss = -11030.4404296875
Iteration 18500: Loss = -11030.4404296875
Iteration 18600: Loss = -11030.4404296875
Iteration 18700: Loss = -11030.4404296875
Iteration 18800: Loss = -11030.4404296875
Iteration 18900: Loss = -11030.439453125
Iteration 19000: Loss = -11030.4404296875
1
Iteration 19100: Loss = -11030.439453125
Iteration 19200: Loss = -11030.439453125
Iteration 19300: Loss = -11030.439453125
Iteration 19400: Loss = -11030.4384765625
Iteration 19500: Loss = -11030.439453125
1
Iteration 19600: Loss = -11030.4384765625
Iteration 19700: Loss = -11030.4384765625
Iteration 19800: Loss = -11030.439453125
1
Iteration 19900: Loss = -11030.4384765625
Iteration 20000: Loss = -11030.4375
Iteration 20100: Loss = -11030.4375
Iteration 20200: Loss = -11030.4375
Iteration 20300: Loss = -11030.4384765625
1
Iteration 20400: Loss = -11030.4375
Iteration 20500: Loss = -11030.4365234375
Iteration 20600: Loss = -11030.4365234375
Iteration 20700: Loss = -11030.412109375
Iteration 20800: Loss = -11030.412109375
Iteration 20900: Loss = -11030.4130859375
1
Iteration 21000: Loss = -11030.412109375
Iteration 21100: Loss = -11030.4140625
1
Iteration 21200: Loss = -11030.412109375
Iteration 21300: Loss = -11030.4140625
1
Iteration 21400: Loss = -11030.412109375
Iteration 21500: Loss = -11030.4111328125
Iteration 21600: Loss = -11030.4111328125
Iteration 21700: Loss = -11030.4111328125
Iteration 21800: Loss = -11030.41015625
Iteration 21900: Loss = -11030.41015625
Iteration 22000: Loss = -11030.41015625
Iteration 22100: Loss = -11030.41015625
Iteration 22200: Loss = -11030.41015625
Iteration 22300: Loss = -11030.412109375
1
Iteration 22400: Loss = -11030.4111328125
2
Iteration 22500: Loss = -11030.4091796875
Iteration 22600: Loss = -11030.41015625
1
Iteration 22700: Loss = -11030.3857421875
Iteration 22800: Loss = -11030.384765625
Iteration 22900: Loss = -11030.3857421875
1
Iteration 23000: Loss = -11030.38671875
2
Iteration 23100: Loss = -11030.384765625
Iteration 23200: Loss = -11030.384765625
Iteration 23300: Loss = -11030.05859375
Iteration 23400: Loss = -11030.05859375
Iteration 23500: Loss = -11030.05859375
Iteration 23600: Loss = -11030.05859375
Iteration 23700: Loss = -11030.0576171875
Iteration 23800: Loss = -11030.05859375
1
Iteration 23900: Loss = -11030.05859375
2
Iteration 24000: Loss = -11030.0595703125
3
Iteration 24100: Loss = -11030.0615234375
4
Iteration 24200: Loss = -11030.05859375
5
Iteration 24300: Loss = -11030.033203125
Iteration 24400: Loss = -11027.7080078125
Iteration 24500: Loss = -11027.7021484375
Iteration 24600: Loss = -11027.703125
1
Iteration 24700: Loss = -11027.7021484375
Iteration 24800: Loss = -11027.7001953125
Iteration 24900: Loss = -11027.701171875
1
Iteration 25000: Loss = -11027.701171875
2
Iteration 25100: Loss = -11027.7001953125
Iteration 25200: Loss = -11027.40625
Iteration 25300: Loss = -11027.4033203125
Iteration 25400: Loss = -11027.4033203125
Iteration 25500: Loss = -11027.4033203125
Iteration 25600: Loss = -11027.4033203125
Iteration 25700: Loss = -11027.4033203125
Iteration 25800: Loss = -11027.40234375
Iteration 25900: Loss = -11027.4033203125
1
Iteration 26000: Loss = -11027.4033203125
2
Iteration 26100: Loss = -11027.376953125
Iteration 26200: Loss = -11025.80078125
Iteration 26300: Loss = -11025.8017578125
1
Iteration 26400: Loss = -11025.7998046875
Iteration 26500: Loss = -11025.80078125
1
Iteration 26600: Loss = -11025.7998046875
Iteration 26700: Loss = -11025.80078125
1
Iteration 26800: Loss = -11025.8017578125
2
Iteration 26900: Loss = -11025.80078125
3
Iteration 27000: Loss = -11025.80078125
4
Iteration 27100: Loss = -11025.80078125
5
Iteration 27200: Loss = -11025.7998046875
Iteration 27300: Loss = -11025.7998046875
Iteration 27400: Loss = -11025.798828125
Iteration 27500: Loss = -11025.798828125
Iteration 27600: Loss = -11025.755859375
Iteration 27700: Loss = -11025.755859375
Iteration 27800: Loss = -11025.755859375
Iteration 27900: Loss = -11025.734375
Iteration 28000: Loss = -11025.7333984375
Iteration 28100: Loss = -11025.7333984375
Iteration 28200: Loss = -11025.736328125
1
Iteration 28300: Loss = -11025.734375
2
Iteration 28400: Loss = -11025.7333984375
Iteration 28500: Loss = -11025.734375
1
Iteration 28600: Loss = -11025.734375
2
Iteration 28700: Loss = -11025.734375
3
Iteration 28800: Loss = -11025.7333984375
Iteration 28900: Loss = -11025.7333984375
Iteration 29000: Loss = -11025.7333984375
Iteration 29100: Loss = -11025.7333984375
Iteration 29200: Loss = -11025.7333984375
Iteration 29300: Loss = -11025.7314453125
Iteration 29400: Loss = -11025.732421875
1
Iteration 29500: Loss = -11025.73046875
Iteration 29600: Loss = -11020.2392578125
Iteration 29700: Loss = -11019.9873046875
Iteration 29800: Loss = -11019.970703125
Iteration 29900: Loss = -11019.9638671875
pi: tensor([[0.7684, 0.2316],
        [0.2673, 0.7327]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5064, 0.4936], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2604, 0.1052],
         [0.0378, 0.2068]],

        [[0.1096, 0.0959],
         [0.0349, 0.0388]],

        [[0.1292, 0.0915],
         [0.9520, 0.1024]],

        [[0.8124, 0.1100],
         [0.2140, 0.5894]],

        [[0.1793, 0.1012],
         [0.0082, 0.9850]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 8
Adjusted Rand Index: 0.7026153394064061
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.8833667227947977
Average Adjusted Rand Index: 0.8856521078996936
[0.0012007967358244876, 0.8833667227947977] [0.0031515151515151517, 0.8856521078996936] [11195.2958984375, 11019.9609375]
-------------------------------------
This iteration is 92
True Objective function: Loss = -10852.200009350014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24412.4296875
Iteration 100: Loss = -16101.9736328125
Iteration 200: Loss = -11996.1611328125
Iteration 300: Loss = -11435.4072265625
Iteration 400: Loss = -11303.3212890625
Iteration 500: Loss = -11252.12109375
Iteration 600: Loss = -11225.0087890625
Iteration 700: Loss = -11205.1796875
Iteration 800: Loss = -11190.55859375
Iteration 900: Loss = -11177.6474609375
Iteration 1000: Loss = -11168.892578125
Iteration 1100: Loss = -11161.8544921875
Iteration 1200: Loss = -11153.4365234375
Iteration 1300: Loss = -11147.3603515625
Iteration 1400: Loss = -11141.591796875
Iteration 1500: Loss = -11135.3759765625
Iteration 1600: Loss = -11129.328125
Iteration 1700: Loss = -11124.6748046875
Iteration 1800: Loss = -11118.2275390625
Iteration 1900: Loss = -11112.302734375
Iteration 2000: Loss = -11104.1162109375
Iteration 2100: Loss = -11094.44140625
Iteration 2200: Loss = -11086.794921875
Iteration 2300: Loss = -11078.89453125
Iteration 2400: Loss = -11069.0830078125
Iteration 2500: Loss = -11060.7734375
Iteration 2600: Loss = -11051.0341796875
Iteration 2700: Loss = -11041.693359375
Iteration 2800: Loss = -11034.263671875
Iteration 2900: Loss = -11029.49609375
Iteration 3000: Loss = -11026.857421875
Iteration 3100: Loss = -11024.998046875
Iteration 3200: Loss = -11023.5888671875
Iteration 3300: Loss = -11022.189453125
Iteration 3400: Loss = -11020.9169921875
Iteration 3500: Loss = -11020.15625
Iteration 3600: Loss = -11019.4833984375
Iteration 3700: Loss = -11018.71875
Iteration 3800: Loss = -11018.1396484375
Iteration 3900: Loss = -11017.73828125
Iteration 4000: Loss = -11017.408203125
Iteration 4100: Loss = -11017.12109375
Iteration 4200: Loss = -11016.869140625
Iteration 4300: Loss = -11016.6455078125
Iteration 4400: Loss = -11016.4443359375
Iteration 4500: Loss = -11016.263671875
Iteration 4600: Loss = -11016.09765625
Iteration 4700: Loss = -11015.94921875
Iteration 4800: Loss = -11015.8134765625
Iteration 4900: Loss = -11015.6865234375
Iteration 5000: Loss = -11015.5712890625
Iteration 5100: Loss = -11015.4638671875
Iteration 5200: Loss = -11015.36328125
Iteration 5300: Loss = -11015.2626953125
Iteration 5400: Loss = -11015.169921875
Iteration 5500: Loss = -11015.0771484375
Iteration 5600: Loss = -11014.978515625
Iteration 5700: Loss = -11014.8515625
Iteration 5800: Loss = -11014.5439453125
Iteration 5900: Loss = -11011.103515625
Iteration 6000: Loss = -11009.2373046875
Iteration 6100: Loss = -11007.2587890625
Iteration 6200: Loss = -11006.4208984375
Iteration 6300: Loss = -11005.619140625
Iteration 6400: Loss = -11004.59765625
Iteration 6500: Loss = -11003.1357421875
Iteration 6600: Loss = -11001.28125
Iteration 6700: Loss = -10994.041015625
Iteration 6800: Loss = -10991.791015625
Iteration 6900: Loss = -10990.712890625
Iteration 7000: Loss = -10990.0830078125
Iteration 7100: Loss = -10989.5751953125
Iteration 7200: Loss = -10987.09375
Iteration 7300: Loss = -10985.1376953125
Iteration 7400: Loss = -10984.427734375
Iteration 7500: Loss = -10981.2119140625
Iteration 7600: Loss = -10979.6201171875
Iteration 7700: Loss = -10978.4150390625
Iteration 7800: Loss = -10975.8720703125
Iteration 7900: Loss = -10974.712890625
Iteration 8000: Loss = -10972.7392578125
Iteration 8100: Loss = -10969.1552734375
Iteration 8200: Loss = -10954.7841796875
Iteration 8300: Loss = -10951.3388671875
Iteration 8400: Loss = -10950.990234375
Iteration 8500: Loss = -10950.1376953125
Iteration 8600: Loss = -10945.8974609375
Iteration 8700: Loss = -10945.833984375
Iteration 8800: Loss = -10945.78515625
Iteration 8900: Loss = -10940.9921875
Iteration 9000: Loss = -10937.4248046875
Iteration 9100: Loss = -10937.2275390625
Iteration 9200: Loss = -10937.1748046875
Iteration 9300: Loss = -10937.126953125
Iteration 9400: Loss = -10936.232421875
Iteration 9500: Loss = -10936.1474609375
Iteration 9600: Loss = -10936.1376953125
Iteration 9700: Loss = -10936.1279296875
Iteration 9800: Loss = -10936.12109375
Iteration 9900: Loss = -10936.115234375
Iteration 10000: Loss = -10935.8134765625
Iteration 10100: Loss = -10935.7998046875
Iteration 10200: Loss = -10935.794921875
Iteration 10300: Loss = -10935.791015625
Iteration 10400: Loss = -10935.7861328125
Iteration 10500: Loss = -10935.7802734375
Iteration 10600: Loss = -10933.873046875
Iteration 10700: Loss = -10933.806640625
Iteration 10800: Loss = -10933.62109375
Iteration 10900: Loss = -10933.609375
Iteration 11000: Loss = -10933.6064453125
Iteration 11100: Loss = -10933.6005859375
Iteration 11200: Loss = -10933.5986328125
Iteration 11300: Loss = -10933.595703125
Iteration 11400: Loss = -10933.5927734375
Iteration 11500: Loss = -10933.5869140625
Iteration 11600: Loss = -10933.16796875
Iteration 11700: Loss = -10933.1630859375
Iteration 11800: Loss = -10933.16015625
Iteration 11900: Loss = -10933.1591796875
Iteration 12000: Loss = -10933.1572265625
Iteration 12100: Loss = -10933.1552734375
Iteration 12200: Loss = -10933.15625
1
Iteration 12300: Loss = -10933.1533203125
Iteration 12400: Loss = -10933.15234375
Iteration 12500: Loss = -10933.08203125
Iteration 12600: Loss = -10933.0791015625
Iteration 12700: Loss = -10933.080078125
1
Iteration 12800: Loss = -10933.0771484375
Iteration 12900: Loss = -10933.076171875
Iteration 13000: Loss = -10933.076171875
Iteration 13100: Loss = -10933.07421875
Iteration 13200: Loss = -10933.07421875
Iteration 13300: Loss = -10933.0732421875
Iteration 13400: Loss = -10933.072265625
Iteration 13500: Loss = -10933.0712890625
Iteration 13600: Loss = -10933.0703125
Iteration 13700: Loss = -10933.0703125
Iteration 13800: Loss = -10933.068359375
Iteration 13900: Loss = -10933.0673828125
Iteration 14000: Loss = -10933.0615234375
Iteration 14100: Loss = -10933.060546875
Iteration 14200: Loss = -10933.0595703125
Iteration 14300: Loss = -10933.060546875
1
Iteration 14400: Loss = -10933.0595703125
Iteration 14500: Loss = -10933.0595703125
Iteration 14600: Loss = -10933.056640625
Iteration 14700: Loss = -10933.0546875
Iteration 14800: Loss = -10933.056640625
1
Iteration 14900: Loss = -10933.0546875
Iteration 15000: Loss = -10933.0546875
Iteration 15100: Loss = -10933.052734375
Iteration 15200: Loss = -10933.0634765625
1
Iteration 15300: Loss = -10933.0537109375
2
Iteration 15400: Loss = -10933.052734375
Iteration 15500: Loss = -10933.052734375
Iteration 15600: Loss = -10933.0517578125
Iteration 15700: Loss = -10933.0517578125
Iteration 15800: Loss = -10933.05078125
Iteration 15900: Loss = -10933.05078125
Iteration 16000: Loss = -10933.0498046875
Iteration 16100: Loss = -10933.0498046875
Iteration 16200: Loss = -10933.05078125
1
Iteration 16300: Loss = -10933.0498046875
Iteration 16400: Loss = -10933.0498046875
Iteration 16500: Loss = -10933.0498046875
Iteration 16600: Loss = -10933.05078125
1
Iteration 16700: Loss = -10933.0498046875
Iteration 16800: Loss = -10933.0517578125
1
Iteration 16900: Loss = -10933.048828125
Iteration 17000: Loss = -10933.048828125
Iteration 17100: Loss = -10933.048828125
Iteration 17200: Loss = -10933.0498046875
1
Iteration 17300: Loss = -10933.048828125
Iteration 17400: Loss = -10933.048828125
Iteration 17500: Loss = -10933.048828125
Iteration 17600: Loss = -10933.048828125
Iteration 17700: Loss = -10933.0498046875
1
Iteration 17800: Loss = -10933.048828125
Iteration 17900: Loss = -10933.048828125
Iteration 18000: Loss = -10933.048828125
Iteration 18100: Loss = -10933.048828125
Iteration 18200: Loss = -10933.0478515625
Iteration 18300: Loss = -10933.046875
Iteration 18400: Loss = -10933.048828125
1
Iteration 18500: Loss = -10933.0478515625
2
Iteration 18600: Loss = -10933.048828125
3
Iteration 18700: Loss = -10933.046875
Iteration 18800: Loss = -10933.0478515625
1
Iteration 18900: Loss = -10933.046875
Iteration 19000: Loss = -10933.0478515625
1
Iteration 19100: Loss = -10933.0478515625
2
Iteration 19200: Loss = -10933.0478515625
3
Iteration 19300: Loss = -10933.0478515625
4
Iteration 19400: Loss = -10933.0478515625
5
Iteration 19500: Loss = -10933.0478515625
6
Iteration 19600: Loss = -10933.048828125
7
Iteration 19700: Loss = -10933.048828125
8
Iteration 19800: Loss = -10933.0478515625
9
Iteration 19900: Loss = -10933.046875
Iteration 20000: Loss = -10933.0478515625
1
Iteration 20100: Loss = -10933.0478515625
2
Iteration 20200: Loss = -10933.048828125
3
Iteration 20300: Loss = -10933.046875
Iteration 20400: Loss = -10933.0478515625
1
Iteration 20500: Loss = -10933.0478515625
2
Iteration 20600: Loss = -10933.0478515625
3
Iteration 20700: Loss = -10933.046875
Iteration 20800: Loss = -10933.046875
Iteration 20900: Loss = -10933.0478515625
1
Iteration 21000: Loss = -10933.048828125
2
Iteration 21100: Loss = -10933.046875
Iteration 21200: Loss = -10933.0478515625
1
Iteration 21300: Loss = -10933.0478515625
2
Iteration 21400: Loss = -10933.0478515625
3
Iteration 21500: Loss = -10933.0478515625
4
Iteration 21600: Loss = -10933.0458984375
Iteration 21700: Loss = -10933.0478515625
1
Iteration 21800: Loss = -10933.0478515625
2
Iteration 21900: Loss = -10933.0478515625
3
Iteration 22000: Loss = -10933.046875
4
Iteration 22100: Loss = -10933.0478515625
5
Iteration 22200: Loss = -10933.0478515625
6
Iteration 22300: Loss = -10933.046875
7
Iteration 22400: Loss = -10933.0478515625
8
Iteration 22500: Loss = -10933.048828125
9
Iteration 22600: Loss = -10933.046875
10
Iteration 22700: Loss = -10933.046875
11
Iteration 22800: Loss = -10933.046875
12
Iteration 22900: Loss = -10933.046875
13
Iteration 23000: Loss = -10933.046875
14
Iteration 23100: Loss = -10933.05078125
15
Stopping early at iteration 23100 due to no improvement.
pi: tensor([[9.9999e-01, 6.9261e-06],
        [4.0255e-02, 9.5974e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.9905e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1880, 0.1664],
         [0.1019, 0.1693]],

        [[0.3898, 0.7254],
         [0.9832, 0.8866]],

        [[0.3545, 0.0979],
         [0.9330, 0.9477]],

        [[0.0216, 0.1186],
         [0.3135, 0.1559]],

        [[0.2457, 0.0936],
         [0.9458, 0.0245]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0019137235069308285
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 65
Adjusted Rand Index: 0.08420239060076323
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 71
Adjusted Rand Index: 0.1662842488864894
Global Adjusted Rand Index: 0.03025093231127617
Average Adjusted Rand Index: 0.049714583196064366
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26138.60546875
Iteration 100: Loss = -17138.89453125
Iteration 200: Loss = -12375.0478515625
Iteration 300: Loss = -11276.6416015625
Iteration 400: Loss = -11113.5283203125
Iteration 500: Loss = -11051.11328125
Iteration 600: Loss = -11019.05078125
Iteration 700: Loss = -10999.7109375
Iteration 800: Loss = -10986.9384765625
Iteration 900: Loss = -10977.9619140625
Iteration 1000: Loss = -10971.35546875
Iteration 1100: Loss = -10966.322265625
Iteration 1200: Loss = -10962.384765625
Iteration 1300: Loss = -10959.2373046875
Iteration 1400: Loss = -10956.6748046875
Iteration 1500: Loss = -10954.5595703125
Iteration 1600: Loss = -10952.7890625
Iteration 1700: Loss = -10951.2939453125
Iteration 1800: Loss = -10950.0234375
Iteration 1900: Loss = -10948.931640625
Iteration 2000: Loss = -10947.994140625
Iteration 2100: Loss = -10947.1806640625
Iteration 2200: Loss = -10946.47265625
Iteration 2300: Loss = -10945.849609375
Iteration 2400: Loss = -10945.3017578125
Iteration 2500: Loss = -10944.8173828125
Iteration 2600: Loss = -10944.3876953125
Iteration 2700: Loss = -10944.0048828125
Iteration 2800: Loss = -10943.662109375
Iteration 2900: Loss = -10943.3525390625
Iteration 3000: Loss = -10943.0751953125
Iteration 3100: Loss = -10942.8212890625
Iteration 3200: Loss = -10942.5927734375
Iteration 3300: Loss = -10942.3828125
Iteration 3400: Loss = -10942.1904296875
Iteration 3500: Loss = -10942.013671875
Iteration 3600: Loss = -10941.8486328125
Iteration 3700: Loss = -10941.6962890625
Iteration 3800: Loss = -10941.556640625
Iteration 3900: Loss = -10941.42578125
Iteration 4000: Loss = -10941.30078125
Iteration 4100: Loss = -10941.1845703125
Iteration 4200: Loss = -10941.0712890625
Iteration 4300: Loss = -10940.966796875
Iteration 4400: Loss = -10940.8642578125
Iteration 4500: Loss = -10940.7626953125
Iteration 4600: Loss = -10940.6650390625
Iteration 4700: Loss = -10940.57421875
Iteration 4800: Loss = -10940.4873046875
Iteration 4900: Loss = -10940.4052734375
Iteration 5000: Loss = -10940.330078125
Iteration 5100: Loss = -10940.2607421875
Iteration 5200: Loss = -10940.197265625
Iteration 5300: Loss = -10940.1416015625
Iteration 5400: Loss = -10940.087890625
Iteration 5500: Loss = -10940.0361328125
Iteration 5600: Loss = -10939.990234375
Iteration 5700: Loss = -10939.94921875
Iteration 5800: Loss = -10939.91015625
Iteration 5900: Loss = -10939.8720703125
Iteration 6000: Loss = -10939.8359375
Iteration 6100: Loss = -10939.802734375
Iteration 6200: Loss = -10939.771484375
Iteration 6300: Loss = -10939.7392578125
Iteration 6400: Loss = -10939.7099609375
Iteration 6500: Loss = -10939.681640625
Iteration 6600: Loss = -10939.65625
Iteration 6700: Loss = -10939.6298828125
Iteration 6800: Loss = -10939.6015625
Iteration 6900: Loss = -10939.578125
Iteration 7000: Loss = -10939.55078125
Iteration 7100: Loss = -10939.525390625
Iteration 7200: Loss = -10939.4990234375
Iteration 7300: Loss = -10939.4716796875
Iteration 7400: Loss = -10939.4423828125
Iteration 7500: Loss = -10939.4091796875
Iteration 7600: Loss = -10939.3720703125
Iteration 7700: Loss = -10939.3271484375
Iteration 7800: Loss = -10939.26953125
Iteration 7900: Loss = -10939.212890625
Iteration 8000: Loss = -10939.1611328125
Iteration 8100: Loss = -10939.115234375
Iteration 8200: Loss = -10939.07421875
Iteration 8300: Loss = -10939.033203125
Iteration 8400: Loss = -10938.99609375
Iteration 8500: Loss = -10938.9560546875
Iteration 8600: Loss = -10938.9130859375
Iteration 8700: Loss = -10938.8662109375
Iteration 8800: Loss = -10938.80859375
Iteration 8900: Loss = -10938.5849609375
Iteration 9000: Loss = -10938.0517578125
Iteration 9100: Loss = -10937.892578125
Iteration 9200: Loss = -10937.705078125
Iteration 9300: Loss = -10936.8837890625
Iteration 9400: Loss = -10931.650390625
Iteration 9500: Loss = -10926.5966796875
Iteration 9600: Loss = -10926.1083984375
Iteration 9700: Loss = -10925.9072265625
Iteration 9800: Loss = -10925.5791015625
Iteration 9900: Loss = -10925.4560546875
Iteration 10000: Loss = -10925.4248046875
Iteration 10100: Loss = -10925.4033203125
Iteration 10200: Loss = -10925.3828125
Iteration 10300: Loss = -10925.365234375
Iteration 10400: Loss = -10925.3466796875
Iteration 10500: Loss = -10925.3310546875
Iteration 10600: Loss = -10925.318359375
Iteration 10700: Loss = -10925.306640625
Iteration 10800: Loss = -10925.2919921875
Iteration 10900: Loss = -10925.2861328125
Iteration 11000: Loss = -10925.2802734375
Iteration 11100: Loss = -10925.2734375
Iteration 11200: Loss = -10925.26953125
Iteration 11300: Loss = -10925.265625
Iteration 11400: Loss = -10925.2626953125
Iteration 11500: Loss = -10925.2587890625
Iteration 11600: Loss = -10925.25390625
Iteration 11700: Loss = -10925.244140625
Iteration 11800: Loss = -10925.1953125
Iteration 11900: Loss = -10925.1767578125
Iteration 12000: Loss = -10925.173828125
Iteration 12100: Loss = -10925.171875
Iteration 12200: Loss = -10925.1708984375
Iteration 12300: Loss = -10925.16796875
Iteration 12400: Loss = -10925.166015625
Iteration 12500: Loss = -10925.1650390625
Iteration 12600: Loss = -10925.1640625
Iteration 12700: Loss = -10925.1630859375
Iteration 12800: Loss = -10925.16015625
Iteration 12900: Loss = -10925.158203125
Iteration 13000: Loss = -10925.1572265625
Iteration 13100: Loss = -10925.154296875
Iteration 13200: Loss = -10925.1533203125
Iteration 13300: Loss = -10925.15234375
Iteration 13400: Loss = -10925.1513671875
Iteration 13500: Loss = -10925.1484375
Iteration 13600: Loss = -10925.1484375
Iteration 13700: Loss = -10925.146484375
Iteration 13800: Loss = -10925.1474609375
1
Iteration 13900: Loss = -10925.1455078125
Iteration 14000: Loss = -10925.14453125
Iteration 14100: Loss = -10925.1396484375
Iteration 14200: Loss = -10925.1279296875
Iteration 14300: Loss = -10925.125
Iteration 14400: Loss = -10925.1240234375
Iteration 14500: Loss = -10925.123046875
Iteration 14600: Loss = -10925.123046875
Iteration 14700: Loss = -10925.1201171875
Iteration 14800: Loss = -10925.1181640625
Iteration 14900: Loss = -10925.1181640625
Iteration 15000: Loss = -10925.1171875
Iteration 15100: Loss = -10925.1162109375
Iteration 15200: Loss = -10925.1171875
1
Iteration 15300: Loss = -10925.11328125
Iteration 15400: Loss = -10925.115234375
1
Iteration 15500: Loss = -10925.11328125
Iteration 15600: Loss = -10925.11328125
Iteration 15700: Loss = -10925.111328125
Iteration 15800: Loss = -10925.111328125
Iteration 15900: Loss = -10925.1103515625
Iteration 16000: Loss = -10925.1123046875
1
Iteration 16100: Loss = -10925.109375
Iteration 16200: Loss = -10925.1103515625
1
Iteration 16300: Loss = -10925.109375
Iteration 16400: Loss = -10925.1083984375
Iteration 16500: Loss = -10925.1083984375
Iteration 16600: Loss = -10925.1103515625
1
Iteration 16700: Loss = -10925.1083984375
Iteration 16800: Loss = -10925.1103515625
1
Iteration 16900: Loss = -10925.109375
2
Iteration 17000: Loss = -10925.109375
3
Iteration 17100: Loss = -10925.109375
4
Iteration 17200: Loss = -10925.1083984375
Iteration 17300: Loss = -10925.1083984375
Iteration 17400: Loss = -10925.1064453125
Iteration 17500: Loss = -10925.1083984375
1
Iteration 17600: Loss = -10925.107421875
2
Iteration 17700: Loss = -10925.1083984375
3
Iteration 17800: Loss = -10925.10546875
Iteration 17900: Loss = -10925.107421875
1
Iteration 18000: Loss = -10925.107421875
2
Iteration 18100: Loss = -10925.107421875
3
Iteration 18200: Loss = -10925.1064453125
4
Iteration 18300: Loss = -10925.1064453125
5
Iteration 18400: Loss = -10925.1064453125
6
Iteration 18500: Loss = -10925.1064453125
7
Iteration 18600: Loss = -10925.10546875
Iteration 18700: Loss = -10925.107421875
1
Iteration 18800: Loss = -10925.107421875
2
Iteration 18900: Loss = -10925.10546875
Iteration 19000: Loss = -10925.107421875
1
Iteration 19100: Loss = -10925.1064453125
2
Iteration 19200: Loss = -10925.10546875
Iteration 19300: Loss = -10925.1064453125
1
Iteration 19400: Loss = -10925.1064453125
2
Iteration 19500: Loss = -10925.1064453125
3
Iteration 19600: Loss = -10925.10546875
Iteration 19700: Loss = -10925.1064453125
1
Iteration 19800: Loss = -10925.10546875
Iteration 19900: Loss = -10925.1025390625
Iteration 20000: Loss = -10925.1025390625
Iteration 20100: Loss = -10925.1015625
Iteration 20200: Loss = -10925.1015625
Iteration 20300: Loss = -10925.1015625
Iteration 20400: Loss = -10925.1015625
Iteration 20500: Loss = -10925.1005859375
Iteration 20600: Loss = -10925.1015625
1
Iteration 20700: Loss = -10925.1025390625
2
Iteration 20800: Loss = -10925.1025390625
3
Iteration 20900: Loss = -10925.1015625
4
Iteration 21000: Loss = -10925.1015625
5
Iteration 21100: Loss = -10925.1015625
6
Iteration 21200: Loss = -10925.1005859375
Iteration 21300: Loss = -10925.1015625
1
Iteration 21400: Loss = -10925.1025390625
2
Iteration 21500: Loss = -10925.1005859375
Iteration 21600: Loss = -10925.1025390625
1
Iteration 21700: Loss = -10925.1025390625
2
Iteration 21800: Loss = -10925.1015625
3
Iteration 21900: Loss = -10925.1005859375
Iteration 22000: Loss = -10925.1005859375
Iteration 22100: Loss = -10925.1015625
1
Iteration 22200: Loss = -10925.1005859375
Iteration 22300: Loss = -10925.1005859375
Iteration 22400: Loss = -10925.1005859375
Iteration 22500: Loss = -10925.1015625
1
Iteration 22600: Loss = -10925.099609375
Iteration 22700: Loss = -10925.1005859375
1
Iteration 22800: Loss = -10925.1005859375
2
Iteration 22900: Loss = -10925.1015625
3
Iteration 23000: Loss = -10925.1015625
4
Iteration 23100: Loss = -10925.1015625
5
Iteration 23200: Loss = -10925.1005859375
6
Iteration 23300: Loss = -10925.1025390625
7
Iteration 23400: Loss = -10925.1025390625
8
Iteration 23500: Loss = -10925.095703125
Iteration 23600: Loss = -10925.072265625
Iteration 23700: Loss = -10924.9443359375
Iteration 23800: Loss = -10914.8232421875
Iteration 23900: Loss = -10911.916015625
Iteration 24000: Loss = -10907.443359375
Iteration 24100: Loss = -10901.9375
Iteration 24200: Loss = -10895.8974609375
Iteration 24300: Loss = -10892.4267578125
Iteration 24400: Loss = -10887.5751953125
Iteration 24500: Loss = -10881.216796875
Iteration 24600: Loss = -10864.2587890625
Iteration 24700: Loss = -10849.796875
Iteration 24800: Loss = -10849.4677734375
Iteration 24900: Loss = -10828.087890625
Iteration 25000: Loss = -10823.72265625
Iteration 25100: Loss = -10816.6572265625
Iteration 25200: Loss = -10816.5400390625
Iteration 25300: Loss = -10816.470703125
Iteration 25400: Loss = -10816.44140625
Iteration 25500: Loss = -10816.423828125
Iteration 25600: Loss = -10816.4091796875
Iteration 25700: Loss = -10816.4013671875
Iteration 25800: Loss = -10816.3935546875
Iteration 25900: Loss = -10816.3876953125
Iteration 26000: Loss = -10816.3837890625
Iteration 26100: Loss = -10816.3779296875
Iteration 26200: Loss = -10816.3759765625
Iteration 26300: Loss = -10816.3720703125
Iteration 26400: Loss = -10816.3720703125
Iteration 26500: Loss = -10816.369140625
Iteration 26600: Loss = -10816.3681640625
Iteration 26700: Loss = -10816.3662109375
Iteration 26800: Loss = -10816.3642578125
Iteration 26900: Loss = -10816.36328125
Iteration 27000: Loss = -10816.361328125
Iteration 27100: Loss = -10816.361328125
Iteration 27200: Loss = -10816.359375
Iteration 27300: Loss = -10816.359375
Iteration 27400: Loss = -10816.359375
Iteration 27500: Loss = -10816.3505859375
Iteration 27600: Loss = -10816.349609375
Iteration 27700: Loss = -10816.3486328125
Iteration 27800: Loss = -10816.349609375
1
Iteration 27900: Loss = -10816.3486328125
Iteration 28000: Loss = -10816.34765625
Iteration 28100: Loss = -10816.3466796875
Iteration 28200: Loss = -10816.3466796875
Iteration 28300: Loss = -10816.3447265625
Iteration 28400: Loss = -10816.345703125
1
Iteration 28500: Loss = -10816.34375
Iteration 28600: Loss = -10816.3447265625
1
Iteration 28700: Loss = -10816.3447265625
2
Iteration 28800: Loss = -10816.3447265625
3
Iteration 28900: Loss = -10816.34375
Iteration 29000: Loss = -10816.3447265625
1
Iteration 29100: Loss = -10816.34375
Iteration 29200: Loss = -10816.34375
Iteration 29300: Loss = -10816.3427734375
Iteration 29400: Loss = -10816.34375
1
Iteration 29500: Loss = -10816.3427734375
Iteration 29600: Loss = -10816.3427734375
Iteration 29700: Loss = -10816.3212890625
Iteration 29800: Loss = -10816.322265625
1
Iteration 29900: Loss = -10816.322265625
2
pi: tensor([[0.7916, 0.2084],
        [0.1942, 0.8058]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6137, 0.3863], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2410, 0.0988],
         [0.0787, 0.1933]],

        [[0.7868, 0.0992],
         [0.2713, 0.0861]],

        [[0.9712, 0.1113],
         [0.9328, 0.9929]],

        [[0.3180, 0.1043],
         [0.7076, 0.0483]],

        [[0.9175, 0.0953],
         [0.6582, 0.0268]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6362268122151362
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448420005390695
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721314419105764
Global Adjusted Rand Index: 0.7739458026652207
Average Adjusted Rand Index: 0.7745136496140753
[0.03025093231127617, 0.7739458026652207] [0.049714583196064366, 0.7745136496140753] [10933.05078125, 10816.3212890625]
-------------------------------------
This iteration is 93
True Objective function: Loss = -10904.181639377
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -54047.2109375
Iteration 100: Loss = -30820.5859375
Iteration 200: Loss = -15991.9775390625
Iteration 300: Loss = -12550.9677734375
Iteration 400: Loss = -11829.6904296875
Iteration 500: Loss = -11443.58203125
Iteration 600: Loss = -11216.990234375
Iteration 700: Loss = -11158.2275390625
Iteration 800: Loss = -11125.5478515625
Iteration 900: Loss = -11102.7978515625
Iteration 1000: Loss = -11080.9921875
Iteration 1100: Loss = -11065.68359375
Iteration 1200: Loss = -11055.7822265625
Iteration 1300: Loss = -11047.849609375
Iteration 1400: Loss = -11041.3662109375
Iteration 1500: Loss = -11035.98828125
Iteration 1600: Loss = -11031.4716796875
Iteration 1700: Loss = -11027.638671875
Iteration 1800: Loss = -11024.3447265625
Iteration 1900: Loss = -11021.2626953125
Iteration 2000: Loss = -11018.8154296875
Iteration 2100: Loss = -11016.6806640625
Iteration 2200: Loss = -11014.8056640625
Iteration 2300: Loss = -11013.1455078125
Iteration 2400: Loss = -11011.66796875
Iteration 2500: Loss = -11010.3505859375
Iteration 2600: Loss = -11009.1689453125
Iteration 2700: Loss = -11008.1044921875
Iteration 2800: Loss = -11007.1455078125
Iteration 2900: Loss = -11006.275390625
Iteration 3000: Loss = -11005.48828125
Iteration 3100: Loss = -11004.76953125
Iteration 3200: Loss = -11004.1162109375
Iteration 3300: Loss = -11003.5185546875
Iteration 3400: Loss = -11002.970703125
Iteration 3500: Loss = -11002.466796875
Iteration 3600: Loss = -11002.0068359375
Iteration 3700: Loss = -11001.5810546875
Iteration 3800: Loss = -11001.185546875
Iteration 3900: Loss = -11000.8232421875
Iteration 4000: Loss = -11000.48828125
Iteration 4100: Loss = -11000.177734375
Iteration 4200: Loss = -10999.890625
Iteration 4300: Loss = -10999.6240234375
Iteration 4400: Loss = -10999.3779296875
Iteration 4500: Loss = -10999.1494140625
Iteration 4600: Loss = -10998.9375
Iteration 4700: Loss = -10998.7392578125
Iteration 4800: Loss = -10998.5546875
Iteration 4900: Loss = -10998.3818359375
Iteration 5000: Loss = -10998.224609375
Iteration 5100: Loss = -10998.076171875
Iteration 5200: Loss = -10997.9365234375
Iteration 5300: Loss = -10997.80859375
Iteration 5400: Loss = -10997.6875
Iteration 5500: Loss = -10997.5732421875
Iteration 5600: Loss = -10997.4677734375
Iteration 5700: Loss = -10997.3681640625
Iteration 5800: Loss = -10997.271484375
Iteration 5900: Loss = -10997.1845703125
Iteration 6000: Loss = -10997.1005859375
Iteration 6100: Loss = -10997.0205078125
Iteration 6200: Loss = -10996.9462890625
Iteration 6300: Loss = -10996.8759765625
Iteration 6400: Loss = -10996.8095703125
Iteration 6500: Loss = -10996.748046875
Iteration 6600: Loss = -10996.685546875
Iteration 6700: Loss = -10996.62890625
Iteration 6800: Loss = -10996.5751953125
Iteration 6900: Loss = -10996.5244140625
Iteration 7000: Loss = -10996.474609375
Iteration 7100: Loss = -10996.4296875
Iteration 7200: Loss = -10996.38671875
Iteration 7300: Loss = -10996.3447265625
Iteration 7400: Loss = -10996.306640625
Iteration 7500: Loss = -10996.267578125
Iteration 7600: Loss = -10996.2333984375
Iteration 7700: Loss = -10996.2001953125
Iteration 7800: Loss = -10996.169921875
Iteration 7900: Loss = -10996.1376953125
Iteration 8000: Loss = -10996.1103515625
Iteration 8100: Loss = -10996.0830078125
Iteration 8200: Loss = -10996.0556640625
Iteration 8300: Loss = -10996.03125
Iteration 8400: Loss = -10996.0087890625
Iteration 8500: Loss = -10995.9853515625
Iteration 8600: Loss = -10995.9658203125
Iteration 8700: Loss = -10995.9443359375
Iteration 8800: Loss = -10995.9248046875
Iteration 8900: Loss = -10995.9072265625
Iteration 9000: Loss = -10995.890625
Iteration 9100: Loss = -10995.873046875
Iteration 9200: Loss = -10995.8564453125
Iteration 9300: Loss = -10995.8408203125
Iteration 9400: Loss = -10995.828125
Iteration 9500: Loss = -10995.8125
Iteration 9600: Loss = -10995.798828125
Iteration 9700: Loss = -10995.7880859375
Iteration 9800: Loss = -10995.775390625
Iteration 9900: Loss = -10995.763671875
Iteration 10000: Loss = -10995.7529296875
Iteration 10100: Loss = -10995.7431640625
Iteration 10200: Loss = -10995.7333984375
Iteration 10300: Loss = -10995.7236328125
Iteration 10400: Loss = -10995.7138671875
Iteration 10500: Loss = -10995.70703125
Iteration 10600: Loss = -10995.6982421875
Iteration 10700: Loss = -10995.689453125
Iteration 10800: Loss = -10995.6826171875
Iteration 10900: Loss = -10995.6767578125
Iteration 11000: Loss = -10995.66796875
Iteration 11100: Loss = -10995.6630859375
Iteration 11200: Loss = -10995.65625
Iteration 11300: Loss = -10995.6513671875
Iteration 11400: Loss = -10995.64453125
Iteration 11500: Loss = -10995.638671875
Iteration 11600: Loss = -10995.6328125
Iteration 11700: Loss = -10995.6298828125
Iteration 11800: Loss = -10995.6240234375
Iteration 11900: Loss = -10995.619140625
Iteration 12000: Loss = -10995.615234375
Iteration 12100: Loss = -10995.61328125
Iteration 12200: Loss = -10995.609375
Iteration 12300: Loss = -10995.603515625
Iteration 12400: Loss = -10995.6005859375
Iteration 12500: Loss = -10995.599609375
Iteration 12600: Loss = -10995.595703125
Iteration 12700: Loss = -10995.5927734375
Iteration 12800: Loss = -10995.587890625
Iteration 12900: Loss = -10995.5869140625
Iteration 13000: Loss = -10995.5849609375
Iteration 13100: Loss = -10995.5830078125
Iteration 13200: Loss = -10995.5791015625
Iteration 13300: Loss = -10995.578125
Iteration 13400: Loss = -10995.57421875
Iteration 13500: Loss = -10995.5712890625
Iteration 13600: Loss = -10995.5693359375
Iteration 13700: Loss = -10995.568359375
Iteration 13800: Loss = -10995.5673828125
Iteration 13900: Loss = -10995.5654296875
Iteration 14000: Loss = -10995.5634765625
Iteration 14100: Loss = -10995.5625
Iteration 14200: Loss = -10995.5595703125
Iteration 14300: Loss = -10995.5595703125
Iteration 14400: Loss = -10995.5556640625
Iteration 14500: Loss = -10995.5556640625
Iteration 14600: Loss = -10995.55859375
1
Iteration 14700: Loss = -10995.5546875
Iteration 14800: Loss = -10995.5537109375
Iteration 14900: Loss = -10995.5517578125
Iteration 15000: Loss = -10995.5498046875
Iteration 15100: Loss = -10995.5498046875
Iteration 15200: Loss = -10995.548828125
Iteration 15300: Loss = -10995.5478515625
Iteration 15400: Loss = -10995.546875
Iteration 15500: Loss = -10995.5458984375
Iteration 15600: Loss = -10995.544921875
Iteration 15700: Loss = -10995.544921875
Iteration 15800: Loss = -10995.5439453125
Iteration 15900: Loss = -10995.5439453125
Iteration 16000: Loss = -10995.5439453125
Iteration 16100: Loss = -10995.5439453125
Iteration 16200: Loss = -10995.541015625
Iteration 16300: Loss = -10995.5419921875
1
Iteration 16400: Loss = -10995.5419921875
2
Iteration 16500: Loss = -10995.5400390625
Iteration 16600: Loss = -10995.5400390625
Iteration 16700: Loss = -10995.5390625
Iteration 16800: Loss = -10995.5390625
Iteration 16900: Loss = -10995.541015625
1
Iteration 17000: Loss = -10995.541015625
2
Iteration 17100: Loss = -10995.5390625
Iteration 17200: Loss = -10995.5400390625
1
Iteration 17300: Loss = -10995.5380859375
Iteration 17400: Loss = -10995.5380859375
Iteration 17500: Loss = -10995.537109375
Iteration 17600: Loss = -10995.537109375
Iteration 17700: Loss = -10995.537109375
Iteration 17800: Loss = -10995.5380859375
1
Iteration 17900: Loss = -10995.5361328125
Iteration 18000: Loss = -10995.5361328125
Iteration 18100: Loss = -10995.5380859375
1
Iteration 18200: Loss = -10995.5361328125
Iteration 18300: Loss = -10995.53515625
Iteration 18400: Loss = -10995.53515625
Iteration 18500: Loss = -10995.53515625
Iteration 18600: Loss = -10995.53515625
Iteration 18700: Loss = -10995.5361328125
1
Iteration 18800: Loss = -10995.5341796875
Iteration 18900: Loss = -10995.53515625
1
Iteration 19000: Loss = -10995.5341796875
Iteration 19100: Loss = -10995.53515625
1
Iteration 19200: Loss = -10995.5341796875
Iteration 19300: Loss = -10995.5341796875
Iteration 19400: Loss = -10995.53515625
1
Iteration 19500: Loss = -10995.533203125
Iteration 19600: Loss = -10995.5341796875
1
Iteration 19700: Loss = -10995.533203125
Iteration 19800: Loss = -10995.5341796875
1
Iteration 19900: Loss = -10995.533203125
Iteration 20000: Loss = -10995.533203125
Iteration 20100: Loss = -10995.5341796875
1
Iteration 20200: Loss = -10995.533203125
Iteration 20300: Loss = -10995.53515625
1
Iteration 20400: Loss = -10995.53515625
2
Iteration 20500: Loss = -10995.5322265625
Iteration 20600: Loss = -10995.533203125
1
Iteration 20700: Loss = -10995.533203125
2
Iteration 20800: Loss = -10995.5341796875
3
Iteration 20900: Loss = -10995.533203125
4
Iteration 21000: Loss = -10995.533203125
5
Iteration 21100: Loss = -10995.533203125
6
Iteration 21200: Loss = -10995.5322265625
Iteration 21300: Loss = -10995.533203125
1
Iteration 21400: Loss = -10995.533203125
2
Iteration 21500: Loss = -10995.5341796875
3
Iteration 21600: Loss = -10995.533203125
4
Iteration 21700: Loss = -10995.5341796875
5
Iteration 21800: Loss = -10995.53125
Iteration 21900: Loss = -10995.533203125
1
Iteration 22000: Loss = -10995.5341796875
2
Iteration 22100: Loss = -10995.533203125
3
Iteration 22200: Loss = -10995.533203125
4
Iteration 22300: Loss = -10995.5322265625
5
Iteration 22400: Loss = -10995.5322265625
6
Iteration 22500: Loss = -10995.5341796875
7
Iteration 22600: Loss = -10995.5322265625
8
Iteration 22700: Loss = -10995.533203125
9
Iteration 22800: Loss = -10995.53515625
10
Iteration 22900: Loss = -10995.533203125
11
Iteration 23000: Loss = -10995.533203125
12
Iteration 23100: Loss = -10995.5361328125
13
Iteration 23200: Loss = -10995.5322265625
14
Iteration 23300: Loss = -10995.533203125
15
Stopping early at iteration 23300 due to no improvement.
pi: tensor([[1.4665e-03, 9.9853e-01],
        [1.0378e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.9659e-04, 9.9940e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.8802, 0.2057],
         [0.6739, 0.1628]],

        [[0.1810, 0.2348],
         [0.0571, 0.8601]],

        [[0.0336, 0.1010],
         [0.6846, 0.9568]],

        [[0.2025, 0.2344],
         [0.9763, 0.0648]],

        [[0.8621, 0.1178],
         [0.0071, 0.1917]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30085.1171875
Iteration 100: Loss = -19359.248046875
Iteration 200: Loss = -13507.6298828125
Iteration 300: Loss = -12107.1640625
Iteration 400: Loss = -11624.7392578125
Iteration 500: Loss = -11397.8994140625
Iteration 600: Loss = -11299.2578125
Iteration 700: Loss = -11246.9677734375
Iteration 800: Loss = -11214.466796875
Iteration 900: Loss = -11180.671875
Iteration 1000: Loss = -11166.78515625
Iteration 1100: Loss = -11157.4306640625
Iteration 1200: Loss = -11149.4130859375
Iteration 1300: Loss = -11141.431640625
Iteration 1400: Loss = -11134.6279296875
Iteration 1500: Loss = -11129.5078125
Iteration 1600: Loss = -11124.7353515625
Iteration 1700: Loss = -11118.095703125
Iteration 1800: Loss = -11111.7451171875
Iteration 1900: Loss = -11104.6357421875
Iteration 2000: Loss = -11095.6435546875
Iteration 2100: Loss = -11085.298828125
Iteration 2200: Loss = -11071.1806640625
Iteration 2300: Loss = -11052.3583984375
Iteration 2400: Loss = -11044.716796875
Iteration 2500: Loss = -11039.5419921875
Iteration 2600: Loss = -11030.9326171875
Iteration 2700: Loss = -11025.9716796875
Iteration 2800: Loss = -11023.7373046875
Iteration 2900: Loss = -11022.2021484375
Iteration 3000: Loss = -11021.056640625
Iteration 3100: Loss = -11020.1640625
Iteration 3200: Loss = -11019.4462890625
Iteration 3300: Loss = -11018.8486328125
Iteration 3400: Loss = -11018.345703125
Iteration 3500: Loss = -11017.921875
Iteration 3600: Loss = -11017.56640625
Iteration 3700: Loss = -11017.26171875
Iteration 3800: Loss = -11016.9912109375
Iteration 3900: Loss = -11016.7548828125
Iteration 4000: Loss = -11016.5439453125
Iteration 4100: Loss = -11016.3544921875
Iteration 4200: Loss = -11016.181640625
Iteration 4300: Loss = -11016.025390625
Iteration 4400: Loss = -11015.87890625
Iteration 4500: Loss = -11015.720703125
Iteration 4600: Loss = -11012.380859375
Iteration 4700: Loss = -11007.4892578125
Iteration 4800: Loss = -11007.2158203125
Iteration 4900: Loss = -11007.0537109375
Iteration 5000: Loss = -11006.921875
Iteration 5100: Loss = -11006.8095703125
Iteration 5200: Loss = -11006.7099609375
Iteration 5300: Loss = -11006.62109375
Iteration 5400: Loss = -11006.5400390625
Iteration 5500: Loss = -11006.466796875
Iteration 5600: Loss = -11006.400390625
Iteration 5700: Loss = -11006.337890625
Iteration 5800: Loss = -11006.283203125
Iteration 5900: Loss = -11006.23046875
Iteration 6000: Loss = -11006.181640625
Iteration 6100: Loss = -11006.134765625
Iteration 6200: Loss = -11006.0947265625
Iteration 6300: Loss = -11006.0537109375
Iteration 6400: Loss = -11006.013671875
Iteration 6500: Loss = -11005.9541015625
Iteration 6600: Loss = -11005.7001953125
Iteration 6700: Loss = -10996.3388671875
Iteration 6800: Loss = -10995.8095703125
Iteration 6900: Loss = -10995.6005859375
Iteration 7000: Loss = -10995.46875
Iteration 7100: Loss = -10995.37109375
Iteration 7200: Loss = -10995.2939453125
Iteration 7300: Loss = -10995.228515625
Iteration 7400: Loss = -10995.1728515625
Iteration 7500: Loss = -10995.1240234375
Iteration 7600: Loss = -10995.0810546875
Iteration 7700: Loss = -10995.0419921875
Iteration 7800: Loss = -10995.0068359375
Iteration 7900: Loss = -10994.9755859375
Iteration 8000: Loss = -10994.9443359375
Iteration 8100: Loss = -10994.9189453125
Iteration 8200: Loss = -10994.8935546875
Iteration 8300: Loss = -10994.87109375
Iteration 8400: Loss = -10994.8515625
Iteration 8500: Loss = -10994.830078125
Iteration 8600: Loss = -10994.8125
Iteration 8700: Loss = -10994.796875
Iteration 8800: Loss = -10994.7802734375
Iteration 8900: Loss = -10994.7666015625
Iteration 9000: Loss = -10994.7529296875
Iteration 9100: Loss = -10994.7412109375
Iteration 9200: Loss = -10994.7275390625
Iteration 9300: Loss = -10994.7177734375
Iteration 9400: Loss = -10994.70703125
Iteration 9500: Loss = -10994.697265625
Iteration 9600: Loss = -10994.6884765625
Iteration 9700: Loss = -10994.6796875
Iteration 9800: Loss = -10994.6728515625
Iteration 9900: Loss = -10994.666015625
Iteration 10000: Loss = -10994.6572265625
Iteration 10100: Loss = -10994.6494140625
Iteration 10200: Loss = -10994.6435546875
Iteration 10300: Loss = -10994.63671875
Iteration 10400: Loss = -10994.6328125
Iteration 10500: Loss = -10994.625
Iteration 10600: Loss = -10994.62109375
Iteration 10700: Loss = -10994.6162109375
Iteration 10800: Loss = -10994.611328125
Iteration 10900: Loss = -10994.6083984375
Iteration 11000: Loss = -10994.6044921875
Iteration 11100: Loss = -10994.599609375
Iteration 11200: Loss = -10994.595703125
Iteration 11300: Loss = -10994.591796875
Iteration 11400: Loss = -10994.58984375
Iteration 11500: Loss = -10994.5859375
Iteration 11600: Loss = -10994.58203125
Iteration 11700: Loss = -10994.5810546875
Iteration 11800: Loss = -10994.578125
Iteration 11900: Loss = -10994.576171875
Iteration 12000: Loss = -10994.572265625
Iteration 12100: Loss = -10994.5712890625
Iteration 12200: Loss = -10994.568359375
Iteration 12300: Loss = -10994.56640625
Iteration 12400: Loss = -10994.564453125
Iteration 12500: Loss = -10994.5625
Iteration 12600: Loss = -10994.5615234375
Iteration 12700: Loss = -10994.560546875
Iteration 12800: Loss = -10994.55859375
Iteration 12900: Loss = -10994.5556640625
Iteration 13000: Loss = -10994.5546875
Iteration 13100: Loss = -10994.5537109375
Iteration 13200: Loss = -10994.55078125
Iteration 13300: Loss = -10994.5517578125
1
Iteration 13400: Loss = -10994.5498046875
Iteration 13500: Loss = -10994.548828125
Iteration 13600: Loss = -10994.548828125
Iteration 13700: Loss = -10994.546875
Iteration 13800: Loss = -10994.5458984375
Iteration 13900: Loss = -10994.544921875
Iteration 14000: Loss = -10994.544921875
Iteration 14100: Loss = -10994.5439453125
Iteration 14200: Loss = -10994.5419921875
Iteration 14300: Loss = -10994.54296875
1
Iteration 14400: Loss = -10994.541015625
Iteration 14500: Loss = -10994.541015625
Iteration 14600: Loss = -10994.541015625
Iteration 14700: Loss = -10994.5380859375
Iteration 14800: Loss = -10994.5380859375
Iteration 14900: Loss = -10994.5390625
1
Iteration 15000: Loss = -10994.537109375
Iteration 15100: Loss = -10994.537109375
Iteration 15200: Loss = -10994.537109375
Iteration 15300: Loss = -10994.537109375
Iteration 15400: Loss = -10994.5361328125
Iteration 15500: Loss = -10994.5341796875
Iteration 15600: Loss = -10994.5361328125
1
Iteration 15700: Loss = -10994.5361328125
2
Iteration 15800: Loss = -10994.5341796875
Iteration 15900: Loss = -10994.533203125
Iteration 16000: Loss = -10994.533203125
Iteration 16100: Loss = -10994.533203125
Iteration 16200: Loss = -10994.533203125
Iteration 16300: Loss = -10994.5322265625
Iteration 16400: Loss = -10994.5341796875
1
Iteration 16500: Loss = -10994.5322265625
Iteration 16600: Loss = -10994.53515625
1
Iteration 16700: Loss = -10994.5322265625
Iteration 16800: Loss = -10994.5302734375
Iteration 16900: Loss = -10994.53125
1
Iteration 17000: Loss = -10994.5322265625
2
Iteration 17100: Loss = -10994.5322265625
3
Iteration 17200: Loss = -10994.5322265625
4
Iteration 17300: Loss = -10994.5322265625
5
Iteration 17400: Loss = -10994.5322265625
6
Iteration 17500: Loss = -10994.5302734375
Iteration 17600: Loss = -10994.5302734375
Iteration 17700: Loss = -10994.5302734375
Iteration 17800: Loss = -10994.5302734375
Iteration 17900: Loss = -10994.5302734375
Iteration 18000: Loss = -10994.5283203125
Iteration 18100: Loss = -10994.5302734375
1
Iteration 18200: Loss = -10994.529296875
2
Iteration 18300: Loss = -10994.53125
3
Iteration 18400: Loss = -10994.5302734375
4
Iteration 18500: Loss = -10994.5283203125
Iteration 18600: Loss = -10994.529296875
1
Iteration 18700: Loss = -10994.529296875
2
Iteration 18800: Loss = -10994.529296875
3
Iteration 18900: Loss = -10994.529296875
4
Iteration 19000: Loss = -10994.529296875
5
Iteration 19100: Loss = -10994.529296875
6
Iteration 19200: Loss = -10994.529296875
7
Iteration 19300: Loss = -10994.529296875
8
Iteration 19400: Loss = -10994.53125
9
Iteration 19500: Loss = -10994.53125
10
Iteration 19600: Loss = -10994.529296875
11
Iteration 19700: Loss = -10994.529296875
12
Iteration 19800: Loss = -10994.529296875
13
Iteration 19900: Loss = -10994.53125
14
Iteration 20000: Loss = -10994.5283203125
Iteration 20100: Loss = -10994.52734375
Iteration 20200: Loss = -10994.529296875
1
Iteration 20300: Loss = -10994.529296875
2
Iteration 20400: Loss = -10994.52734375
Iteration 20500: Loss = -10994.529296875
1
Iteration 20600: Loss = -10994.529296875
2
Iteration 20700: Loss = -10994.529296875
3
Iteration 20800: Loss = -10994.5283203125
4
Iteration 20900: Loss = -10994.529296875
5
Iteration 21000: Loss = -10994.529296875
6
Iteration 21100: Loss = -10994.529296875
7
Iteration 21200: Loss = -10994.529296875
8
Iteration 21300: Loss = -10994.529296875
9
Iteration 21400: Loss = -10994.529296875
10
Iteration 21500: Loss = -10994.529296875
11
Iteration 21600: Loss = -10994.529296875
12
Iteration 21700: Loss = -10994.529296875
13
Iteration 21800: Loss = -10994.529296875
14
Iteration 21900: Loss = -10994.529296875
15
Stopping early at iteration 21900 due to no improvement.
pi: tensor([[4.5031e-07, 1.0000e+00],
        [1.0000e+00, 4.9635e-07]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 5.3644e-07], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1601, 0.1682],
         [0.7118, 0.1669]],

        [[0.6915, 0.1630],
         [0.9257, 0.0388]],

        [[0.8823, 0.1648],
         [0.2484, 0.5748]],

        [[0.7054, 0.1955],
         [0.0140, 0.8936]],

        [[0.0208, 0.7017],
         [0.2054, 0.0343]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018328108809234837
Average Adjusted Rand Index: 0.0
[0.0, -0.0018328108809234837] [0.0, 0.0] [10995.533203125, 10994.529296875]
-------------------------------------
This iteration is 94
True Objective function: Loss = -10760.486142162514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29323.62109375
Iteration 100: Loss = -19515.78125
Iteration 200: Loss = -12660.5478515625
Iteration 300: Loss = -11462.5654296875
Iteration 400: Loss = -11240.662109375
Iteration 500: Loss = -11147.142578125
Iteration 600: Loss = -11092.96875
Iteration 700: Loss = -11056.333984375
Iteration 800: Loss = -11029.0068359375
Iteration 900: Loss = -11005.5361328125
Iteration 1000: Loss = -10990.2744140625
Iteration 1100: Loss = -10977.982421875
Iteration 1200: Loss = -10969.4560546875
Iteration 1300: Loss = -10962.3779296875
Iteration 1400: Loss = -10956.2177734375
Iteration 1500: Loss = -10951.103515625
Iteration 1600: Loss = -10946.974609375
Iteration 1700: Loss = -10943.375
Iteration 1800: Loss = -10939.951171875
Iteration 1900: Loss = -10936.2900390625
Iteration 2000: Loss = -10932.1787109375
Iteration 2100: Loss = -10927.8798828125
Iteration 2200: Loss = -10921.5703125
Iteration 2300: Loss = -10916.951171875
Iteration 2400: Loss = -10912.5322265625
Iteration 2500: Loss = -10907.814453125
Iteration 2600: Loss = -10902.3603515625
Iteration 2700: Loss = -10897.21484375
Iteration 2800: Loss = -10894.5322265625
Iteration 2900: Loss = -10891.455078125
Iteration 3000: Loss = -10889.015625
Iteration 3100: Loss = -10886.5732421875
Iteration 3200: Loss = -10884.1552734375
Iteration 3300: Loss = -10881.783203125
Iteration 3400: Loss = -10878.923828125
Iteration 3500: Loss = -10876.8271484375
Iteration 3600: Loss = -10875.2646484375
Iteration 3700: Loss = -10873.6025390625
Iteration 3800: Loss = -10871.5029296875
Iteration 3900: Loss = -10868.72265625
Iteration 4000: Loss = -10867.6357421875
Iteration 4100: Loss = -10866.8291015625
Iteration 4200: Loss = -10865.3095703125
Iteration 4300: Loss = -10863.37109375
Iteration 4400: Loss = -10861.2998046875
Iteration 4500: Loss = -10860.212890625
Iteration 4600: Loss = -10858.3935546875
Iteration 4700: Loss = -10856.3349609375
Iteration 4800: Loss = -10855.880859375
Iteration 4900: Loss = -10852.498046875
Iteration 5000: Loss = -10851.908203125
Iteration 5100: Loss = -10850.9921875
Iteration 5200: Loss = -10850.185546875
Iteration 5300: Loss = -10849.1474609375
Iteration 5400: Loss = -10848.8154296875
Iteration 5500: Loss = -10847.2841796875
Iteration 5600: Loss = -10846.759765625
Iteration 5700: Loss = -10846.4541015625
Iteration 5800: Loss = -10844.9248046875
Iteration 5900: Loss = -10844.7626953125
Iteration 6000: Loss = -10844.6875
Iteration 6100: Loss = -10844.408203125
Iteration 6200: Loss = -10843.5869140625
Iteration 6300: Loss = -10842.265625
Iteration 6400: Loss = -10840.4873046875
Iteration 6500: Loss = -10840.4296875
Iteration 6600: Loss = -10840.3818359375
Iteration 6700: Loss = -10840.3056640625
Iteration 6800: Loss = -10839.283203125
Iteration 6900: Loss = -10839.189453125
Iteration 7000: Loss = -10839.15625
Iteration 7100: Loss = -10839.126953125
Iteration 7200: Loss = -10839.099609375
Iteration 7300: Loss = -10839.06640625
Iteration 7400: Loss = -10839.0302734375
Iteration 7500: Loss = -10838.955078125
Iteration 7600: Loss = -10837.78515625
Iteration 7700: Loss = -10836.205078125
Iteration 7800: Loss = -10833.9423828125
Iteration 7900: Loss = -10832.9736328125
Iteration 8000: Loss = -10832.234375
Iteration 8100: Loss = -10832.1748046875
Iteration 8200: Loss = -10832.1435546875
Iteration 8300: Loss = -10832.1181640625
Iteration 8400: Loss = -10832.1005859375
Iteration 8500: Loss = -10832.083984375
Iteration 8600: Loss = -10832.0703125
Iteration 8700: Loss = -10832.056640625
Iteration 8800: Loss = -10832.044921875
Iteration 8900: Loss = -10832.0341796875
Iteration 9000: Loss = -10832.0234375
Iteration 9100: Loss = -10832.013671875
Iteration 9200: Loss = -10832.0029296875
Iteration 9300: Loss = -10831.9951171875
Iteration 9400: Loss = -10831.986328125
Iteration 9500: Loss = -10831.9794921875
Iteration 9600: Loss = -10831.97265625
Iteration 9700: Loss = -10831.96484375
Iteration 9800: Loss = -10831.958984375
Iteration 9900: Loss = -10831.9521484375
Iteration 10000: Loss = -10831.947265625
Iteration 10100: Loss = -10831.9423828125
Iteration 10200: Loss = -10831.9375
Iteration 10300: Loss = -10831.931640625
Iteration 10400: Loss = -10831.9287109375
Iteration 10500: Loss = -10831.921875
Iteration 10600: Loss = -10831.9189453125
Iteration 10700: Loss = -10831.916015625
Iteration 10800: Loss = -10831.912109375
Iteration 10900: Loss = -10831.9091796875
Iteration 11000: Loss = -10831.904296875
Iteration 11100: Loss = -10831.9013671875
Iteration 11200: Loss = -10831.8974609375
Iteration 11300: Loss = -10831.8955078125
Iteration 11400: Loss = -10831.892578125
Iteration 11500: Loss = -10831.890625
Iteration 11600: Loss = -10831.888671875
Iteration 11700: Loss = -10831.884765625
Iteration 11800: Loss = -10831.8837890625
Iteration 11900: Loss = -10831.8818359375
Iteration 12000: Loss = -10831.8779296875
Iteration 12100: Loss = -10831.876953125
Iteration 12200: Loss = -10831.8759765625
Iteration 12300: Loss = -10831.875
Iteration 12400: Loss = -10831.8720703125
Iteration 12500: Loss = -10831.8701171875
Iteration 12600: Loss = -10831.8701171875
Iteration 12700: Loss = -10831.8681640625
Iteration 12800: Loss = -10831.869140625
1
Iteration 12900: Loss = -10831.865234375
Iteration 13000: Loss = -10831.86328125
Iteration 13100: Loss = -10831.86328125
Iteration 13200: Loss = -10831.8623046875
Iteration 13300: Loss = -10831.8603515625
Iteration 13400: Loss = -10831.8603515625
Iteration 13500: Loss = -10831.8583984375
Iteration 13600: Loss = -10831.857421875
Iteration 13700: Loss = -10831.857421875
Iteration 13800: Loss = -10831.8564453125
Iteration 13900: Loss = -10831.8564453125
Iteration 14000: Loss = -10831.85546875
Iteration 14100: Loss = -10831.853515625
Iteration 14200: Loss = -10831.853515625
Iteration 14300: Loss = -10831.853515625
Iteration 14400: Loss = -10831.8525390625
Iteration 14500: Loss = -10831.8525390625
Iteration 14600: Loss = -10831.8525390625
Iteration 14700: Loss = -10831.8505859375
Iteration 14800: Loss = -10831.8515625
1
Iteration 14900: Loss = -10831.8515625
2
Iteration 15000: Loss = -10831.8515625
3
Iteration 15100: Loss = -10831.84765625
Iteration 15200: Loss = -10831.8505859375
1
Iteration 15300: Loss = -10831.8486328125
2
Iteration 15400: Loss = -10831.84765625
Iteration 15500: Loss = -10831.84765625
Iteration 15600: Loss = -10831.8466796875
Iteration 15700: Loss = -10831.8466796875
Iteration 15800: Loss = -10831.8466796875
Iteration 15900: Loss = -10831.8466796875
Iteration 16000: Loss = -10831.8466796875
Iteration 16100: Loss = -10831.8466796875
Iteration 16200: Loss = -10831.845703125
Iteration 16300: Loss = -10831.845703125
Iteration 16400: Loss = -10831.845703125
Iteration 16500: Loss = -10831.845703125
Iteration 16600: Loss = -10831.8447265625
Iteration 16700: Loss = -10831.8466796875
1
Iteration 16800: Loss = -10831.8447265625
Iteration 16900: Loss = -10831.8427734375
Iteration 17000: Loss = -10831.8447265625
1
Iteration 17100: Loss = -10831.84375
2
Iteration 17200: Loss = -10831.8447265625
3
Iteration 17300: Loss = -10831.8427734375
Iteration 17400: Loss = -10831.84375
1
Iteration 17500: Loss = -10831.84375
2
Iteration 17600: Loss = -10831.841796875
Iteration 17700: Loss = -10831.84375
1
Iteration 17800: Loss = -10831.84375
2
Iteration 17900: Loss = -10831.841796875
Iteration 18000: Loss = -10831.841796875
Iteration 18100: Loss = -10831.84375
1
Iteration 18200: Loss = -10831.84375
2
Iteration 18300: Loss = -10831.841796875
Iteration 18400: Loss = -10831.841796875
Iteration 18500: Loss = -10831.841796875
Iteration 18600: Loss = -10831.8427734375
1
Iteration 18700: Loss = -10831.8408203125
Iteration 18800: Loss = -10831.841796875
1
Iteration 18900: Loss = -10831.841796875
2
Iteration 19000: Loss = -10831.841796875
3
Iteration 19100: Loss = -10831.8427734375
4
Iteration 19200: Loss = -10831.841796875
5
Iteration 19300: Loss = -10831.8408203125
Iteration 19400: Loss = -10831.8408203125
Iteration 19500: Loss = -10831.8408203125
Iteration 19600: Loss = -10831.8408203125
Iteration 19700: Loss = -10831.841796875
1
Iteration 19800: Loss = -10831.8408203125
Iteration 19900: Loss = -10831.8408203125
Iteration 20000: Loss = -10831.8427734375
1
Iteration 20100: Loss = -10831.83984375
Iteration 20200: Loss = -10831.841796875
1
Iteration 20300: Loss = -10831.841796875
2
Iteration 20400: Loss = -10831.83984375
Iteration 20500: Loss = -10831.8408203125
1
Iteration 20600: Loss = -10831.841796875
2
Iteration 20700: Loss = -10831.83984375
Iteration 20800: Loss = -10831.841796875
1
Iteration 20900: Loss = -10831.841796875
2
Iteration 21000: Loss = -10831.8408203125
3
Iteration 21100: Loss = -10831.8408203125
4
Iteration 21200: Loss = -10831.8408203125
5
Iteration 21300: Loss = -10831.841796875
6
Iteration 21400: Loss = -10831.841796875
7
Iteration 21500: Loss = -10831.841796875
8
Iteration 21600: Loss = -10831.8408203125
9
Iteration 21700: Loss = -10831.8408203125
10
Iteration 21800: Loss = -10831.8408203125
11
Iteration 21900: Loss = -10831.83984375
Iteration 22000: Loss = -10831.83984375
Iteration 22100: Loss = -10831.8388671875
Iteration 22200: Loss = -10831.8408203125
1
Iteration 22300: Loss = -10831.8388671875
Iteration 22400: Loss = -10831.83984375
1
Iteration 22500: Loss = -10831.8388671875
Iteration 22600: Loss = -10831.837890625
Iteration 22700: Loss = -10831.837890625
Iteration 22800: Loss = -10831.837890625
Iteration 22900: Loss = -10831.8369140625
Iteration 23000: Loss = -10831.83984375
1
Iteration 23100: Loss = -10831.837890625
2
Iteration 23200: Loss = -10831.8359375
Iteration 23300: Loss = -10831.833984375
Iteration 23400: Loss = -10831.8271484375
Iteration 23500: Loss = -10831.818359375
Iteration 23600: Loss = -10831.80859375
Iteration 23700: Loss = -10831.8037109375
Iteration 23800: Loss = -10831.802734375
Iteration 23900: Loss = -10831.802734375
Iteration 24000: Loss = -10831.802734375
Iteration 24100: Loss = -10831.8037109375
1
Iteration 24200: Loss = -10831.802734375
Iteration 24300: Loss = -10831.8037109375
1
Iteration 24400: Loss = -10831.8037109375
2
Iteration 24500: Loss = -10831.8017578125
Iteration 24600: Loss = -10831.8037109375
1
Iteration 24700: Loss = -10831.802734375
2
Iteration 24800: Loss = -10831.802734375
3
Iteration 24900: Loss = -10831.8017578125
Iteration 25000: Loss = -10831.8017578125
Iteration 25100: Loss = -10831.8017578125
Iteration 25200: Loss = -10831.802734375
1
Iteration 25300: Loss = -10831.802734375
2
Iteration 25400: Loss = -10831.802734375
3
Iteration 25500: Loss = -10831.802734375
4
Iteration 25600: Loss = -10831.8017578125
Iteration 25700: Loss = -10831.802734375
1
Iteration 25800: Loss = -10831.8017578125
Iteration 25900: Loss = -10831.802734375
1
Iteration 26000: Loss = -10831.802734375
2
Iteration 26100: Loss = -10831.8037109375
3
Iteration 26200: Loss = -10831.8017578125
Iteration 26300: Loss = -10831.802734375
1
Iteration 26400: Loss = -10831.802734375
2
Iteration 26500: Loss = -10831.8017578125
Iteration 26600: Loss = -10831.802734375
1
Iteration 26700: Loss = -10831.802734375
2
Iteration 26800: Loss = -10831.8017578125
Iteration 26900: Loss = -10831.8017578125
Iteration 27000: Loss = -10831.8017578125
Iteration 27100: Loss = -10831.8046875
1
Iteration 27200: Loss = -10831.802734375
2
Iteration 27300: Loss = -10831.8017578125
Iteration 27400: Loss = -10831.802734375
1
Iteration 27500: Loss = -10831.8037109375
2
Iteration 27600: Loss = -10831.8017578125
Iteration 27700: Loss = -10831.802734375
1
Iteration 27800: Loss = -10831.8017578125
Iteration 27900: Loss = -10831.8046875
1
Iteration 28000: Loss = -10831.8037109375
2
Iteration 28100: Loss = -10831.8017578125
Iteration 28200: Loss = -10831.8017578125
Iteration 28300: Loss = -10831.802734375
1
Iteration 28400: Loss = -10831.8017578125
Iteration 28500: Loss = -10831.802734375
1
Iteration 28600: Loss = -10831.802734375
2
Iteration 28700: Loss = -10831.802734375
3
Iteration 28800: Loss = -10831.802734375
4
Iteration 28900: Loss = -10831.802734375
5
Iteration 29000: Loss = -10831.802734375
6
Iteration 29100: Loss = -10831.8017578125
Iteration 29200: Loss = -10831.8017578125
Iteration 29300: Loss = -10831.802734375
1
Iteration 29400: Loss = -10831.8037109375
2
Iteration 29500: Loss = -10831.802734375
3
Iteration 29600: Loss = -10831.8017578125
Iteration 29700: Loss = -10831.8017578125
Iteration 29800: Loss = -10831.802734375
1
Iteration 29900: Loss = -10831.802734375
2
pi: tensor([[1.0000e+00, 5.3739e-07],
        [4.6567e-01, 5.3433e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.3131e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1591, 0.1670],
         [0.9153, 0.1578]],

        [[0.9900, 0.1614],
         [0.1976, 0.0154]],

        [[0.7477, 0.1572],
         [0.6334, 0.5987]],

        [[0.9570, 0.1618],
         [0.9226, 0.0877]],

        [[0.2995, 0.1544],
         [0.0575, 0.9179]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001764441811601581
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36282.62890625
Iteration 100: Loss = -23058.98046875
Iteration 200: Loss = -13505.0908203125
Iteration 300: Loss = -11721.4853515625
Iteration 400: Loss = -11370.208984375
Iteration 500: Loss = -11220.1796875
Iteration 600: Loss = -11136.1591796875
Iteration 700: Loss = -11082.8583984375
Iteration 800: Loss = -11045.0947265625
Iteration 900: Loss = -11015.576171875
Iteration 1000: Loss = -10994.4482421875
Iteration 1100: Loss = -10977.234375
Iteration 1200: Loss = -10963.7333984375
Iteration 1300: Loss = -10952.8994140625
Iteration 1400: Loss = -10942.744140625
Iteration 1500: Loss = -10933.5576171875
Iteration 1600: Loss = -10925.3193359375
Iteration 1700: Loss = -10918.3212890625
Iteration 1800: Loss = -10911.0
Iteration 1900: Loss = -10903.341796875
Iteration 2000: Loss = -10896.7060546875
Iteration 2100: Loss = -10889.296875
Iteration 2200: Loss = -10883.1171875
Iteration 2300: Loss = -10876.583984375
Iteration 2400: Loss = -10869.4453125
Iteration 2500: Loss = -10866.111328125
Iteration 2600: Loss = -10861.6298828125
Iteration 2700: Loss = -10859.927734375
Iteration 2800: Loss = -10858.095703125
Iteration 2900: Loss = -10854.529296875
Iteration 3000: Loss = -10852.7392578125
Iteration 3100: Loss = -10848.8720703125
Iteration 3200: Loss = -10843.46875
Iteration 3300: Loss = -10842.1787109375
Iteration 3400: Loss = -10841.232421875
Iteration 3500: Loss = -10840.451171875
Iteration 3600: Loss = -10839.779296875
Iteration 3700: Loss = -10839.1884765625
Iteration 3800: Loss = -10838.6630859375
Iteration 3900: Loss = -10838.1884765625
Iteration 4000: Loss = -10837.759765625
Iteration 4100: Loss = -10837.3564453125
Iteration 4200: Loss = -10837.0
Iteration 4300: Loss = -10836.6708984375
Iteration 4400: Loss = -10836.37109375
Iteration 4500: Loss = -10836.0927734375
Iteration 4600: Loss = -10835.833984375
Iteration 4700: Loss = -10835.5947265625
Iteration 4800: Loss = -10835.3740234375
Iteration 4900: Loss = -10835.1689453125
Iteration 5000: Loss = -10834.9755859375
Iteration 5100: Loss = -10834.7958984375
Iteration 5200: Loss = -10834.6279296875
Iteration 5300: Loss = -10834.4716796875
Iteration 5400: Loss = -10834.3232421875
Iteration 5500: Loss = -10834.185546875
Iteration 5600: Loss = -10834.056640625
Iteration 5700: Loss = -10833.9365234375
Iteration 5800: Loss = -10833.8212890625
Iteration 5900: Loss = -10833.7158203125
Iteration 6000: Loss = -10833.615234375
Iteration 6100: Loss = -10833.521484375
Iteration 6200: Loss = -10833.4306640625
Iteration 6300: Loss = -10833.34765625
Iteration 6400: Loss = -10833.2685546875
Iteration 6500: Loss = -10833.1943359375
Iteration 6600: Loss = -10833.1240234375
Iteration 6700: Loss = -10833.0576171875
Iteration 6800: Loss = -10832.9951171875
Iteration 6900: Loss = -10832.9365234375
Iteration 7000: Loss = -10832.880859375
Iteration 7100: Loss = -10832.826171875
Iteration 7200: Loss = -10832.7763671875
Iteration 7300: Loss = -10832.7294921875
Iteration 7400: Loss = -10832.6845703125
Iteration 7500: Loss = -10832.64453125
Iteration 7600: Loss = -10832.603515625
Iteration 7700: Loss = -10832.56640625
Iteration 7800: Loss = -10832.529296875
Iteration 7900: Loss = -10832.4970703125
Iteration 8000: Loss = -10832.4638671875
Iteration 8100: Loss = -10832.43359375
Iteration 8200: Loss = -10832.4052734375
Iteration 8300: Loss = -10832.376953125
Iteration 8400: Loss = -10832.3525390625
Iteration 8500: Loss = -10832.3271484375
Iteration 8600: Loss = -10832.3056640625
Iteration 8700: Loss = -10832.28125
Iteration 8800: Loss = -10832.259765625
Iteration 8900: Loss = -10832.240234375
Iteration 9000: Loss = -10832.2216796875
Iteration 9100: Loss = -10832.2041015625
Iteration 9200: Loss = -10832.1875
Iteration 9300: Loss = -10832.1708984375
Iteration 9400: Loss = -10832.154296875
Iteration 9500: Loss = -10832.140625
Iteration 9600: Loss = -10832.1279296875
Iteration 9700: Loss = -10832.115234375
Iteration 9800: Loss = -10832.1015625
Iteration 9900: Loss = -10832.091796875
Iteration 10000: Loss = -10832.0791015625
Iteration 10100: Loss = -10832.0693359375
Iteration 10200: Loss = -10832.05859375
Iteration 10300: Loss = -10832.048828125
Iteration 10400: Loss = -10832.041015625
Iteration 10500: Loss = -10832.0322265625
Iteration 10600: Loss = -10832.0234375
Iteration 10700: Loss = -10832.0166015625
Iteration 10800: Loss = -10832.0078125
Iteration 10900: Loss = -10832.0009765625
Iteration 11000: Loss = -10831.9951171875
Iteration 11100: Loss = -10831.9873046875
Iteration 11200: Loss = -10831.982421875
Iteration 11300: Loss = -10831.9755859375
Iteration 11400: Loss = -10831.970703125
Iteration 11500: Loss = -10831.966796875
Iteration 11600: Loss = -10831.962890625
Iteration 11700: Loss = -10831.9560546875
Iteration 11800: Loss = -10831.9541015625
Iteration 11900: Loss = -10831.9482421875
Iteration 12000: Loss = -10831.9453125
Iteration 12100: Loss = -10831.9404296875
Iteration 12200: Loss = -10831.935546875
Iteration 12300: Loss = -10831.9345703125
Iteration 12400: Loss = -10831.9306640625
Iteration 12500: Loss = -10831.927734375
Iteration 12600: Loss = -10831.9248046875
Iteration 12700: Loss = -10831.9208984375
Iteration 12800: Loss = -10831.9189453125
Iteration 12900: Loss = -10831.916015625
Iteration 13000: Loss = -10831.9130859375
Iteration 13100: Loss = -10831.9130859375
Iteration 13200: Loss = -10831.91015625
Iteration 13300: Loss = -10831.90625
Iteration 13400: Loss = -10831.9052734375
Iteration 13500: Loss = -10831.904296875
Iteration 13600: Loss = -10831.9033203125
Iteration 13700: Loss = -10831.900390625
Iteration 13800: Loss = -10831.8994140625
Iteration 13900: Loss = -10831.896484375
Iteration 14000: Loss = -10831.8955078125
Iteration 14100: Loss = -10831.8955078125
Iteration 14200: Loss = -10831.8935546875
Iteration 14300: Loss = -10831.8935546875
Iteration 14400: Loss = -10831.892578125
Iteration 14500: Loss = -10831.8896484375
Iteration 14600: Loss = -10831.8876953125
Iteration 14700: Loss = -10831.888671875
1
Iteration 14800: Loss = -10831.88671875
Iteration 14900: Loss = -10831.88671875
Iteration 15000: Loss = -10831.8857421875
Iteration 15100: Loss = -10831.8837890625
Iteration 15200: Loss = -10831.884765625
1
Iteration 15300: Loss = -10831.8818359375
Iteration 15400: Loss = -10831.8828125
1
Iteration 15500: Loss = -10831.8828125
2
Iteration 15600: Loss = -10831.880859375
Iteration 15700: Loss = -10831.87890625
Iteration 15800: Loss = -10831.87890625
Iteration 15900: Loss = -10831.87890625
Iteration 16000: Loss = -10831.880859375
1
Iteration 16100: Loss = -10831.8779296875
Iteration 16200: Loss = -10831.8779296875
Iteration 16300: Loss = -10831.876953125
Iteration 16400: Loss = -10831.876953125
Iteration 16500: Loss = -10831.876953125
Iteration 16600: Loss = -10831.876953125
Iteration 16700: Loss = -10831.8759765625
Iteration 16800: Loss = -10831.8759765625
Iteration 16900: Loss = -10831.876953125
1
Iteration 17000: Loss = -10831.8740234375
Iteration 17100: Loss = -10831.8759765625
1
Iteration 17200: Loss = -10831.875
2
Iteration 17300: Loss = -10831.8740234375
Iteration 17400: Loss = -10831.875
1
Iteration 17500: Loss = -10831.873046875
Iteration 17600: Loss = -10831.8740234375
1
Iteration 17700: Loss = -10831.875
2
Iteration 17800: Loss = -10831.8759765625
3
Iteration 17900: Loss = -10831.8720703125
Iteration 18000: Loss = -10831.873046875
1
Iteration 18100: Loss = -10831.8720703125
Iteration 18200: Loss = -10831.8740234375
1
Iteration 18300: Loss = -10831.873046875
2
Iteration 18400: Loss = -10831.8720703125
Iteration 18500: Loss = -10831.873046875
1
Iteration 18600: Loss = -10831.873046875
2
Iteration 18700: Loss = -10831.8720703125
Iteration 18800: Loss = -10831.873046875
1
Iteration 18900: Loss = -10831.8720703125
Iteration 19000: Loss = -10831.8720703125
Iteration 19100: Loss = -10831.873046875
1
Iteration 19200: Loss = -10831.87109375
Iteration 19300: Loss = -10831.873046875
1
Iteration 19400: Loss = -10831.8701171875
Iteration 19500: Loss = -10831.87109375
1
Iteration 19600: Loss = -10831.8701171875
Iteration 19700: Loss = -10831.8720703125
1
Iteration 19800: Loss = -10831.8720703125
2
Iteration 19900: Loss = -10831.87109375
3
Iteration 20000: Loss = -10831.8701171875
Iteration 20100: Loss = -10831.8720703125
1
Iteration 20200: Loss = -10831.87109375
2
Iteration 20300: Loss = -10831.8720703125
3
Iteration 20400: Loss = -10831.87109375
4
Iteration 20500: Loss = -10831.869140625
Iteration 20600: Loss = -10831.87109375
1
Iteration 20700: Loss = -10831.8701171875
2
Iteration 20800: Loss = -10831.87109375
3
Iteration 20900: Loss = -10831.8701171875
4
Iteration 21000: Loss = -10831.869140625
Iteration 21100: Loss = -10831.8701171875
1
Iteration 21200: Loss = -10831.869140625
Iteration 21300: Loss = -10831.8701171875
1
Iteration 21400: Loss = -10831.8681640625
Iteration 21500: Loss = -10831.869140625
1
Iteration 21600: Loss = -10831.8681640625
Iteration 21700: Loss = -10831.8701171875
1
Iteration 21800: Loss = -10831.8681640625
Iteration 21900: Loss = -10831.869140625
1
Iteration 22000: Loss = -10831.8681640625
Iteration 22100: Loss = -10831.869140625
1
Iteration 22200: Loss = -10831.8701171875
2
Iteration 22300: Loss = -10831.869140625
3
Iteration 22400: Loss = -10831.869140625
4
Iteration 22500: Loss = -10831.869140625
5
Iteration 22600: Loss = -10831.869140625
6
Iteration 22700: Loss = -10831.869140625
7
Iteration 22800: Loss = -10831.869140625
8
Iteration 22900: Loss = -10831.869140625
9
Iteration 23000: Loss = -10831.869140625
10
Iteration 23100: Loss = -10831.869140625
11
Iteration 23200: Loss = -10831.869140625
12
Iteration 23300: Loss = -10831.869140625
13
Iteration 23400: Loss = -10831.8701171875
14
Iteration 23500: Loss = -10831.869140625
15
Stopping early at iteration 23500 due to no improvement.
pi: tensor([[1.0000e+00, 2.1134e-06],
        [1.0000e+00, 5.9616e-08]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.2080e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.1812],
         [0.0614, 0.1570]],

        [[0.1249, 0.4370],
         [0.6293, 0.4188]],

        [[0.9865, 0.1550],
         [0.5813, 0.9803]],

        [[0.9632, 0.1614],
         [0.1236, 0.8065]],

        [[0.0331, 0.4532],
         [0.7913, 0.5098]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0012439690256998406
Average Adjusted Rand Index: 0.0
[0.001764441811601581, 0.0012439690256998406] [0.0, 0.0] [10831.802734375, 10831.869140625]
-------------------------------------
This iteration is 95
True Objective function: Loss = -10853.209754451182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32492.169921875
Iteration 100: Loss = -18902.080078125
Iteration 200: Loss = -12525.3583984375
Iteration 300: Loss = -11480.4248046875
Iteration 400: Loss = -11250.75
Iteration 500: Loss = -11135.3525390625
Iteration 600: Loss = -11049.1044921875
Iteration 700: Loss = -10996.1044921875
Iteration 800: Loss = -10968.4130859375
Iteration 900: Loss = -10941.7890625
Iteration 1000: Loss = -10910.2998046875
Iteration 1100: Loss = -10901.54296875
Iteration 1200: Loss = -10896.2705078125
Iteration 1300: Loss = -10892.3857421875
Iteration 1400: Loss = -10889.36328125
Iteration 1500: Loss = -10886.931640625
Iteration 1600: Loss = -10884.9296875
Iteration 1700: Loss = -10883.2490234375
Iteration 1800: Loss = -10881.8154296875
Iteration 1900: Loss = -10880.5830078125
Iteration 2000: Loss = -10879.5087890625
Iteration 2100: Loss = -10878.5673828125
Iteration 2200: Loss = -10877.7392578125
Iteration 2300: Loss = -10876.998046875
Iteration 2400: Loss = -10876.341796875
Iteration 2500: Loss = -10875.751953125
Iteration 2600: Loss = -10875.2197265625
Iteration 2700: Loss = -10874.740234375
Iteration 2800: Loss = -10874.3056640625
Iteration 2900: Loss = -10873.9091796875
Iteration 3000: Loss = -10873.5478515625
Iteration 3100: Loss = -10873.216796875
Iteration 3200: Loss = -10872.9140625
Iteration 3300: Loss = -10872.6357421875
Iteration 3400: Loss = -10872.37890625
Iteration 3500: Loss = -10872.1416015625
Iteration 3600: Loss = -10871.9208984375
Iteration 3700: Loss = -10871.7197265625
Iteration 3800: Loss = -10871.5322265625
Iteration 3900: Loss = -10871.357421875
Iteration 4000: Loss = -10871.1943359375
Iteration 4100: Loss = -10871.041015625
Iteration 4200: Loss = -10870.9013671875
Iteration 4300: Loss = -10870.7685546875
Iteration 4400: Loss = -10870.64453125
Iteration 4500: Loss = -10870.5283203125
Iteration 4600: Loss = -10870.419921875
Iteration 4700: Loss = -10870.3173828125
Iteration 4800: Loss = -10870.22265625
Iteration 4900: Loss = -10870.1337890625
Iteration 5000: Loss = -10870.0478515625
Iteration 5100: Loss = -10869.96875
Iteration 5200: Loss = -10869.89453125
Iteration 5300: Loss = -10869.82421875
Iteration 5400: Loss = -10869.7568359375
Iteration 5500: Loss = -10869.6953125
Iteration 5600: Loss = -10869.63671875
Iteration 5700: Loss = -10869.580078125
Iteration 5800: Loss = -10869.5263671875
Iteration 5900: Loss = -10869.4775390625
Iteration 6000: Loss = -10869.431640625
Iteration 6100: Loss = -10869.3857421875
Iteration 6200: Loss = -10869.3427734375
Iteration 6300: Loss = -10869.302734375
Iteration 6400: Loss = -10869.2646484375
Iteration 6500: Loss = -10869.2314453125
Iteration 6600: Loss = -10869.197265625
Iteration 6700: Loss = -10869.1650390625
Iteration 6800: Loss = -10869.134765625
Iteration 6900: Loss = -10869.103515625
Iteration 7000: Loss = -10869.0791015625
Iteration 7100: Loss = -10869.052734375
Iteration 7200: Loss = -10869.0283203125
Iteration 7300: Loss = -10869.00390625
Iteration 7400: Loss = -10868.982421875
Iteration 7500: Loss = -10868.9609375
Iteration 7600: Loss = -10868.9404296875
Iteration 7700: Loss = -10868.921875
Iteration 7800: Loss = -10868.9033203125
Iteration 7900: Loss = -10868.8857421875
Iteration 8000: Loss = -10868.8701171875
Iteration 8100: Loss = -10868.85546875
Iteration 8200: Loss = -10868.83984375
Iteration 8300: Loss = -10868.826171875
Iteration 8400: Loss = -10868.8115234375
Iteration 8500: Loss = -10868.7978515625
Iteration 8600: Loss = -10868.787109375
Iteration 8700: Loss = -10868.7744140625
Iteration 8800: Loss = -10868.7646484375
Iteration 8900: Loss = -10868.75390625
Iteration 9000: Loss = -10868.744140625
Iteration 9100: Loss = -10868.7353515625
Iteration 9200: Loss = -10868.7255859375
Iteration 9300: Loss = -10868.7177734375
Iteration 9400: Loss = -10868.708984375
Iteration 9500: Loss = -10868.701171875
Iteration 9600: Loss = -10868.6962890625
Iteration 9700: Loss = -10868.6865234375
Iteration 9800: Loss = -10868.681640625
Iteration 9900: Loss = -10868.673828125
Iteration 10000: Loss = -10868.66796875
Iteration 10100: Loss = -10868.6630859375
Iteration 10200: Loss = -10868.65625
Iteration 10300: Loss = -10868.650390625
Iteration 10400: Loss = -10868.646484375
Iteration 10500: Loss = -10868.6396484375
Iteration 10600: Loss = -10868.634765625
Iteration 10700: Loss = -10868.630859375
Iteration 10800: Loss = -10868.6240234375
Iteration 10900: Loss = -10868.6181640625
Iteration 11000: Loss = -10868.6083984375
Iteration 11100: Loss = -10868.58203125
Iteration 11200: Loss = -10868.3935546875
Iteration 11300: Loss = -10867.3212890625
Iteration 11400: Loss = -10867.1953125
Iteration 11500: Loss = -10867.1484375
Iteration 11600: Loss = -10867.111328125
Iteration 11700: Loss = -10867.091796875
Iteration 11800: Loss = -10867.0712890625
Iteration 11900: Loss = -10867.0224609375
Iteration 12000: Loss = -10866.240234375
Iteration 12100: Loss = -10866.1484375
Iteration 12200: Loss = -10866.1181640625
Iteration 12300: Loss = -10866.107421875
Iteration 12400: Loss = -10866.09765625
Iteration 12500: Loss = -10866.0947265625
Iteration 12600: Loss = -10866.091796875
Iteration 12700: Loss = -10866.087890625
Iteration 12800: Loss = -10866.0859375
Iteration 12900: Loss = -10866.08203125
Iteration 13000: Loss = -10866.08203125
Iteration 13100: Loss = -10866.080078125
Iteration 13200: Loss = -10866.078125
Iteration 13300: Loss = -10866.0771484375
Iteration 13400: Loss = -10866.07421875
Iteration 13500: Loss = -10866.07421875
Iteration 13600: Loss = -10866.072265625
Iteration 13700: Loss = -10866.0703125
Iteration 13800: Loss = -10866.072265625
1
Iteration 13900: Loss = -10866.068359375
Iteration 14000: Loss = -10866.0693359375
1
Iteration 14100: Loss = -10866.068359375
Iteration 14200: Loss = -10866.06640625
Iteration 14300: Loss = -10866.0654296875
Iteration 14400: Loss = -10866.064453125
Iteration 14500: Loss = -10866.0634765625
Iteration 14600: Loss = -10866.0625
Iteration 14700: Loss = -10866.060546875
Iteration 14800: Loss = -10866.0576171875
Iteration 14900: Loss = -10866.0556640625
Iteration 15000: Loss = -10866.0517578125
Iteration 15100: Loss = -10866.0458984375
Iteration 15200: Loss = -10866.0263671875
Iteration 15300: Loss = -10865.87890625
Iteration 15400: Loss = -10865.3310546875
Iteration 15500: Loss = -10865.150390625
Iteration 15600: Loss = -10864.7958984375
Iteration 15700: Loss = -10864.7294921875
Iteration 15800: Loss = -10864.6884765625
Iteration 15900: Loss = -10864.2470703125
Iteration 16000: Loss = -10864.22265625
Iteration 16100: Loss = -10864.2099609375
Iteration 16200: Loss = -10864.193359375
Iteration 16300: Loss = -10864.1904296875
Iteration 16400: Loss = -10864.1396484375
Iteration 16500: Loss = -10864.1376953125
Iteration 16600: Loss = -10864.1318359375
Iteration 16700: Loss = -10864.111328125
Iteration 16800: Loss = -10864.1025390625
Iteration 16900: Loss = -10864.0966796875
Iteration 17000: Loss = -10864.0927734375
Iteration 17100: Loss = -10864.08984375
Iteration 17200: Loss = -10864.087890625
Iteration 17300: Loss = -10864.087890625
Iteration 17400: Loss = -10864.0859375
Iteration 17500: Loss = -10864.0830078125
Iteration 17600: Loss = -10864.0791015625
Iteration 17700: Loss = -10864.0771484375
Iteration 17800: Loss = -10864.0751953125
Iteration 17900: Loss = -10864.0732421875
Iteration 18000: Loss = -10864.072265625
Iteration 18100: Loss = -10864.0693359375
Iteration 18200: Loss = -10864.0693359375
Iteration 18300: Loss = -10864.0693359375
Iteration 18400: Loss = -10864.068359375
Iteration 18500: Loss = -10864.0673828125
Iteration 18600: Loss = -10864.0693359375
1
Iteration 18700: Loss = -10864.068359375
2
Iteration 18800: Loss = -10864.068359375
3
Iteration 18900: Loss = -10864.0673828125
Iteration 19000: Loss = -10864.06640625
Iteration 19100: Loss = -10864.068359375
1
Iteration 19200: Loss = -10864.0673828125
2
Iteration 19300: Loss = -10864.0673828125
3
Iteration 19400: Loss = -10864.0673828125
4
Iteration 19500: Loss = -10864.068359375
5
Iteration 19600: Loss = -10864.06640625
Iteration 19700: Loss = -10864.0673828125
1
Iteration 19800: Loss = -10864.0673828125
2
Iteration 19900: Loss = -10864.0673828125
3
Iteration 20000: Loss = -10864.0673828125
4
Iteration 20100: Loss = -10864.0693359375
5
Iteration 20200: Loss = -10864.0673828125
6
Iteration 20300: Loss = -10864.06640625
Iteration 20400: Loss = -10864.0673828125
1
Iteration 20500: Loss = -10864.06640625
Iteration 20600: Loss = -10864.06640625
Iteration 20700: Loss = -10864.068359375
1
Iteration 20800: Loss = -10864.0673828125
2
Iteration 20900: Loss = -10864.06640625
Iteration 21000: Loss = -10864.068359375
1
Iteration 21100: Loss = -10864.06640625
Iteration 21200: Loss = -10864.0693359375
1
Iteration 21300: Loss = -10864.0673828125
2
Iteration 21400: Loss = -10864.0703125
3
Iteration 21500: Loss = -10864.06640625
Iteration 21600: Loss = -10864.0654296875
Iteration 21700: Loss = -10864.0654296875
Iteration 21800: Loss = -10864.0673828125
1
Iteration 21900: Loss = -10864.06640625
2
Iteration 22000: Loss = -10864.06640625
3
Iteration 22100: Loss = -10864.06640625
4
Iteration 22200: Loss = -10864.0673828125
5
Iteration 22300: Loss = -10864.0673828125
6
Iteration 22400: Loss = -10864.06640625
7
Iteration 22500: Loss = -10864.0673828125
8
Iteration 22600: Loss = -10864.0654296875
Iteration 22700: Loss = -10864.06640625
1
Iteration 22800: Loss = -10864.0673828125
2
Iteration 22900: Loss = -10864.06640625
3
Iteration 23000: Loss = -10864.0654296875
Iteration 23100: Loss = -10864.0673828125
1
Iteration 23200: Loss = -10864.0673828125
2
Iteration 23300: Loss = -10864.0673828125
3
Iteration 23400: Loss = -10864.0673828125
4
Iteration 23500: Loss = -10864.0673828125
5
Iteration 23600: Loss = -10864.0654296875
Iteration 23700: Loss = -10864.06640625
1
Iteration 23800: Loss = -10864.0673828125
2
Iteration 23900: Loss = -10864.0673828125
3
Iteration 24000: Loss = -10864.0673828125
4
Iteration 24100: Loss = -10864.0654296875
Iteration 24200: Loss = -10864.06640625
1
Iteration 24300: Loss = -10864.068359375
2
Iteration 24400: Loss = -10864.06640625
3
Iteration 24500: Loss = -10864.06640625
4
Iteration 24600: Loss = -10864.0673828125
5
Iteration 24700: Loss = -10864.0673828125
6
Iteration 24800: Loss = -10864.06640625
7
Iteration 24900: Loss = -10864.06640625
8
Iteration 25000: Loss = -10864.06640625
9
Iteration 25100: Loss = -10864.0673828125
10
Iteration 25200: Loss = -10864.068359375
11
Iteration 25300: Loss = -10864.0673828125
12
Iteration 25400: Loss = -10864.0654296875
Iteration 25500: Loss = -10864.0673828125
1
Iteration 25600: Loss = -10864.0654296875
Iteration 25700: Loss = -10864.06640625
1
Iteration 25800: Loss = -10864.068359375
2
Iteration 25900: Loss = -10864.0673828125
3
Iteration 26000: Loss = -10864.0673828125
4
Iteration 26100: Loss = -10864.0673828125
5
Iteration 26200: Loss = -10864.06640625
6
Iteration 26300: Loss = -10864.068359375
7
Iteration 26400: Loss = -10864.0673828125
8
Iteration 26500: Loss = -10864.068359375
9
Iteration 26600: Loss = -10864.06640625
10
Iteration 26700: Loss = -10864.0673828125
11
Iteration 26800: Loss = -10864.0673828125
12
Iteration 26900: Loss = -10864.06640625
13
Iteration 27000: Loss = -10864.0693359375
14
Iteration 27100: Loss = -10864.06640625
15
Stopping early at iteration 27100 due to no improvement.
pi: tensor([[0.4063, 0.5937],
        [0.0128, 0.9872]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0618, 0.9382], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3436, 0.0780],
         [0.0539, 0.1594]],

        [[0.0260, 0.1804],
         [0.3276, 0.9701]],

        [[0.3078, 0.1469],
         [0.8249, 0.9797]],

        [[0.0385, 0.2750],
         [0.3110, 0.1473]],

        [[0.6229, 0.2266],
         [0.4176, 0.9216]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.023297732239618434
Global Adjusted Rand Index: 0.0021857731767020395
Average Adjusted Rand Index: 0.00569137952322356
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41821.96484375
Iteration 100: Loss = -23737.125
Iteration 200: Loss = -13543.56640625
Iteration 300: Loss = -11781.005859375
Iteration 400: Loss = -11424.7021484375
Iteration 500: Loss = -11278.876953125
Iteration 600: Loss = -11206.8671875
Iteration 700: Loss = -11146.9599609375
Iteration 800: Loss = -11091.919921875
Iteration 900: Loss = -11026.9873046875
Iteration 1000: Loss = -10988.2470703125
Iteration 1100: Loss = -10966.95703125
Iteration 1200: Loss = -10956.3779296875
Iteration 1300: Loss = -10948.248046875
Iteration 1400: Loss = -10941.0048828125
Iteration 1500: Loss = -10930.7744140625
Iteration 1600: Loss = -10925.9326171875
Iteration 1700: Loss = -10922.732421875
Iteration 1800: Loss = -10920.16015625
Iteration 1900: Loss = -10917.9892578125
Iteration 2000: Loss = -10916.1171875
Iteration 2100: Loss = -10914.4765625
Iteration 2200: Loss = -10913.0087890625
Iteration 2300: Loss = -10901.185546875
Iteration 2400: Loss = -10899.6708984375
Iteration 2500: Loss = -10898.5966796875
Iteration 2600: Loss = -10897.66796875
Iteration 2700: Loss = -10896.818359375
Iteration 2800: Loss = -10895.923828125
Iteration 2900: Loss = -10891.6953125
Iteration 3000: Loss = -10889.8486328125
Iteration 3100: Loss = -10889.1865234375
Iteration 3200: Loss = -10888.6494140625
Iteration 3300: Loss = -10888.17578125
Iteration 3400: Loss = -10887.74609375
Iteration 3500: Loss = -10887.3525390625
Iteration 3600: Loss = -10886.98828125
Iteration 3700: Loss = -10886.6494140625
Iteration 3800: Loss = -10886.3310546875
Iteration 3900: Loss = -10886.0361328125
Iteration 4000: Loss = -10885.7548828125
Iteration 4100: Loss = -10885.48046875
Iteration 4200: Loss = -10878.548828125
Iteration 4300: Loss = -10873.01171875
Iteration 4400: Loss = -10872.408203125
Iteration 4500: Loss = -10872.0791015625
Iteration 4600: Loss = -10871.82421875
Iteration 4700: Loss = -10871.6083984375
Iteration 4800: Loss = -10871.4169921875
Iteration 4900: Loss = -10871.2421875
Iteration 5000: Loss = -10871.0849609375
Iteration 5100: Loss = -10870.9384765625
Iteration 5200: Loss = -10870.8046875
Iteration 5300: Loss = -10870.6767578125
Iteration 5400: Loss = -10870.5595703125
Iteration 5500: Loss = -10870.4521484375
Iteration 5600: Loss = -10870.3486328125
Iteration 5700: Loss = -10870.2529296875
Iteration 5800: Loss = -10870.1630859375
Iteration 5900: Loss = -10870.0810546875
Iteration 6000: Loss = -10870.0
Iteration 6100: Loss = -10869.92578125
Iteration 6200: Loss = -10869.8515625
Iteration 6300: Loss = -10869.7861328125
Iteration 6400: Loss = -10869.7236328125
Iteration 6500: Loss = -10869.6650390625
Iteration 6600: Loss = -10869.6064453125
Iteration 6700: Loss = -10869.5556640625
Iteration 6800: Loss = -10869.5048828125
Iteration 6900: Loss = -10869.4580078125
Iteration 7000: Loss = -10869.4150390625
Iteration 7100: Loss = -10869.3720703125
Iteration 7200: Loss = -10869.3310546875
Iteration 7300: Loss = -10869.2939453125
Iteration 7400: Loss = -10869.2568359375
Iteration 7500: Loss = -10869.22265625
Iteration 7600: Loss = -10869.1904296875
Iteration 7700: Loss = -10869.1611328125
Iteration 7800: Loss = -10869.1318359375
Iteration 7900: Loss = -10869.103515625
Iteration 8000: Loss = -10869.0771484375
Iteration 8100: Loss = -10869.05078125
Iteration 8200: Loss = -10869.02734375
Iteration 8300: Loss = -10869.00390625
Iteration 8400: Loss = -10868.9833984375
Iteration 8500: Loss = -10868.9638671875
Iteration 8600: Loss = -10868.94140625
Iteration 8700: Loss = -10868.9248046875
Iteration 8800: Loss = -10868.90625
Iteration 8900: Loss = -10868.88671875
Iteration 9000: Loss = -10868.873046875
Iteration 9100: Loss = -10868.8583984375
Iteration 9200: Loss = -10868.8427734375
Iteration 9300: Loss = -10868.8291015625
Iteration 9400: Loss = -10868.81640625
Iteration 9500: Loss = -10868.8046875
Iteration 9600: Loss = -10868.7919921875
Iteration 9700: Loss = -10868.779296875
Iteration 9800: Loss = -10868.7685546875
Iteration 9900: Loss = -10868.7587890625
Iteration 10000: Loss = -10868.7490234375
Iteration 10100: Loss = -10868.7392578125
Iteration 10200: Loss = -10868.7294921875
Iteration 10300: Loss = -10868.72265625
Iteration 10400: Loss = -10868.71484375
Iteration 10500: Loss = -10868.7060546875
Iteration 10600: Loss = -10868.701171875
Iteration 10700: Loss = -10868.69140625
Iteration 10800: Loss = -10868.6845703125
Iteration 10900: Loss = -10868.677734375
Iteration 11000: Loss = -10868.6728515625
Iteration 11100: Loss = -10868.6669921875
Iteration 11200: Loss = -10868.6611328125
Iteration 11300: Loss = -10868.6552734375
Iteration 11400: Loss = -10868.650390625
Iteration 11500: Loss = -10868.646484375
Iteration 11600: Loss = -10868.640625
Iteration 11700: Loss = -10868.63671875
Iteration 11800: Loss = -10868.6318359375
Iteration 11900: Loss = -10868.626953125
Iteration 12000: Loss = -10868.6240234375
Iteration 12100: Loss = -10868.6201171875
Iteration 12200: Loss = -10868.6162109375
Iteration 12300: Loss = -10868.61328125
Iteration 12400: Loss = -10868.6083984375
Iteration 12500: Loss = -10868.6044921875
Iteration 12600: Loss = -10868.5986328125
Iteration 12700: Loss = -10868.5927734375
Iteration 12800: Loss = -10868.580078125
Iteration 12900: Loss = -10868.544921875
Iteration 13000: Loss = -10868.44140625
Iteration 13100: Loss = -10867.419921875
Iteration 13200: Loss = -10867.357421875
Iteration 13300: Loss = -10867.2939453125
Iteration 13400: Loss = -10867.2666015625
Iteration 13500: Loss = -10867.240234375
Iteration 13600: Loss = -10867.2099609375
Iteration 13700: Loss = -10867.1826171875
Iteration 13800: Loss = -10867.1552734375
Iteration 13900: Loss = -10867.1328125
Iteration 14000: Loss = -10867.109375
Iteration 14100: Loss = -10867.0869140625
Iteration 14200: Loss = -10867.06640625
Iteration 14300: Loss = -10867.044921875
Iteration 14400: Loss = -10867.01171875
Iteration 14500: Loss = -10866.158203125
Iteration 14600: Loss = -10866.0927734375
Iteration 14700: Loss = -10866.0908203125
Iteration 14800: Loss = -10866.0869140625
Iteration 14900: Loss = -10866.0859375
Iteration 15000: Loss = -10866.083984375
Iteration 15100: Loss = -10866.080078125
Iteration 15200: Loss = -10866.080078125
Iteration 15300: Loss = -10866.0771484375
Iteration 15400: Loss = -10866.076171875
Iteration 15500: Loss = -10866.07421875
Iteration 15600: Loss = -10866.0732421875
Iteration 15700: Loss = -10866.07421875
1
Iteration 15800: Loss = -10866.0732421875
Iteration 15900: Loss = -10866.0712890625
Iteration 16000: Loss = -10866.0712890625
Iteration 16100: Loss = -10866.0712890625
Iteration 16200: Loss = -10866.0693359375
Iteration 16300: Loss = -10866.0693359375
Iteration 16400: Loss = -10866.0693359375
Iteration 16500: Loss = -10866.068359375
Iteration 16600: Loss = -10866.068359375
Iteration 16700: Loss = -10866.0673828125
Iteration 16800: Loss = -10866.0693359375
1
Iteration 16900: Loss = -10866.0654296875
Iteration 17000: Loss = -10866.064453125
Iteration 17100: Loss = -10866.0595703125
Iteration 17200: Loss = -10866.0595703125
Iteration 17300: Loss = -10866.0576171875
Iteration 17400: Loss = -10866.05859375
1
Iteration 17500: Loss = -10866.056640625
Iteration 17600: Loss = -10866.05859375
1
Iteration 17700: Loss = -10866.05859375
2
Iteration 17800: Loss = -10866.056640625
Iteration 17900: Loss = -10866.05859375
1
Iteration 18000: Loss = -10866.0556640625
Iteration 18100: Loss = -10866.056640625
1
Iteration 18200: Loss = -10866.056640625
2
Iteration 18300: Loss = -10866.0556640625
Iteration 18400: Loss = -10866.0556640625
Iteration 18500: Loss = -10866.0556640625
Iteration 18600: Loss = -10866.0556640625
Iteration 18700: Loss = -10866.0556640625
Iteration 18800: Loss = -10866.0546875
Iteration 18900: Loss = -10866.0556640625
1
Iteration 19000: Loss = -10866.0556640625
2
Iteration 19100: Loss = -10866.0537109375
Iteration 19200: Loss = -10866.0556640625
1
Iteration 19300: Loss = -10866.0556640625
2
Iteration 19400: Loss = -10866.0546875
3
Iteration 19500: Loss = -10866.0546875
4
Iteration 19600: Loss = -10866.056640625
5
Iteration 19700: Loss = -10866.0537109375
Iteration 19800: Loss = -10866.0537109375
Iteration 19900: Loss = -10866.052734375
Iteration 20000: Loss = -10866.0556640625
1
Iteration 20100: Loss = -10866.052734375
Iteration 20200: Loss = -10866.0537109375
1
Iteration 20300: Loss = -10866.0537109375
2
Iteration 20400: Loss = -10866.0537109375
3
Iteration 20500: Loss = -10866.0537109375
4
Iteration 20600: Loss = -10866.0537109375
5
Iteration 20700: Loss = -10866.0537109375
6
Iteration 20800: Loss = -10866.0546875
7
Iteration 20900: Loss = -10866.052734375
Iteration 21000: Loss = -10866.0537109375
1
Iteration 21100: Loss = -10866.052734375
Iteration 21200: Loss = -10866.052734375
Iteration 21300: Loss = -10866.052734375
Iteration 21400: Loss = -10866.0537109375
1
Iteration 21500: Loss = -10866.052734375
Iteration 21600: Loss = -10866.052734375
Iteration 21700: Loss = -10866.052734375
Iteration 21800: Loss = -10866.0537109375
1
Iteration 21900: Loss = -10866.0517578125
Iteration 22000: Loss = -10866.052734375
1
Iteration 22100: Loss = -10866.0537109375
2
Iteration 22200: Loss = -10866.052734375
3
Iteration 22300: Loss = -10866.052734375
4
Iteration 22400: Loss = -10866.052734375
5
Iteration 22500: Loss = -10866.052734375
6
Iteration 22600: Loss = -10866.052734375
7
Iteration 22700: Loss = -10866.0517578125
Iteration 22800: Loss = -10866.052734375
1
Iteration 22900: Loss = -10866.052734375
2
Iteration 23000: Loss = -10866.052734375
3
Iteration 23100: Loss = -10866.052734375
4
Iteration 23200: Loss = -10866.0537109375
5
Iteration 23300: Loss = -10866.0517578125
Iteration 23400: Loss = -10866.0517578125
Iteration 23500: Loss = -10866.0517578125
Iteration 23600: Loss = -10866.0517578125
Iteration 23700: Loss = -10866.052734375
1
Iteration 23800: Loss = -10866.05078125
Iteration 23900: Loss = -10866.052734375
1
Iteration 24000: Loss = -10866.0537109375
2
Iteration 24100: Loss = -10866.0517578125
3
Iteration 24200: Loss = -10866.052734375
4
Iteration 24300: Loss = -10866.0517578125
5
Iteration 24400: Loss = -10866.0517578125
6
Iteration 24500: Loss = -10866.052734375
7
Iteration 24600: Loss = -10866.0517578125
8
Iteration 24700: Loss = -10866.0517578125
9
Iteration 24800: Loss = -10866.0537109375
10
Iteration 24900: Loss = -10866.052734375
11
Iteration 25000: Loss = -10866.052734375
12
Iteration 25100: Loss = -10866.0537109375
13
Iteration 25200: Loss = -10866.052734375
14
Iteration 25300: Loss = -10866.0517578125
15
Stopping early at iteration 25300 due to no improvement.
pi: tensor([[2.4896e-02, 9.7510e-01],
        [8.9989e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0599, 0.9401], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4873, 0.0763],
         [0.0241, 0.1614]],

        [[0.0112, 0.1824],
         [0.1232, 0.0272]],

        [[0.9755, 0.1615],
         [0.0095, 0.0287]],

        [[0.0653, 0.1552],
         [0.8772, 0.1566]],

        [[0.2698, 0.2159],
         [0.2640, 0.0078]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011392109569709318
Average Adjusted Rand Index: 0.00020299801231112943
[0.0021857731767020395, -0.0011392109569709318] [0.00569137952322356, 0.00020299801231112943] [10864.06640625, 10866.0517578125]
-------------------------------------
This iteration is 96
True Objective function: Loss = -10986.450595959708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29756.419921875
Iteration 100: Loss = -21009.0
Iteration 200: Loss = -13657.8974609375
Iteration 300: Loss = -11823.4248046875
Iteration 400: Loss = -11493.892578125
Iteration 500: Loss = -11380.73046875
Iteration 600: Loss = -11321.9052734375
Iteration 700: Loss = -11284.685546875
Iteration 800: Loss = -11256.4228515625
Iteration 900: Loss = -11234.576171875
Iteration 1000: Loss = -11213.669921875
Iteration 1100: Loss = -11196.7763671875
Iteration 1200: Loss = -11182.6611328125
Iteration 1300: Loss = -11170.701171875
Iteration 1400: Loss = -11161.3623046875
Iteration 1500: Loss = -11153.517578125
Iteration 1600: Loss = -11145.99609375
Iteration 1700: Loss = -11137.8193359375
Iteration 1800: Loss = -11131.7099609375
Iteration 1900: Loss = -11128.3427734375
Iteration 2000: Loss = -11120.7578125
Iteration 2100: Loss = -11115.6962890625
Iteration 2200: Loss = -11113.4599609375
Iteration 2300: Loss = -11111.8017578125
Iteration 2400: Loss = -11110.4375
Iteration 2500: Loss = -11109.2822265625
Iteration 2600: Loss = -11108.2470703125
Iteration 2700: Loss = -11105.7578125
Iteration 2800: Loss = -11104.072265625
Iteration 2900: Loss = -11103.2880859375
Iteration 3000: Loss = -11102.6591796875
Iteration 3100: Loss = -11102.0458984375
Iteration 3200: Loss = -11099.955078125
Iteration 3300: Loss = -11098.6005859375
Iteration 3400: Loss = -11097.986328125
Iteration 3500: Loss = -11097.525390625
Iteration 3600: Loss = -11097.1455078125
Iteration 3700: Loss = -11096.8134765625
Iteration 3800: Loss = -11096.521484375
Iteration 3900: Loss = -11096.259765625
Iteration 4000: Loss = -11096.0166015625
Iteration 4100: Loss = -11095.7958984375
Iteration 4200: Loss = -11095.5908203125
Iteration 4300: Loss = -11095.3935546875
Iteration 4400: Loss = -11095.1943359375
Iteration 4500: Loss = -11094.9765625
Iteration 4600: Loss = -11094.7080078125
Iteration 4700: Loss = -11094.3896484375
Iteration 4800: Loss = -11094.1259765625
Iteration 4900: Loss = -11093.9375
Iteration 5000: Loss = -11093.783203125
Iteration 5100: Loss = -11093.65234375
Iteration 5200: Loss = -11093.5380859375
Iteration 5300: Loss = -11093.4326171875
Iteration 5400: Loss = -11093.3349609375
Iteration 5500: Loss = -11093.2529296875
Iteration 5600: Loss = -11093.1796875
Iteration 5700: Loss = -11092.3984375
Iteration 5800: Loss = -11092.3349609375
Iteration 5900: Loss = -11092.2802734375
Iteration 6000: Loss = -11092.2294921875
Iteration 6100: Loss = -11092.1806640625
Iteration 6200: Loss = -11092.13671875
Iteration 6300: Loss = -11092.0947265625
Iteration 6400: Loss = -11092.0537109375
Iteration 6500: Loss = -11092.0166015625
Iteration 6600: Loss = -11091.98046875
Iteration 6700: Loss = -11091.947265625
Iteration 6800: Loss = -11091.912109375
Iteration 6900: Loss = -11091.0322265625
Iteration 7000: Loss = -11090.908203125
Iteration 7100: Loss = -11090.806640625
Iteration 7200: Loss = -11090.7138671875
Iteration 7300: Loss = -11090.6396484375
Iteration 7400: Loss = -11090.587890625
Iteration 7500: Loss = -11090.55078125
Iteration 7600: Loss = -11090.5244140625
Iteration 7700: Loss = -11090.5009765625
Iteration 7800: Loss = -11090.4814453125
Iteration 7900: Loss = -11090.46484375
Iteration 8000: Loss = -11090.44921875
Iteration 8100: Loss = -11090.4345703125
Iteration 8200: Loss = -11090.421875
Iteration 8300: Loss = -11090.408203125
Iteration 8400: Loss = -11090.3955078125
Iteration 8500: Loss = -11090.3837890625
Iteration 8600: Loss = -11090.373046875
Iteration 8700: Loss = -11090.36328125
Iteration 8800: Loss = -11090.3525390625
Iteration 8900: Loss = -11090.3427734375
Iteration 9000: Loss = -11090.333984375
Iteration 9100: Loss = -11090.3251953125
Iteration 9200: Loss = -11090.3173828125
Iteration 9300: Loss = -11090.30859375
Iteration 9400: Loss = -11090.302734375
Iteration 9500: Loss = -11090.2958984375
Iteration 9600: Loss = -11090.2900390625
Iteration 9700: Loss = -11090.2841796875
Iteration 9800: Loss = -11090.2763671875
Iteration 9900: Loss = -11090.271484375
Iteration 10000: Loss = -11090.265625
Iteration 10100: Loss = -11090.2607421875
Iteration 10200: Loss = -11090.2568359375
Iteration 10300: Loss = -11090.251953125
Iteration 10400: Loss = -11090.24609375
Iteration 10500: Loss = -11090.2421875
Iteration 10600: Loss = -11090.23828125
Iteration 10700: Loss = -11090.2353515625
Iteration 10800: Loss = -11090.232421875
Iteration 10900: Loss = -11090.2275390625
Iteration 11000: Loss = -11090.224609375
Iteration 11100: Loss = -11090.2216796875
Iteration 11200: Loss = -11090.2177734375
Iteration 11300: Loss = -11090.2158203125
Iteration 11400: Loss = -11090.212890625
Iteration 11500: Loss = -11090.2099609375
Iteration 11600: Loss = -11090.2080078125
Iteration 11700: Loss = -11090.20703125
Iteration 11800: Loss = -11090.203125
Iteration 11900: Loss = -11090.2001953125
Iteration 12000: Loss = -11090.19921875
Iteration 12100: Loss = -11090.1962890625
Iteration 12200: Loss = -11090.1943359375
Iteration 12300: Loss = -11090.193359375
Iteration 12400: Loss = -11090.1923828125
Iteration 12500: Loss = -11090.1904296875
Iteration 12600: Loss = -11090.1884765625
Iteration 12700: Loss = -11090.185546875
Iteration 12800: Loss = -11090.18359375
Iteration 12900: Loss = -11090.1845703125
1
Iteration 13000: Loss = -11090.1826171875
Iteration 13100: Loss = -11090.181640625
Iteration 13200: Loss = -11090.1796875
Iteration 13300: Loss = -11090.177734375
Iteration 13400: Loss = -11090.177734375
Iteration 13500: Loss = -11090.17578125
Iteration 13600: Loss = -11090.17578125
Iteration 13700: Loss = -11090.1748046875
Iteration 13800: Loss = -11090.173828125
Iteration 13900: Loss = -11090.1728515625
Iteration 14000: Loss = -11090.1728515625
Iteration 14100: Loss = -11090.1708984375
Iteration 14200: Loss = -11090.169921875
Iteration 14300: Loss = -11090.1708984375
1
Iteration 14400: Loss = -11090.169921875
Iteration 14500: Loss = -11090.16796875
Iteration 14600: Loss = -11090.1728515625
1
Iteration 14700: Loss = -11090.16796875
Iteration 14800: Loss = -11090.1669921875
Iteration 14900: Loss = -11090.166015625
Iteration 15000: Loss = -11090.166015625
Iteration 15100: Loss = -11090.1669921875
1
Iteration 15200: Loss = -11090.1650390625
Iteration 15300: Loss = -11090.166015625
1
Iteration 15400: Loss = -11090.1650390625
Iteration 15500: Loss = -11090.1640625
Iteration 15600: Loss = -11090.1630859375
Iteration 15700: Loss = -11090.134765625
Iteration 15800: Loss = -11089.875
Iteration 15900: Loss = -11089.8759765625
1
Iteration 16000: Loss = -11089.875
Iteration 16100: Loss = -11089.8740234375
Iteration 16200: Loss = -11089.8720703125
Iteration 16300: Loss = -11089.873046875
1
Iteration 16400: Loss = -11089.8720703125
Iteration 16500: Loss = -11089.8720703125
Iteration 16600: Loss = -11089.8740234375
1
Iteration 16700: Loss = -11089.87109375
Iteration 16800: Loss = -11089.8720703125
1
Iteration 16900: Loss = -11089.8720703125
2
Iteration 17000: Loss = -11089.87109375
Iteration 17100: Loss = -11089.8720703125
1
Iteration 17200: Loss = -11089.873046875
2
Iteration 17300: Loss = -11089.8720703125
3
Iteration 17400: Loss = -11089.8740234375
4
Iteration 17500: Loss = -11089.87109375
Iteration 17600: Loss = -11089.87109375
Iteration 17700: Loss = -11089.8701171875
Iteration 17800: Loss = -11089.8701171875
Iteration 17900: Loss = -11089.8701171875
Iteration 18000: Loss = -11089.8701171875
Iteration 18100: Loss = -11089.8701171875
Iteration 18200: Loss = -11089.8701171875
Iteration 18300: Loss = -11089.8701171875
Iteration 18400: Loss = -11089.8701171875
Iteration 18500: Loss = -11089.8701171875
Iteration 18600: Loss = -11089.87109375
1
Iteration 18700: Loss = -11089.8701171875
Iteration 18800: Loss = -11089.8701171875
Iteration 18900: Loss = -11089.8701171875
Iteration 19000: Loss = -11089.8701171875
Iteration 19100: Loss = -11089.869140625
Iteration 19200: Loss = -11089.8701171875
1
Iteration 19300: Loss = -11089.8681640625
Iteration 19400: Loss = -11089.8671875
Iteration 19500: Loss = -11089.87109375
1
Iteration 19600: Loss = -11089.8681640625
2
Iteration 19700: Loss = -11089.8662109375
Iteration 19800: Loss = -11089.8671875
1
Iteration 19900: Loss = -11089.8681640625
2
Iteration 20000: Loss = -11089.8681640625
3
Iteration 20100: Loss = -11089.8681640625
4
Iteration 20200: Loss = -11089.8681640625
5
Iteration 20300: Loss = -11089.8671875
6
Iteration 20400: Loss = -11089.8671875
7
Iteration 20500: Loss = -11089.8662109375
Iteration 20600: Loss = -11089.8671875
1
Iteration 20700: Loss = -11089.8671875
2
Iteration 20800: Loss = -11089.8671875
3
Iteration 20900: Loss = -11089.8662109375
Iteration 21000: Loss = -11089.8662109375
Iteration 21100: Loss = -11089.8671875
1
Iteration 21200: Loss = -11089.8662109375
Iteration 21300: Loss = -11089.8662109375
Iteration 21400: Loss = -11089.8662109375
Iteration 21500: Loss = -11089.8671875
1
Iteration 21600: Loss = -11089.8662109375
Iteration 21700: Loss = -11089.8671875
1
Iteration 21800: Loss = -11089.8671875
2
Iteration 21900: Loss = -11089.8662109375
Iteration 22000: Loss = -11089.8662109375
Iteration 22100: Loss = -11089.8662109375
Iteration 22200: Loss = -11089.8662109375
Iteration 22300: Loss = -11089.8662109375
Iteration 22400: Loss = -11089.87109375
1
Iteration 22500: Loss = -11089.8662109375
Iteration 22600: Loss = -11089.8671875
1
Iteration 22700: Loss = -11089.8671875
2
Iteration 22800: Loss = -11089.8671875
3
Iteration 22900: Loss = -11089.8662109375
Iteration 23000: Loss = -11089.8662109375
Iteration 23100: Loss = -11089.8671875
1
Iteration 23200: Loss = -11089.8671875
2
Iteration 23300: Loss = -11089.8671875
3
Iteration 23400: Loss = -11089.865234375
Iteration 23500: Loss = -11089.8671875
1
Iteration 23600: Loss = -11089.8671875
2
Iteration 23700: Loss = -11089.8671875
3
Iteration 23800: Loss = -11089.8662109375
4
Iteration 23900: Loss = -11089.865234375
Iteration 24000: Loss = -11089.8662109375
1
Iteration 24100: Loss = -11089.8671875
2
Iteration 24200: Loss = -11089.8671875
3
Iteration 24300: Loss = -11089.865234375
Iteration 24400: Loss = -11089.8671875
1
Iteration 24500: Loss = -11089.8662109375
2
Iteration 24600: Loss = -11089.8662109375
3
Iteration 24700: Loss = -11089.8662109375
4
Iteration 24800: Loss = -11089.8671875
5
Iteration 24900: Loss = -11089.8671875
6
Iteration 25000: Loss = -11089.8701171875
7
Iteration 25100: Loss = -11089.8662109375
8
Iteration 25200: Loss = -11089.8671875
9
Iteration 25300: Loss = -11089.8671875
10
Iteration 25400: Loss = -11089.865234375
Iteration 25500: Loss = -11089.8662109375
1
Iteration 25600: Loss = -11089.8662109375
2
Iteration 25700: Loss = -11089.8662109375
3
Iteration 25800: Loss = -11089.8662109375
4
Iteration 25900: Loss = -11089.8662109375
5
Iteration 26000: Loss = -11089.8662109375
6
Iteration 26100: Loss = -11089.8662109375
7
Iteration 26200: Loss = -11089.8662109375
8
Iteration 26300: Loss = -11089.865234375
Iteration 26400: Loss = -11089.8662109375
1
Iteration 26500: Loss = -11089.8671875
2
Iteration 26600: Loss = -11089.865234375
Iteration 26700: Loss = -11089.8671875
1
Iteration 26800: Loss = -11089.865234375
Iteration 26900: Loss = -11089.8662109375
1
Iteration 27000: Loss = -11089.8671875
2
Iteration 27100: Loss = -11089.8671875
3
Iteration 27200: Loss = -11089.8662109375
4
Iteration 27300: Loss = -11089.8662109375
5
Iteration 27400: Loss = -11089.8662109375
6
Iteration 27500: Loss = -11089.8671875
7
Iteration 27600: Loss = -11089.8662109375
8
Iteration 27700: Loss = -11089.8671875
9
Iteration 27800: Loss = -11089.8662109375
10
Iteration 27900: Loss = -11089.8662109375
11
Iteration 28000: Loss = -11089.8671875
12
Iteration 28100: Loss = -11089.8671875
13
Iteration 28200: Loss = -11089.8671875
14
Iteration 28300: Loss = -11089.8662109375
15
Stopping early at iteration 28300 due to no improvement.
pi: tensor([[1.0000e+00, 3.2797e-06],
        [9.4007e-01, 5.9928e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.5848e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1632, 0.1717],
         [0.7365, 0.1644]],

        [[0.0867, 0.2341],
         [0.1840, 0.9891]],

        [[0.2988, 0.2412],
         [0.1021, 0.6534]],

        [[0.0071, 0.2168],
         [0.8683, 0.9788]],

        [[0.9512, 0.5746],
         [0.1689, 0.4586]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000145713110828393
Average Adjusted Rand Index: -0.0018018988527126576
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29183.509765625
Iteration 100: Loss = -20083.869140625
Iteration 200: Loss = -13922.3671875
Iteration 300: Loss = -12356.7109375
Iteration 400: Loss = -11609.1669921875
Iteration 500: Loss = -11383.615234375
Iteration 600: Loss = -11322.0087890625
Iteration 700: Loss = -11285.3173828125
Iteration 800: Loss = -11259.197265625
Iteration 900: Loss = -11237.5751953125
Iteration 1000: Loss = -11223.3671875
Iteration 1100: Loss = -11209.884765625
Iteration 1200: Loss = -11199.6240234375
Iteration 1300: Loss = -11189.7275390625
Iteration 1400: Loss = -11184.0498046875
Iteration 1500: Loss = -11177.75
Iteration 1600: Loss = -11171.5634765625
Iteration 1700: Loss = -11166.6953125
Iteration 1800: Loss = -11159.55078125
Iteration 1900: Loss = -11151.76953125
Iteration 2000: Loss = -11144.9599609375
Iteration 2100: Loss = -11139.9814453125
Iteration 2200: Loss = -11135.5048828125
Iteration 2300: Loss = -11131.5302734375
Iteration 2400: Loss = -11128.484375
Iteration 2500: Loss = -11126.6865234375
Iteration 2600: Loss = -11125.36328125
Iteration 2700: Loss = -11124.306640625
Iteration 2800: Loss = -11123.3876953125
Iteration 2900: Loss = -11122.470703125
Iteration 3000: Loss = -11121.33203125
Iteration 3100: Loss = -11119.1298828125
Iteration 3200: Loss = -11117.8173828125
Iteration 3300: Loss = -11116.6318359375
Iteration 3400: Loss = -11114.7734375
Iteration 3500: Loss = -11113.87109375
Iteration 3600: Loss = -11113.357421875
Iteration 3700: Loss = -11112.9521484375
Iteration 3800: Loss = -11112.50390625
Iteration 3900: Loss = -11112.177734375
Iteration 4000: Loss = -11111.93359375
Iteration 4100: Loss = -11111.7265625
Iteration 4200: Loss = -11111.537109375
Iteration 4300: Loss = -11111.3359375
Iteration 4400: Loss = -11111.15234375
Iteration 4500: Loss = -11110.95703125
Iteration 4600: Loss = -11110.79296875
Iteration 4700: Loss = -11110.6552734375
Iteration 4800: Loss = -11110.51953125
Iteration 4900: Loss = -11109.9892578125
Iteration 5000: Loss = -11109.82421875
Iteration 5100: Loss = -11109.71484375
Iteration 5200: Loss = -11109.6123046875
Iteration 5300: Loss = -11109.5078125
Iteration 5400: Loss = -11109.373046875
Iteration 5500: Loss = -11109.2197265625
Iteration 5600: Loss = -11109.130859375
Iteration 5700: Loss = -11109.0439453125
Iteration 5800: Loss = -11108.927734375
Iteration 5900: Loss = -11108.5517578125
Iteration 6000: Loss = -11107.8173828125
Iteration 6100: Loss = -11107.708984375
Iteration 6200: Loss = -11106.6572265625
Iteration 6300: Loss = -11106.5888671875
Iteration 6400: Loss = -11106.5263671875
Iteration 6500: Loss = -11106.4228515625
Iteration 6600: Loss = -11106.3056640625
Iteration 6700: Loss = -11106.2607421875
Iteration 6800: Loss = -11106.2236328125
Iteration 6900: Loss = -11106.1865234375
Iteration 7000: Loss = -11106.15234375
Iteration 7100: Loss = -11106.1181640625
Iteration 7200: Loss = -11106.087890625
Iteration 7300: Loss = -11106.05859375
Iteration 7400: Loss = -11106.025390625
Iteration 7500: Loss = -11105.8701171875
Iteration 7600: Loss = -11105.66796875
Iteration 7700: Loss = -11105.6298828125
Iteration 7800: Loss = -11105.58203125
Iteration 7900: Loss = -11105.4580078125
Iteration 8000: Loss = -11104.908203125
Iteration 8100: Loss = -11103.6552734375
Iteration 8200: Loss = -11102.6279296875
Iteration 8300: Loss = -11102.1357421875
Iteration 8400: Loss = -11101.8564453125
Iteration 8500: Loss = -11101.7080078125
Iteration 8600: Loss = -11101.5244140625
Iteration 8700: Loss = -11101.4658203125
Iteration 8800: Loss = -11101.400390625
Iteration 8900: Loss = -11101.224609375
Iteration 9000: Loss = -11101.1455078125
Iteration 9100: Loss = -11101.1328125
Iteration 9200: Loss = -11101.1181640625
Iteration 9300: Loss = -11100.9794921875
Iteration 9400: Loss = -11100.8232421875
Iteration 9500: Loss = -11100.7919921875
Iteration 9600: Loss = -11100.744140625
Iteration 9700: Loss = -11100.5615234375
Iteration 9800: Loss = -11100.365234375
Iteration 9900: Loss = -11100.3349609375
Iteration 10000: Loss = -11100.3154296875
Iteration 10100: Loss = -11100.2578125
Iteration 10200: Loss = -11099.9990234375
Iteration 10300: Loss = -11099.908203125
Iteration 10400: Loss = -11099.888671875
Iteration 10500: Loss = -11099.8583984375
Iteration 10600: Loss = -11099.796875
Iteration 10700: Loss = -11099.7392578125
Iteration 10800: Loss = -11099.7177734375
Iteration 10900: Loss = -11099.7021484375
Iteration 11000: Loss = -11099.6591796875
Iteration 11100: Loss = -11099.615234375
Iteration 11200: Loss = -11099.5908203125
Iteration 11300: Loss = -11099.58203125
Iteration 11400: Loss = -11099.576171875
Iteration 11500: Loss = -11099.5693359375
Iteration 11600: Loss = -11099.5634765625
Iteration 11700: Loss = -11099.5576171875
Iteration 11800: Loss = -11099.5458984375
Iteration 11900: Loss = -11099.529296875
Iteration 12000: Loss = -11099.5009765625
Iteration 12100: Loss = -11099.4814453125
Iteration 12200: Loss = -11099.4677734375
Iteration 12300: Loss = -11099.4580078125
Iteration 12400: Loss = -11099.4501953125
Iteration 12500: Loss = -11099.44140625
Iteration 12600: Loss = -11099.4365234375
Iteration 12700: Loss = -11099.431640625
Iteration 12800: Loss = -11099.4287109375
Iteration 12900: Loss = -11099.423828125
Iteration 13000: Loss = -11099.423828125
Iteration 13100: Loss = -11099.4189453125
Iteration 13200: Loss = -11099.4169921875
Iteration 13300: Loss = -11099.416015625
Iteration 13400: Loss = -11099.412109375
Iteration 13500: Loss = -11099.412109375
Iteration 13600: Loss = -11099.4091796875
Iteration 13700: Loss = -11099.4072265625
Iteration 13800: Loss = -11099.404296875
Iteration 13900: Loss = -11099.4033203125
Iteration 14000: Loss = -11099.39453125
Iteration 14100: Loss = -11099.3876953125
Iteration 14200: Loss = -11099.38671875
Iteration 14300: Loss = -11099.3857421875
Iteration 14400: Loss = -11099.3837890625
Iteration 14500: Loss = -11099.3837890625
Iteration 14600: Loss = -11099.3818359375
Iteration 14700: Loss = -11099.3818359375
Iteration 14800: Loss = -11099.3818359375
Iteration 14900: Loss = -11099.3818359375
Iteration 15000: Loss = -11099.3798828125
Iteration 15100: Loss = -11098.2021484375
Iteration 15200: Loss = -11098.197265625
Iteration 15300: Loss = -11098.1962890625
Iteration 15400: Loss = -11098.1953125
Iteration 15500: Loss = -11098.1953125
Iteration 15600: Loss = -11098.1953125
Iteration 15700: Loss = -11098.1943359375
Iteration 15800: Loss = -11098.193359375
Iteration 15900: Loss = -11098.1943359375
1
Iteration 16000: Loss = -11098.193359375
Iteration 16100: Loss = -11098.193359375
Iteration 16200: Loss = -11098.1923828125
Iteration 16300: Loss = -11098.193359375
1
Iteration 16400: Loss = -11098.1943359375
2
Iteration 16500: Loss = -11098.193359375
3
Iteration 16600: Loss = -11098.1923828125
Iteration 16700: Loss = -11098.19140625
Iteration 16800: Loss = -11098.1923828125
1
Iteration 16900: Loss = -11098.1591796875
Iteration 17000: Loss = -11096.953125
Iteration 17100: Loss = -11096.939453125
Iteration 17200: Loss = -11096.9384765625
Iteration 17300: Loss = -11096.9384765625
Iteration 17400: Loss = -11096.9375
Iteration 17500: Loss = -11096.9375
Iteration 17600: Loss = -11096.9375
Iteration 17700: Loss = -11096.6064453125
Iteration 17800: Loss = -11096.0078125
Iteration 17900: Loss = -11095.939453125
Iteration 18000: Loss = -11095.0712890625
Iteration 18100: Loss = -11095.0712890625
Iteration 18200: Loss = -11093.9755859375
Iteration 18300: Loss = -11093.9619140625
Iteration 18400: Loss = -11093.962890625
1
Iteration 18500: Loss = -11093.962890625
2
Iteration 18600: Loss = -11093.9619140625
Iteration 18700: Loss = -11093.9619140625
Iteration 18800: Loss = -11093.9638671875
1
Iteration 18900: Loss = -11093.962890625
2
Iteration 19000: Loss = -11093.962890625
3
Iteration 19100: Loss = -11092.291015625
Iteration 19200: Loss = -11092.275390625
Iteration 19300: Loss = -11092.2734375
Iteration 19400: Loss = -11092.2734375
Iteration 19500: Loss = -11092.275390625
1
Iteration 19600: Loss = -11092.2734375
Iteration 19700: Loss = -11092.2744140625
1
Iteration 19800: Loss = -11092.2744140625
2
Iteration 19900: Loss = -11092.2724609375
Iteration 20000: Loss = -11092.2744140625
1
Iteration 20100: Loss = -11092.2744140625
2
Iteration 20200: Loss = -11092.2734375
3
Iteration 20300: Loss = -11092.2734375
4
Iteration 20400: Loss = -11090.49609375
Iteration 20500: Loss = -11090.486328125
Iteration 20600: Loss = -11090.4853515625
Iteration 20700: Loss = -11090.486328125
1
Iteration 20800: Loss = -11090.486328125
2
Iteration 20900: Loss = -11090.4873046875
3
Iteration 21000: Loss = -11090.486328125
4
Iteration 21100: Loss = -11090.4853515625
Iteration 21200: Loss = -11090.4853515625
Iteration 21300: Loss = -11090.486328125
1
Iteration 21400: Loss = -11090.4873046875
2
Iteration 21500: Loss = -11090.486328125
3
Iteration 21600: Loss = -11090.486328125
4
Iteration 21700: Loss = -11090.4873046875
5
Iteration 21800: Loss = -11090.486328125
6
Iteration 21900: Loss = -11090.4853515625
Iteration 22000: Loss = -11090.4853515625
Iteration 22100: Loss = -11090.486328125
1
Iteration 22200: Loss = -11090.4853515625
Iteration 22300: Loss = -11090.486328125
1
Iteration 22400: Loss = -11090.4873046875
2
Iteration 22500: Loss = -11090.4873046875
3
Iteration 22600: Loss = -11090.48828125
4
Iteration 22700: Loss = -11090.486328125
5
Iteration 22800: Loss = -11090.486328125
6
Iteration 22900: Loss = -11090.486328125
7
Iteration 23000: Loss = -11090.4873046875
8
Iteration 23100: Loss = -11090.486328125
9
Iteration 23200: Loss = -11090.4873046875
10
Iteration 23300: Loss = -11090.4853515625
Iteration 23400: Loss = -11090.48828125
1
Iteration 23500: Loss = -11090.486328125
2
Iteration 23600: Loss = -11090.4853515625
Iteration 23700: Loss = -11090.486328125
1
Iteration 23800: Loss = -11090.486328125
2
Iteration 23900: Loss = -11090.486328125
3
Iteration 24000: Loss = -11090.486328125
4
Iteration 24100: Loss = -11090.486328125
5
Iteration 24200: Loss = -11090.486328125
6
Iteration 24300: Loss = -11090.486328125
7
Iteration 24400: Loss = -11090.486328125
8
Iteration 24500: Loss = -11090.486328125
9
Iteration 24600: Loss = -11090.486328125
10
Iteration 24700: Loss = -11090.486328125
11
Iteration 24800: Loss = -11090.486328125
12
Iteration 24900: Loss = -11090.486328125
13
Iteration 25000: Loss = -11090.4853515625
Iteration 25100: Loss = -11090.486328125
1
Iteration 25200: Loss = -11090.486328125
2
Iteration 25300: Loss = -11090.486328125
3
Iteration 25400: Loss = -11090.486328125
4
Iteration 25500: Loss = -11090.486328125
5
Iteration 25600: Loss = -11090.48828125
6
Iteration 25700: Loss = -11090.486328125
7
Iteration 25800: Loss = -11090.48828125
8
Iteration 25900: Loss = -11090.4873046875
9
Iteration 26000: Loss = -11090.4833984375
Iteration 26100: Loss = -11089.4873046875
Iteration 26200: Loss = -11089.4873046875
Iteration 26300: Loss = -11089.4873046875
Iteration 26400: Loss = -11089.4892578125
1
Iteration 26500: Loss = -11089.48828125
2
Iteration 26600: Loss = -11089.48828125
3
Iteration 26700: Loss = -11089.4873046875
Iteration 26800: Loss = -11089.4873046875
Iteration 26900: Loss = -11087.8583984375
Iteration 27000: Loss = -11087.71484375
Iteration 27100: Loss = -11087.71484375
Iteration 27200: Loss = -11087.712890625
Iteration 27300: Loss = -11087.71484375
1
Iteration 27400: Loss = -11087.7138671875
2
Iteration 27500: Loss = -11087.71484375
3
Iteration 27600: Loss = -11087.7138671875
4
Iteration 27700: Loss = -11087.71484375
5
Iteration 27800: Loss = -11087.7138671875
6
Iteration 27900: Loss = -11087.7138671875
7
Iteration 28000: Loss = -11087.716796875
8
Iteration 28100: Loss = -11087.7138671875
9
Iteration 28200: Loss = -11087.71484375
10
Iteration 28300: Loss = -11087.7138671875
11
Iteration 28400: Loss = -11087.71484375
12
Iteration 28500: Loss = -11087.7138671875
13
Iteration 28600: Loss = -11087.71484375
14
Iteration 28700: Loss = -11087.7138671875
15
Stopping early at iteration 28700 due to no improvement.
pi: tensor([[6.3273e-02, 9.3673e-01],
        [1.4453e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1669, 0.0428],
         [0.1214, 0.1631]],

        [[0.0348, 0.2332],
         [0.0714, 0.5047]],

        [[0.7620, 0.2414],
         [0.1668, 0.9709]],

        [[0.9929, 0.2177],
         [0.0133, 0.9929]],

        [[0.8899, 0.2380],
         [0.9915, 0.9480]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00026295296676737796
Average Adjusted Rand Index: -0.0018018988527126576
[0.000145713110828393, -0.00026295296676737796] [-0.0018018988527126576, -0.0018018988527126576] [11089.8662109375, 11087.7138671875]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11073.150955462863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33561.25
Iteration 100: Loss = -19523.634765625
Iteration 200: Loss = -12897.0322265625
Iteration 300: Loss = -11690.96875
Iteration 400: Loss = -11463.302734375
Iteration 500: Loss = -11365.9921875
Iteration 600: Loss = -11302.51953125
Iteration 700: Loss = -11265.662109375
Iteration 800: Loss = -11232.2587890625
Iteration 900: Loss = -11209.712890625
Iteration 1000: Loss = -11196.9609375
Iteration 1100: Loss = -11183.5947265625
Iteration 1200: Loss = -11173.4541015625
Iteration 1300: Loss = -11167.33984375
Iteration 1400: Loss = -11163.0029296875
Iteration 1500: Loss = -11157.8896484375
Iteration 1600: Loss = -11155.63671875
Iteration 1700: Loss = -11153.6708984375
Iteration 1800: Loss = -11147.1787109375
Iteration 1900: Loss = -11141.9912109375
Iteration 2000: Loss = -11140.7587890625
Iteration 2100: Loss = -11139.8037109375
Iteration 2200: Loss = -11139.0107421875
Iteration 2300: Loss = -11138.3251953125
Iteration 2400: Loss = -11137.7255859375
Iteration 2500: Loss = -11137.1904296875
Iteration 2600: Loss = -11136.7119140625
Iteration 2700: Loss = -11136.28515625
Iteration 2800: Loss = -11135.9013671875
Iteration 2900: Loss = -11135.5537109375
Iteration 3000: Loss = -11135.236328125
Iteration 3100: Loss = -11134.953125
Iteration 3200: Loss = -11134.693359375
Iteration 3300: Loss = -11134.4638671875
Iteration 3400: Loss = -11134.2568359375
Iteration 3500: Loss = -11134.072265625
Iteration 3600: Loss = -11133.91015625
Iteration 3700: Loss = -11133.7626953125
Iteration 3800: Loss = -11133.6328125
Iteration 3900: Loss = -11133.5166015625
Iteration 4000: Loss = -11133.412109375
Iteration 4100: Loss = -11133.3193359375
Iteration 4200: Loss = -11133.2333984375
Iteration 4300: Loss = -11133.1533203125
Iteration 4400: Loss = -11133.078125
Iteration 4500: Loss = -11133.009765625
Iteration 4600: Loss = -11132.9482421875
Iteration 4700: Loss = -11132.8896484375
Iteration 4800: Loss = -11132.8349609375
Iteration 4900: Loss = -11132.7841796875
Iteration 5000: Loss = -11132.7353515625
Iteration 5100: Loss = -11132.6884765625
Iteration 5200: Loss = -11132.642578125
Iteration 5300: Loss = -11132.6005859375
Iteration 5400: Loss = -11132.5595703125
Iteration 5500: Loss = -11132.51953125
Iteration 5600: Loss = -11132.4794921875
Iteration 5700: Loss = -11132.4384765625
Iteration 5800: Loss = -11132.3984375
Iteration 5900: Loss = -11132.3544921875
Iteration 6000: Loss = -11132.306640625
Iteration 6100: Loss = -11132.2470703125
Iteration 6200: Loss = -11132.1689453125
Iteration 6300: Loss = -11132.0537109375
Iteration 6400: Loss = -11131.876953125
Iteration 6500: Loss = -11131.619140625
Iteration 6600: Loss = -11131.353515625
Iteration 6700: Loss = -11131.1337890625
Iteration 6800: Loss = -11130.96875
Iteration 6900: Loss = -11130.828125
Iteration 7000: Loss = -11130.658203125
Iteration 7100: Loss = -11130.3173828125
Iteration 7200: Loss = -11130.06640625
Iteration 7300: Loss = -11129.892578125
Iteration 7400: Loss = -11129.705078125
Iteration 7500: Loss = -11129.5166015625
Iteration 7600: Loss = -11129.41015625
Iteration 7700: Loss = -11129.3408203125
Iteration 7800: Loss = -11129.28515625
Iteration 7900: Loss = -11129.2412109375
Iteration 8000: Loss = -11129.201171875
Iteration 8100: Loss = -11129.1650390625
Iteration 8200: Loss = -11129.1357421875
Iteration 8300: Loss = -11129.1103515625
Iteration 8400: Loss = -11129.087890625
Iteration 8500: Loss = -11129.068359375
Iteration 8600: Loss = -11129.0498046875
Iteration 8700: Loss = -11129.0322265625
Iteration 8800: Loss = -11129.017578125
Iteration 8900: Loss = -11129.0029296875
Iteration 9000: Loss = -11128.9912109375
Iteration 9100: Loss = -11128.9794921875
Iteration 9200: Loss = -11128.9677734375
Iteration 9300: Loss = -11128.95703125
Iteration 9400: Loss = -11128.947265625
Iteration 9500: Loss = -11128.939453125
Iteration 9600: Loss = -11128.931640625
Iteration 9700: Loss = -11128.9228515625
Iteration 9800: Loss = -11128.9140625
Iteration 9900: Loss = -11128.9072265625
Iteration 10000: Loss = -11128.9013671875
Iteration 10100: Loss = -11128.896484375
Iteration 10200: Loss = -11128.8896484375
Iteration 10300: Loss = -11128.8857421875
Iteration 10400: Loss = -11128.876953125
Iteration 10500: Loss = -11128.875
Iteration 10600: Loss = -11128.8701171875
Iteration 10700: Loss = -11128.8642578125
Iteration 10800: Loss = -11128.861328125
Iteration 10900: Loss = -11128.8583984375
Iteration 11000: Loss = -11128.8515625
Iteration 11100: Loss = -11128.849609375
Iteration 11200: Loss = -11128.8466796875
Iteration 11300: Loss = -11128.841796875
Iteration 11400: Loss = -11128.83984375
Iteration 11500: Loss = -11128.8359375
Iteration 11600: Loss = -11128.833984375
Iteration 11700: Loss = -11128.8310546875
Iteration 11800: Loss = -11128.8291015625
Iteration 11900: Loss = -11128.8251953125
Iteration 12000: Loss = -11128.8232421875
Iteration 12100: Loss = -11128.8203125
Iteration 12200: Loss = -11128.8193359375
Iteration 12300: Loss = -11128.814453125
Iteration 12400: Loss = -11128.8134765625
Iteration 12500: Loss = -11128.8134765625
Iteration 12600: Loss = -11128.8095703125
Iteration 12700: Loss = -11128.8095703125
Iteration 12800: Loss = -11128.80859375
Iteration 12900: Loss = -11128.8056640625
Iteration 13000: Loss = -11128.806640625
1
Iteration 13100: Loss = -11128.8037109375
Iteration 13200: Loss = -11128.802734375
Iteration 13300: Loss = -11128.7998046875
Iteration 13400: Loss = -11128.80078125
1
Iteration 13500: Loss = -11128.7998046875
Iteration 13600: Loss = -11128.798828125
Iteration 13700: Loss = -11128.796875
Iteration 13800: Loss = -11128.7958984375
Iteration 13900: Loss = -11128.794921875
Iteration 14000: Loss = -11128.794921875
Iteration 14100: Loss = -11128.7939453125
Iteration 14200: Loss = -11128.7939453125
Iteration 14300: Loss = -11128.79296875
Iteration 14400: Loss = -11128.79296875
Iteration 14500: Loss = -11128.7919921875
Iteration 14600: Loss = -11128.7919921875
Iteration 14700: Loss = -11128.7900390625
Iteration 14800: Loss = -11128.7890625
Iteration 14900: Loss = -11128.7890625
Iteration 15000: Loss = -11128.7880859375
Iteration 15100: Loss = -11128.7890625
1
Iteration 15200: Loss = -11128.7890625
2
Iteration 15300: Loss = -11128.7890625
3
Iteration 15400: Loss = -11128.7890625
4
Iteration 15500: Loss = -11128.787109375
Iteration 15600: Loss = -11128.787109375
Iteration 15700: Loss = -11128.7880859375
1
Iteration 15800: Loss = -11128.787109375
Iteration 15900: Loss = -11128.7880859375
1
Iteration 16000: Loss = -11128.787109375
Iteration 16100: Loss = -11128.7861328125
Iteration 16200: Loss = -11128.7861328125
Iteration 16300: Loss = -11128.7861328125
Iteration 16400: Loss = -11128.7861328125
Iteration 16500: Loss = -11128.787109375
1
Iteration 16600: Loss = -11128.7841796875
Iteration 16700: Loss = -11128.78515625
1
Iteration 16800: Loss = -11128.78515625
2
Iteration 16900: Loss = -11128.7841796875
Iteration 17000: Loss = -11128.783203125
Iteration 17100: Loss = -11128.7841796875
1
Iteration 17200: Loss = -11128.783203125
Iteration 17300: Loss = -11128.783203125
Iteration 17400: Loss = -11128.783203125
Iteration 17500: Loss = -11128.7822265625
Iteration 17600: Loss = -11128.7822265625
Iteration 17700: Loss = -11128.7822265625
Iteration 17800: Loss = -11128.783203125
1
Iteration 17900: Loss = -11128.783203125
2
Iteration 18000: Loss = -11128.7841796875
3
Iteration 18100: Loss = -11128.7822265625
Iteration 18200: Loss = -11128.7822265625
Iteration 18300: Loss = -11128.7802734375
Iteration 18400: Loss = -11128.783203125
1
Iteration 18500: Loss = -11128.78125
2
Iteration 18600: Loss = -11128.7802734375
Iteration 18700: Loss = -11128.771484375
Iteration 18800: Loss = -11128.650390625
Iteration 18900: Loss = -11128.64453125
Iteration 19000: Loss = -11128.642578125
Iteration 19100: Loss = -11128.265625
Iteration 19200: Loss = -11127.8486328125
Iteration 19300: Loss = -11127.8427734375
Iteration 19400: Loss = -11127.8427734375
Iteration 19500: Loss = -11127.83984375
Iteration 19600: Loss = -11127.8408203125
1
Iteration 19700: Loss = -11127.8369140625
Iteration 19800: Loss = -11127.837890625
1
Iteration 19900: Loss = -11127.8369140625
Iteration 20000: Loss = -11127.837890625
1
Iteration 20100: Loss = -11127.8359375
Iteration 20200: Loss = -11127.8369140625
1
Iteration 20300: Loss = -11127.8369140625
2
Iteration 20400: Loss = -11127.8369140625
3
Iteration 20500: Loss = -11127.8349609375
Iteration 20600: Loss = -11127.833984375
Iteration 20700: Loss = -11127.837890625
1
Iteration 20800: Loss = -11127.8359375
2
Iteration 20900: Loss = -11127.8359375
3
Iteration 21000: Loss = -11127.8369140625
4
Iteration 21100: Loss = -11127.8349609375
5
Iteration 21200: Loss = -11127.8369140625
6
Iteration 21300: Loss = -11127.8369140625
7
Iteration 21400: Loss = -11127.8359375
8
Iteration 21500: Loss = -11127.8359375
9
Iteration 21600: Loss = -11127.8349609375
10
Iteration 21700: Loss = -11127.8349609375
11
Iteration 21800: Loss = -11127.8359375
12
Iteration 21900: Loss = -11127.8359375
13
Iteration 22000: Loss = -11127.8369140625
14
Iteration 22100: Loss = -11127.8359375
15
Stopping early at iteration 22100 due to no improvement.
pi: tensor([[1.0000e+00, 6.2290e-07],
        [3.5690e-03, 9.9643e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9866, 0.0134], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1668, 0.0587],
         [0.0154, 0.3915]],

        [[0.6249, 0.2259],
         [0.8364, 0.8450]],

        [[0.8502, 0.1095],
         [0.7758, 0.0219]],

        [[0.8616, 0.1574],
         [0.3921, 0.0282]],

        [[0.0122, 0.1576],
         [0.0116, 0.9865]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.00012483681790258485
Average Adjusted Rand Index: -0.0021303917092411464
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43980.1484375
Iteration 100: Loss = -23914.3125
Iteration 200: Loss = -14106.6923828125
Iteration 300: Loss = -12057.9384765625
Iteration 400: Loss = -11696.76171875
Iteration 500: Loss = -11535.875
Iteration 600: Loss = -11425.1435546875
Iteration 700: Loss = -11344.560546875
Iteration 800: Loss = -11306.1201171875
Iteration 900: Loss = -11276.556640625
Iteration 1000: Loss = -11248.697265625
Iteration 1100: Loss = -11233.3330078125
Iteration 1200: Loss = -11221.26953125
Iteration 1300: Loss = -11203.3974609375
Iteration 1400: Loss = -11195.5185546875
Iteration 1500: Loss = -11190.2138671875
Iteration 1600: Loss = -11178.2890625
Iteration 1700: Loss = -11173.9716796875
Iteration 1800: Loss = -11171.1787109375
Iteration 1900: Loss = -11168.9208984375
Iteration 2000: Loss = -11167.009765625
Iteration 2100: Loss = -11165.3271484375
Iteration 2200: Loss = -11160.5634765625
Iteration 2300: Loss = -11159.17578125
Iteration 2400: Loss = -11158.025390625
Iteration 2500: Loss = -11157.00390625
Iteration 2600: Loss = -11149.2783203125
Iteration 2700: Loss = -11147.783203125
Iteration 2800: Loss = -11146.9404296875
Iteration 2900: Loss = -11146.244140625
Iteration 3000: Loss = -11145.6337890625
Iteration 3100: Loss = -11145.0869140625
Iteration 3200: Loss = -11144.59375
Iteration 3300: Loss = -11144.140625
Iteration 3400: Loss = -11143.72265625
Iteration 3500: Loss = -11143.107421875
Iteration 3600: Loss = -11137.228515625
Iteration 3700: Loss = -11136.7783203125
Iteration 3800: Loss = -11136.439453125
Iteration 3900: Loss = -11136.1416015625
Iteration 4000: Loss = -11135.873046875
Iteration 4100: Loss = -11135.6279296875
Iteration 4200: Loss = -11135.4033203125
Iteration 4300: Loss = -11135.1943359375
Iteration 4400: Loss = -11135.00390625
Iteration 4500: Loss = -11134.8271484375
Iteration 4600: Loss = -11134.6650390625
Iteration 4700: Loss = -11134.51171875
Iteration 4800: Loss = -11134.3720703125
Iteration 4900: Loss = -11134.2431640625
Iteration 5000: Loss = -11134.1240234375
Iteration 5100: Loss = -11134.0126953125
Iteration 5200: Loss = -11133.9052734375
Iteration 5300: Loss = -11133.8076171875
Iteration 5400: Loss = -11133.7158203125
Iteration 5500: Loss = -11133.62890625
Iteration 5600: Loss = -11133.5498046875
Iteration 5700: Loss = -11133.47265625
Iteration 5800: Loss = -11133.3994140625
Iteration 5900: Loss = -11133.3330078125
Iteration 6000: Loss = -11133.2685546875
Iteration 6100: Loss = -11133.20703125
Iteration 6200: Loss = -11133.150390625
Iteration 6300: Loss = -11133.0966796875
Iteration 6400: Loss = -11133.044921875
Iteration 6500: Loss = -11132.994140625
Iteration 6600: Loss = -11132.9482421875
Iteration 6700: Loss = -11132.90625
Iteration 6800: Loss = -11132.86328125
Iteration 6900: Loss = -11132.8232421875
Iteration 7000: Loss = -11132.787109375
Iteration 7100: Loss = -11132.75
Iteration 7200: Loss = -11132.71484375
Iteration 7300: Loss = -11132.681640625
Iteration 7400: Loss = -11132.6513671875
Iteration 7500: Loss = -11132.6220703125
Iteration 7600: Loss = -11132.5947265625
Iteration 7700: Loss = -11132.5673828125
Iteration 7800: Loss = -11132.54296875
Iteration 7900: Loss = -11132.517578125
Iteration 8000: Loss = -11132.49609375
Iteration 8100: Loss = -11132.474609375
Iteration 8200: Loss = -11132.4541015625
Iteration 8300: Loss = -11132.43359375
Iteration 8400: Loss = -11132.4130859375
Iteration 8500: Loss = -11132.396484375
Iteration 8600: Loss = -11132.3798828125
Iteration 8700: Loss = -11132.36328125
Iteration 8800: Loss = -11132.345703125
Iteration 8900: Loss = -11132.33203125
Iteration 9000: Loss = -11132.318359375
Iteration 9100: Loss = -11132.3046875
Iteration 9200: Loss = -11132.2919921875
Iteration 9300: Loss = -11132.279296875
Iteration 9400: Loss = -11132.267578125
Iteration 9500: Loss = -11132.255859375
Iteration 9600: Loss = -11132.244140625
Iteration 9700: Loss = -11132.236328125
Iteration 9800: Loss = -11132.22265625
Iteration 9900: Loss = -11132.2138671875
Iteration 10000: Loss = -11132.205078125
Iteration 10100: Loss = -11132.1953125
Iteration 10200: Loss = -11132.18359375
Iteration 10300: Loss = -11132.17578125
Iteration 10400: Loss = -11132.16796875
Iteration 10500: Loss = -11132.15625
Iteration 10600: Loss = -11132.14453125
Iteration 10700: Loss = -11132.12890625
Iteration 10800: Loss = -11132.09375
Iteration 10900: Loss = -11131.8291015625
Iteration 11000: Loss = -11131.095703125
Iteration 11100: Loss = -11130.86328125
Iteration 11200: Loss = -11129.98828125
Iteration 11300: Loss = -11129.900390625
Iteration 11400: Loss = -11129.853515625
Iteration 11500: Loss = -11129.8203125
Iteration 11600: Loss = -11129.7919921875
Iteration 11700: Loss = -11129.767578125
Iteration 11800: Loss = -11129.74609375
Iteration 11900: Loss = -11129.7255859375
Iteration 12000: Loss = -11129.7099609375
Iteration 12100: Loss = -11129.6943359375
Iteration 12200: Loss = -11129.6787109375
Iteration 12300: Loss = -11129.6640625
Iteration 12400: Loss = -11129.6513671875
Iteration 12500: Loss = -11129.6396484375
Iteration 12600: Loss = -11129.626953125
Iteration 12700: Loss = -11129.6162109375
Iteration 12800: Loss = -11129.6064453125
Iteration 12900: Loss = -11129.595703125
Iteration 13000: Loss = -11129.587890625
Iteration 13100: Loss = -11129.578125
Iteration 13200: Loss = -11129.5712890625
Iteration 13300: Loss = -11129.5634765625
Iteration 13400: Loss = -11129.5556640625
Iteration 13500: Loss = -11129.5478515625
Iteration 13600: Loss = -11129.5419921875
Iteration 13700: Loss = -11129.53515625
Iteration 13800: Loss = -11129.529296875
Iteration 13900: Loss = -11129.5234375
Iteration 14000: Loss = -11129.5185546875
Iteration 14100: Loss = -11129.5146484375
Iteration 14200: Loss = -11129.5068359375
Iteration 14300: Loss = -11129.50390625
Iteration 14400: Loss = -11129.4990234375
Iteration 14500: Loss = -11129.49609375
Iteration 14600: Loss = -11129.4921875
Iteration 14700: Loss = -11129.48828125
Iteration 14800: Loss = -11129.484375
Iteration 14900: Loss = -11129.4794921875
Iteration 15000: Loss = -11129.478515625
Iteration 15100: Loss = -11129.474609375
Iteration 15200: Loss = -11129.47265625
Iteration 15300: Loss = -11129.46875
Iteration 15400: Loss = -11129.466796875
Iteration 15500: Loss = -11129.4658203125
Iteration 15600: Loss = -11129.462890625
Iteration 15700: Loss = -11129.4609375
Iteration 15800: Loss = -11129.458984375
Iteration 15900: Loss = -11129.4619140625
1
Iteration 16000: Loss = -11129.4560546875
Iteration 16100: Loss = -11129.453125
Iteration 16200: Loss = -11129.4541015625
1
Iteration 16300: Loss = -11129.451171875
Iteration 16400: Loss = -11129.4501953125
Iteration 16500: Loss = -11129.447265625
Iteration 16600: Loss = -11129.4501953125
1
Iteration 16700: Loss = -11129.4443359375
Iteration 16800: Loss = -11129.4462890625
1
Iteration 16900: Loss = -11129.4443359375
Iteration 17000: Loss = -11129.443359375
Iteration 17100: Loss = -11129.4404296875
Iteration 17200: Loss = -11129.4404296875
Iteration 17300: Loss = -11129.44140625
1
Iteration 17400: Loss = -11129.4384765625
Iteration 17500: Loss = -11129.4384765625
Iteration 17600: Loss = -11129.4306640625
Iteration 17700: Loss = -11128.5498046875
Iteration 17800: Loss = -11128.5361328125
Iteration 17900: Loss = -11128.53125
Iteration 18000: Loss = -11128.5263671875
Iteration 18100: Loss = -11128.5263671875
Iteration 18200: Loss = -11128.5244140625
Iteration 18300: Loss = -11128.5224609375
Iteration 18400: Loss = -11128.5224609375
Iteration 18500: Loss = -11128.5205078125
Iteration 18600: Loss = -11128.51953125
Iteration 18700: Loss = -11128.517578125
Iteration 18800: Loss = -11128.517578125
Iteration 18900: Loss = -11128.517578125
Iteration 19000: Loss = -11128.515625
Iteration 19100: Loss = -11128.513671875
Iteration 19200: Loss = -11128.5146484375
1
Iteration 19300: Loss = -11128.5126953125
Iteration 19400: Loss = -11128.5146484375
1
Iteration 19500: Loss = -11128.5126953125
Iteration 19600: Loss = -11128.51171875
Iteration 19700: Loss = -11128.5107421875
Iteration 19800: Loss = -11128.5107421875
Iteration 19900: Loss = -11128.5107421875
Iteration 20000: Loss = -11128.51171875
1
Iteration 20100: Loss = -11128.5087890625
Iteration 20200: Loss = -11128.51171875
1
Iteration 20300: Loss = -11128.509765625
2
Iteration 20400: Loss = -11128.5087890625
Iteration 20500: Loss = -11128.5078125
Iteration 20600: Loss = -11128.5087890625
1
Iteration 20700: Loss = -11128.5087890625
2
Iteration 20800: Loss = -11128.5078125
Iteration 20900: Loss = -11128.5068359375
Iteration 21000: Loss = -11128.5078125
1
Iteration 21100: Loss = -11128.5068359375
Iteration 21200: Loss = -11128.5078125
1
Iteration 21300: Loss = -11128.5048828125
Iteration 21400: Loss = -11128.5078125
1
Iteration 21500: Loss = -11128.505859375
2
Iteration 21600: Loss = -11128.5068359375
3
Iteration 21700: Loss = -11128.505859375
4
Iteration 21800: Loss = -11128.505859375
5
Iteration 21900: Loss = -11128.5048828125
Iteration 22000: Loss = -11128.50390625
Iteration 22100: Loss = -11128.505859375
1
Iteration 22200: Loss = -11128.505859375
2
Iteration 22300: Loss = -11128.5048828125
3
Iteration 22400: Loss = -11128.5048828125
4
Iteration 22500: Loss = -11128.5048828125
5
Iteration 22600: Loss = -11128.5048828125
6
Iteration 22700: Loss = -11128.50390625
Iteration 22800: Loss = -11128.5048828125
1
Iteration 22900: Loss = -11128.50390625
Iteration 23000: Loss = -11128.50390625
Iteration 23100: Loss = -11128.505859375
1
Iteration 23200: Loss = -11128.5048828125
2
Iteration 23300: Loss = -11128.5048828125
3
Iteration 23400: Loss = -11128.5048828125
4
Iteration 23500: Loss = -11128.5048828125
5
Iteration 23600: Loss = -11128.50390625
Iteration 23700: Loss = -11128.505859375
1
Iteration 23800: Loss = -11128.505859375
2
Iteration 23900: Loss = -11128.5048828125
3
Iteration 24000: Loss = -11128.50390625
Iteration 24100: Loss = -11128.5048828125
1
Iteration 24200: Loss = -11128.5048828125
2
Iteration 24300: Loss = -11128.50390625
Iteration 24400: Loss = -11128.50390625
Iteration 24500: Loss = -11128.50390625
Iteration 24600: Loss = -11128.505859375
1
Iteration 24700: Loss = -11128.505859375
2
Iteration 24800: Loss = -11128.50390625
Iteration 24900: Loss = -11128.5068359375
1
Iteration 25000: Loss = -11128.5029296875
Iteration 25100: Loss = -11128.5048828125
1
Iteration 25200: Loss = -11128.5029296875
Iteration 25300: Loss = -11128.5048828125
1
Iteration 25400: Loss = -11128.50390625
2
Iteration 25500: Loss = -11128.50390625
3
Iteration 25600: Loss = -11128.50390625
4
Iteration 25700: Loss = -11128.50390625
5
Iteration 25800: Loss = -11128.5048828125
6
Iteration 25900: Loss = -11128.5048828125
7
Iteration 26000: Loss = -11128.5048828125
8
Iteration 26100: Loss = -11128.5029296875
Iteration 26200: Loss = -11128.50390625
1
Iteration 26300: Loss = -11128.50390625
2
Iteration 26400: Loss = -11128.50390625
3
Iteration 26500: Loss = -11128.50390625
4
Iteration 26600: Loss = -11128.50390625
5
Iteration 26700: Loss = -11128.50390625
6
Iteration 26800: Loss = -11128.50390625
7
Iteration 26900: Loss = -11128.50390625
8
Iteration 27000: Loss = -11128.5048828125
9
Iteration 27100: Loss = -11128.5048828125
10
Iteration 27200: Loss = -11128.5048828125
11
Iteration 27300: Loss = -11128.50390625
12
Iteration 27400: Loss = -11128.50390625
13
Iteration 27500: Loss = -11128.50390625
14
Iteration 27600: Loss = -11128.50390625
15
Stopping early at iteration 27600 due to no improvement.
pi: tensor([[1.0000e+00, 6.4797e-07],
        [4.6803e-01, 5.3197e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9724, 0.0276], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1672, 0.0744],
         [0.9556, 0.9999]],

        [[0.6160, 0.2067],
         [0.0140, 0.8468]],

        [[0.0375, 0.1010],
         [0.0468, 0.7228]],

        [[0.9233, 0.1515],
         [0.0583, 0.1271]],

        [[0.3255, 0.1515],
         [0.5580, 0.1080]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.1812094985451128e-06
Average Adjusted Rand Index: -0.0030069060821583974
[-0.00012483681790258485, 3.1812094985451128e-06] [-0.0021303917092411464, -0.0030069060821583974] [11127.8359375, 11128.50390625]
-------------------------------------
This iteration is 98
True Objective function: Loss = -10870.652198835178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29988.576171875
Iteration 100: Loss = -21360.99609375
Iteration 200: Loss = -13135.5673828125
Iteration 300: Loss = -11553.3720703125
Iteration 400: Loss = -11285.4697265625
Iteration 500: Loss = -11196.541015625
Iteration 600: Loss = -11151.1572265625
Iteration 700: Loss = -11123.3125
Iteration 800: Loss = -11104.49609375
Iteration 900: Loss = -11090.5166015625
Iteration 1000: Loss = -11077.794921875
Iteration 1100: Loss = -11066.1298828125
Iteration 1200: Loss = -11057.2470703125
Iteration 1300: Loss = -11050.3974609375
Iteration 1400: Loss = -11045.8759765625
Iteration 1500: Loss = -11040.263671875
Iteration 1600: Loss = -11035.7060546875
Iteration 1700: Loss = -11030.9013671875
Iteration 1800: Loss = -11026.1337890625
Iteration 1900: Loss = -11021.4716796875
Iteration 2000: Loss = -11017.595703125
Iteration 2100: Loss = -11012.390625
Iteration 2200: Loss = -11008.728515625
Iteration 2300: Loss = -11006.1513671875
Iteration 2400: Loss = -11003.2548828125
Iteration 2500: Loss = -11000.0966796875
Iteration 2600: Loss = -10997.642578125
Iteration 2700: Loss = -10995.765625
Iteration 2800: Loss = -10994.2001953125
Iteration 2900: Loss = -10990.7470703125
Iteration 3000: Loss = -10989.59765625
Iteration 3100: Loss = -10988.671875
Iteration 3200: Loss = -10987.90625
Iteration 3300: Loss = -10987.2568359375
Iteration 3400: Loss = -10986.689453125
Iteration 3500: Loss = -10986.169921875
Iteration 3600: Loss = -10985.7138671875
Iteration 3700: Loss = -10985.330078125
Iteration 3800: Loss = -10984.9931640625
Iteration 3900: Loss = -10984.693359375
Iteration 4000: Loss = -10984.421875
Iteration 4100: Loss = -10984.177734375
Iteration 4200: Loss = -10983.9541015625
Iteration 4300: Loss = -10983.75
Iteration 4400: Loss = -10983.5595703125
Iteration 4500: Loss = -10983.3818359375
Iteration 4600: Loss = -10983.197265625
Iteration 4700: Loss = -10982.216796875
Iteration 4800: Loss = -10980.212890625
Iteration 4900: Loss = -10980.025390625
Iteration 5000: Loss = -10979.884765625
Iteration 5100: Loss = -10979.7626953125
Iteration 5200: Loss = -10979.6552734375
Iteration 5300: Loss = -10979.5537109375
Iteration 5400: Loss = -10979.4619140625
Iteration 5500: Loss = -10979.376953125
Iteration 5600: Loss = -10979.2978515625
Iteration 5700: Loss = -10979.2216796875
Iteration 5800: Loss = -10979.1484375
Iteration 5900: Loss = -10979.01171875
Iteration 6000: Loss = -10975.736328125
Iteration 6100: Loss = -10975.4970703125
Iteration 6200: Loss = -10975.3427734375
Iteration 6300: Loss = -10975.2255859375
Iteration 6400: Loss = -10975.1298828125
Iteration 6500: Loss = -10975.0498046875
Iteration 6600: Loss = -10974.9814453125
Iteration 6700: Loss = -10974.9228515625
Iteration 6800: Loss = -10974.8701171875
Iteration 6900: Loss = -10974.8232421875
Iteration 7000: Loss = -10974.783203125
Iteration 7100: Loss = -10974.7451171875
Iteration 7200: Loss = -10974.7119140625
Iteration 7300: Loss = -10974.6787109375
Iteration 7400: Loss = -10974.6513671875
Iteration 7500: Loss = -10974.623046875
Iteration 7600: Loss = -10974.59765625
Iteration 7700: Loss = -10974.576171875
Iteration 7800: Loss = -10974.5537109375
Iteration 7900: Loss = -10974.5341796875
Iteration 8000: Loss = -10974.515625
Iteration 8100: Loss = -10974.4970703125
Iteration 8200: Loss = -10974.4814453125
Iteration 8300: Loss = -10974.4658203125
Iteration 8400: Loss = -10974.451171875
Iteration 8500: Loss = -10974.4375
Iteration 8600: Loss = -10974.423828125
Iteration 8700: Loss = -10974.412109375
Iteration 8800: Loss = -10974.40234375
Iteration 8900: Loss = -10974.3916015625
Iteration 9000: Loss = -10974.3798828125
Iteration 9100: Loss = -10974.37109375
Iteration 9200: Loss = -10974.3603515625
Iteration 9300: Loss = -10974.3505859375
Iteration 9400: Loss = -10974.34375
Iteration 9500: Loss = -10974.3369140625
Iteration 9600: Loss = -10974.328125
Iteration 9700: Loss = -10974.3232421875
Iteration 9800: Loss = -10974.314453125
Iteration 9900: Loss = -10974.30859375
Iteration 10000: Loss = -10974.302734375
Iteration 10100: Loss = -10974.296875
Iteration 10200: Loss = -10974.2919921875
Iteration 10300: Loss = -10974.2861328125
Iteration 10400: Loss = -10974.283203125
Iteration 10500: Loss = -10974.27734375
Iteration 10600: Loss = -10974.2724609375
Iteration 10700: Loss = -10974.271484375
Iteration 10800: Loss = -10974.2646484375
Iteration 10900: Loss = -10974.26171875
Iteration 11000: Loss = -10974.255859375
Iteration 11100: Loss = -10974.2529296875
Iteration 11200: Loss = -10974.2509765625
Iteration 11300: Loss = -10974.248046875
Iteration 11400: Loss = -10974.244140625
Iteration 11500: Loss = -10974.2412109375
Iteration 11600: Loss = -10974.23828125
Iteration 11700: Loss = -10974.2373046875
Iteration 11800: Loss = -10974.234375
Iteration 11900: Loss = -10974.232421875
Iteration 12000: Loss = -10974.23046875
Iteration 12100: Loss = -10974.228515625
Iteration 12200: Loss = -10974.2265625
Iteration 12300: Loss = -10974.2236328125
Iteration 12400: Loss = -10974.22265625
Iteration 12500: Loss = -10974.2216796875
Iteration 12600: Loss = -10974.2197265625
Iteration 12700: Loss = -10974.2177734375
Iteration 12800: Loss = -10974.216796875
Iteration 12900: Loss = -10974.21484375
Iteration 13000: Loss = -10974.2138671875
Iteration 13100: Loss = -10974.212890625
Iteration 13200: Loss = -10974.2109375
Iteration 13300: Loss = -10974.208984375
Iteration 13400: Loss = -10974.2080078125
Iteration 13500: Loss = -10974.20703125
Iteration 13600: Loss = -10974.2060546875
Iteration 13700: Loss = -10974.205078125
Iteration 13800: Loss = -10974.205078125
Iteration 13900: Loss = -10974.205078125
Iteration 14000: Loss = -10974.2041015625
Iteration 14100: Loss = -10974.203125
Iteration 14200: Loss = -10974.203125
Iteration 14300: Loss = -10974.201171875
Iteration 14400: Loss = -10974.2001953125
Iteration 14500: Loss = -10974.2001953125
Iteration 14600: Loss = -10974.2001953125
Iteration 14700: Loss = -10974.19921875
Iteration 14800: Loss = -10974.1982421875
Iteration 14900: Loss = -10974.1982421875
Iteration 15000: Loss = -10974.1943359375
Iteration 15100: Loss = -10974.19140625
Iteration 15200: Loss = -10974.1904296875
Iteration 15300: Loss = -10974.1904296875
Iteration 15400: Loss = -10974.189453125
Iteration 15500: Loss = -10974.189453125
Iteration 15600: Loss = -10974.1904296875
1
Iteration 15700: Loss = -10974.1884765625
Iteration 15800: Loss = -10974.1884765625
Iteration 15900: Loss = -10974.1884765625
Iteration 16000: Loss = -10974.1875
Iteration 16100: Loss = -10974.1875
Iteration 16200: Loss = -10974.1875
Iteration 16300: Loss = -10974.1875
Iteration 16400: Loss = -10974.185546875
Iteration 16500: Loss = -10974.1875
1
Iteration 16600: Loss = -10974.1865234375
2
Iteration 16700: Loss = -10974.185546875
Iteration 16800: Loss = -10974.185546875
Iteration 16900: Loss = -10974.1875
1
Iteration 17000: Loss = -10974.1865234375
2
Iteration 17100: Loss = -10974.185546875
Iteration 17200: Loss = -10974.18359375
Iteration 17300: Loss = -10974.185546875
1
Iteration 17400: Loss = -10974.185546875
2
Iteration 17500: Loss = -10974.185546875
3
Iteration 17600: Loss = -10974.1865234375
4
Iteration 17700: Loss = -10974.18359375
Iteration 17800: Loss = -10974.185546875
1
Iteration 17900: Loss = -10974.185546875
2
Iteration 18000: Loss = -10974.18359375
Iteration 18100: Loss = -10974.18359375
Iteration 18200: Loss = -10974.185546875
1
Iteration 18300: Loss = -10974.185546875
2
Iteration 18400: Loss = -10974.185546875
3
Iteration 18500: Loss = -10974.18359375
Iteration 18600: Loss = -10974.18359375
Iteration 18700: Loss = -10974.185546875
1
Iteration 18800: Loss = -10974.1845703125
2
Iteration 18900: Loss = -10974.18359375
Iteration 19000: Loss = -10974.1845703125
1
Iteration 19100: Loss = -10974.18359375
Iteration 19200: Loss = -10974.1826171875
Iteration 19300: Loss = -10974.18359375
1
Iteration 19400: Loss = -10974.18359375
2
Iteration 19500: Loss = -10974.1826171875
Iteration 19600: Loss = -10974.18359375
1
Iteration 19700: Loss = -10974.1845703125
2
Iteration 19800: Loss = -10974.1826171875
Iteration 19900: Loss = -10974.1826171875
Iteration 20000: Loss = -10974.18359375
1
Iteration 20100: Loss = -10974.1845703125
2
Iteration 20200: Loss = -10974.1826171875
Iteration 20300: Loss = -10974.18359375
1
Iteration 20400: Loss = -10974.181640625
Iteration 20500: Loss = -10974.1826171875
1
Iteration 20600: Loss = -10974.1845703125
2
Iteration 20700: Loss = -10974.18359375
3
Iteration 20800: Loss = -10974.1845703125
4
Iteration 20900: Loss = -10974.1845703125
5
Iteration 21000: Loss = -10974.181640625
Iteration 21100: Loss = -10974.1826171875
1
Iteration 21200: Loss = -10974.1826171875
2
Iteration 21300: Loss = -10974.1826171875
3
Iteration 21400: Loss = -10974.1826171875
4
Iteration 21500: Loss = -10974.181640625
Iteration 21600: Loss = -10974.18359375
1
Iteration 21700: Loss = -10974.1826171875
2
Iteration 21800: Loss = -10974.1826171875
3
Iteration 21900: Loss = -10974.18359375
4
Iteration 22000: Loss = -10974.18359375
5
Iteration 22100: Loss = -10974.1826171875
6
Iteration 22200: Loss = -10974.1826171875
7
Iteration 22300: Loss = -10974.1826171875
8
Iteration 22400: Loss = -10974.18359375
9
Iteration 22500: Loss = -10974.18359375
10
Iteration 22600: Loss = -10974.1826171875
11
Iteration 22700: Loss = -10974.1826171875
12
Iteration 22800: Loss = -10974.1845703125
13
Iteration 22900: Loss = -10974.1826171875
14
Iteration 23000: Loss = -10974.18359375
15
Stopping early at iteration 23000 due to no improvement.
pi: tensor([[0.9895, 0.0105],
        [0.9657, 0.0343]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.2864e-07, 1.0000e+00], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1660, 0.2632],
         [0.8095, 0.1590]],

        [[0.0320, 0.0958],
         [0.0069, 0.1636]],

        [[0.9509, 0.4854],
         [0.0287, 0.0856]],

        [[0.6633, 0.0908],
         [0.0142, 0.9828]],

        [[0.0114, 0.0428],
         [0.9832, 0.9499]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: -0.0010134073425416364
Average Adjusted Rand Index: 0.0013396045902860018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22022.728515625
Iteration 100: Loss = -14933.9443359375
Iteration 200: Loss = -11871.6416015625
Iteration 300: Loss = -11356.6220703125
Iteration 400: Loss = -11237.580078125
Iteration 500: Loss = -11181.529296875
Iteration 600: Loss = -11147.4140625
Iteration 700: Loss = -11122.091796875
Iteration 800: Loss = -11101.8544921875
Iteration 900: Loss = -11087.50390625
Iteration 1000: Loss = -11073.9091796875
Iteration 1100: Loss = -11064.2548828125
Iteration 1200: Loss = -11055.1875
Iteration 1300: Loss = -11048.3349609375
Iteration 1400: Loss = -11039.3154296875
Iteration 1500: Loss = -11034.2724609375
Iteration 1600: Loss = -11030.0341796875
Iteration 1700: Loss = -11024.8212890625
Iteration 1800: Loss = -11020.080078125
Iteration 1900: Loss = -11014.5859375
Iteration 2000: Loss = -11010.3330078125
Iteration 2100: Loss = -11006.13671875
Iteration 2200: Loss = -11003.134765625
Iteration 2300: Loss = -11000.810546875
Iteration 2400: Loss = -10999.181640625
Iteration 2500: Loss = -10996.654296875
Iteration 2600: Loss = -10993.076171875
Iteration 2700: Loss = -10991.900390625
Iteration 2800: Loss = -10990.6103515625
Iteration 2900: Loss = -10987.412109375
Iteration 3000: Loss = -10985.7119140625
Iteration 3100: Loss = -10984.8486328125
Iteration 3200: Loss = -10984.1259765625
Iteration 3300: Loss = -10983.6015625
Iteration 3400: Loss = -10982.9599609375
Iteration 3500: Loss = -10981.412109375
Iteration 3600: Loss = -10979.84375
Iteration 3700: Loss = -10979.134765625
Iteration 3800: Loss = -10978.7431640625
Iteration 3900: Loss = -10978.4794921875
Iteration 4000: Loss = -10978.2783203125
Iteration 4100: Loss = -10978.1083984375
Iteration 4200: Loss = -10977.96484375
Iteration 4300: Loss = -10977.8408203125
Iteration 4400: Loss = -10977.7314453125
Iteration 4500: Loss = -10977.6376953125
Iteration 4600: Loss = -10977.5537109375
Iteration 4700: Loss = -10977.478515625
Iteration 4800: Loss = -10977.4130859375
Iteration 4900: Loss = -10977.3544921875
Iteration 5000: Loss = -10977.302734375
Iteration 5100: Loss = -10977.255859375
Iteration 5200: Loss = -10977.2119140625
Iteration 5300: Loss = -10977.171875
Iteration 5400: Loss = -10977.13671875
Iteration 5500: Loss = -10977.1044921875
Iteration 5600: Loss = -10977.0751953125
Iteration 5700: Loss = -10977.0458984375
Iteration 5800: Loss = -10977.0205078125
Iteration 5900: Loss = -10976.998046875
Iteration 6000: Loss = -10976.9755859375
Iteration 6100: Loss = -10976.9541015625
Iteration 6200: Loss = -10976.9365234375
Iteration 6300: Loss = -10976.9189453125
Iteration 6400: Loss = -10976.90234375
Iteration 6500: Loss = -10976.884765625
Iteration 6600: Loss = -10976.87109375
Iteration 6700: Loss = -10976.857421875
Iteration 6800: Loss = -10976.8447265625
Iteration 6900: Loss = -10976.8310546875
Iteration 7000: Loss = -10976.8154296875
Iteration 7100: Loss = -10976.7939453125
Iteration 7200: Loss = -10976.24609375
Iteration 7300: Loss = -10975.9462890625
Iteration 7400: Loss = -10975.8837890625
Iteration 7500: Loss = -10975.8544921875
Iteration 7600: Loss = -10975.830078125
Iteration 7700: Loss = -10975.8046875
Iteration 7800: Loss = -10975.779296875
Iteration 7900: Loss = -10975.7587890625
Iteration 8000: Loss = -10975.7431640625
Iteration 8100: Loss = -10975.732421875
Iteration 8200: Loss = -10975.72265625
Iteration 8300: Loss = -10975.71484375
Iteration 8400: Loss = -10975.70703125
Iteration 8500: Loss = -10975.7041015625
Iteration 8600: Loss = -10975.6953125
Iteration 8700: Loss = -10975.69140625
Iteration 8800: Loss = -10975.6875
Iteration 8900: Loss = -10975.6826171875
Iteration 9000: Loss = -10975.6787109375
Iteration 9100: Loss = -10975.67578125
Iteration 9200: Loss = -10975.671875
Iteration 9300: Loss = -10975.6669921875
Iteration 9400: Loss = -10975.6650390625
Iteration 9500: Loss = -10975.6611328125
Iteration 9600: Loss = -10975.658203125
Iteration 9700: Loss = -10975.65625
Iteration 9800: Loss = -10975.65234375
Iteration 9900: Loss = -10975.6484375
Iteration 10000: Loss = -10975.3017578125
Iteration 10100: Loss = -10973.0166015625
Iteration 10200: Loss = -10972.767578125
Iteration 10300: Loss = -10972.681640625
Iteration 10400: Loss = -10972.65625
Iteration 10500: Loss = -10972.6435546875
Iteration 10600: Loss = -10972.638671875
Iteration 10700: Loss = -10972.634765625
Iteration 10800: Loss = -10972.6318359375
Iteration 10900: Loss = -10972.6279296875
Iteration 11000: Loss = -10972.626953125
Iteration 11100: Loss = -10972.5048828125
Iteration 11200: Loss = -10972.4453125
Iteration 11300: Loss = -10972.4208984375
Iteration 11400: Loss = -10972.416015625
Iteration 11500: Loss = -10972.4150390625
Iteration 11600: Loss = -10972.4111328125
Iteration 11700: Loss = -10972.4033203125
Iteration 11800: Loss = -10972.380859375
Iteration 11900: Loss = -10972.376953125
Iteration 12000: Loss = -10972.3701171875
Iteration 12100: Loss = -10972.353515625
Iteration 12200: Loss = -10972.3515625
Iteration 12300: Loss = -10972.3505859375
Iteration 12400: Loss = -10972.34765625
Iteration 12500: Loss = -10972.3466796875
Iteration 12600: Loss = -10972.34375
Iteration 12700: Loss = -10972.34375
Iteration 12800: Loss = -10972.3408203125
Iteration 12900: Loss = -10972.341796875
1
Iteration 13000: Loss = -10972.3408203125
Iteration 13100: Loss = -10972.33984375
Iteration 13200: Loss = -10972.337890625
Iteration 13300: Loss = -10972.3369140625
Iteration 13400: Loss = -10972.3359375
Iteration 13500: Loss = -10972.3359375
Iteration 13600: Loss = -10972.3359375
Iteration 13700: Loss = -10972.333984375
Iteration 13800: Loss = -10972.330078125
Iteration 13900: Loss = -10972.3291015625
Iteration 14000: Loss = -10972.3291015625
Iteration 14100: Loss = -10972.3271484375
Iteration 14200: Loss = -10972.3291015625
1
Iteration 14300: Loss = -10972.3271484375
Iteration 14400: Loss = -10972.3232421875
Iteration 14500: Loss = -10972.322265625
Iteration 14600: Loss = -10972.322265625
Iteration 14700: Loss = -10972.3232421875
1
Iteration 14800: Loss = -10972.3115234375
Iteration 14900: Loss = -10972.287109375
Iteration 15000: Loss = -10972.28515625
Iteration 15100: Loss = -10972.2841796875
Iteration 15200: Loss = -10972.283203125
Iteration 15300: Loss = -10972.2822265625
Iteration 15400: Loss = -10972.28125
Iteration 15500: Loss = -10972.279296875
Iteration 15600: Loss = -10972.2783203125
Iteration 15700: Loss = -10972.275390625
Iteration 15800: Loss = -10972.27734375
1
Iteration 15900: Loss = -10972.2763671875
2
Iteration 16000: Loss = -10972.275390625
Iteration 16100: Loss = -10972.275390625
Iteration 16200: Loss = -10972.275390625
Iteration 16300: Loss = -10972.2763671875
1
Iteration 16400: Loss = -10972.2744140625
Iteration 16500: Loss = -10972.2744140625
Iteration 16600: Loss = -10972.2744140625
Iteration 16700: Loss = -10972.2744140625
Iteration 16800: Loss = -10972.275390625
1
Iteration 16900: Loss = -10972.275390625
2
Iteration 17000: Loss = -10972.2744140625
Iteration 17100: Loss = -10972.2744140625
Iteration 17200: Loss = -10972.2734375
Iteration 17300: Loss = -10972.2734375
Iteration 17400: Loss = -10972.2734375
Iteration 17500: Loss = -10972.2734375
Iteration 17600: Loss = -10972.2724609375
Iteration 17700: Loss = -10972.2744140625
1
Iteration 17800: Loss = -10972.2734375
2
Iteration 17900: Loss = -10972.271484375
Iteration 18000: Loss = -10972.2734375
1
Iteration 18100: Loss = -10972.2734375
2
Iteration 18200: Loss = -10972.271484375
Iteration 18300: Loss = -10972.2724609375
1
Iteration 18400: Loss = -10972.2724609375
2
Iteration 18500: Loss = -10972.2734375
3
Iteration 18600: Loss = -10972.2724609375
4
Iteration 18700: Loss = -10972.2724609375
5
Iteration 18800: Loss = -10972.2734375
6
Iteration 18900: Loss = -10972.2734375
7
Iteration 19000: Loss = -10972.2724609375
8
Iteration 19100: Loss = -10972.2734375
9
Iteration 19200: Loss = -10972.2734375
10
Iteration 19300: Loss = -10972.2734375
11
Iteration 19400: Loss = -10972.2724609375
12
Iteration 19500: Loss = -10972.2724609375
13
Iteration 19600: Loss = -10972.2724609375
14
Iteration 19700: Loss = -10972.2724609375
15
Stopping early at iteration 19700 due to no improvement.
pi: tensor([[0.1395, 0.8605],
        [0.0120, 0.9880]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1148, 0.8852], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0942, 0.1207],
         [0.0406, 0.1657]],

        [[0.8381, 0.0979],
         [0.9706, 0.0302]],

        [[0.5176, 0.2510],
         [0.8489, 0.9914]],

        [[0.7532, 0.5896],
         [0.9845, 0.0083]],

        [[0.0723, 0.0432],
         [0.2428, 0.0489]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: 0.0065317017556330015
Average Adjusted Rand Index: 0.005726379357958248
[-0.0010134073425416364, 0.0065317017556330015] [0.0013396045902860018, 0.005726379357958248] [10974.18359375, 10972.2724609375]
-------------------------------------
This iteration is 99
True Objective function: Loss = -10781.988177382842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27156.791015625
Iteration 100: Loss = -16647.484375
Iteration 200: Loss = -12179.3427734375
Iteration 300: Loss = -11250.708984375
Iteration 400: Loss = -11077.576171875
Iteration 500: Loss = -11009.318359375
Iteration 600: Loss = -10974.234375
Iteration 700: Loss = -10943.814453125
Iteration 800: Loss = -10924.1630859375
Iteration 900: Loss = -10914.7587890625
Iteration 1000: Loss = -10907.115234375
Iteration 1100: Loss = -10902.486328125
Iteration 1200: Loss = -10898.6962890625
Iteration 1300: Loss = -10893.064453125
Iteration 1400: Loss = -10890.083984375
Iteration 1500: Loss = -10888.0087890625
Iteration 1600: Loss = -10886.4091796875
Iteration 1700: Loss = -10885.123046875
Iteration 1800: Loss = -10884.0615234375
Iteration 1900: Loss = -10883.1708984375
Iteration 2000: Loss = -10882.4052734375
Iteration 2100: Loss = -10877.5322265625
Iteration 2200: Loss = -10876.703125
Iteration 2300: Loss = -10876.0634765625
Iteration 2400: Loss = -10875.517578125
Iteration 2500: Loss = -10875.0419921875
Iteration 2600: Loss = -10874.62109375
Iteration 2700: Loss = -10874.25
Iteration 2800: Loss = -10873.9189453125
Iteration 2900: Loss = -10873.6201171875
Iteration 3000: Loss = -10873.353515625
Iteration 3100: Loss = -10873.11328125
Iteration 3200: Loss = -10872.892578125
Iteration 3300: Loss = -10872.693359375
Iteration 3400: Loss = -10872.51171875
Iteration 3500: Loss = -10872.345703125
Iteration 3600: Loss = -10872.1953125
Iteration 3700: Loss = -10872.0546875
Iteration 3800: Loss = -10871.927734375
Iteration 3900: Loss = -10871.810546875
Iteration 4000: Loss = -10871.7041015625
Iteration 4100: Loss = -10871.6044921875
Iteration 4200: Loss = -10871.51171875
Iteration 4300: Loss = -10871.4287109375
Iteration 4400: Loss = -10871.349609375
Iteration 4500: Loss = -10871.27734375
Iteration 4600: Loss = -10871.2099609375
Iteration 4700: Loss = -10871.1484375
Iteration 4800: Loss = -10871.0888671875
Iteration 4900: Loss = -10871.03515625
Iteration 5000: Loss = -10870.9853515625
Iteration 5100: Loss = -10870.9384765625
Iteration 5200: Loss = -10870.89453125
Iteration 5300: Loss = -10870.8525390625
Iteration 5400: Loss = -10870.814453125
Iteration 5500: Loss = -10870.7783203125
Iteration 5600: Loss = -10870.7451171875
Iteration 5700: Loss = -10870.712890625
Iteration 5800: Loss = -10870.6845703125
Iteration 5900: Loss = -10870.6572265625
Iteration 6000: Loss = -10870.630859375
Iteration 6100: Loss = -10870.6064453125
Iteration 6200: Loss = -10870.5830078125
Iteration 6300: Loss = -10870.5634765625
Iteration 6400: Loss = -10870.5419921875
Iteration 6500: Loss = -10870.5244140625
Iteration 6600: Loss = -10870.5068359375
Iteration 6700: Loss = -10870.4892578125
Iteration 6800: Loss = -10870.47265625
Iteration 6900: Loss = -10870.4580078125
Iteration 7000: Loss = -10870.443359375
Iteration 7100: Loss = -10870.4306640625
Iteration 7200: Loss = -10870.41796875
Iteration 7300: Loss = -10870.4052734375
Iteration 7400: Loss = -10870.3955078125
Iteration 7500: Loss = -10870.384765625
Iteration 7600: Loss = -10870.3740234375
Iteration 7700: Loss = -10870.365234375
Iteration 7800: Loss = -10870.3544921875
Iteration 7900: Loss = -10870.345703125
Iteration 8000: Loss = -10870.337890625
Iteration 8100: Loss = -10870.330078125
Iteration 8200: Loss = -10870.3212890625
Iteration 8300: Loss = -10870.3154296875
Iteration 8400: Loss = -10870.3076171875
Iteration 8500: Loss = -10870.302734375
Iteration 8600: Loss = -10870.2958984375
Iteration 8700: Loss = -10870.291015625
Iteration 8800: Loss = -10870.283203125
Iteration 8900: Loss = -10870.279296875
Iteration 9000: Loss = -10870.2744140625
Iteration 9100: Loss = -10870.26953125
Iteration 9200: Loss = -10870.265625
Iteration 9300: Loss = -10870.259765625
Iteration 9400: Loss = -10870.255859375
Iteration 9500: Loss = -10870.251953125
Iteration 9600: Loss = -10870.248046875
Iteration 9700: Loss = -10870.2451171875
Iteration 9800: Loss = -10870.240234375
Iteration 9900: Loss = -10870.23828125
Iteration 10000: Loss = -10870.2353515625
Iteration 10100: Loss = -10870.232421875
Iteration 10200: Loss = -10870.23046875
Iteration 10300: Loss = -10870.2255859375
Iteration 10400: Loss = -10870.2255859375
Iteration 10500: Loss = -10870.224609375
Iteration 10600: Loss = -10870.2236328125
Iteration 10700: Loss = -10870.220703125
Iteration 10800: Loss = -10870.21875
Iteration 10900: Loss = -10870.2177734375
Iteration 11000: Loss = -10870.216796875
Iteration 11100: Loss = -10870.2177734375
1
Iteration 11200: Loss = -10870.2158203125
Iteration 11300: Loss = -10870.2138671875
Iteration 11400: Loss = -10870.2138671875
Iteration 11500: Loss = -10870.21484375
1
Iteration 11600: Loss = -10870.2119140625
Iteration 11700: Loss = -10870.2119140625
Iteration 11800: Loss = -10870.2099609375
Iteration 11900: Loss = -10870.2099609375
Iteration 12000: Loss = -10870.2099609375
Iteration 12100: Loss = -10870.2080078125
Iteration 12200: Loss = -10870.2080078125
Iteration 12300: Loss = -10870.2080078125
Iteration 12400: Loss = -10870.20703125
Iteration 12500: Loss = -10870.20703125
Iteration 12600: Loss = -10870.205078125
Iteration 12700: Loss = -10870.205078125
Iteration 12800: Loss = -10870.2041015625
Iteration 12900: Loss = -10870.2021484375
Iteration 13000: Loss = -10870.2021484375
Iteration 13100: Loss = -10870.2021484375
Iteration 13200: Loss = -10870.201171875
Iteration 13300: Loss = -10870.1982421875
Iteration 13400: Loss = -10870.197265625
Iteration 13500: Loss = -10870.1962890625
Iteration 13600: Loss = -10870.193359375
Iteration 13700: Loss = -10870.19140625
Iteration 13800: Loss = -10870.1904296875
Iteration 13900: Loss = -10870.185546875
Iteration 14000: Loss = -10870.181640625
Iteration 14100: Loss = -10870.1767578125
Iteration 14200: Loss = -10870.1689453125
Iteration 14300: Loss = -10870.1591796875
Iteration 14400: Loss = -10870.1484375
Iteration 14500: Loss = -10870.130859375
Iteration 14600: Loss = -10870.1005859375
Iteration 14700: Loss = -10870.0263671875
Iteration 14800: Loss = -10869.806640625
Iteration 14900: Loss = -10869.6171875
Iteration 15000: Loss = -10869.5888671875
Iteration 15100: Loss = -10869.576171875
Iteration 15200: Loss = -10869.5751953125
Iteration 15300: Loss = -10869.57421875
Iteration 15400: Loss = -10869.57421875
Iteration 15500: Loss = -10869.5732421875
Iteration 15600: Loss = -10869.572265625
Iteration 15700: Loss = -10869.572265625
Iteration 15800: Loss = -10869.572265625
Iteration 15900: Loss = -10869.572265625
Iteration 16000: Loss = -10869.5732421875
1
Iteration 16100: Loss = -10869.5712890625
Iteration 16200: Loss = -10869.5703125
Iteration 16300: Loss = -10869.572265625
1
Iteration 16400: Loss = -10869.5712890625
2
Iteration 16500: Loss = -10869.5732421875
3
Iteration 16600: Loss = -10869.572265625
4
Iteration 16700: Loss = -10869.5712890625
5
Iteration 16800: Loss = -10869.5712890625
6
Iteration 16900: Loss = -10869.5732421875
7
Iteration 17000: Loss = -10869.5712890625
8
Iteration 17100: Loss = -10869.572265625
9
Iteration 17200: Loss = -10869.5712890625
10
Iteration 17300: Loss = -10869.572265625
11
Iteration 17400: Loss = -10869.5703125
Iteration 17500: Loss = -10869.5703125
Iteration 17600: Loss = -10869.5712890625
1
Iteration 17700: Loss = -10869.572265625
2
Iteration 17800: Loss = -10869.5712890625
3
Iteration 17900: Loss = -10869.5712890625
4
Iteration 18000: Loss = -10869.5712890625
5
Iteration 18100: Loss = -10869.572265625
6
Iteration 18200: Loss = -10869.5703125
Iteration 18300: Loss = -10869.5703125
Iteration 18400: Loss = -10869.5712890625
1
Iteration 18500: Loss = -10869.5703125
Iteration 18600: Loss = -10869.5712890625
1
Iteration 18700: Loss = -10869.5712890625
2
Iteration 18800: Loss = -10869.5712890625
3
Iteration 18900: Loss = -10869.5693359375
Iteration 19000: Loss = -10869.5703125
1
Iteration 19100: Loss = -10869.5703125
2
Iteration 19200: Loss = -10869.5703125
3
Iteration 19300: Loss = -10869.5703125
4
Iteration 19400: Loss = -10869.5712890625
5
Iteration 19500: Loss = -10869.5703125
6
Iteration 19600: Loss = -10869.572265625
7
Iteration 19700: Loss = -10869.5712890625
8
Iteration 19800: Loss = -10869.5703125
9
Iteration 19900: Loss = -10869.5712890625
10
Iteration 20000: Loss = -10869.5712890625
11
Iteration 20100: Loss = -10869.5703125
12
Iteration 20200: Loss = -10869.572265625
13
Iteration 20300: Loss = -10869.5703125
14
Iteration 20400: Loss = -10869.5712890625
15
Stopping early at iteration 20400 due to no improvement.
pi: tensor([[0.0011, 0.9989],
        [0.0859, 0.9141]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.8235e-04, 9.9982e-01], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1460, 0.1314],
         [0.8752, 0.1608]],

        [[0.9145, 0.1324],
         [0.9474, 0.7980]],

        [[0.8800, 0.1548],
         [0.2334, 0.1860]],

        [[0.0094, 0.1537],
         [0.9856, 0.7137]],

        [[0.1824, 0.1713],
         [0.9793, 0.9397]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -57006.3671875
Iteration 100: Loss = -40949.26171875
Iteration 200: Loss = -26622.39453125
Iteration 300: Loss = -16918.37890625
Iteration 400: Loss = -12913.2578125
Iteration 500: Loss = -11599.4541015625
Iteration 600: Loss = -11144.8427734375
Iteration 700: Loss = -10998.2119140625
Iteration 800: Loss = -10944.775390625
Iteration 900: Loss = -10926.0869140625
Iteration 1000: Loss = -10916.8857421875
Iteration 1100: Loss = -10907.7763671875
Iteration 1200: Loss = -10899.67578125
Iteration 1300: Loss = -10891.2294921875
Iteration 1400: Loss = -10888.0908203125
Iteration 1500: Loss = -10882.3505859375
Iteration 1600: Loss = -10880.185546875
Iteration 1700: Loss = -10878.9423828125
Iteration 1800: Loss = -10877.984375
Iteration 1900: Loss = -10877.193359375
Iteration 2000: Loss = -10876.52734375
Iteration 2100: Loss = -10875.955078125
Iteration 2200: Loss = -10875.45703125
Iteration 2300: Loss = -10875.0205078125
Iteration 2400: Loss = -10874.6328125
Iteration 2500: Loss = -10874.291015625
Iteration 2600: Loss = -10873.982421875
Iteration 2700: Loss = -10873.70703125
Iteration 2800: Loss = -10873.45703125
Iteration 2900: Loss = -10873.2294921875
Iteration 3000: Loss = -10873.0224609375
Iteration 3100: Loss = -10872.8359375
Iteration 3200: Loss = -10872.6630859375
Iteration 3300: Loss = -10872.505859375
Iteration 3400: Loss = -10872.3603515625
Iteration 3500: Loss = -10872.2255859375
Iteration 3600: Loss = -10872.1025390625
Iteration 3700: Loss = -10871.9892578125
Iteration 3800: Loss = -10871.8818359375
Iteration 3900: Loss = -10871.7841796875
Iteration 4000: Loss = -10871.6923828125
Iteration 4100: Loss = -10871.607421875
Iteration 4200: Loss = -10871.529296875
Iteration 4300: Loss = -10871.453125
Iteration 4400: Loss = -10871.384765625
Iteration 4500: Loss = -10871.3193359375
Iteration 4600: Loss = -10871.2578125
Iteration 4700: Loss = -10871.2021484375
Iteration 4800: Loss = -10871.1474609375
Iteration 4900: Loss = -10871.09765625
Iteration 5000: Loss = -10871.0498046875
Iteration 5100: Loss = -10871.0068359375
Iteration 5200: Loss = -10870.9638671875
Iteration 5300: Loss = -10870.92578125
Iteration 5400: Loss = -10870.8876953125
Iteration 5500: Loss = -10870.8515625
Iteration 5600: Loss = -10870.8193359375
Iteration 5700: Loss = -10870.7880859375
Iteration 5800: Loss = -10870.7578125
Iteration 5900: Loss = -10870.73046875
Iteration 6000: Loss = -10870.703125
Iteration 6100: Loss = -10870.6767578125
Iteration 6200: Loss = -10870.6552734375
Iteration 6300: Loss = -10870.6318359375
Iteration 6400: Loss = -10870.6123046875
Iteration 6500: Loss = -10870.591796875
Iteration 6600: Loss = -10870.572265625
Iteration 6700: Loss = -10870.552734375
Iteration 6800: Loss = -10870.537109375
Iteration 6900: Loss = -10870.5205078125
Iteration 7000: Loss = -10870.5048828125
Iteration 7100: Loss = -10870.48828125
Iteration 7200: Loss = -10870.4755859375
Iteration 7300: Loss = -10870.462890625
Iteration 7400: Loss = -10870.451171875
Iteration 7500: Loss = -10870.4365234375
Iteration 7600: Loss = -10870.427734375
Iteration 7700: Loss = -10870.416015625
Iteration 7800: Loss = -10870.4052734375
Iteration 7900: Loss = -10870.396484375
Iteration 8000: Loss = -10870.3876953125
Iteration 8100: Loss = -10870.37890625
Iteration 8200: Loss = -10870.3701171875
Iteration 8300: Loss = -10870.3642578125
Iteration 8400: Loss = -10870.3544921875
Iteration 8500: Loss = -10870.34765625
Iteration 8600: Loss = -10870.341796875
Iteration 8700: Loss = -10870.3349609375
Iteration 8800: Loss = -10870.3291015625
Iteration 8900: Loss = -10870.322265625
Iteration 9000: Loss = -10870.3173828125
Iteration 9100: Loss = -10870.3134765625
Iteration 9200: Loss = -10870.306640625
Iteration 9300: Loss = -10870.302734375
Iteration 9400: Loss = -10870.296875
Iteration 9500: Loss = -10870.29296875
Iteration 9600: Loss = -10870.2890625
Iteration 9700: Loss = -10870.28515625
Iteration 9800: Loss = -10870.28125
Iteration 9900: Loss = -10870.2783203125
Iteration 10000: Loss = -10870.275390625
Iteration 10100: Loss = -10870.2734375
Iteration 10200: Loss = -10870.26953125
Iteration 10300: Loss = -10870.2666015625
Iteration 10400: Loss = -10870.2646484375
Iteration 10500: Loss = -10870.2626953125
Iteration 10600: Loss = -10870.2587890625
Iteration 10700: Loss = -10870.2568359375
Iteration 10800: Loss = -10870.2548828125
Iteration 10900: Loss = -10870.2529296875
Iteration 11000: Loss = -10870.251953125
Iteration 11100: Loss = -10870.25
Iteration 11200: Loss = -10870.2470703125
Iteration 11300: Loss = -10870.2470703125
Iteration 11400: Loss = -10870.2431640625
Iteration 11500: Loss = -10870.2431640625
Iteration 11600: Loss = -10870.2421875
Iteration 11700: Loss = -10870.240234375
Iteration 11800: Loss = -10870.2392578125
Iteration 11900: Loss = -10870.2373046875
Iteration 12000: Loss = -10870.236328125
Iteration 12100: Loss = -10870.2353515625
Iteration 12200: Loss = -10870.234375
Iteration 12300: Loss = -10870.232421875
Iteration 12400: Loss = -10870.232421875
Iteration 12500: Loss = -10870.2314453125
Iteration 12600: Loss = -10870.2314453125
Iteration 12700: Loss = -10870.23046875
Iteration 12800: Loss = -10870.228515625
Iteration 12900: Loss = -10870.228515625
Iteration 13000: Loss = -10870.2275390625
Iteration 13100: Loss = -10870.228515625
1
Iteration 13200: Loss = -10870.2255859375
Iteration 13300: Loss = -10870.224609375
Iteration 13400: Loss = -10870.2255859375
1
Iteration 13500: Loss = -10870.2236328125
Iteration 13600: Loss = -10870.2236328125
Iteration 13700: Loss = -10870.22265625
Iteration 13800: Loss = -10870.2236328125
1
Iteration 13900: Loss = -10870.22265625
Iteration 14000: Loss = -10870.2216796875
Iteration 14100: Loss = -10870.2216796875
Iteration 14200: Loss = -10870.220703125
Iteration 14300: Loss = -10870.220703125
Iteration 14400: Loss = -10870.220703125
Iteration 14500: Loss = -10870.2197265625
Iteration 14600: Loss = -10870.2197265625
Iteration 14700: Loss = -10870.220703125
1
Iteration 14800: Loss = -10870.220703125
2
Iteration 14900: Loss = -10870.2197265625
Iteration 15000: Loss = -10870.2197265625
Iteration 15100: Loss = -10870.2197265625
Iteration 15200: Loss = -10870.21875
Iteration 15300: Loss = -10870.2197265625
1
Iteration 15400: Loss = -10870.21875
Iteration 15500: Loss = -10870.21875
Iteration 15600: Loss = -10870.216796875
Iteration 15700: Loss = -10870.21875
1
Iteration 15800: Loss = -10870.21875
2
Iteration 15900: Loss = -10870.216796875
Iteration 16000: Loss = -10870.21875
1
Iteration 16100: Loss = -10870.2177734375
2
Iteration 16200: Loss = -10870.2158203125
Iteration 16300: Loss = -10870.216796875
1
Iteration 16400: Loss = -10870.2158203125
Iteration 16500: Loss = -10870.2158203125
Iteration 16600: Loss = -10870.2177734375
1
Iteration 16700: Loss = -10870.216796875
2
Iteration 16800: Loss = -10870.2177734375
3
Iteration 16900: Loss = -10870.21484375
Iteration 17000: Loss = -10870.216796875
1
Iteration 17100: Loss = -10870.21484375
Iteration 17200: Loss = -10870.21875
1
Iteration 17300: Loss = -10870.21484375
Iteration 17400: Loss = -10870.21484375
Iteration 17500: Loss = -10870.21484375
Iteration 17600: Loss = -10870.21484375
Iteration 17700: Loss = -10870.2158203125
1
Iteration 17800: Loss = -10870.21484375
Iteration 17900: Loss = -10870.21484375
Iteration 18000: Loss = -10870.21484375
Iteration 18100: Loss = -10870.2158203125
1
Iteration 18200: Loss = -10870.2138671875
Iteration 18300: Loss = -10870.21484375
1
Iteration 18400: Loss = -10870.2138671875
Iteration 18500: Loss = -10870.21484375
1
Iteration 18600: Loss = -10870.21484375
2
Iteration 18700: Loss = -10870.21484375
3
Iteration 18800: Loss = -10870.2158203125
4
Iteration 18900: Loss = -10870.21484375
5
Iteration 19000: Loss = -10870.21484375
6
Iteration 19100: Loss = -10870.21484375
7
Iteration 19200: Loss = -10870.21484375
8
Iteration 19300: Loss = -10870.2138671875
Iteration 19400: Loss = -10870.2138671875
Iteration 19500: Loss = -10870.2138671875
Iteration 19600: Loss = -10870.2138671875
Iteration 19700: Loss = -10870.2138671875
Iteration 19800: Loss = -10870.212890625
Iteration 19900: Loss = -10870.2138671875
1
Iteration 20000: Loss = -10870.2138671875
2
Iteration 20100: Loss = -10870.2158203125
3
Iteration 20200: Loss = -10870.21484375
4
Iteration 20300: Loss = -10870.2138671875
5
Iteration 20400: Loss = -10870.2138671875
6
Iteration 20500: Loss = -10870.2158203125
7
Iteration 20600: Loss = -10870.2138671875
8
Iteration 20700: Loss = -10870.212890625
Iteration 20800: Loss = -10870.2138671875
1
Iteration 20900: Loss = -10870.2138671875
2
Iteration 21000: Loss = -10870.2138671875
3
Iteration 21100: Loss = -10870.212890625
Iteration 21200: Loss = -10870.2138671875
1
Iteration 21300: Loss = -10870.2138671875
2
Iteration 21400: Loss = -10870.2138671875
3
Iteration 21500: Loss = -10870.2138671875
4
Iteration 21600: Loss = -10870.2138671875
5
Iteration 21700: Loss = -10870.21484375
6
Iteration 21800: Loss = -10870.2158203125
7
Iteration 21900: Loss = -10870.2138671875
8
Iteration 22000: Loss = -10870.2138671875
9
Iteration 22100: Loss = -10870.2138671875
10
Iteration 22200: Loss = -10870.212890625
Iteration 22300: Loss = -10870.2138671875
1
Iteration 22400: Loss = -10870.2158203125
2
Iteration 22500: Loss = -10870.21484375
3
Iteration 22600: Loss = -10870.2138671875
4
Iteration 22700: Loss = -10870.2138671875
5
Iteration 22800: Loss = -10870.2138671875
6
Iteration 22900: Loss = -10870.216796875
7
Iteration 23000: Loss = -10870.2138671875
8
Iteration 23100: Loss = -10870.2158203125
9
Iteration 23200: Loss = -10870.2138671875
10
Iteration 23300: Loss = -10870.2138671875
11
Iteration 23400: Loss = -10870.2138671875
12
Iteration 23500: Loss = -10870.21484375
13
Iteration 23600: Loss = -10870.2138671875
14
Iteration 23700: Loss = -10870.2138671875
15
Stopping early at iteration 23700 due to no improvement.
pi: tensor([[1.0000e+00, 4.5772e-06],
        [9.8567e-01, 1.4335e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 2.2648e-05], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1597, 0.1648],
         [0.0798, 0.8948]],

        [[0.1467, 0.1388],
         [0.8896, 0.0549]],

        [[0.7229, 0.6088],
         [0.9161, 0.0271]],

        [[0.0281, 0.1558],
         [0.6829, 0.6395]],

        [[0.0544, 0.1767],
         [0.0280, 0.2748]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [10869.5712890625, 10870.2138671875]
