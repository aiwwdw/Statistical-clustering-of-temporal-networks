nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  1%|          | 1/100 [13:34<22:24:15, 814.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  2%|▏         | 2/100 [30:24<25:18:05, 929.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  3%|▎         | 3/100 [48:47<27:10:35, 1008.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  4%|▍         | 4/100 [59:49<23:14:41, 871.68s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  5%|▌         | 5/100 [1:14:39<23:11:03, 878.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  6%|▌         | 6/100 [1:31:23<24:03:14, 921.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  7%|▋         | 7/100 [1:46:08<23:29:30, 909.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  8%|▊         | 8/100 [1:58:27<21:51:10, 855.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
  9%|▉         | 9/100 [2:12:48<21:39:31, 856.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 10%|█         | 10/100 [2:26:51<21:19:05, 852.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 11%|█         | 11/100 [2:38:27<19:53:26, 804.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 12%|█▏        | 12/100 [2:52:48<20:05:05, 821.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 13%|█▎        | 13/100 [3:08:32<20:45:30, 858.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 14%|█▍        | 14/100 [3:19:44<19:10:14, 802.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 15%|█▌        | 15/100 [3:33:47<19:14:06, 814.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 16%|█▌        | 16/100 [3:48:53<19:38:50, 842.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 17%|█▋        | 17/100 [4:07:03<21:07:54, 916.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 18%|█▊        | 18/100 [4:15:54<18:14:21, 800.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 19%|█▉        | 19/100 [4:26:17<16:48:55, 747.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 20%|██        | 20/100 [4:41:06<17:33:10, 789.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 21%|██        | 21/100 [4:58:12<18:53:13, 860.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 22%|██▏       | 22/100 [5:13:29<19:01:04, 877.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 23%|██▎       | 23/100 [5:26:28<18:08:11, 847.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 24%|██▍       | 24/100 [5:38:46<17:12:21, 815.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 25%|██▌       | 25/100 [5:52:38<17:05:22, 820.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 26%|██▌       | 26/100 [6:03:28<15:48:39, 769.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 27%|██▋       | 27/100 [6:20:01<16:57:35, 836.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 28%|██▊       | 28/100 [6:34:06<16:46:39, 838.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 29%|██▉       | 29/100 [6:50:14<17:18:35, 877.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 30%|███       | 30/100 [7:05:46<17:22:54, 893.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 31%|███       | 31/100 [7:23:20<18:03:19, 942.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 32%|███▏      | 32/100 [7:40:47<18:23:12, 973.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 33%|███▎      | 33/100 [7:58:54<18:45:07, 1007.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 34%|███▍      | 34/100 [8:12:27<17:23:56, 949.04s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 35%|███▌      | 35/100 [8:29:01<17:22:56, 962.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11784.263629579336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23967.8671875
inf tensor(23967.8672, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12327.9970703125
tensor(23967.8672, grad_fn=<NegBackward0>) tensor(12327.9971, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12326.365234375
tensor(12327.9971, grad_fn=<NegBackward0>) tensor(12326.3652, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12325.6298828125
tensor(12326.3652, grad_fn=<NegBackward0>) tensor(12325.6299, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12325.39453125
tensor(12325.6299, grad_fn=<NegBackward0>) tensor(12325.3945, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12325.1484375
tensor(12325.3945, grad_fn=<NegBackward0>) tensor(12325.1484, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12324.7490234375
tensor(12325.1484, grad_fn=<NegBackward0>) tensor(12324.7490, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12279.5400390625
tensor(12324.7490, grad_fn=<NegBackward0>) tensor(12279.5400, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12208.98046875
tensor(12279.5400, grad_fn=<NegBackward0>) tensor(12208.9805, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12208.4697265625
tensor(12208.9805, grad_fn=<NegBackward0>) tensor(12208.4697, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12208.142578125
tensor(12208.4697, grad_fn=<NegBackward0>) tensor(12208.1426, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12207.9521484375
tensor(12208.1426, grad_fn=<NegBackward0>) tensor(12207.9521, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12207.810546875
tensor(12207.9521, grad_fn=<NegBackward0>) tensor(12207.8105, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12207.6728515625
tensor(12207.8105, grad_fn=<NegBackward0>) tensor(12207.6729, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12207.5361328125
tensor(12207.6729, grad_fn=<NegBackward0>) tensor(12207.5361, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12207.419921875
tensor(12207.5361, grad_fn=<NegBackward0>) tensor(12207.4199, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12207.34765625
tensor(12207.4199, grad_fn=<NegBackward0>) tensor(12207.3477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12207.3134765625
tensor(12207.3477, grad_fn=<NegBackward0>) tensor(12207.3135, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12207.298828125
tensor(12207.3135, grad_fn=<NegBackward0>) tensor(12207.2988, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12207.2890625
tensor(12207.2988, grad_fn=<NegBackward0>) tensor(12207.2891, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12207.283203125
tensor(12207.2891, grad_fn=<NegBackward0>) tensor(12207.2832, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12207.27734375
tensor(12207.2832, grad_fn=<NegBackward0>) tensor(12207.2773, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12207.2734375
tensor(12207.2773, grad_fn=<NegBackward0>) tensor(12207.2734, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12207.2705078125
tensor(12207.2734, grad_fn=<NegBackward0>) tensor(12207.2705, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12207.2646484375
tensor(12207.2705, grad_fn=<NegBackward0>) tensor(12207.2646, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12207.2529296875
tensor(12207.2646, grad_fn=<NegBackward0>) tensor(12207.2529, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12207.2314453125
tensor(12207.2529, grad_fn=<NegBackward0>) tensor(12207.2314, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12207.2236328125
tensor(12207.2314, grad_fn=<NegBackward0>) tensor(12207.2236, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12207.220703125
tensor(12207.2236, grad_fn=<NegBackward0>) tensor(12207.2207, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12207.2158203125
tensor(12207.2207, grad_fn=<NegBackward0>) tensor(12207.2158, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12207.21484375
tensor(12207.2158, grad_fn=<NegBackward0>) tensor(12207.2148, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12207.2119140625
tensor(12207.2148, grad_fn=<NegBackward0>) tensor(12207.2119, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12207.2119140625
tensor(12207.2119, grad_fn=<NegBackward0>) tensor(12207.2119, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12207.2119140625
tensor(12207.2119, grad_fn=<NegBackward0>) tensor(12207.2119, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12207.2119140625
tensor(12207.2119, grad_fn=<NegBackward0>) tensor(12207.2119, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12207.2080078125
tensor(12207.2119, grad_fn=<NegBackward0>) tensor(12207.2080, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12207.2099609375
tensor(12207.2080, grad_fn=<NegBackward0>) tensor(12207.2100, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12207.208984375
tensor(12207.2080, grad_fn=<NegBackward0>) tensor(12207.2090, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -12207.2080078125
tensor(12207.2080, grad_fn=<NegBackward0>) tensor(12207.2080, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12207.2080078125
tensor(12207.2080, grad_fn=<NegBackward0>) tensor(12207.2080, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12207.20703125
tensor(12207.2080, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12207.2080078125
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2080, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12207.2080078125
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2080, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -12207.20703125
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12207.2060546875
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12207.20703125
tensor(12207.2061, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12207.205078125
tensor(12207.2061, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12207.2060546875
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12207.205078125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12207.20703125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12207.205078125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12207.205078125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12207.205078125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12207.2060546875
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12207.205078125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12207.2060546875
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12207.2041015625
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12207.205078125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12207.205078125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12207.205078125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -12207.2041015625
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12207.205078125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12207.2041015625
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12207.2021484375
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2021, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12207.2041015625
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12207.2109375
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2109, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12207.2041015625
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12207.203125
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -12207.2041015625
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.0262, 0.9738],
        [0.0570, 0.9430]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5597, 0.4403], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3310, 0.0898],
         [0.6072, 0.2038]],

        [[0.5577, 0.1151],
         [0.5936, 0.5549]],

        [[0.5331, 0.1882],
         [0.5403, 0.5699]],

        [[0.6965, 0.2264],
         [0.5794, 0.5927]],

        [[0.5682, 0.1877],
         [0.6091, 0.5135]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.054573442627465676
Average Adjusted Rand Index: 0.20033680801538306
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20880.224609375
inf tensor(20880.2246, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12328.529296875
tensor(20880.2246, grad_fn=<NegBackward0>) tensor(12328.5293, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12327.7744140625
tensor(12328.5293, grad_fn=<NegBackward0>) tensor(12327.7744, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12325.744140625
tensor(12327.7744, grad_fn=<NegBackward0>) tensor(12325.7441, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12325.3515625
tensor(12325.7441, grad_fn=<NegBackward0>) tensor(12325.3516, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12324.96484375
tensor(12325.3516, grad_fn=<NegBackward0>) tensor(12324.9648, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12237.94140625
tensor(12324.9648, grad_fn=<NegBackward0>) tensor(12237.9414, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12209.041015625
tensor(12237.9414, grad_fn=<NegBackward0>) tensor(12209.0410, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12208.765625
tensor(12209.0410, grad_fn=<NegBackward0>) tensor(12208.7656, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12208.5869140625
tensor(12208.7656, grad_fn=<NegBackward0>) tensor(12208.5869, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12208.380859375
tensor(12208.5869, grad_fn=<NegBackward0>) tensor(12208.3809, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12208.08984375
tensor(12208.3809, grad_fn=<NegBackward0>) tensor(12208.0898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12207.8212890625
tensor(12208.0898, grad_fn=<NegBackward0>) tensor(12207.8213, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12207.626953125
tensor(12207.8213, grad_fn=<NegBackward0>) tensor(12207.6270, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12207.5
tensor(12207.6270, grad_fn=<NegBackward0>) tensor(12207.5000, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12207.4296875
tensor(12207.5000, grad_fn=<NegBackward0>) tensor(12207.4297, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12207.380859375
tensor(12207.4297, grad_fn=<NegBackward0>) tensor(12207.3809, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12207.349609375
tensor(12207.3809, grad_fn=<NegBackward0>) tensor(12207.3496, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12207.33203125
tensor(12207.3496, grad_fn=<NegBackward0>) tensor(12207.3320, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12207.31640625
tensor(12207.3320, grad_fn=<NegBackward0>) tensor(12207.3164, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12207.3056640625
tensor(12207.3164, grad_fn=<NegBackward0>) tensor(12207.3057, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12207.296875
tensor(12207.3057, grad_fn=<NegBackward0>) tensor(12207.2969, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12207.2890625
tensor(12207.2969, grad_fn=<NegBackward0>) tensor(12207.2891, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12207.2822265625
tensor(12207.2891, grad_fn=<NegBackward0>) tensor(12207.2822, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12207.2763671875
tensor(12207.2822, grad_fn=<NegBackward0>) tensor(12207.2764, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12207.271484375
tensor(12207.2764, grad_fn=<NegBackward0>) tensor(12207.2715, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12207.2685546875
tensor(12207.2715, grad_fn=<NegBackward0>) tensor(12207.2686, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12207.265625
tensor(12207.2686, grad_fn=<NegBackward0>) tensor(12207.2656, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12207.263671875
tensor(12207.2656, grad_fn=<NegBackward0>) tensor(12207.2637, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12207.2587890625
tensor(12207.2637, grad_fn=<NegBackward0>) tensor(12207.2588, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12207.2578125
tensor(12207.2588, grad_fn=<NegBackward0>) tensor(12207.2578, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12207.2568359375
tensor(12207.2578, grad_fn=<NegBackward0>) tensor(12207.2568, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12207.2548828125
tensor(12207.2568, grad_fn=<NegBackward0>) tensor(12207.2549, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12207.25390625
tensor(12207.2549, grad_fn=<NegBackward0>) tensor(12207.2539, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12207.25
tensor(12207.2539, grad_fn=<NegBackward0>) tensor(12207.2500, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12207.248046875
tensor(12207.2500, grad_fn=<NegBackward0>) tensor(12207.2480, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12207.2353515625
tensor(12207.2480, grad_fn=<NegBackward0>) tensor(12207.2354, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12207.21875
tensor(12207.2354, grad_fn=<NegBackward0>) tensor(12207.2188, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12207.21484375
tensor(12207.2188, grad_fn=<NegBackward0>) tensor(12207.2148, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12207.2109375
tensor(12207.2148, grad_fn=<NegBackward0>) tensor(12207.2109, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12207.208984375
tensor(12207.2109, grad_fn=<NegBackward0>) tensor(12207.2090, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12207.20703125
tensor(12207.2090, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12207.20703125
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12207.20703125
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12207.2060546875
tensor(12207.2070, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12207.20703125
tensor(12207.2061, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12207.20703125
tensor(12207.2061, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12207.20703125
tensor(12207.2061, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -12207.205078125
tensor(12207.2061, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12207.2060546875
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12207.20703125
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12207.2041015625
tensor(12207.2051, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12207.2041015625
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12207.2060546875
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12207.205078125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -12207.20703125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2070, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -12207.2041015625
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12207.2041015625
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12207.203125
tensor(12207.2041, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12207.203125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12207.2060546875
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12207.203125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12207.2041015625
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12207.205078125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2051, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12207.203125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12207.2041015625
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12207.2060546875
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12207.203125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12207.2041015625
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12207.2060546875
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2061, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12207.2041015625
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -12207.203125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12207.3818359375
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.3818, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12207.203125
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12207.21484375
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2148, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12207.2021484375
tensor(12207.2031, grad_fn=<NegBackward0>) tensor(12207.2021, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12207.2119140625
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2119, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12207.203125
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2031, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12207.216796875
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2168, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -12207.2041015625
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2041, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -12207.2236328125
tensor(12207.2021, grad_fn=<NegBackward0>) tensor(12207.2236, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.0262, 0.9738],
        [0.0574, 0.9426]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5596, 0.4404], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3310, 0.0898],
         [0.5154, 0.2041]],

        [[0.6122, 0.1151],
         [0.5839, 0.6876]],

        [[0.6449, 0.1884],
         [0.6704, 0.5758]],

        [[0.5162, 0.2264],
         [0.5985, 0.6682]],

        [[0.6513, 0.1877],
         [0.5094, 0.7075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.054573442627465676
Average Adjusted Rand Index: 0.20033680801538306
[0.054573442627465676, 0.054573442627465676] [0.20033680801538306, 0.20033680801538306] [12207.2041015625, 12207.2236328125]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11819.677384221859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20769.88671875
inf tensor(20769.8867, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12262.44140625
tensor(20769.8867, grad_fn=<NegBackward0>) tensor(12262.4414, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12262.0322265625
tensor(12262.4414, grad_fn=<NegBackward0>) tensor(12262.0322, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12261.9140625
tensor(12262.0322, grad_fn=<NegBackward0>) tensor(12261.9141, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12261.80859375
tensor(12261.9141, grad_fn=<NegBackward0>) tensor(12261.8086, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12261.6826171875
tensor(12261.8086, grad_fn=<NegBackward0>) tensor(12261.6826, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12261.5673828125
tensor(12261.6826, grad_fn=<NegBackward0>) tensor(12261.5674, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12261.478515625
tensor(12261.5674, grad_fn=<NegBackward0>) tensor(12261.4785, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12261.408203125
tensor(12261.4785, grad_fn=<NegBackward0>) tensor(12261.4082, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12261.3427734375
tensor(12261.4082, grad_fn=<NegBackward0>) tensor(12261.3428, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12261.2705078125
tensor(12261.3428, grad_fn=<NegBackward0>) tensor(12261.2705, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12261.2109375
tensor(12261.2705, grad_fn=<NegBackward0>) tensor(12261.2109, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12261.189453125
tensor(12261.2109, grad_fn=<NegBackward0>) tensor(12261.1895, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12261.17578125
tensor(12261.1895, grad_fn=<NegBackward0>) tensor(12261.1758, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12261.1630859375
tensor(12261.1758, grad_fn=<NegBackward0>) tensor(12261.1631, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12261.1513671875
tensor(12261.1631, grad_fn=<NegBackward0>) tensor(12261.1514, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12261.142578125
tensor(12261.1514, grad_fn=<NegBackward0>) tensor(12261.1426, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12261.1298828125
tensor(12261.1426, grad_fn=<NegBackward0>) tensor(12261.1299, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12261.1162109375
tensor(12261.1299, grad_fn=<NegBackward0>) tensor(12261.1162, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12261.0537109375
tensor(12261.1162, grad_fn=<NegBackward0>) tensor(12261.0537, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12260.828125
tensor(12261.0537, grad_fn=<NegBackward0>) tensor(12260.8281, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12260.6728515625
tensor(12260.8281, grad_fn=<NegBackward0>) tensor(12260.6729, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12260.587890625
tensor(12260.6729, grad_fn=<NegBackward0>) tensor(12260.5879, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12260.5390625
tensor(12260.5879, grad_fn=<NegBackward0>) tensor(12260.5391, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12260.5126953125
tensor(12260.5391, grad_fn=<NegBackward0>) tensor(12260.5127, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12260.49609375
tensor(12260.5127, grad_fn=<NegBackward0>) tensor(12260.4961, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12260.484375
tensor(12260.4961, grad_fn=<NegBackward0>) tensor(12260.4844, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12260.474609375
tensor(12260.4844, grad_fn=<NegBackward0>) tensor(12260.4746, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12260.4697265625
tensor(12260.4746, grad_fn=<NegBackward0>) tensor(12260.4697, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12260.4658203125
tensor(12260.4697, grad_fn=<NegBackward0>) tensor(12260.4658, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12260.4619140625
tensor(12260.4658, grad_fn=<NegBackward0>) tensor(12260.4619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12260.4599609375
tensor(12260.4619, grad_fn=<NegBackward0>) tensor(12260.4600, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12260.4560546875
tensor(12260.4600, grad_fn=<NegBackward0>) tensor(12260.4561, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12260.4541015625
tensor(12260.4561, grad_fn=<NegBackward0>) tensor(12260.4541, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12260.4521484375
tensor(12260.4541, grad_fn=<NegBackward0>) tensor(12260.4521, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12260.4501953125
tensor(12260.4521, grad_fn=<NegBackward0>) tensor(12260.4502, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12260.4482421875
tensor(12260.4502, grad_fn=<NegBackward0>) tensor(12260.4482, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12260.447265625
tensor(12260.4482, grad_fn=<NegBackward0>) tensor(12260.4473, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12260.447265625
tensor(12260.4473, grad_fn=<NegBackward0>) tensor(12260.4473, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12260.4443359375
tensor(12260.4473, grad_fn=<NegBackward0>) tensor(12260.4443, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12260.443359375
tensor(12260.4443, grad_fn=<NegBackward0>) tensor(12260.4434, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12260.443359375
tensor(12260.4434, grad_fn=<NegBackward0>) tensor(12260.4434, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12260.4423828125
tensor(12260.4434, grad_fn=<NegBackward0>) tensor(12260.4424, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12260.44140625
tensor(12260.4424, grad_fn=<NegBackward0>) tensor(12260.4414, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12260.4404296875
tensor(12260.4414, grad_fn=<NegBackward0>) tensor(12260.4404, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12260.4404296875
tensor(12260.4404, grad_fn=<NegBackward0>) tensor(12260.4404, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12260.4384765625
tensor(12260.4404, grad_fn=<NegBackward0>) tensor(12260.4385, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12260.439453125
tensor(12260.4385, grad_fn=<NegBackward0>) tensor(12260.4395, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12260.4384765625
tensor(12260.4385, grad_fn=<NegBackward0>) tensor(12260.4385, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12260.4375
tensor(12260.4385, grad_fn=<NegBackward0>) tensor(12260.4375, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12260.4365234375
tensor(12260.4375, grad_fn=<NegBackward0>) tensor(12260.4365, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12260.4345703125
tensor(12260.4365, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12260.4365234375
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4365, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12260.4365234375
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4365, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12260.4365234375
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4365, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -12260.4345703125
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12260.4345703125
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12260.4345703125
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12260.43359375
tensor(12260.4346, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12260.43359375
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12260.4345703125
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12260.43359375
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12260.43359375
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12260.4345703125
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12260.4345703125
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4346, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12260.43359375
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12260.43359375
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12261.0556640625
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12261.0557, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12260.4326171875
tensor(12260.4336, grad_fn=<NegBackward0>) tensor(12260.4326, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12260.43359375
tensor(12260.4326, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12260.4326171875
tensor(12260.4326, grad_fn=<NegBackward0>) tensor(12260.4326, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12260.4326171875
tensor(12260.4326, grad_fn=<NegBackward0>) tensor(12260.4326, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12260.43359375
tensor(12260.4326, grad_fn=<NegBackward0>) tensor(12260.4336, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12261.0849609375
tensor(12260.4326, grad_fn=<NegBackward0>) tensor(12261.0850, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12260.431640625
tensor(12260.4326, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12260.431640625
tensor(12260.4316, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12260.4326171875
tensor(12260.4316, grad_fn=<NegBackward0>) tensor(12260.4326, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12260.431640625
tensor(12260.4316, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12260.4326171875
tensor(12260.4316, grad_fn=<NegBackward0>) tensor(12260.4326, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12260.80078125
tensor(12260.4316, grad_fn=<NegBackward0>) tensor(12260.8008, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12260.4306640625
tensor(12260.4316, grad_fn=<NegBackward0>) tensor(12260.4307, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12260.431640625
tensor(12260.4307, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12260.431640625
tensor(12260.4307, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12260.431640625
tensor(12260.4307, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12260.6220703125
tensor(12260.4307, grad_fn=<NegBackward0>) tensor(12260.6221, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12260.431640625
tensor(12260.4307, grad_fn=<NegBackward0>) tensor(12260.4316, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[2.8799e-04, 9.9971e-01],
        [1.4591e-02, 9.8541e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0200, 0.9800], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2555, 0.2646],
         [0.6868, 0.1942]],

        [[0.6869, 0.2399],
         [0.5064, 0.5079]],

        [[0.6236, 0.2063],
         [0.6687, 0.6752]],

        [[0.6324, 0.3047],
         [0.6858, 0.6554]],

        [[0.6479, 0.3223],
         [0.5458, 0.6851]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0001576638651779441
Average Adjusted Rand Index: -0.000609869589596532
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20273.09375
inf tensor(20273.0938, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12262.1455078125
tensor(20273.0938, grad_fn=<NegBackward0>) tensor(12262.1455, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12261.3056640625
tensor(12262.1455, grad_fn=<NegBackward0>) tensor(12261.3057, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12257.76171875
tensor(12261.3057, grad_fn=<NegBackward0>) tensor(12257.7617, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12257.5673828125
tensor(12257.7617, grad_fn=<NegBackward0>) tensor(12257.5674, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12257.513671875
tensor(12257.5674, grad_fn=<NegBackward0>) tensor(12257.5137, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12257.48046875
tensor(12257.5137, grad_fn=<NegBackward0>) tensor(12257.4805, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12257.447265625
tensor(12257.4805, grad_fn=<NegBackward0>) tensor(12257.4473, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12257.390625
tensor(12257.4473, grad_fn=<NegBackward0>) tensor(12257.3906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12257.298828125
tensor(12257.3906, grad_fn=<NegBackward0>) tensor(12257.2988, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12257.2099609375
tensor(12257.2988, grad_fn=<NegBackward0>) tensor(12257.2100, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12257.146484375
tensor(12257.2100, grad_fn=<NegBackward0>) tensor(12257.1465, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12257.1025390625
tensor(12257.1465, grad_fn=<NegBackward0>) tensor(12257.1025, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12257.072265625
tensor(12257.1025, grad_fn=<NegBackward0>) tensor(12257.0723, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12257.05078125
tensor(12257.0723, grad_fn=<NegBackward0>) tensor(12257.0508, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12257.03515625
tensor(12257.0508, grad_fn=<NegBackward0>) tensor(12257.0352, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12257.021484375
tensor(12257.0352, grad_fn=<NegBackward0>) tensor(12257.0215, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12257.009765625
tensor(12257.0215, grad_fn=<NegBackward0>) tensor(12257.0098, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12257.001953125
tensor(12257.0098, grad_fn=<NegBackward0>) tensor(12257.0020, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12256.9931640625
tensor(12257.0020, grad_fn=<NegBackward0>) tensor(12256.9932, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12256.9814453125
tensor(12256.9932, grad_fn=<NegBackward0>) tensor(12256.9814, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12256.96875
tensor(12256.9814, grad_fn=<NegBackward0>) tensor(12256.9688, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12256.9404296875
tensor(12256.9688, grad_fn=<NegBackward0>) tensor(12256.9404, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12256.861328125
tensor(12256.9404, grad_fn=<NegBackward0>) tensor(12256.8613, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12256.712890625
tensor(12256.8613, grad_fn=<NegBackward0>) tensor(12256.7129, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12256.6220703125
tensor(12256.7129, grad_fn=<NegBackward0>) tensor(12256.6221, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12244.806640625
tensor(12256.6221, grad_fn=<NegBackward0>) tensor(12244.8066, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12193.8525390625
tensor(12244.8066, grad_fn=<NegBackward0>) tensor(12193.8525, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11917.328125
tensor(12193.8525, grad_fn=<NegBackward0>) tensor(11917.3281, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11875.06640625
tensor(11917.3281, grad_fn=<NegBackward0>) tensor(11875.0664, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11864.4443359375
tensor(11875.0664, grad_fn=<NegBackward0>) tensor(11864.4443, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11855.9873046875
tensor(11864.4443, grad_fn=<NegBackward0>) tensor(11855.9873, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11855.935546875
tensor(11855.9873, grad_fn=<NegBackward0>) tensor(11855.9355, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11855.9052734375
tensor(11855.9355, grad_fn=<NegBackward0>) tensor(11855.9053, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11855.8994140625
tensor(11855.9053, grad_fn=<NegBackward0>) tensor(11855.8994, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11855.8935546875
tensor(11855.8994, grad_fn=<NegBackward0>) tensor(11855.8936, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11855.8896484375
tensor(11855.8936, grad_fn=<NegBackward0>) tensor(11855.8896, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11855.884765625
tensor(11855.8896, grad_fn=<NegBackward0>) tensor(11855.8848, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11855.8828125
tensor(11855.8848, grad_fn=<NegBackward0>) tensor(11855.8828, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11855.8759765625
tensor(11855.8828, grad_fn=<NegBackward0>) tensor(11855.8760, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11855.861328125
tensor(11855.8760, grad_fn=<NegBackward0>) tensor(11855.8613, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11848.716796875
tensor(11855.8613, grad_fn=<NegBackward0>) tensor(11848.7168, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11848.705078125
tensor(11848.7168, grad_fn=<NegBackward0>) tensor(11848.7051, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11848.66796875
tensor(11848.7051, grad_fn=<NegBackward0>) tensor(11848.6680, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11848.66796875
tensor(11848.6680, grad_fn=<NegBackward0>) tensor(11848.6680, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11848.6669921875
tensor(11848.6680, grad_fn=<NegBackward0>) tensor(11848.6670, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11848.666015625
tensor(11848.6670, grad_fn=<NegBackward0>) tensor(11848.6660, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11848.6650390625
tensor(11848.6660, grad_fn=<NegBackward0>) tensor(11848.6650, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11848.6630859375
tensor(11848.6650, grad_fn=<NegBackward0>) tensor(11848.6631, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11848.6630859375
tensor(11848.6631, grad_fn=<NegBackward0>) tensor(11848.6631, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11848.662109375
tensor(11848.6631, grad_fn=<NegBackward0>) tensor(11848.6621, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11848.662109375
tensor(11848.6621, grad_fn=<NegBackward0>) tensor(11848.6621, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11848.6611328125
tensor(11848.6621, grad_fn=<NegBackward0>) tensor(11848.6611, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11848.671875
tensor(11848.6611, grad_fn=<NegBackward0>) tensor(11848.6719, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11848.6591796875
tensor(11848.6611, grad_fn=<NegBackward0>) tensor(11848.6592, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11848.404296875
tensor(11848.6592, grad_fn=<NegBackward0>) tensor(11848.4043, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11848.4033203125
tensor(11848.4043, grad_fn=<NegBackward0>) tensor(11848.4033, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11848.4052734375
tensor(11848.4033, grad_fn=<NegBackward0>) tensor(11848.4053, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11848.404296875
tensor(11848.4033, grad_fn=<NegBackward0>) tensor(11848.4043, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11848.4033203125
tensor(11848.4033, grad_fn=<NegBackward0>) tensor(11848.4033, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11848.4033203125
tensor(11848.4033, grad_fn=<NegBackward0>) tensor(11848.4033, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11848.40234375
tensor(11848.4033, grad_fn=<NegBackward0>) tensor(11848.4023, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11848.3671875
tensor(11848.4023, grad_fn=<NegBackward0>) tensor(11848.3672, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11848.365234375
tensor(11848.3672, grad_fn=<NegBackward0>) tensor(11848.3652, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11848.3642578125
tensor(11848.3652, grad_fn=<NegBackward0>) tensor(11848.3643, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11848.3642578125
tensor(11848.3643, grad_fn=<NegBackward0>) tensor(11848.3643, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11848.3662109375
tensor(11848.3643, grad_fn=<NegBackward0>) tensor(11848.3662, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11848.3642578125
tensor(11848.3643, grad_fn=<NegBackward0>) tensor(11848.3643, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11833.1484375
tensor(11848.3643, grad_fn=<NegBackward0>) tensor(11833.1484, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11833.1484375
tensor(11833.1484, grad_fn=<NegBackward0>) tensor(11833.1484, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11833.1484375
tensor(11833.1484, grad_fn=<NegBackward0>) tensor(11833.1484, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11833.1552734375
tensor(11833.1484, grad_fn=<NegBackward0>) tensor(11833.1553, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11833.1474609375
tensor(11833.1484, grad_fn=<NegBackward0>) tensor(11833.1475, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11833.1455078125
tensor(11833.1475, grad_fn=<NegBackward0>) tensor(11833.1455, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11833.1455078125
tensor(11833.1455, grad_fn=<NegBackward0>) tensor(11833.1455, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11833.146484375
tensor(11833.1455, grad_fn=<NegBackward0>) tensor(11833.1465, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11833.1455078125
tensor(11833.1455, grad_fn=<NegBackward0>) tensor(11833.1455, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11833.14453125
tensor(11833.1455, grad_fn=<NegBackward0>) tensor(11833.1445, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11833.1484375
tensor(11833.1445, grad_fn=<NegBackward0>) tensor(11833.1484, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11833.185546875
tensor(11833.1445, grad_fn=<NegBackward0>) tensor(11833.1855, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11833.1455078125
tensor(11833.1445, grad_fn=<NegBackward0>) tensor(11833.1455, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11833.134765625
tensor(11833.1445, grad_fn=<NegBackward0>) tensor(11833.1348, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11833.134765625
tensor(11833.1348, grad_fn=<NegBackward0>) tensor(11833.1348, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11833.1337890625
tensor(11833.1348, grad_fn=<NegBackward0>) tensor(11833.1338, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11816.1142578125
tensor(11833.1338, grad_fn=<NegBackward0>) tensor(11816.1143, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11816.111328125
tensor(11816.1143, grad_fn=<NegBackward0>) tensor(11816.1113, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11816.11328125
tensor(11816.1113, grad_fn=<NegBackward0>) tensor(11816.1133, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11816.1171875
tensor(11816.1113, grad_fn=<NegBackward0>) tensor(11816.1172, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11816.16015625
tensor(11816.1113, grad_fn=<NegBackward0>) tensor(11816.1602, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11816.1044921875
tensor(11816.1113, grad_fn=<NegBackward0>) tensor(11816.1045, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11816.107421875
tensor(11816.1045, grad_fn=<NegBackward0>) tensor(11816.1074, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11816.1123046875
tensor(11816.1045, grad_fn=<NegBackward0>) tensor(11816.1123, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11816.1044921875
tensor(11816.1045, grad_fn=<NegBackward0>) tensor(11816.1045, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11816.115234375
tensor(11816.1045, grad_fn=<NegBackward0>) tensor(11816.1152, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11816.0712890625
tensor(11816.1045, grad_fn=<NegBackward0>) tensor(11816.0713, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11816.072265625
tensor(11816.0713, grad_fn=<NegBackward0>) tensor(11816.0723, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11816.078125
tensor(11816.0713, grad_fn=<NegBackward0>) tensor(11816.0781, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11816.0703125
tensor(11816.0713, grad_fn=<NegBackward0>) tensor(11816.0703, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11816.07421875
tensor(11816.0703, grad_fn=<NegBackward0>) tensor(11816.0742, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11816.087890625
tensor(11816.0703, grad_fn=<NegBackward0>) tensor(11816.0879, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7629, 0.2371],
        [0.2754, 0.7246]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4486, 0.5514], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2858, 0.0984],
         [0.5779, 0.2986]],

        [[0.6802, 0.0938],
         [0.5694, 0.6794]],

        [[0.6078, 0.1106],
         [0.6845, 0.7287]],

        [[0.6047, 0.1034],
         [0.7280, 0.6701]],

        [[0.5515, 0.1032],
         [0.7211, 0.5771]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9524808202447418
Average Adjusted Rand Index: 0.9524837513757015
[-0.0001576638651779441, 0.9524808202447418] [-0.000609869589596532, 0.9524837513757015] [12260.431640625, 11816.0810546875]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11711.539729957021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23678.318359375
inf tensor(23678.3184, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12230.2197265625
tensor(23678.3184, grad_fn=<NegBackward0>) tensor(12230.2197, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12229.75
tensor(12230.2197, grad_fn=<NegBackward0>) tensor(12229.7500, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12229.666015625
tensor(12229.7500, grad_fn=<NegBackward0>) tensor(12229.6660, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12229.6181640625
tensor(12229.6660, grad_fn=<NegBackward0>) tensor(12229.6182, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12229.587890625
tensor(12229.6182, grad_fn=<NegBackward0>) tensor(12229.5879, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12229.564453125
tensor(12229.5879, grad_fn=<NegBackward0>) tensor(12229.5645, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12229.55078125
tensor(12229.5645, grad_fn=<NegBackward0>) tensor(12229.5508, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12229.537109375
tensor(12229.5508, grad_fn=<NegBackward0>) tensor(12229.5371, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12229.5283203125
tensor(12229.5371, grad_fn=<NegBackward0>) tensor(12229.5283, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12229.51953125
tensor(12229.5283, grad_fn=<NegBackward0>) tensor(12229.5195, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12229.51171875
tensor(12229.5195, grad_fn=<NegBackward0>) tensor(12229.5117, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12229.5048828125
tensor(12229.5117, grad_fn=<NegBackward0>) tensor(12229.5049, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12229.5
tensor(12229.5049, grad_fn=<NegBackward0>) tensor(12229.5000, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12229.494140625
tensor(12229.5000, grad_fn=<NegBackward0>) tensor(12229.4941, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12229.48828125
tensor(12229.4941, grad_fn=<NegBackward0>) tensor(12229.4883, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12229.482421875
tensor(12229.4883, grad_fn=<NegBackward0>) tensor(12229.4824, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12229.4775390625
tensor(12229.4824, grad_fn=<NegBackward0>) tensor(12229.4775, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12229.470703125
tensor(12229.4775, grad_fn=<NegBackward0>) tensor(12229.4707, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12229.466796875
tensor(12229.4707, grad_fn=<NegBackward0>) tensor(12229.4668, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12229.462890625
tensor(12229.4668, grad_fn=<NegBackward0>) tensor(12229.4629, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12229.45703125
tensor(12229.4629, grad_fn=<NegBackward0>) tensor(12229.4570, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12229.453125
tensor(12229.4570, grad_fn=<NegBackward0>) tensor(12229.4531, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12229.4482421875
tensor(12229.4531, grad_fn=<NegBackward0>) tensor(12229.4482, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12229.443359375
tensor(12229.4482, grad_fn=<NegBackward0>) tensor(12229.4434, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12229.439453125
tensor(12229.4434, grad_fn=<NegBackward0>) tensor(12229.4395, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12229.4345703125
tensor(12229.4395, grad_fn=<NegBackward0>) tensor(12229.4346, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12229.4296875
tensor(12229.4346, grad_fn=<NegBackward0>) tensor(12229.4297, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12229.4267578125
tensor(12229.4297, grad_fn=<NegBackward0>) tensor(12229.4268, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12229.419921875
tensor(12229.4268, grad_fn=<NegBackward0>) tensor(12229.4199, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12229.416015625
tensor(12229.4199, grad_fn=<NegBackward0>) tensor(12229.4160, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12229.4111328125
tensor(12229.4160, grad_fn=<NegBackward0>) tensor(12229.4111, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12229.408203125
tensor(12229.4111, grad_fn=<NegBackward0>) tensor(12229.4082, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12229.40625
tensor(12229.4082, grad_fn=<NegBackward0>) tensor(12229.4062, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12229.404296875
tensor(12229.4062, grad_fn=<NegBackward0>) tensor(12229.4043, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12229.40234375
tensor(12229.4043, grad_fn=<NegBackward0>) tensor(12229.4023, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12229.400390625
tensor(12229.4023, grad_fn=<NegBackward0>) tensor(12229.4004, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12229.4013671875
tensor(12229.4004, grad_fn=<NegBackward0>) tensor(12229.4014, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12229.3994140625
tensor(12229.4004, grad_fn=<NegBackward0>) tensor(12229.3994, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12229.3984375
tensor(12229.3994, grad_fn=<NegBackward0>) tensor(12229.3984, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12229.3994140625
tensor(12229.3984, grad_fn=<NegBackward0>) tensor(12229.3994, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12229.3994140625
tensor(12229.3984, grad_fn=<NegBackward0>) tensor(12229.3994, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12229.3984375
tensor(12229.3984, grad_fn=<NegBackward0>) tensor(12229.3984, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12229.3974609375
tensor(12229.3984, grad_fn=<NegBackward0>) tensor(12229.3975, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12229.3974609375
tensor(12229.3975, grad_fn=<NegBackward0>) tensor(12229.3975, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12229.396484375
tensor(12229.3975, grad_fn=<NegBackward0>) tensor(12229.3965, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12229.3955078125
tensor(12229.3965, grad_fn=<NegBackward0>) tensor(12229.3955, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12229.396484375
tensor(12229.3955, grad_fn=<NegBackward0>) tensor(12229.3965, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12229.396484375
tensor(12229.3955, grad_fn=<NegBackward0>) tensor(12229.3965, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -12229.3955078125
tensor(12229.3955, grad_fn=<NegBackward0>) tensor(12229.3955, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12229.39453125
tensor(12229.3955, grad_fn=<NegBackward0>) tensor(12229.3945, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12229.3955078125
tensor(12229.3945, grad_fn=<NegBackward0>) tensor(12229.3955, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12229.392578125
tensor(12229.3945, grad_fn=<NegBackward0>) tensor(12229.3926, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12229.3935546875
tensor(12229.3926, grad_fn=<NegBackward0>) tensor(12229.3936, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12229.390625
tensor(12229.3926, grad_fn=<NegBackward0>) tensor(12229.3906, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12229.3916015625
tensor(12229.3906, grad_fn=<NegBackward0>) tensor(12229.3916, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12229.3857421875
tensor(12229.3906, grad_fn=<NegBackward0>) tensor(12229.3857, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12229.3828125
tensor(12229.3857, grad_fn=<NegBackward0>) tensor(12229.3828, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12229.3759765625
tensor(12229.3828, grad_fn=<NegBackward0>) tensor(12229.3760, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12229.3671875
tensor(12229.3760, grad_fn=<NegBackward0>) tensor(12229.3672, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12229.353515625
tensor(12229.3672, grad_fn=<NegBackward0>) tensor(12229.3535, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12229.3359375
tensor(12229.3535, grad_fn=<NegBackward0>) tensor(12229.3359, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12229.3212890625
tensor(12229.3359, grad_fn=<NegBackward0>) tensor(12229.3213, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12229.3095703125
tensor(12229.3213, grad_fn=<NegBackward0>) tensor(12229.3096, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12229.3193359375
tensor(12229.3096, grad_fn=<NegBackward0>) tensor(12229.3193, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12229.294921875
tensor(12229.3096, grad_fn=<NegBackward0>) tensor(12229.2949, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12229.29296875
tensor(12229.2949, grad_fn=<NegBackward0>) tensor(12229.2930, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12229.2900390625
tensor(12229.2930, grad_fn=<NegBackward0>) tensor(12229.2900, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12229.287109375
tensor(12229.2900, grad_fn=<NegBackward0>) tensor(12229.2871, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12229.28515625
tensor(12229.2871, grad_fn=<NegBackward0>) tensor(12229.2852, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12229.2841796875
tensor(12229.2852, grad_fn=<NegBackward0>) tensor(12229.2842, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12229.283203125
tensor(12229.2842, grad_fn=<NegBackward0>) tensor(12229.2832, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12229.3212890625
tensor(12229.2832, grad_fn=<NegBackward0>) tensor(12229.3213, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12229.2783203125
tensor(12229.2832, grad_fn=<NegBackward0>) tensor(12229.2783, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12229.27734375
tensor(12229.2783, grad_fn=<NegBackward0>) tensor(12229.2773, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12229.2763671875
tensor(12229.2773, grad_fn=<NegBackward0>) tensor(12229.2764, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12229.2744140625
tensor(12229.2764, grad_fn=<NegBackward0>) tensor(12229.2744, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12229.2734375
tensor(12229.2744, grad_fn=<NegBackward0>) tensor(12229.2734, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12229.2724609375
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2725, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12229.2724609375
tensor(12229.2725, grad_fn=<NegBackward0>) tensor(12229.2725, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12229.2705078125
tensor(12229.2725, grad_fn=<NegBackward0>) tensor(12229.2705, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12229.2685546875
tensor(12229.2705, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12229.267578125
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2676, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12229.2685546875
tensor(12229.2676, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12229.2685546875
tensor(12229.2676, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -12229.28125
tensor(12229.2676, grad_fn=<NegBackward0>) tensor(12229.2812, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -12229.2666015625
tensor(12229.2676, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12229.2685546875
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12229.267578125
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2676, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12229.2666015625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12229.2744140625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2744, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12229.2666015625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12229.267578125
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2676, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12229.2666015625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12229.265625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2656, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12229.2685546875
tensor(12229.2656, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12229.2666015625
tensor(12229.2656, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12229.265625
tensor(12229.2656, grad_fn=<NegBackward0>) tensor(12229.2656, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12229.271484375
tensor(12229.2656, grad_fn=<NegBackward0>) tensor(12229.2715, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12229.265625
tensor(12229.2656, grad_fn=<NegBackward0>) tensor(12229.2656, grad_fn=<NegBackward0>)
pi: tensor([[3.0008e-01, 6.9992e-01],
        [2.7900e-04, 9.9972e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8757, 0.1243], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.1977],
         [0.6996, 0.1943]],

        [[0.5639, 0.1916],
         [0.6206, 0.6847]],

        [[0.6946, 0.2023],
         [0.5682, 0.5705]],

        [[0.6228, 0.2527],
         [0.5266, 0.6634]],

        [[0.6811, 0.2112],
         [0.6033, 0.5220]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0010286999242137477
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19209.57421875
inf tensor(19209.5742, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12229.865234375
tensor(19209.5742, grad_fn=<NegBackward0>) tensor(12229.8652, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12229.58203125
tensor(12229.8652, grad_fn=<NegBackward0>) tensor(12229.5820, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12229.533203125
tensor(12229.5820, grad_fn=<NegBackward0>) tensor(12229.5332, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12229.51171875
tensor(12229.5332, grad_fn=<NegBackward0>) tensor(12229.5117, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12229.4990234375
tensor(12229.5117, grad_fn=<NegBackward0>) tensor(12229.4990, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12229.4853515625
tensor(12229.4990, grad_fn=<NegBackward0>) tensor(12229.4854, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12229.4765625
tensor(12229.4854, grad_fn=<NegBackward0>) tensor(12229.4766, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12229.466796875
tensor(12229.4766, grad_fn=<NegBackward0>) tensor(12229.4668, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12229.4560546875
tensor(12229.4668, grad_fn=<NegBackward0>) tensor(12229.4561, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12229.4443359375
tensor(12229.4561, grad_fn=<NegBackward0>) tensor(12229.4443, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12229.4326171875
tensor(12229.4443, grad_fn=<NegBackward0>) tensor(12229.4326, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12229.421875
tensor(12229.4326, grad_fn=<NegBackward0>) tensor(12229.4219, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12229.4111328125
tensor(12229.4219, grad_fn=<NegBackward0>) tensor(12229.4111, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12229.4033203125
tensor(12229.4111, grad_fn=<NegBackward0>) tensor(12229.4033, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12229.3984375
tensor(12229.4033, grad_fn=<NegBackward0>) tensor(12229.3984, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12229.3935546875
tensor(12229.3984, grad_fn=<NegBackward0>) tensor(12229.3936, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12229.392578125
tensor(12229.3936, grad_fn=<NegBackward0>) tensor(12229.3926, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12229.3876953125
tensor(12229.3926, grad_fn=<NegBackward0>) tensor(12229.3877, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12229.3857421875
tensor(12229.3877, grad_fn=<NegBackward0>) tensor(12229.3857, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12229.3828125
tensor(12229.3857, grad_fn=<NegBackward0>) tensor(12229.3828, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12229.3818359375
tensor(12229.3828, grad_fn=<NegBackward0>) tensor(12229.3818, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12229.37890625
tensor(12229.3818, grad_fn=<NegBackward0>) tensor(12229.3789, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12229.375
tensor(12229.3789, grad_fn=<NegBackward0>) tensor(12229.3750, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12229.3720703125
tensor(12229.3750, grad_fn=<NegBackward0>) tensor(12229.3721, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12229.3671875
tensor(12229.3721, grad_fn=<NegBackward0>) tensor(12229.3672, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12229.3603515625
tensor(12229.3672, grad_fn=<NegBackward0>) tensor(12229.3604, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12229.353515625
tensor(12229.3604, grad_fn=<NegBackward0>) tensor(12229.3535, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12229.345703125
tensor(12229.3535, grad_fn=<NegBackward0>) tensor(12229.3457, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12229.337890625
tensor(12229.3457, grad_fn=<NegBackward0>) tensor(12229.3379, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12229.3310546875
tensor(12229.3379, grad_fn=<NegBackward0>) tensor(12229.3311, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12229.3232421875
tensor(12229.3311, grad_fn=<NegBackward0>) tensor(12229.3232, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12229.318359375
tensor(12229.3232, grad_fn=<NegBackward0>) tensor(12229.3184, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12229.314453125
tensor(12229.3184, grad_fn=<NegBackward0>) tensor(12229.3145, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12229.3076171875
tensor(12229.3145, grad_fn=<NegBackward0>) tensor(12229.3076, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12229.3046875
tensor(12229.3076, grad_fn=<NegBackward0>) tensor(12229.3047, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12229.30078125
tensor(12229.3047, grad_fn=<NegBackward0>) tensor(12229.3008, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12229.2998046875
tensor(12229.3008, grad_fn=<NegBackward0>) tensor(12229.2998, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12229.2978515625
tensor(12229.2998, grad_fn=<NegBackward0>) tensor(12229.2979, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12229.296875
tensor(12229.2979, grad_fn=<NegBackward0>) tensor(12229.2969, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12229.294921875
tensor(12229.2969, grad_fn=<NegBackward0>) tensor(12229.2949, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12229.29296875
tensor(12229.2949, grad_fn=<NegBackward0>) tensor(12229.2930, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12229.2919921875
tensor(12229.2930, grad_fn=<NegBackward0>) tensor(12229.2920, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12229.291015625
tensor(12229.2920, grad_fn=<NegBackward0>) tensor(12229.2910, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12229.2900390625
tensor(12229.2910, grad_fn=<NegBackward0>) tensor(12229.2900, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12229.2900390625
tensor(12229.2900, grad_fn=<NegBackward0>) tensor(12229.2900, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12229.2890625
tensor(12229.2900, grad_fn=<NegBackward0>) tensor(12229.2891, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12229.2880859375
tensor(12229.2891, grad_fn=<NegBackward0>) tensor(12229.2881, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12229.287109375
tensor(12229.2881, grad_fn=<NegBackward0>) tensor(12229.2871, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12229.287109375
tensor(12229.2871, grad_fn=<NegBackward0>) tensor(12229.2871, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12229.28515625
tensor(12229.2871, grad_fn=<NegBackward0>) tensor(12229.2852, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12229.2861328125
tensor(12229.2852, grad_fn=<NegBackward0>) tensor(12229.2861, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12229.28515625
tensor(12229.2852, grad_fn=<NegBackward0>) tensor(12229.2852, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12229.283203125
tensor(12229.2852, grad_fn=<NegBackward0>) tensor(12229.2832, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12229.2841796875
tensor(12229.2832, grad_fn=<NegBackward0>) tensor(12229.2842, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12229.2841796875
tensor(12229.2832, grad_fn=<NegBackward0>) tensor(12229.2842, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12229.283203125
tensor(12229.2832, grad_fn=<NegBackward0>) tensor(12229.2832, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12229.2822265625
tensor(12229.2832, grad_fn=<NegBackward0>) tensor(12229.2822, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12229.2841796875
tensor(12229.2822, grad_fn=<NegBackward0>) tensor(12229.2842, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12229.28125
tensor(12229.2822, grad_fn=<NegBackward0>) tensor(12229.2812, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12229.28125
tensor(12229.2812, grad_fn=<NegBackward0>) tensor(12229.2812, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12229.2802734375
tensor(12229.2812, grad_fn=<NegBackward0>) tensor(12229.2803, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12229.2802734375
tensor(12229.2803, grad_fn=<NegBackward0>) tensor(12229.2803, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12229.2822265625
tensor(12229.2803, grad_fn=<NegBackward0>) tensor(12229.2822, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12229.279296875
tensor(12229.2803, grad_fn=<NegBackward0>) tensor(12229.2793, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12229.279296875
tensor(12229.2793, grad_fn=<NegBackward0>) tensor(12229.2793, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12229.2783203125
tensor(12229.2793, grad_fn=<NegBackward0>) tensor(12229.2783, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12229.28125
tensor(12229.2783, grad_fn=<NegBackward0>) tensor(12229.2812, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12229.2763671875
tensor(12229.2783, grad_fn=<NegBackward0>) tensor(12229.2764, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12229.279296875
tensor(12229.2764, grad_fn=<NegBackward0>) tensor(12229.2793, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12229.2734375
tensor(12229.2764, grad_fn=<NegBackward0>) tensor(12229.2734, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12229.2734375
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2734, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12229.2734375
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2734, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12229.2744140625
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2744, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12229.2900390625
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2900, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12229.2744140625
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2744, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12229.2724609375
tensor(12229.2734, grad_fn=<NegBackward0>) tensor(12229.2725, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12229.26953125
tensor(12229.2725, grad_fn=<NegBackward0>) tensor(12229.2695, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12229.2705078125
tensor(12229.2695, grad_fn=<NegBackward0>) tensor(12229.2705, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12229.2685546875
tensor(12229.2695, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12229.4267578125
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.4268, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12229.2685546875
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12229.2685546875
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12229.2685546875
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12229.2705078125
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2705, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12229.2685546875
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12229.2734375
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2734, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12229.2666015625
tensor(12229.2686, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12229.349609375
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.3496, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12229.2666015625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12229.267578125
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2676, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12229.271484375
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2715, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12229.267578125
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2676, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12229.3056640625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.3057, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -12229.2666015625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12229.2685546875
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2686, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12229.2666015625
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2666, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12229.2646484375
tensor(12229.2666, grad_fn=<NegBackward0>) tensor(12229.2646, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12229.2841796875
tensor(12229.2646, grad_fn=<NegBackward0>) tensor(12229.2842, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12229.2744140625
tensor(12229.2646, grad_fn=<NegBackward0>) tensor(12229.2744, grad_fn=<NegBackward0>)
2
pi: tensor([[2.9843e-01, 7.0157e-01],
        [2.4505e-04, 9.9975e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8971, 0.1029], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.1978],
         [0.5383, 0.1944]],

        [[0.5598, 0.1916],
         [0.6572, 0.6751]],

        [[0.6197, 0.2022],
         [0.5537, 0.6439]],

        [[0.6547, 0.2528],
         [0.6206, 0.6323]],

        [[0.6773, 0.2113],
         [0.6786, 0.5958]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0010286999242137477
Average Adjusted Rand Index: 0.0
[0.0010286999242137477, 0.0010286999242137477] [0.0, 0.0] [12229.267578125, 12229.265625]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11765.60719486485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20315.494140625
inf tensor(20315.4941, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12312.7421875
tensor(20315.4941, grad_fn=<NegBackward0>) tensor(12312.7422, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12311.9775390625
tensor(12312.7422, grad_fn=<NegBackward0>) tensor(12311.9775, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12311.0908203125
tensor(12311.9775, grad_fn=<NegBackward0>) tensor(12311.0908, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12310.2666015625
tensor(12311.0908, grad_fn=<NegBackward0>) tensor(12310.2666, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12309.4033203125
tensor(12310.2666, grad_fn=<NegBackward0>) tensor(12309.4033, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12308.66015625
tensor(12309.4033, grad_fn=<NegBackward0>) tensor(12308.6602, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12308.2744140625
tensor(12308.6602, grad_fn=<NegBackward0>) tensor(12308.2744, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12307.9150390625
tensor(12308.2744, grad_fn=<NegBackward0>) tensor(12307.9150, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12307.611328125
tensor(12307.9150, grad_fn=<NegBackward0>) tensor(12307.6113, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12307.390625
tensor(12307.6113, grad_fn=<NegBackward0>) tensor(12307.3906, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12307.24609375
tensor(12307.3906, grad_fn=<NegBackward0>) tensor(12307.2461, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12307.1552734375
tensor(12307.2461, grad_fn=<NegBackward0>) tensor(12307.1553, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12307.08984375
tensor(12307.1553, grad_fn=<NegBackward0>) tensor(12307.0898, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12307.0244140625
tensor(12307.0898, grad_fn=<NegBackward0>) tensor(12307.0244, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12306.916015625
tensor(12307.0244, grad_fn=<NegBackward0>) tensor(12306.9160, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12306.759765625
tensor(12306.9160, grad_fn=<NegBackward0>) tensor(12306.7598, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12306.6669921875
tensor(12306.7598, grad_fn=<NegBackward0>) tensor(12306.6670, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12306.595703125
tensor(12306.6670, grad_fn=<NegBackward0>) tensor(12306.5957, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12306.5322265625
tensor(12306.5957, grad_fn=<NegBackward0>) tensor(12306.5322, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12306.46875
tensor(12306.5322, grad_fn=<NegBackward0>) tensor(12306.4688, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12306.3955078125
tensor(12306.4688, grad_fn=<NegBackward0>) tensor(12306.3955, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12306.3017578125
tensor(12306.3955, grad_fn=<NegBackward0>) tensor(12306.3018, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12306.201171875
tensor(12306.3018, grad_fn=<NegBackward0>) tensor(12306.2012, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12306.1259765625
tensor(12306.2012, grad_fn=<NegBackward0>) tensor(12306.1260, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12306.091796875
tensor(12306.1260, grad_fn=<NegBackward0>) tensor(12306.0918, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12306.078125
tensor(12306.0918, grad_fn=<NegBackward0>) tensor(12306.0781, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12306.0712890625
tensor(12306.0781, grad_fn=<NegBackward0>) tensor(12306.0713, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12306.0673828125
tensor(12306.0713, grad_fn=<NegBackward0>) tensor(12306.0674, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12306.0654296875
tensor(12306.0674, grad_fn=<NegBackward0>) tensor(12306.0654, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12306.0615234375
tensor(12306.0654, grad_fn=<NegBackward0>) tensor(12306.0615, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12306.0595703125
tensor(12306.0615, grad_fn=<NegBackward0>) tensor(12306.0596, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12306.056640625
tensor(12306.0596, grad_fn=<NegBackward0>) tensor(12306.0566, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12306.068359375
tensor(12306.0566, grad_fn=<NegBackward0>) tensor(12306.0684, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12306.0537109375
tensor(12306.0566, grad_fn=<NegBackward0>) tensor(12306.0537, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12306.052734375
tensor(12306.0537, grad_fn=<NegBackward0>) tensor(12306.0527, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12306.0517578125
tensor(12306.0527, grad_fn=<NegBackward0>) tensor(12306.0518, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12306.0498046875
tensor(12306.0518, grad_fn=<NegBackward0>) tensor(12306.0498, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12306.0498046875
tensor(12306.0498, grad_fn=<NegBackward0>) tensor(12306.0498, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12306.0478515625
tensor(12306.0498, grad_fn=<NegBackward0>) tensor(12306.0479, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12306.0458984375
tensor(12306.0479, grad_fn=<NegBackward0>) tensor(12306.0459, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12306.046875
tensor(12306.0459, grad_fn=<NegBackward0>) tensor(12306.0469, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12306.0458984375
tensor(12306.0459, grad_fn=<NegBackward0>) tensor(12306.0459, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12306.046875
tensor(12306.0459, grad_fn=<NegBackward0>) tensor(12306.0469, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12306.044921875
tensor(12306.0459, grad_fn=<NegBackward0>) tensor(12306.0449, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12306.044921875
tensor(12306.0449, grad_fn=<NegBackward0>) tensor(12306.0449, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12306.044921875
tensor(12306.0449, grad_fn=<NegBackward0>) tensor(12306.0449, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12306.0439453125
tensor(12306.0449, grad_fn=<NegBackward0>) tensor(12306.0439, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12306.0458984375
tensor(12306.0439, grad_fn=<NegBackward0>) tensor(12306.0459, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12306.0419921875
tensor(12306.0439, grad_fn=<NegBackward0>) tensor(12306.0420, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12306.041015625
tensor(12306.0420, grad_fn=<NegBackward0>) tensor(12306.0410, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12306.041015625
tensor(12306.0410, grad_fn=<NegBackward0>) tensor(12306.0410, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12306.0419921875
tensor(12306.0410, grad_fn=<NegBackward0>) tensor(12306.0420, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12306.0400390625
tensor(12306.0410, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12306.041015625
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0410, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12306.07421875
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0742, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12306.0400390625
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12306.0400390625
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12306.0390625
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0391, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12306.0380859375
tensor(12306.0391, grad_fn=<NegBackward0>) tensor(12306.0381, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12306.0439453125
tensor(12306.0381, grad_fn=<NegBackward0>) tensor(12306.0439, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12306.0400390625
tensor(12306.0381, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -12306.0400390625
tensor(12306.0381, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -12306.0400390625
tensor(12306.0381, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -12306.0400390625
tensor(12306.0381, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[1.0242e-04, 9.9990e-01],
        [4.0062e-01, 5.9938e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9581, 0.0419], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2140, 0.1842],
         [0.5635, 0.1882]],

        [[0.5700, 0.0914],
         [0.5980, 0.5325]],

        [[0.5684, 0.2032],
         [0.6820, 0.6415]],

        [[0.6029, 0.2128],
         [0.5213, 0.7213]],

        [[0.6069, 0.1983],
         [0.6995, 0.5015]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.006722961540734027
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019696076451945015
Average Adjusted Rand Index: -0.0014463759727449637
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23328.24609375
inf tensor(23328.2461, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12312.4033203125
tensor(23328.2461, grad_fn=<NegBackward0>) tensor(12312.4033, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12311.708984375
tensor(12312.4033, grad_fn=<NegBackward0>) tensor(12311.7090, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12310.8623046875
tensor(12311.7090, grad_fn=<NegBackward0>) tensor(12310.8623, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12309.8388671875
tensor(12310.8623, grad_fn=<NegBackward0>) tensor(12309.8389, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12309.3720703125
tensor(12309.8389, grad_fn=<NegBackward0>) tensor(12309.3721, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12309.080078125
tensor(12309.3721, grad_fn=<NegBackward0>) tensor(12309.0801, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12308.7373046875
tensor(12309.0801, grad_fn=<NegBackward0>) tensor(12308.7373, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12308.29296875
tensor(12308.7373, grad_fn=<NegBackward0>) tensor(12308.2930, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12307.830078125
tensor(12308.2930, grad_fn=<NegBackward0>) tensor(12307.8301, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12307.3828125
tensor(12307.8301, grad_fn=<NegBackward0>) tensor(12307.3828, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12307.0478515625
tensor(12307.3828, grad_fn=<NegBackward0>) tensor(12307.0479, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12306.8349609375
tensor(12307.0479, grad_fn=<NegBackward0>) tensor(12306.8350, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12306.7021484375
tensor(12306.8350, grad_fn=<NegBackward0>) tensor(12306.7021, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12306.607421875
tensor(12306.7021, grad_fn=<NegBackward0>) tensor(12306.6074, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12306.537109375
tensor(12306.6074, grad_fn=<NegBackward0>) tensor(12306.5371, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12306.4716796875
tensor(12306.5371, grad_fn=<NegBackward0>) tensor(12306.4717, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12306.404296875
tensor(12306.4717, grad_fn=<NegBackward0>) tensor(12306.4043, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12306.3203125
tensor(12306.4043, grad_fn=<NegBackward0>) tensor(12306.3203, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12306.212890625
tensor(12306.3203, grad_fn=<NegBackward0>) tensor(12306.2129, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12306.119140625
tensor(12306.2129, grad_fn=<NegBackward0>) tensor(12306.1191, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12306.07421875
tensor(12306.1191, grad_fn=<NegBackward0>) tensor(12306.0742, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12306.0595703125
tensor(12306.0742, grad_fn=<NegBackward0>) tensor(12306.0596, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12306.0546875
tensor(12306.0596, grad_fn=<NegBackward0>) tensor(12306.0547, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12306.052734375
tensor(12306.0547, grad_fn=<NegBackward0>) tensor(12306.0527, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12306.05078125
tensor(12306.0527, grad_fn=<NegBackward0>) tensor(12306.0508, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12306.048828125
tensor(12306.0508, grad_fn=<NegBackward0>) tensor(12306.0488, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12306.0478515625
tensor(12306.0488, grad_fn=<NegBackward0>) tensor(12306.0479, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12306.046875
tensor(12306.0479, grad_fn=<NegBackward0>) tensor(12306.0469, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12306.0458984375
tensor(12306.0469, grad_fn=<NegBackward0>) tensor(12306.0459, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12306.072265625
tensor(12306.0459, grad_fn=<NegBackward0>) tensor(12306.0723, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -12306.044921875
tensor(12306.0459, grad_fn=<NegBackward0>) tensor(12306.0449, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12306.0439453125
tensor(12306.0449, grad_fn=<NegBackward0>) tensor(12306.0439, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12306.04296875
tensor(12306.0439, grad_fn=<NegBackward0>) tensor(12306.0430, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12306.04296875
tensor(12306.0430, grad_fn=<NegBackward0>) tensor(12306.0430, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12306.0478515625
tensor(12306.0430, grad_fn=<NegBackward0>) tensor(12306.0479, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12306.041015625
tensor(12306.0430, grad_fn=<NegBackward0>) tensor(12306.0410, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12306.041015625
tensor(12306.0410, grad_fn=<NegBackward0>) tensor(12306.0410, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12306.041015625
tensor(12306.0410, grad_fn=<NegBackward0>) tensor(12306.0410, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12306.0400390625
tensor(12306.0410, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12306.0830078125
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0830, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12306.0400390625
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0400, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12306.0390625
tensor(12306.0400, grad_fn=<NegBackward0>) tensor(12306.0391, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12306.0390625
tensor(12306.0391, grad_fn=<NegBackward0>) tensor(12306.0391, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12306.0390625
tensor(12306.0391, grad_fn=<NegBackward0>) tensor(12306.0391, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12306.0380859375
tensor(12306.0391, grad_fn=<NegBackward0>) tensor(12306.0381, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12306.037109375
tensor(12306.0381, grad_fn=<NegBackward0>) tensor(12306.0371, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12306.0380859375
tensor(12306.0371, grad_fn=<NegBackward0>) tensor(12306.0381, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12306.0390625
tensor(12306.0371, grad_fn=<NegBackward0>) tensor(12306.0391, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -12306.037109375
tensor(12306.0371, grad_fn=<NegBackward0>) tensor(12306.0371, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12306.0380859375
tensor(12306.0371, grad_fn=<NegBackward0>) tensor(12306.0381, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12306.0380859375
tensor(12306.0371, grad_fn=<NegBackward0>) tensor(12306.0381, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12306.0361328125
tensor(12306.0371, grad_fn=<NegBackward0>) tensor(12306.0361, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12306.037109375
tensor(12306.0361, grad_fn=<NegBackward0>) tensor(12306.0371, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12306.037109375
tensor(12306.0361, grad_fn=<NegBackward0>) tensor(12306.0371, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -12306.037109375
tensor(12306.0361, grad_fn=<NegBackward0>) tensor(12306.0371, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -12306.0791015625
tensor(12306.0361, grad_fn=<NegBackward0>) tensor(12306.0791, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -12306.037109375
tensor(12306.0361, grad_fn=<NegBackward0>) tensor(12306.0371, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[6.3184e-05, 9.9994e-01],
        [4.0202e-01, 5.9798e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9581, 0.0419], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2140, 0.1842],
         [0.5721, 0.1882]],

        [[0.5489, 0.0914],
         [0.7113, 0.5773]],

        [[0.5091, 0.2032],
         [0.5025, 0.5762]],

        [[0.5935, 0.2128],
         [0.6995, 0.6314]],

        [[0.6747, 0.1983],
         [0.6505, 0.7011]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.006944868242789664
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001921235247104867
Average Adjusted Rand Index: -0.001490757313156091
[-0.0019696076451945015, -0.001921235247104867] [-0.0014463759727449637, -0.001490757313156091] [12306.0400390625, 12306.037109375]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11939.454871638682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22114.451171875
inf tensor(22114.4512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12409.0830078125
tensor(22114.4512, grad_fn=<NegBackward0>) tensor(12409.0830, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12408.7275390625
tensor(12409.0830, grad_fn=<NegBackward0>) tensor(12408.7275, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12408.6162109375
tensor(12408.7275, grad_fn=<NegBackward0>) tensor(12408.6162, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12408.541015625
tensor(12408.6162, grad_fn=<NegBackward0>) tensor(12408.5410, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12408.4853515625
tensor(12408.5410, grad_fn=<NegBackward0>) tensor(12408.4854, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12408.4404296875
tensor(12408.4854, grad_fn=<NegBackward0>) tensor(12408.4404, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12408.404296875
tensor(12408.4404, grad_fn=<NegBackward0>) tensor(12408.4043, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12408.373046875
tensor(12408.4043, grad_fn=<NegBackward0>) tensor(12408.3730, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12408.3447265625
tensor(12408.3730, grad_fn=<NegBackward0>) tensor(12408.3447, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12408.3154296875
tensor(12408.3447, grad_fn=<NegBackward0>) tensor(12408.3154, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12408.283203125
tensor(12408.3154, grad_fn=<NegBackward0>) tensor(12408.2832, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12408.2099609375
tensor(12408.2832, grad_fn=<NegBackward0>) tensor(12408.2100, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12405.4169921875
tensor(12408.2100, grad_fn=<NegBackward0>) tensor(12405.4170, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12402.865234375
tensor(12405.4170, grad_fn=<NegBackward0>) tensor(12402.8652, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12402.3662109375
tensor(12402.8652, grad_fn=<NegBackward0>) tensor(12402.3662, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12402.0732421875
tensor(12402.3662, grad_fn=<NegBackward0>) tensor(12402.0732, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12401.87109375
tensor(12402.0732, grad_fn=<NegBackward0>) tensor(12401.8711, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12401.7060546875
tensor(12401.8711, grad_fn=<NegBackward0>) tensor(12401.7061, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12401.541015625
tensor(12401.7061, grad_fn=<NegBackward0>) tensor(12401.5410, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12401.3515625
tensor(12401.5410, grad_fn=<NegBackward0>) tensor(12401.3516, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12401.1337890625
tensor(12401.3516, grad_fn=<NegBackward0>) tensor(12401.1338, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12400.904296875
tensor(12401.1338, grad_fn=<NegBackward0>) tensor(12400.9043, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12400.6689453125
tensor(12400.9043, grad_fn=<NegBackward0>) tensor(12400.6689, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12400.458984375
tensor(12400.6689, grad_fn=<NegBackward0>) tensor(12400.4590, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12400.291015625
tensor(12400.4590, grad_fn=<NegBackward0>) tensor(12400.2910, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12400.1611328125
tensor(12400.2910, grad_fn=<NegBackward0>) tensor(12400.1611, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12400.046875
tensor(12400.1611, grad_fn=<NegBackward0>) tensor(12400.0469, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12399.95703125
tensor(12400.0469, grad_fn=<NegBackward0>) tensor(12399.9570, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12399.89453125
tensor(12399.9570, grad_fn=<NegBackward0>) tensor(12399.8945, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12399.8505859375
tensor(12399.8945, grad_fn=<NegBackward0>) tensor(12399.8506, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12399.818359375
tensor(12399.8506, grad_fn=<NegBackward0>) tensor(12399.8184, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12399.7958984375
tensor(12399.8184, grad_fn=<NegBackward0>) tensor(12399.7959, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12399.78125
tensor(12399.7959, grad_fn=<NegBackward0>) tensor(12399.7812, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12399.767578125
tensor(12399.7812, grad_fn=<NegBackward0>) tensor(12399.7676, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12399.7568359375
tensor(12399.7676, grad_fn=<NegBackward0>) tensor(12399.7568, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12399.7490234375
tensor(12399.7568, grad_fn=<NegBackward0>) tensor(12399.7490, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12399.7421875
tensor(12399.7490, grad_fn=<NegBackward0>) tensor(12399.7422, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12399.7373046875
tensor(12399.7422, grad_fn=<NegBackward0>) tensor(12399.7373, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12399.732421875
tensor(12399.7373, grad_fn=<NegBackward0>) tensor(12399.7324, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12399.728515625
tensor(12399.7324, grad_fn=<NegBackward0>) tensor(12399.7285, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12399.724609375
tensor(12399.7285, grad_fn=<NegBackward0>) tensor(12399.7246, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12399.7236328125
tensor(12399.7246, grad_fn=<NegBackward0>) tensor(12399.7236, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12399.7216796875
tensor(12399.7236, grad_fn=<NegBackward0>) tensor(12399.7217, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12399.7197265625
tensor(12399.7217, grad_fn=<NegBackward0>) tensor(12399.7197, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12399.7158203125
tensor(12399.7197, grad_fn=<NegBackward0>) tensor(12399.7158, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12399.7158203125
tensor(12399.7158, grad_fn=<NegBackward0>) tensor(12399.7158, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12399.71484375
tensor(12399.7158, grad_fn=<NegBackward0>) tensor(12399.7148, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12399.7236328125
tensor(12399.7148, grad_fn=<NegBackward0>) tensor(12399.7236, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12399.7119140625
tensor(12399.7148, grad_fn=<NegBackward0>) tensor(12399.7119, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12399.7099609375
tensor(12399.7119, grad_fn=<NegBackward0>) tensor(12399.7100, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12399.7099609375
tensor(12399.7100, grad_fn=<NegBackward0>) tensor(12399.7100, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12399.708984375
tensor(12399.7100, grad_fn=<NegBackward0>) tensor(12399.7090, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12399.708984375
tensor(12399.7090, grad_fn=<NegBackward0>) tensor(12399.7090, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12399.70703125
tensor(12399.7090, grad_fn=<NegBackward0>) tensor(12399.7070, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12399.70703125
tensor(12399.7070, grad_fn=<NegBackward0>) tensor(12399.7070, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12399.7060546875
tensor(12399.7070, grad_fn=<NegBackward0>) tensor(12399.7061, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12399.705078125
tensor(12399.7061, grad_fn=<NegBackward0>) tensor(12399.7051, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12399.7041015625
tensor(12399.7051, grad_fn=<NegBackward0>) tensor(12399.7041, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12399.703125
tensor(12399.7041, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12399.703125
tensor(12399.7031, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12399.703125
tensor(12399.7031, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12399.703125
tensor(12399.7031, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12399.7021484375
tensor(12399.7031, grad_fn=<NegBackward0>) tensor(12399.7021, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12399.7021484375
tensor(12399.7021, grad_fn=<NegBackward0>) tensor(12399.7021, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12399.7001953125
tensor(12399.7021, grad_fn=<NegBackward0>) tensor(12399.7002, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12399.701171875
tensor(12399.7002, grad_fn=<NegBackward0>) tensor(12399.7012, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12399.701171875
tensor(12399.7002, grad_fn=<NegBackward0>) tensor(12399.7012, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12399.701171875
tensor(12399.7002, grad_fn=<NegBackward0>) tensor(12399.7012, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12399.7001953125
tensor(12399.7002, grad_fn=<NegBackward0>) tensor(12399.7002, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12399.7001953125
tensor(12399.7002, grad_fn=<NegBackward0>) tensor(12399.7002, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12399.6982421875
tensor(12399.7002, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12399.7041015625
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.7041, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12399.7001953125
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.7002, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12399.69921875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12399.69921875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -12399.697265625
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12399.720703125
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.7207, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12399.697265625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12399.697265625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12399.6982421875
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12399.697265625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12399.69921875
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12399.6982421875
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12399.708984375
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.7090, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12399.6962890625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6963, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12399.6982421875
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12399.845703125
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.8457, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12399.6962890625
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.6963, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12399.7041015625
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.7041, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12399.703125
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12399.7509765625
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.7510, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12399.7001953125
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.7002, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -12399.697265625
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[4.2394e-01, 5.7606e-01],
        [3.4632e-04, 9.9965e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9383, 0.0617], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.1902],
         [0.5278, 0.2064]],

        [[0.5326, 0.2065],
         [0.5255, 0.5477]],

        [[0.6201, 0.2051],
         [0.5960, 0.5600]],

        [[0.6460, 0.1687],
         [0.6523, 0.6699]],

        [[0.5638, 0.0787],
         [0.6685, 0.5541]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.02523388575915466
Global Adjusted Rand Index: 0.011497628133745769
Average Adjusted Rand Index: 0.008761537266816758
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20341.208984375
inf tensor(20341.2090, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12408.681640625
tensor(20341.2090, grad_fn=<NegBackward0>) tensor(12408.6816, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12408.298828125
tensor(12408.6816, grad_fn=<NegBackward0>) tensor(12408.2988, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12407.0625
tensor(12408.2988, grad_fn=<NegBackward0>) tensor(12407.0625, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12403.0078125
tensor(12407.0625, grad_fn=<NegBackward0>) tensor(12403.0078, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12402.4306640625
tensor(12403.0078, grad_fn=<NegBackward0>) tensor(12402.4307, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12402.2109375
tensor(12402.4307, grad_fn=<NegBackward0>) tensor(12402.2109, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12402.0068359375
tensor(12402.2109, grad_fn=<NegBackward0>) tensor(12402.0068, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12401.7783203125
tensor(12402.0068, grad_fn=<NegBackward0>) tensor(12401.7783, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12401.5419921875
tensor(12401.7783, grad_fn=<NegBackward0>) tensor(12401.5420, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12401.1748046875
tensor(12401.5420, grad_fn=<NegBackward0>) tensor(12401.1748, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12400.6884765625
tensor(12401.1748, grad_fn=<NegBackward0>) tensor(12400.6885, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12400.3662109375
tensor(12400.6885, grad_fn=<NegBackward0>) tensor(12400.3662, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12400.1142578125
tensor(12400.3662, grad_fn=<NegBackward0>) tensor(12400.1143, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12399.9599609375
tensor(12400.1143, grad_fn=<NegBackward0>) tensor(12399.9600, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12399.875
tensor(12399.9600, grad_fn=<NegBackward0>) tensor(12399.8750, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12399.8232421875
tensor(12399.8750, grad_fn=<NegBackward0>) tensor(12399.8232, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12399.7919921875
tensor(12399.8232, grad_fn=<NegBackward0>) tensor(12399.7920, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12399.76953125
tensor(12399.7920, grad_fn=<NegBackward0>) tensor(12399.7695, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12399.7548828125
tensor(12399.7695, grad_fn=<NegBackward0>) tensor(12399.7549, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12399.744140625
tensor(12399.7549, grad_fn=<NegBackward0>) tensor(12399.7441, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12399.7373046875
tensor(12399.7441, grad_fn=<NegBackward0>) tensor(12399.7373, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12399.73046875
tensor(12399.7373, grad_fn=<NegBackward0>) tensor(12399.7305, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12399.7255859375
tensor(12399.7305, grad_fn=<NegBackward0>) tensor(12399.7256, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12399.7236328125
tensor(12399.7256, grad_fn=<NegBackward0>) tensor(12399.7236, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12399.7197265625
tensor(12399.7236, grad_fn=<NegBackward0>) tensor(12399.7197, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12399.7177734375
tensor(12399.7197, grad_fn=<NegBackward0>) tensor(12399.7178, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12399.71484375
tensor(12399.7178, grad_fn=<NegBackward0>) tensor(12399.7148, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12399.7138671875
tensor(12399.7148, grad_fn=<NegBackward0>) tensor(12399.7139, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12399.7119140625
tensor(12399.7139, grad_fn=<NegBackward0>) tensor(12399.7119, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12399.7109375
tensor(12399.7119, grad_fn=<NegBackward0>) tensor(12399.7109, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12399.7099609375
tensor(12399.7109, grad_fn=<NegBackward0>) tensor(12399.7100, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12399.7109375
tensor(12399.7100, grad_fn=<NegBackward0>) tensor(12399.7109, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12399.70703125
tensor(12399.7100, grad_fn=<NegBackward0>) tensor(12399.7070, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12399.7060546875
tensor(12399.7070, grad_fn=<NegBackward0>) tensor(12399.7061, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12399.7060546875
tensor(12399.7061, grad_fn=<NegBackward0>) tensor(12399.7061, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12399.705078125
tensor(12399.7061, grad_fn=<NegBackward0>) tensor(12399.7051, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12399.7041015625
tensor(12399.7051, grad_fn=<NegBackward0>) tensor(12399.7041, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12399.7041015625
tensor(12399.7041, grad_fn=<NegBackward0>) tensor(12399.7041, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12399.70703125
tensor(12399.7041, grad_fn=<NegBackward0>) tensor(12399.7070, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12399.7021484375
tensor(12399.7041, grad_fn=<NegBackward0>) tensor(12399.7021, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12399.703125
tensor(12399.7021, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12399.7021484375
tensor(12399.7021, grad_fn=<NegBackward0>) tensor(12399.7021, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12399.7158203125
tensor(12399.7021, grad_fn=<NegBackward0>) tensor(12399.7158, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12399.701171875
tensor(12399.7021, grad_fn=<NegBackward0>) tensor(12399.7012, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12399.69921875
tensor(12399.7012, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12399.69921875
tensor(12399.6992, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12399.69921875
tensor(12399.6992, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12399.7001953125
tensor(12399.6992, grad_fn=<NegBackward0>) tensor(12399.7002, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12399.6982421875
tensor(12399.6992, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12399.69921875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12399.6982421875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12399.69921875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6992, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12399.6982421875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12399.6982421875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12399.6982421875
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12399.697265625
tensor(12399.6982, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12399.6982421875
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12399.697265625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12399.697265625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12399.697265625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12399.708984375
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.7090, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12399.6962890625
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6963, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12399.703125
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12399.6953125
tensor(12399.6963, grad_fn=<NegBackward0>) tensor(12399.6953, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12399.703125
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.7031, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12399.6953125
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.6953, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12399.6982421875
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.6982, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12399.697265625
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12399.6962890625
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.6963, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12399.7041015625
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.7041, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -12399.697265625
tensor(12399.6953, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[9.9966e-01, 3.4196e-04],
        [5.7700e-01, 4.2300e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0612, 0.9388], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2063, 0.1902],
         [0.5698, 0.1942]],

        [[0.5034, 0.2066],
         [0.6935, 0.6208]],

        [[0.6731, 0.2051],
         [0.5300, 0.6930]],

        [[0.5008, 0.1687],
         [0.6937, 0.6671]],

        [[0.6742, 0.0787],
         [0.7142, 0.7172]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.00038912871648324923
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.02523388575915466
Global Adjusted Rand Index: 0.011497628133745769
Average Adjusted Rand Index: 0.008761537266816758
[0.011497628133745769, 0.011497628133745769] [0.008761537266816758, 0.008761537266816758] [12399.697265625, 12399.697265625]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11749.172768219172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20637.20703125
inf tensor(20637.2070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12277.8955078125
tensor(20637.2070, grad_fn=<NegBackward0>) tensor(12277.8955, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12277.4287109375
tensor(12277.8955, grad_fn=<NegBackward0>) tensor(12277.4287, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12277.0966796875
tensor(12277.4287, grad_fn=<NegBackward0>) tensor(12277.0967, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12276.275390625
tensor(12277.0967, grad_fn=<NegBackward0>) tensor(12276.2754, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12274.6083984375
tensor(12276.2754, grad_fn=<NegBackward0>) tensor(12274.6084, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12273.828125
tensor(12274.6084, grad_fn=<NegBackward0>) tensor(12273.8281, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12273.369140625
tensor(12273.8281, grad_fn=<NegBackward0>) tensor(12273.3691, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12272.994140625
tensor(12273.3691, grad_fn=<NegBackward0>) tensor(12272.9941, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12272.724609375
tensor(12272.9941, grad_fn=<NegBackward0>) tensor(12272.7246, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12272.4990234375
tensor(12272.7246, grad_fn=<NegBackward0>) tensor(12272.4990, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12272.2978515625
tensor(12272.4990, grad_fn=<NegBackward0>) tensor(12272.2979, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12272.1298828125
tensor(12272.2979, grad_fn=<NegBackward0>) tensor(12272.1299, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12271.9560546875
tensor(12272.1299, grad_fn=<NegBackward0>) tensor(12271.9561, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12271.7197265625
tensor(12271.9561, grad_fn=<NegBackward0>) tensor(12271.7197, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12271.4755859375
tensor(12271.7197, grad_fn=<NegBackward0>) tensor(12271.4756, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12271.326171875
tensor(12271.4756, grad_fn=<NegBackward0>) tensor(12271.3262, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12271.234375
tensor(12271.3262, grad_fn=<NegBackward0>) tensor(12271.2344, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12271.1640625
tensor(12271.2344, grad_fn=<NegBackward0>) tensor(12271.1641, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12271.09765625
tensor(12271.1641, grad_fn=<NegBackward0>) tensor(12271.0977, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12271.0361328125
tensor(12271.0977, grad_fn=<NegBackward0>) tensor(12271.0361, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12270.982421875
tensor(12271.0361, grad_fn=<NegBackward0>) tensor(12270.9824, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12270.935546875
tensor(12270.9824, grad_fn=<NegBackward0>) tensor(12270.9355, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12270.8955078125
tensor(12270.9355, grad_fn=<NegBackward0>) tensor(12270.8955, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12270.8623046875
tensor(12270.8955, grad_fn=<NegBackward0>) tensor(12270.8623, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12270.833984375
tensor(12270.8623, grad_fn=<NegBackward0>) tensor(12270.8340, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12270.8134765625
tensor(12270.8340, grad_fn=<NegBackward0>) tensor(12270.8135, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12270.7958984375
tensor(12270.8135, grad_fn=<NegBackward0>) tensor(12270.7959, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12270.7783203125
tensor(12270.7959, grad_fn=<NegBackward0>) tensor(12270.7783, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12270.767578125
tensor(12270.7783, grad_fn=<NegBackward0>) tensor(12270.7676, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12270.755859375
tensor(12270.7676, grad_fn=<NegBackward0>) tensor(12270.7559, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12270.748046875
tensor(12270.7559, grad_fn=<NegBackward0>) tensor(12270.7480, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12270.73828125
tensor(12270.7480, grad_fn=<NegBackward0>) tensor(12270.7383, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12270.734375
tensor(12270.7383, grad_fn=<NegBackward0>) tensor(12270.7344, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12270.728515625
tensor(12270.7344, grad_fn=<NegBackward0>) tensor(12270.7285, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12270.72265625
tensor(12270.7285, grad_fn=<NegBackward0>) tensor(12270.7227, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12270.7177734375
tensor(12270.7227, grad_fn=<NegBackward0>) tensor(12270.7178, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12270.71484375
tensor(12270.7178, grad_fn=<NegBackward0>) tensor(12270.7148, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12270.708984375
tensor(12270.7148, grad_fn=<NegBackward0>) tensor(12270.7090, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12270.70703125
tensor(12270.7090, grad_fn=<NegBackward0>) tensor(12270.7070, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12270.7021484375
tensor(12270.7070, grad_fn=<NegBackward0>) tensor(12270.7021, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12270.7001953125
tensor(12270.7021, grad_fn=<NegBackward0>) tensor(12270.7002, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12270.69921875
tensor(12270.7002, grad_fn=<NegBackward0>) tensor(12270.6992, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12270.697265625
tensor(12270.6992, grad_fn=<NegBackward0>) tensor(12270.6973, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12270.6953125
tensor(12270.6973, grad_fn=<NegBackward0>) tensor(12270.6953, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12270.69140625
tensor(12270.6953, grad_fn=<NegBackward0>) tensor(12270.6914, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12270.6884765625
tensor(12270.6914, grad_fn=<NegBackward0>) tensor(12270.6885, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12270.689453125
tensor(12270.6885, grad_fn=<NegBackward0>) tensor(12270.6895, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12270.6865234375
tensor(12270.6885, grad_fn=<NegBackward0>) tensor(12270.6865, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12270.6845703125
tensor(12270.6865, grad_fn=<NegBackward0>) tensor(12270.6846, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12270.6845703125
tensor(12270.6846, grad_fn=<NegBackward0>) tensor(12270.6846, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12270.6826171875
tensor(12270.6846, grad_fn=<NegBackward0>) tensor(12270.6826, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12270.681640625
tensor(12270.6826, grad_fn=<NegBackward0>) tensor(12270.6816, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12270.6806640625
tensor(12270.6816, grad_fn=<NegBackward0>) tensor(12270.6807, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12270.6796875
tensor(12270.6807, grad_fn=<NegBackward0>) tensor(12270.6797, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12270.6787109375
tensor(12270.6797, grad_fn=<NegBackward0>) tensor(12270.6787, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12270.677734375
tensor(12270.6787, grad_fn=<NegBackward0>) tensor(12270.6777, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12270.6767578125
tensor(12270.6777, grad_fn=<NegBackward0>) tensor(12270.6768, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12270.67578125
tensor(12270.6768, grad_fn=<NegBackward0>) tensor(12270.6758, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12270.6748046875
tensor(12270.6758, grad_fn=<NegBackward0>) tensor(12270.6748, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12270.6748046875
tensor(12270.6748, grad_fn=<NegBackward0>) tensor(12270.6748, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12270.673828125
tensor(12270.6748, grad_fn=<NegBackward0>) tensor(12270.6738, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12270.6728515625
tensor(12270.6738, grad_fn=<NegBackward0>) tensor(12270.6729, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12270.671875
tensor(12270.6729, grad_fn=<NegBackward0>) tensor(12270.6719, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12270.6728515625
tensor(12270.6719, grad_fn=<NegBackward0>) tensor(12270.6729, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12270.671875
tensor(12270.6719, grad_fn=<NegBackward0>) tensor(12270.6719, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12270.8779296875
tensor(12270.6719, grad_fn=<NegBackward0>) tensor(12270.8779, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12270.669921875
tensor(12270.6719, grad_fn=<NegBackward0>) tensor(12270.6699, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12270.671875
tensor(12270.6699, grad_fn=<NegBackward0>) tensor(12270.6719, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12270.6689453125
tensor(12270.6699, grad_fn=<NegBackward0>) tensor(12270.6689, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12270.6953125
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6953, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12270.6689453125
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6689, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12270.669921875
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6699, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12271.6474609375
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12271.6475, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12270.6669921875
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6670, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12270.66796875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12270.66796875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12270.66796875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -12270.66796875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -12270.6669921875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6670, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12270.666015625
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12270.7412109375
tensor(12270.6660, grad_fn=<NegBackward0>) tensor(12270.7412, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12270.66796875
tensor(12270.6660, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12270.6650390625
tensor(12270.6660, grad_fn=<NegBackward0>) tensor(12270.6650, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12270.666015625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12270.6650390625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6650, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12270.6650390625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6650, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12270.6640625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6641, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12270.6640625
tensor(12270.6641, grad_fn=<NegBackward0>) tensor(12270.6641, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12270.6669921875
tensor(12270.6641, grad_fn=<NegBackward0>) tensor(12270.6670, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12270.666015625
tensor(12270.6641, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12270.6650390625
tensor(12270.6641, grad_fn=<NegBackward0>) tensor(12270.6650, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12270.6640625
tensor(12270.6641, grad_fn=<NegBackward0>) tensor(12270.6641, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12270.6630859375
tensor(12270.6641, grad_fn=<NegBackward0>) tensor(12270.6631, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12270.6630859375
tensor(12270.6631, grad_fn=<NegBackward0>) tensor(12270.6631, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12270.6640625
tensor(12270.6631, grad_fn=<NegBackward0>) tensor(12270.6641, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12270.6845703125
tensor(12270.6631, grad_fn=<NegBackward0>) tensor(12270.6846, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12270.6640625
tensor(12270.6631, grad_fn=<NegBackward0>) tensor(12270.6641, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -12270.6630859375
tensor(12270.6631, grad_fn=<NegBackward0>) tensor(12270.6631, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12270.6630859375
tensor(12270.6631, grad_fn=<NegBackward0>) tensor(12270.6631, grad_fn=<NegBackward0>)
pi: tensor([[1.0000e+00, 3.8743e-06],
        [4.5211e-04, 9.9955e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9813, 0.0187], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1972, 0.1547],
         [0.6653, 0.2049]],

        [[0.5872, 0.2220],
         [0.5203, 0.6723]],

        [[0.7302, 0.1834],
         [0.6162, 0.7108]],

        [[0.6896, 0.0873],
         [0.5906, 0.6240]],

        [[0.5889, 0.2987],
         [0.6254, 0.5218]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: 0.0011536996915455686
Average Adjusted Rand Index: 0.0012779021416470114
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21164.828125
inf tensor(21164.8281, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12278.3017578125
tensor(21164.8281, grad_fn=<NegBackward0>) tensor(12278.3018, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12277.5771484375
tensor(12278.3018, grad_fn=<NegBackward0>) tensor(12277.5771, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12277.2275390625
tensor(12277.5771, grad_fn=<NegBackward0>) tensor(12277.2275, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12276.8330078125
tensor(12277.2275, grad_fn=<NegBackward0>) tensor(12276.8330, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12276.552734375
tensor(12276.8330, grad_fn=<NegBackward0>) tensor(12276.5527, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12276.2568359375
tensor(12276.5527, grad_fn=<NegBackward0>) tensor(12276.2568, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12275.109375
tensor(12276.2568, grad_fn=<NegBackward0>) tensor(12275.1094, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12273.966796875
tensor(12275.1094, grad_fn=<NegBackward0>) tensor(12273.9668, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12273.537109375
tensor(12273.9668, grad_fn=<NegBackward0>) tensor(12273.5371, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12273.16015625
tensor(12273.5371, grad_fn=<NegBackward0>) tensor(12273.1602, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12272.767578125
tensor(12273.1602, grad_fn=<NegBackward0>) tensor(12272.7676, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12272.4619140625
tensor(12272.7676, grad_fn=<NegBackward0>) tensor(12272.4619, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12272.2041015625
tensor(12272.4619, grad_fn=<NegBackward0>) tensor(12272.2041, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12271.9228515625
tensor(12272.2041, grad_fn=<NegBackward0>) tensor(12271.9229, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12271.5927734375
tensor(12271.9229, grad_fn=<NegBackward0>) tensor(12271.5928, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12271.3798828125
tensor(12271.5928, grad_fn=<NegBackward0>) tensor(12271.3799, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12271.24609375
tensor(12271.3799, grad_fn=<NegBackward0>) tensor(12271.2461, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12271.1494140625
tensor(12271.2461, grad_fn=<NegBackward0>) tensor(12271.1494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12271.0693359375
tensor(12271.1494, grad_fn=<NegBackward0>) tensor(12271.0693, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12271.005859375
tensor(12271.0693, grad_fn=<NegBackward0>) tensor(12271.0059, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12270.955078125
tensor(12271.0059, grad_fn=<NegBackward0>) tensor(12270.9551, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12270.9150390625
tensor(12270.9551, grad_fn=<NegBackward0>) tensor(12270.9150, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12270.8818359375
tensor(12270.9150, grad_fn=<NegBackward0>) tensor(12270.8818, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12270.85546875
tensor(12270.8818, grad_fn=<NegBackward0>) tensor(12270.8555, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12270.8330078125
tensor(12270.8555, grad_fn=<NegBackward0>) tensor(12270.8330, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12270.814453125
tensor(12270.8330, grad_fn=<NegBackward0>) tensor(12270.8145, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12270.7978515625
tensor(12270.8145, grad_fn=<NegBackward0>) tensor(12270.7979, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12270.7841796875
tensor(12270.7979, grad_fn=<NegBackward0>) tensor(12270.7842, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12270.7705078125
tensor(12270.7842, grad_fn=<NegBackward0>) tensor(12270.7705, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12270.76171875
tensor(12270.7705, grad_fn=<NegBackward0>) tensor(12270.7617, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12270.751953125
tensor(12270.7617, grad_fn=<NegBackward0>) tensor(12270.7520, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12270.744140625
tensor(12270.7520, grad_fn=<NegBackward0>) tensor(12270.7441, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12270.73828125
tensor(12270.7441, grad_fn=<NegBackward0>) tensor(12270.7383, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12270.7314453125
tensor(12270.7383, grad_fn=<NegBackward0>) tensor(12270.7314, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12270.7265625
tensor(12270.7314, grad_fn=<NegBackward0>) tensor(12270.7266, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12270.7236328125
tensor(12270.7266, grad_fn=<NegBackward0>) tensor(12270.7236, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12270.716796875
tensor(12270.7236, grad_fn=<NegBackward0>) tensor(12270.7168, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12270.7119140625
tensor(12270.7168, grad_fn=<NegBackward0>) tensor(12270.7119, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12270.708984375
tensor(12270.7119, grad_fn=<NegBackward0>) tensor(12270.7090, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12270.70703125
tensor(12270.7090, grad_fn=<NegBackward0>) tensor(12270.7070, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12270.7041015625
tensor(12270.7070, grad_fn=<NegBackward0>) tensor(12270.7041, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12270.701171875
tensor(12270.7041, grad_fn=<NegBackward0>) tensor(12270.7012, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12270.6982421875
tensor(12270.7012, grad_fn=<NegBackward0>) tensor(12270.6982, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12270.6943359375
tensor(12270.6982, grad_fn=<NegBackward0>) tensor(12270.6943, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12270.693359375
tensor(12270.6943, grad_fn=<NegBackward0>) tensor(12270.6934, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12270.69140625
tensor(12270.6934, grad_fn=<NegBackward0>) tensor(12270.6914, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12270.6904296875
tensor(12270.6914, grad_fn=<NegBackward0>) tensor(12270.6904, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12270.6875
tensor(12270.6904, grad_fn=<NegBackward0>) tensor(12270.6875, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12270.6865234375
tensor(12270.6875, grad_fn=<NegBackward0>) tensor(12270.6865, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12270.6845703125
tensor(12270.6865, grad_fn=<NegBackward0>) tensor(12270.6846, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12270.6826171875
tensor(12270.6846, grad_fn=<NegBackward0>) tensor(12270.6826, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12270.68359375
tensor(12270.6826, grad_fn=<NegBackward0>) tensor(12270.6836, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12270.681640625
tensor(12270.6826, grad_fn=<NegBackward0>) tensor(12270.6816, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12270.6796875
tensor(12270.6816, grad_fn=<NegBackward0>) tensor(12270.6797, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12270.6796875
tensor(12270.6797, grad_fn=<NegBackward0>) tensor(12270.6797, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12270.6787109375
tensor(12270.6797, grad_fn=<NegBackward0>) tensor(12270.6787, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12270.677734375
tensor(12270.6787, grad_fn=<NegBackward0>) tensor(12270.6777, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12270.6767578125
tensor(12270.6777, grad_fn=<NegBackward0>) tensor(12270.6768, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12270.6767578125
tensor(12270.6768, grad_fn=<NegBackward0>) tensor(12270.6768, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12270.6748046875
tensor(12270.6768, grad_fn=<NegBackward0>) tensor(12270.6748, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12270.6728515625
tensor(12270.6748, grad_fn=<NegBackward0>) tensor(12270.6729, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12270.6728515625
tensor(12270.6729, grad_fn=<NegBackward0>) tensor(12270.6729, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12270.673828125
tensor(12270.6729, grad_fn=<NegBackward0>) tensor(12270.6738, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12270.671875
tensor(12270.6729, grad_fn=<NegBackward0>) tensor(12270.6719, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12270.671875
tensor(12270.6719, grad_fn=<NegBackward0>) tensor(12270.6719, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12270.6708984375
tensor(12270.6719, grad_fn=<NegBackward0>) tensor(12270.6709, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12270.671875
tensor(12270.6709, grad_fn=<NegBackward0>) tensor(12270.6719, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12271.3369140625
tensor(12270.6709, grad_fn=<NegBackward0>) tensor(12271.3369, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12270.669921875
tensor(12270.6709, grad_fn=<NegBackward0>) tensor(12270.6699, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12270.6689453125
tensor(12270.6699, grad_fn=<NegBackward0>) tensor(12270.6689, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12270.669921875
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6699, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12270.9150390625
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.9150, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12270.6689453125
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6689, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12270.66796875
tensor(12270.6689, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12270.66796875
tensor(12270.6680, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12270.6689453125
tensor(12270.6680, grad_fn=<NegBackward0>) tensor(12270.6689, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12270.6669921875
tensor(12270.6680, grad_fn=<NegBackward0>) tensor(12270.6670, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12270.66796875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12270.66796875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6680, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12271.25
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12271.2500, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -12270.6669921875
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6670, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12270.6650390625
tensor(12270.6670, grad_fn=<NegBackward0>) tensor(12270.6650, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12270.6669921875
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6670, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12270.666015625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -12270.666015625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -12270.666015625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -12270.666015625
tensor(12270.6650, grad_fn=<NegBackward0>) tensor(12270.6660, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[9.9932e-01, 6.8234e-04],
        [5.4998e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0187, 0.9813], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2049, 0.1547],
         [0.5881, 0.1972]],

        [[0.6336, 0.2221],
         [0.7188, 0.5199]],

        [[0.6346, 0.1834],
         [0.6279, 0.6549]],

        [[0.5349, 0.0873],
         [0.5250, 0.5846]],

        [[0.5725, 0.2987],
         [0.6010, 0.6399]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: 0.0011536996915455686
Average Adjusted Rand Index: 0.0012779021416470114
[0.0011536996915455686, 0.0011536996915455686] [0.0012779021416470114, 0.0012779021416470114] [12270.69921875, 12270.666015625]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11752.675697906672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22646.404296875
inf tensor(22646.4043, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12194.18359375
tensor(22646.4043, grad_fn=<NegBackward0>) tensor(12194.1836, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12194.0205078125
tensor(12194.1836, grad_fn=<NegBackward0>) tensor(12194.0205, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12193.931640625
tensor(12194.0205, grad_fn=<NegBackward0>) tensor(12193.9316, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12193.798828125
tensor(12193.9316, grad_fn=<NegBackward0>) tensor(12193.7988, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12193.685546875
tensor(12193.7988, grad_fn=<NegBackward0>) tensor(12193.6855, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12193.607421875
tensor(12193.6855, grad_fn=<NegBackward0>) tensor(12193.6074, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12193.517578125
tensor(12193.6074, grad_fn=<NegBackward0>) tensor(12193.5176, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12193.4462890625
tensor(12193.5176, grad_fn=<NegBackward0>) tensor(12193.4463, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12193.359375
tensor(12193.4463, grad_fn=<NegBackward0>) tensor(12193.3594, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12193.2275390625
tensor(12193.3594, grad_fn=<NegBackward0>) tensor(12193.2275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12193.0107421875
tensor(12193.2275, grad_fn=<NegBackward0>) tensor(12193.0107, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12192.6796875
tensor(12193.0107, grad_fn=<NegBackward0>) tensor(12192.6797, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12192.474609375
tensor(12192.6797, grad_fn=<NegBackward0>) tensor(12192.4746, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12192.4228515625
tensor(12192.4746, grad_fn=<NegBackward0>) tensor(12192.4229, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12192.40625
tensor(12192.4229, grad_fn=<NegBackward0>) tensor(12192.4062, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12192.3955078125
tensor(12192.4062, grad_fn=<NegBackward0>) tensor(12192.3955, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12192.3857421875
tensor(12192.3955, grad_fn=<NegBackward0>) tensor(12192.3857, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12192.3720703125
tensor(12192.3857, grad_fn=<NegBackward0>) tensor(12192.3721, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12192.3359375
tensor(12192.3721, grad_fn=<NegBackward0>) tensor(12192.3359, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12192.11328125
tensor(12192.3359, grad_fn=<NegBackward0>) tensor(12192.1133, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12190.810546875
tensor(12192.1133, grad_fn=<NegBackward0>) tensor(12190.8105, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12187.7373046875
tensor(12190.8105, grad_fn=<NegBackward0>) tensor(12187.7373, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12183.2431640625
tensor(12187.7373, grad_fn=<NegBackward0>) tensor(12183.2432, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12182.5810546875
tensor(12183.2432, grad_fn=<NegBackward0>) tensor(12182.5811, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12182.197265625
tensor(12182.5811, grad_fn=<NegBackward0>) tensor(12182.1973, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12181.9345703125
tensor(12182.1973, grad_fn=<NegBackward0>) tensor(12181.9346, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12181.759765625
tensor(12181.9346, grad_fn=<NegBackward0>) tensor(12181.7598, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12181.6455078125
tensor(12181.7598, grad_fn=<NegBackward0>) tensor(12181.6455, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12181.5703125
tensor(12181.6455, grad_fn=<NegBackward0>) tensor(12181.5703, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12181.51953125
tensor(12181.5703, grad_fn=<NegBackward0>) tensor(12181.5195, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12181.4892578125
tensor(12181.5195, grad_fn=<NegBackward0>) tensor(12181.4893, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12181.4677734375
tensor(12181.4893, grad_fn=<NegBackward0>) tensor(12181.4678, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12181.4541015625
tensor(12181.4678, grad_fn=<NegBackward0>) tensor(12181.4541, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12181.4443359375
tensor(12181.4541, grad_fn=<NegBackward0>) tensor(12181.4443, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12181.4384765625
tensor(12181.4443, grad_fn=<NegBackward0>) tensor(12181.4385, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12181.4326171875
tensor(12181.4385, grad_fn=<NegBackward0>) tensor(12181.4326, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12181.431640625
tensor(12181.4326, grad_fn=<NegBackward0>) tensor(12181.4316, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12181.4287109375
tensor(12181.4316, grad_fn=<NegBackward0>) tensor(12181.4287, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12181.4267578125
tensor(12181.4287, grad_fn=<NegBackward0>) tensor(12181.4268, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12181.4267578125
tensor(12181.4268, grad_fn=<NegBackward0>) tensor(12181.4268, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12181.4248046875
tensor(12181.4268, grad_fn=<NegBackward0>) tensor(12181.4248, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12181.423828125
tensor(12181.4248, grad_fn=<NegBackward0>) tensor(12181.4238, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12181.421875
tensor(12181.4238, grad_fn=<NegBackward0>) tensor(12181.4219, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12181.4228515625
tensor(12181.4219, grad_fn=<NegBackward0>) tensor(12181.4229, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12181.4228515625
tensor(12181.4219, grad_fn=<NegBackward0>) tensor(12181.4229, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12181.421875
tensor(12181.4219, grad_fn=<NegBackward0>) tensor(12181.4219, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12181.4208984375
tensor(12181.4219, grad_fn=<NegBackward0>) tensor(12181.4209, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12181.4208984375
tensor(12181.4209, grad_fn=<NegBackward0>) tensor(12181.4209, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12181.421875
tensor(12181.4209, grad_fn=<NegBackward0>) tensor(12181.4219, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12181.4208984375
tensor(12181.4209, grad_fn=<NegBackward0>) tensor(12181.4209, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12181.4189453125
tensor(12181.4209, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12181.419921875
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12181.419921875
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12181.419921875
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -12181.419921875
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -12181.4189453125
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12181.4189453125
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12181.419921875
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12181.4189453125
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12181.4189453125
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12181.41796875
tensor(12181.4189, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12181.419921875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12181.4208984375
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4209, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12181.419921875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -12181.4814453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4814, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12181.419921875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12181.41796875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4180, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12181.419921875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4199, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12181.62109375
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.6211, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12181.4189453125
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4189, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -12181.498046875
tensor(12181.4180, grad_fn=<NegBackward0>) tensor(12181.4980, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.9966, 0.0034],
        [0.2042, 0.7958]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9769, 0.0231], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.2468],
         [0.7034, 0.7718]],

        [[0.6724, 0.1495],
         [0.5849, 0.6366]],

        [[0.5879, 0.0819],
         [0.6906, 0.5907]],

        [[0.5376, 0.2406],
         [0.6296, 0.6612]],

        [[0.5072, 0.0667],
         [0.7075, 0.5955]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: -2.7862102935448927e-05
Average Adjusted Rand Index: 0.002881628711290623
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23231.65625
inf tensor(23231.6562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12195.1455078125
tensor(23231.6562, grad_fn=<NegBackward0>) tensor(12195.1455, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12194.35546875
tensor(12195.1455, grad_fn=<NegBackward0>) tensor(12194.3555, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12194.0791015625
tensor(12194.3555, grad_fn=<NegBackward0>) tensor(12194.0791, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12193.8837890625
tensor(12194.0791, grad_fn=<NegBackward0>) tensor(12193.8838, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12193.8232421875
tensor(12193.8838, grad_fn=<NegBackward0>) tensor(12193.8232, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12193.7763671875
tensor(12193.8232, grad_fn=<NegBackward0>) tensor(12193.7764, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12193.7314453125
tensor(12193.7764, grad_fn=<NegBackward0>) tensor(12193.7314, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12193.685546875
tensor(12193.7314, grad_fn=<NegBackward0>) tensor(12193.6855, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12193.6376953125
tensor(12193.6855, grad_fn=<NegBackward0>) tensor(12193.6377, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12193.5888671875
tensor(12193.6377, grad_fn=<NegBackward0>) tensor(12193.5889, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12193.5341796875
tensor(12193.5889, grad_fn=<NegBackward0>) tensor(12193.5342, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12193.46484375
tensor(12193.5342, grad_fn=<NegBackward0>) tensor(12193.4648, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12193.3447265625
tensor(12193.4648, grad_fn=<NegBackward0>) tensor(12193.3447, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12193.0947265625
tensor(12193.3447, grad_fn=<NegBackward0>) tensor(12193.0947, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12192.6318359375
tensor(12193.0947, grad_fn=<NegBackward0>) tensor(12192.6318, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12192.4755859375
tensor(12192.6318, grad_fn=<NegBackward0>) tensor(12192.4756, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12192.4267578125
tensor(12192.4756, grad_fn=<NegBackward0>) tensor(12192.4268, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12192.3984375
tensor(12192.4268, grad_fn=<NegBackward0>) tensor(12192.3984, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12192.3681640625
tensor(12192.3984, grad_fn=<NegBackward0>) tensor(12192.3682, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12192.279296875
tensor(12192.3682, grad_fn=<NegBackward0>) tensor(12192.2793, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12191.5576171875
tensor(12192.2793, grad_fn=<NegBackward0>) tensor(12191.5576, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12190.2685546875
tensor(12191.5576, grad_fn=<NegBackward0>) tensor(12190.2686, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12183.4501953125
tensor(12190.2686, grad_fn=<NegBackward0>) tensor(12183.4502, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12182.7861328125
tensor(12183.4502, grad_fn=<NegBackward0>) tensor(12182.7861, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12182.3818359375
tensor(12182.7861, grad_fn=<NegBackward0>) tensor(12182.3818, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12182.091796875
tensor(12182.3818, grad_fn=<NegBackward0>) tensor(12182.0918, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12181.8828125
tensor(12182.0918, grad_fn=<NegBackward0>) tensor(12181.8828, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12181.736328125
tensor(12181.8828, grad_fn=<NegBackward0>) tensor(12181.7363, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12181.6357421875
tensor(12181.7363, grad_fn=<NegBackward0>) tensor(12181.6357, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12181.5673828125
tensor(12181.6357, grad_fn=<NegBackward0>) tensor(12181.5674, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12181.521484375
tensor(12181.5674, grad_fn=<NegBackward0>) tensor(12181.5215, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12181.4912109375
tensor(12181.5215, grad_fn=<NegBackward0>) tensor(12181.4912, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12181.4677734375
tensor(12181.4912, grad_fn=<NegBackward0>) tensor(12181.4678, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12181.4140625
tensor(12181.4678, grad_fn=<NegBackward0>) tensor(12181.4141, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12181.3828125
tensor(12181.4141, grad_fn=<NegBackward0>) tensor(12181.3828, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12181.3779296875
tensor(12181.3828, grad_fn=<NegBackward0>) tensor(12181.3779, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12181.3740234375
tensor(12181.3779, grad_fn=<NegBackward0>) tensor(12181.3740, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12181.37109375
tensor(12181.3740, grad_fn=<NegBackward0>) tensor(12181.3711, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12181.369140625
tensor(12181.3711, grad_fn=<NegBackward0>) tensor(12181.3691, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12181.3662109375
tensor(12181.3691, grad_fn=<NegBackward0>) tensor(12181.3662, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12181.3662109375
tensor(12181.3662, grad_fn=<NegBackward0>) tensor(12181.3662, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12181.365234375
tensor(12181.3662, grad_fn=<NegBackward0>) tensor(12181.3652, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12181.3662109375
tensor(12181.3652, grad_fn=<NegBackward0>) tensor(12181.3662, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12181.36328125
tensor(12181.3652, grad_fn=<NegBackward0>) tensor(12181.3633, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12181.3642578125
tensor(12181.3633, grad_fn=<NegBackward0>) tensor(12181.3643, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12181.36328125
tensor(12181.3633, grad_fn=<NegBackward0>) tensor(12181.3633, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12181.3623046875
tensor(12181.3633, grad_fn=<NegBackward0>) tensor(12181.3623, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12181.3623046875
tensor(12181.3623, grad_fn=<NegBackward0>) tensor(12181.3623, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12181.361328125
tensor(12181.3623, grad_fn=<NegBackward0>) tensor(12181.3613, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12181.3603515625
tensor(12181.3613, grad_fn=<NegBackward0>) tensor(12181.3604, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12181.361328125
tensor(12181.3604, grad_fn=<NegBackward0>) tensor(12181.3613, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12181.3623046875
tensor(12181.3604, grad_fn=<NegBackward0>) tensor(12181.3623, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -12181.361328125
tensor(12181.3604, grad_fn=<NegBackward0>) tensor(12181.3613, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -12181.359375
tensor(12181.3604, grad_fn=<NegBackward0>) tensor(12181.3594, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12181.359375
tensor(12181.3594, grad_fn=<NegBackward0>) tensor(12181.3594, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12181.3603515625
tensor(12181.3594, grad_fn=<NegBackward0>) tensor(12181.3604, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12181.3603515625
tensor(12181.3594, grad_fn=<NegBackward0>) tensor(12181.3604, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12181.359375
tensor(12181.3594, grad_fn=<NegBackward0>) tensor(12181.3594, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12181.359375
tensor(12181.3594, grad_fn=<NegBackward0>) tensor(12181.3594, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12181.357421875
tensor(12181.3594, grad_fn=<NegBackward0>) tensor(12181.3574, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12181.359375
tensor(12181.3574, grad_fn=<NegBackward0>) tensor(12181.3594, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12181.3603515625
tensor(12181.3574, grad_fn=<NegBackward0>) tensor(12181.3604, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12181.3603515625
tensor(12181.3574, grad_fn=<NegBackward0>) tensor(12181.3604, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -12181.3603515625
tensor(12181.3574, grad_fn=<NegBackward0>) tensor(12181.3604, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -12181.3583984375
tensor(12181.3574, grad_fn=<NegBackward0>) tensor(12181.3584, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.7975, 0.2025],
        [0.0034, 0.9966]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0232, 0.9768], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.7624, 0.2466],
         [0.5544, 0.1959]],

        [[0.5671, 0.1496],
         [0.5494, 0.6223]],

        [[0.6129, 0.0819],
         [0.5949, 0.6239]],

        [[0.7265, 0.2363],
         [0.5594, 0.6964]],

        [[0.6289, 0.0667],
         [0.5239, 0.6792]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: -2.7862102935448927e-05
Average Adjusted Rand Index: 0.002881628711290623
[-2.7862102935448927e-05, -2.7862102935448927e-05] [0.002881628711290623, 0.002881628711290623] [12181.498046875, 12181.3583984375]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11787.831845287514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20938.802734375
inf tensor(20938.8027, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11778.5048828125
tensor(20938.8027, grad_fn=<NegBackward0>) tensor(11778.5049, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11777.8359375
tensor(11778.5049, grad_fn=<NegBackward0>) tensor(11777.8359, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11777.6806640625
tensor(11777.8359, grad_fn=<NegBackward0>) tensor(11777.6807, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11777.61328125
tensor(11777.6807, grad_fn=<NegBackward0>) tensor(11777.6133, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11777.576171875
tensor(11777.6133, grad_fn=<NegBackward0>) tensor(11777.5762, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11777.5517578125
tensor(11777.5762, grad_fn=<NegBackward0>) tensor(11777.5518, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11777.537109375
tensor(11777.5518, grad_fn=<NegBackward0>) tensor(11777.5371, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11777.54296875
tensor(11777.5371, grad_fn=<NegBackward0>) tensor(11777.5430, grad_fn=<NegBackward0>)
1
Iteration 900: Loss = -11777.51953125
tensor(11777.5371, grad_fn=<NegBackward0>) tensor(11777.5195, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11777.5126953125
tensor(11777.5195, grad_fn=<NegBackward0>) tensor(11777.5127, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11777.509765625
tensor(11777.5127, grad_fn=<NegBackward0>) tensor(11777.5098, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11777.505859375
tensor(11777.5098, grad_fn=<NegBackward0>) tensor(11777.5059, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11777.501953125
tensor(11777.5059, grad_fn=<NegBackward0>) tensor(11777.5020, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11777.4990234375
tensor(11777.5020, grad_fn=<NegBackward0>) tensor(11777.4990, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11777.498046875
tensor(11777.4990, grad_fn=<NegBackward0>) tensor(11777.4980, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11777.4990234375
tensor(11777.4980, grad_fn=<NegBackward0>) tensor(11777.4990, grad_fn=<NegBackward0>)
1
Iteration 1700: Loss = -11777.4951171875
tensor(11777.4980, grad_fn=<NegBackward0>) tensor(11777.4951, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11777.494140625
tensor(11777.4951, grad_fn=<NegBackward0>) tensor(11777.4941, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11777.4931640625
tensor(11777.4941, grad_fn=<NegBackward0>) tensor(11777.4932, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11777.4931640625
tensor(11777.4932, grad_fn=<NegBackward0>) tensor(11777.4932, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11777.4912109375
tensor(11777.4932, grad_fn=<NegBackward0>) tensor(11777.4912, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11777.4912109375
tensor(11777.4912, grad_fn=<NegBackward0>) tensor(11777.4912, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11777.490234375
tensor(11777.4912, grad_fn=<NegBackward0>) tensor(11777.4902, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11777.4892578125
tensor(11777.4902, grad_fn=<NegBackward0>) tensor(11777.4893, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11777.48828125
tensor(11777.4893, grad_fn=<NegBackward0>) tensor(11777.4883, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11777.4970703125
tensor(11777.4883, grad_fn=<NegBackward0>) tensor(11777.4971, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11777.4873046875
tensor(11777.4883, grad_fn=<NegBackward0>) tensor(11777.4873, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11777.4892578125
tensor(11777.4873, grad_fn=<NegBackward0>) tensor(11777.4893, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11777.4873046875
tensor(11777.4873, grad_fn=<NegBackward0>) tensor(11777.4873, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11777.4931640625
tensor(11777.4873, grad_fn=<NegBackward0>) tensor(11777.4932, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11777.486328125
tensor(11777.4873, grad_fn=<NegBackward0>) tensor(11777.4863, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11777.4853515625
tensor(11777.4863, grad_fn=<NegBackward0>) tensor(11777.4854, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11777.486328125
tensor(11777.4854, grad_fn=<NegBackward0>) tensor(11777.4863, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11777.4853515625
tensor(11777.4854, grad_fn=<NegBackward0>) tensor(11777.4854, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11777.48828125
tensor(11777.4854, grad_fn=<NegBackward0>) tensor(11777.4883, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11777.4853515625
tensor(11777.4854, grad_fn=<NegBackward0>) tensor(11777.4854, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11777.484375
tensor(11777.4854, grad_fn=<NegBackward0>) tensor(11777.4844, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11777.4853515625
tensor(11777.4844, grad_fn=<NegBackward0>) tensor(11777.4854, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11777.484375
tensor(11777.4844, grad_fn=<NegBackward0>) tensor(11777.4844, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11777.48828125
tensor(11777.4844, grad_fn=<NegBackward0>) tensor(11777.4883, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11777.4853515625
tensor(11777.4844, grad_fn=<NegBackward0>) tensor(11777.4854, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11777.4833984375
tensor(11777.4844, grad_fn=<NegBackward0>) tensor(11777.4834, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11777.484375
tensor(11777.4834, grad_fn=<NegBackward0>) tensor(11777.4844, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11777.484375
tensor(11777.4834, grad_fn=<NegBackward0>) tensor(11777.4844, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11777.486328125
tensor(11777.4834, grad_fn=<NegBackward0>) tensor(11777.4863, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11777.484375
tensor(11777.4834, grad_fn=<NegBackward0>) tensor(11777.4844, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -11777.484375
tensor(11777.4834, grad_fn=<NegBackward0>) tensor(11777.4844, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4700 due to no improvement.
pi: tensor([[0.7465, 0.2535],
        [0.2273, 0.7727]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4868, 0.5132], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3073, 0.0923],
         [0.7104, 0.3003]],

        [[0.7090, 0.0888],
         [0.6375, 0.5852]],

        [[0.6090, 0.0901],
         [0.5414, 0.6252]],

        [[0.7289, 0.1034],
         [0.5703, 0.7104]],

        [[0.7130, 0.1050],
         [0.6779, 0.6211]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9840320434389982
Average Adjusted Rand Index: 0.983999122327126
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22568.9921875
inf tensor(22568.9922, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12366.08984375
tensor(22568.9922, grad_fn=<NegBackward0>) tensor(12366.0898, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12365.5791015625
tensor(12366.0898, grad_fn=<NegBackward0>) tensor(12365.5791, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12365.431640625
tensor(12365.5791, grad_fn=<NegBackward0>) tensor(12365.4316, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12365.3212890625
tensor(12365.4316, grad_fn=<NegBackward0>) tensor(12365.3213, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12365.232421875
tensor(12365.3213, grad_fn=<NegBackward0>) tensor(12365.2324, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12365.1650390625
tensor(12365.2324, grad_fn=<NegBackward0>) tensor(12365.1650, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12365.1083984375
tensor(12365.1650, grad_fn=<NegBackward0>) tensor(12365.1084, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12365.0546875
tensor(12365.1084, grad_fn=<NegBackward0>) tensor(12365.0547, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12364.9921875
tensor(12365.0547, grad_fn=<NegBackward0>) tensor(12364.9922, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12364.92578125
tensor(12364.9922, grad_fn=<NegBackward0>) tensor(12364.9258, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12364.845703125
tensor(12364.9258, grad_fn=<NegBackward0>) tensor(12364.8457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12364.744140625
tensor(12364.8457, grad_fn=<NegBackward0>) tensor(12364.7441, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12364.603515625
tensor(12364.7441, grad_fn=<NegBackward0>) tensor(12364.6035, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12364.3017578125
tensor(12364.6035, grad_fn=<NegBackward0>) tensor(12364.3018, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12363.1611328125
tensor(12364.3018, grad_fn=<NegBackward0>) tensor(12363.1611, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11892.4755859375
tensor(12363.1611, grad_fn=<NegBackward0>) tensor(11892.4756, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11781.7490234375
tensor(11892.4756, grad_fn=<NegBackward0>) tensor(11781.7490, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11779.32421875
tensor(11781.7490, grad_fn=<NegBackward0>) tensor(11779.3242, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11779.1484375
tensor(11779.3242, grad_fn=<NegBackward0>) tensor(11779.1484, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11779.1337890625
tensor(11779.1484, grad_fn=<NegBackward0>) tensor(11779.1338, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11779.119140625
tensor(11779.1338, grad_fn=<NegBackward0>) tensor(11779.1191, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11779.107421875
tensor(11779.1191, grad_fn=<NegBackward0>) tensor(11779.1074, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11778.009765625
tensor(11779.1074, grad_fn=<NegBackward0>) tensor(11778.0098, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11778.0
tensor(11778.0098, grad_fn=<NegBackward0>) tensor(11778., grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11777.9970703125
tensor(11778., grad_fn=<NegBackward0>) tensor(11777.9971, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11777.994140625
tensor(11777.9971, grad_fn=<NegBackward0>) tensor(11777.9941, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11777.9912109375
tensor(11777.9941, grad_fn=<NegBackward0>) tensor(11777.9912, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11777.9892578125
tensor(11777.9912, grad_fn=<NegBackward0>) tensor(11777.9893, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11777.9873046875
tensor(11777.9893, grad_fn=<NegBackward0>) tensor(11777.9873, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11777.9853515625
tensor(11777.9873, grad_fn=<NegBackward0>) tensor(11777.9854, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11777.984375
tensor(11777.9854, grad_fn=<NegBackward0>) tensor(11777.9844, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11777.9833984375
tensor(11777.9844, grad_fn=<NegBackward0>) tensor(11777.9834, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11777.9814453125
tensor(11777.9834, grad_fn=<NegBackward0>) tensor(11777.9814, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11777.98046875
tensor(11777.9814, grad_fn=<NegBackward0>) tensor(11777.9805, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11777.978515625
tensor(11777.9805, grad_fn=<NegBackward0>) tensor(11777.9785, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11777.9775390625
tensor(11777.9785, grad_fn=<NegBackward0>) tensor(11777.9775, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11777.9765625
tensor(11777.9775, grad_fn=<NegBackward0>) tensor(11777.9766, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11777.9794921875
tensor(11777.9766, grad_fn=<NegBackward0>) tensor(11777.9795, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11777.9765625
tensor(11777.9766, grad_fn=<NegBackward0>) tensor(11777.9766, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11777.974609375
tensor(11777.9766, grad_fn=<NegBackward0>) tensor(11777.9746, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11777.9736328125
tensor(11777.9746, grad_fn=<NegBackward0>) tensor(11777.9736, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11777.97265625
tensor(11777.9736, grad_fn=<NegBackward0>) tensor(11777.9727, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11777.9716796875
tensor(11777.9727, grad_fn=<NegBackward0>) tensor(11777.9717, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11777.9697265625
tensor(11777.9717, grad_fn=<NegBackward0>) tensor(11777.9697, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11777.970703125
tensor(11777.9697, grad_fn=<NegBackward0>) tensor(11777.9707, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11777.97265625
tensor(11777.9697, grad_fn=<NegBackward0>) tensor(11777.9727, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -11777.9658203125
tensor(11777.9697, grad_fn=<NegBackward0>) tensor(11777.9658, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11777.9658203125
tensor(11777.9658, grad_fn=<NegBackward0>) tensor(11777.9658, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11777.9658203125
tensor(11777.9658, grad_fn=<NegBackward0>) tensor(11777.9658, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11777.96875
tensor(11777.9658, grad_fn=<NegBackward0>) tensor(11777.9688, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11777.9619140625
tensor(11777.9658, grad_fn=<NegBackward0>) tensor(11777.9619, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11777.9619140625
tensor(11777.9619, grad_fn=<NegBackward0>) tensor(11777.9619, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11777.9599609375
tensor(11777.9619, grad_fn=<NegBackward0>) tensor(11777.9600, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11777.958984375
tensor(11777.9600, grad_fn=<NegBackward0>) tensor(11777.9590, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11777.9599609375
tensor(11777.9590, grad_fn=<NegBackward0>) tensor(11777.9600, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11777.9599609375
tensor(11777.9590, grad_fn=<NegBackward0>) tensor(11777.9600, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11777.9599609375
tensor(11777.9590, grad_fn=<NegBackward0>) tensor(11777.9600, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11777.958984375
tensor(11777.9590, grad_fn=<NegBackward0>) tensor(11777.9590, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11777.9599609375
tensor(11777.9590, grad_fn=<NegBackward0>) tensor(11777.9600, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11777.95703125
tensor(11777.9590, grad_fn=<NegBackward0>) tensor(11777.9570, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11777.9580078125
tensor(11777.9570, grad_fn=<NegBackward0>) tensor(11777.9580, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11777.9677734375
tensor(11777.9570, grad_fn=<NegBackward0>) tensor(11777.9678, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11777.95703125
tensor(11777.9570, grad_fn=<NegBackward0>) tensor(11777.9570, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11777.95703125
tensor(11777.9570, grad_fn=<NegBackward0>) tensor(11777.9570, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11777.966796875
tensor(11777.9570, grad_fn=<NegBackward0>) tensor(11777.9668, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11777.955078125
tensor(11777.9570, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11777.955078125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11777.955078125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11777.962890625
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9629, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11777.958984375
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9590, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11777.95703125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9570, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11777.9609375
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9609, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11777.955078125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11777.9560546875
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9561, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11777.955078125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11777.955078125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11777.953125
tensor(11777.9551, grad_fn=<NegBackward0>) tensor(11777.9531, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11777.955078125
tensor(11777.9531, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11777.9521484375
tensor(11777.9531, grad_fn=<NegBackward0>) tensor(11777.9521, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11777.955078125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11777.958984375
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9590, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11777.9521484375
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9521, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11777.984375
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9844, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11777.953125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9531, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11777.953125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9531, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11777.953125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9531, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11777.9521484375
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9521, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11777.9521484375
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9521, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11777.966796875
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9668, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11777.953125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9531, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11777.955078125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9551, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11777.953125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9531, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11777.95703125
tensor(11777.9521, grad_fn=<NegBackward0>) tensor(11777.9570, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.2412, 0.7588],
        [0.7588, 0.2412]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4874, 0.5126], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3030, 0.0922],
         [0.6709, 0.3040]],

        [[0.5502, 0.0888],
         [0.6494, 0.7143]],

        [[0.5695, 0.0901],
         [0.7247, 0.5834]],

        [[0.7045, 0.1035],
         [0.7140, 0.5192]],

        [[0.7249, 0.1049],
         [0.5224, 0.5308]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.038078276342865244
Average Adjusted Rand Index: 0.983999122327126
[0.9840320434389982, 0.038078276342865244] [0.983999122327126, 0.983999122327126] [11777.484375, 11777.95703125]
-------------------------------------
This iteration is 8
True Objective function: Loss = -11849.418410444872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20235.3203125
inf tensor(20235.3203, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12347.69140625
tensor(20235.3203, grad_fn=<NegBackward0>) tensor(12347.6914, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12347.4443359375
tensor(12347.6914, grad_fn=<NegBackward0>) tensor(12347.4443, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12347.3466796875
tensor(12347.4443, grad_fn=<NegBackward0>) tensor(12347.3467, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12347.2216796875
tensor(12347.3467, grad_fn=<NegBackward0>) tensor(12347.2217, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12346.998046875
tensor(12347.2217, grad_fn=<NegBackward0>) tensor(12346.9980, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12346.9072265625
tensor(12346.9980, grad_fn=<NegBackward0>) tensor(12346.9072, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12346.8330078125
tensor(12346.9072, grad_fn=<NegBackward0>) tensor(12346.8330, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12346.7548828125
tensor(12346.8330, grad_fn=<NegBackward0>) tensor(12346.7549, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12346.6923828125
tensor(12346.7549, grad_fn=<NegBackward0>) tensor(12346.6924, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12346.6552734375
tensor(12346.6924, grad_fn=<NegBackward0>) tensor(12346.6553, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12346.63671875
tensor(12346.6553, grad_fn=<NegBackward0>) tensor(12346.6367, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12346.625
tensor(12346.6367, grad_fn=<NegBackward0>) tensor(12346.6250, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12346.6171875
tensor(12346.6250, grad_fn=<NegBackward0>) tensor(12346.6172, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12346.6083984375
tensor(12346.6172, grad_fn=<NegBackward0>) tensor(12346.6084, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12346.6005859375
tensor(12346.6084, grad_fn=<NegBackward0>) tensor(12346.6006, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12346.5966796875
tensor(12346.6006, grad_fn=<NegBackward0>) tensor(12346.5967, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12346.5869140625
tensor(12346.5967, grad_fn=<NegBackward0>) tensor(12346.5869, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12346.58203125
tensor(12346.5869, grad_fn=<NegBackward0>) tensor(12346.5820, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12346.578125
tensor(12346.5820, grad_fn=<NegBackward0>) tensor(12346.5781, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12346.5712890625
tensor(12346.5781, grad_fn=<NegBackward0>) tensor(12346.5713, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12346.56640625
tensor(12346.5713, grad_fn=<NegBackward0>) tensor(12346.5664, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12346.560546875
tensor(12346.5664, grad_fn=<NegBackward0>) tensor(12346.5605, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12346.5546875
tensor(12346.5605, grad_fn=<NegBackward0>) tensor(12346.5547, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12346.5498046875
tensor(12346.5547, grad_fn=<NegBackward0>) tensor(12346.5498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12346.546875
tensor(12346.5498, grad_fn=<NegBackward0>) tensor(12346.5469, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12346.541015625
tensor(12346.5469, grad_fn=<NegBackward0>) tensor(12346.5410, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12346.5380859375
tensor(12346.5410, grad_fn=<NegBackward0>) tensor(12346.5381, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12346.5341796875
tensor(12346.5381, grad_fn=<NegBackward0>) tensor(12346.5342, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12346.53125
tensor(12346.5342, grad_fn=<NegBackward0>) tensor(12346.5312, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12346.5263671875
tensor(12346.5312, grad_fn=<NegBackward0>) tensor(12346.5264, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12346.525390625
tensor(12346.5264, grad_fn=<NegBackward0>) tensor(12346.5254, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12346.5224609375
tensor(12346.5254, grad_fn=<NegBackward0>) tensor(12346.5225, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12346.5185546875
tensor(12346.5225, grad_fn=<NegBackward0>) tensor(12346.5186, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12346.5166015625
tensor(12346.5186, grad_fn=<NegBackward0>) tensor(12346.5166, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12346.5146484375
tensor(12346.5166, grad_fn=<NegBackward0>) tensor(12346.5146, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12346.513671875
tensor(12346.5146, grad_fn=<NegBackward0>) tensor(12346.5137, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12346.5107421875
tensor(12346.5137, grad_fn=<NegBackward0>) tensor(12346.5107, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12346.5107421875
tensor(12346.5107, grad_fn=<NegBackward0>) tensor(12346.5107, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12346.5087890625
tensor(12346.5107, grad_fn=<NegBackward0>) tensor(12346.5088, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12346.5068359375
tensor(12346.5088, grad_fn=<NegBackward0>) tensor(12346.5068, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12346.505859375
tensor(12346.5068, grad_fn=<NegBackward0>) tensor(12346.5059, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12346.5048828125
tensor(12346.5059, grad_fn=<NegBackward0>) tensor(12346.5049, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12346.501953125
tensor(12346.5049, grad_fn=<NegBackward0>) tensor(12346.5020, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12346.5029296875
tensor(12346.5020, grad_fn=<NegBackward0>) tensor(12346.5029, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12346.501953125
tensor(12346.5020, grad_fn=<NegBackward0>) tensor(12346.5020, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12346.5009765625
tensor(12346.5020, grad_fn=<NegBackward0>) tensor(12346.5010, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12346.5
tensor(12346.5010, grad_fn=<NegBackward0>) tensor(12346.5000, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12346.498046875
tensor(12346.5000, grad_fn=<NegBackward0>) tensor(12346.4980, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12346.4990234375
tensor(12346.4980, grad_fn=<NegBackward0>) tensor(12346.4990, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12346.5
tensor(12346.4980, grad_fn=<NegBackward0>) tensor(12346.5000, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12346.4970703125
tensor(12346.4980, grad_fn=<NegBackward0>) tensor(12346.4971, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12346.498046875
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.4980, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12346.49609375
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.4961, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12346.49609375
tensor(12346.4961, grad_fn=<NegBackward0>) tensor(12346.4961, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12346.4951171875
tensor(12346.4961, grad_fn=<NegBackward0>) tensor(12346.4951, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12346.4951171875
tensor(12346.4951, grad_fn=<NegBackward0>) tensor(12346.4951, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12346.4951171875
tensor(12346.4951, grad_fn=<NegBackward0>) tensor(12346.4951, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12346.494140625
tensor(12346.4951, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12346.494140625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12346.494140625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12346.494140625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12346.4931640625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4932, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12346.4931640625
tensor(12346.4932, grad_fn=<NegBackward0>) tensor(12346.4932, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12346.5
tensor(12346.4932, grad_fn=<NegBackward0>) tensor(12346.5000, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12346.4921875
tensor(12346.4932, grad_fn=<NegBackward0>) tensor(12346.4922, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12346.4921875
tensor(12346.4922, grad_fn=<NegBackward0>) tensor(12346.4922, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12346.4921875
tensor(12346.4922, grad_fn=<NegBackward0>) tensor(12346.4922, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12346.4912109375
tensor(12346.4922, grad_fn=<NegBackward0>) tensor(12346.4912, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12346.490234375
tensor(12346.4912, grad_fn=<NegBackward0>) tensor(12346.4902, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12346.4921875
tensor(12346.4902, grad_fn=<NegBackward0>) tensor(12346.4922, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12346.4912109375
tensor(12346.4902, grad_fn=<NegBackward0>) tensor(12346.4912, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12346.5009765625
tensor(12346.4902, grad_fn=<NegBackward0>) tensor(12346.5010, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -12346.4951171875
tensor(12346.4902, grad_fn=<NegBackward0>) tensor(12346.4951, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -12346.4892578125
tensor(12346.4902, grad_fn=<NegBackward0>) tensor(12346.4893, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12346.490234375
tensor(12346.4893, grad_fn=<NegBackward0>) tensor(12346.4902, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12346.638671875
tensor(12346.4893, grad_fn=<NegBackward0>) tensor(12346.6387, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12346.4912109375
tensor(12346.4893, grad_fn=<NegBackward0>) tensor(12346.4912, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -12346.490234375
tensor(12346.4893, grad_fn=<NegBackward0>) tensor(12346.4902, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -12346.4912109375
tensor(12346.4893, grad_fn=<NegBackward0>) tensor(12346.4912, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.7490, 0.2510],
        [0.9969, 0.0031]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3660, 0.6340], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.2010],
         [0.7088, 0.2059]],

        [[0.5120, 0.1980],
         [0.5999, 0.5345]],

        [[0.6566, 0.2048],
         [0.6380, 0.6383]],

        [[0.6318, 0.2096],
         [0.7019, 0.5745]],

        [[0.7038, 0.1897],
         [0.5498, 0.5620]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018599660773026315
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20707.03125
inf tensor(20707.0312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12347.8125
tensor(20707.0312, grad_fn=<NegBackward0>) tensor(12347.8125, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12347.5322265625
tensor(12347.8125, grad_fn=<NegBackward0>) tensor(12347.5322, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12347.4384765625
tensor(12347.5322, grad_fn=<NegBackward0>) tensor(12347.4385, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12347.3544921875
tensor(12347.4385, grad_fn=<NegBackward0>) tensor(12347.3545, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12347.236328125
tensor(12347.3545, grad_fn=<NegBackward0>) tensor(12347.2363, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12347.037109375
tensor(12347.2363, grad_fn=<NegBackward0>) tensor(12347.0371, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12346.943359375
tensor(12347.0371, grad_fn=<NegBackward0>) tensor(12346.9434, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12346.8818359375
tensor(12346.9434, grad_fn=<NegBackward0>) tensor(12346.8818, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12346.814453125
tensor(12346.8818, grad_fn=<NegBackward0>) tensor(12346.8145, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12346.7509765625
tensor(12346.8145, grad_fn=<NegBackward0>) tensor(12346.7510, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12346.705078125
tensor(12346.7510, grad_fn=<NegBackward0>) tensor(12346.7051, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12346.6767578125
tensor(12346.7051, grad_fn=<NegBackward0>) tensor(12346.6768, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12346.6591796875
tensor(12346.6768, grad_fn=<NegBackward0>) tensor(12346.6592, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12346.64453125
tensor(12346.6592, grad_fn=<NegBackward0>) tensor(12346.6445, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12346.6337890625
tensor(12346.6445, grad_fn=<NegBackward0>) tensor(12346.6338, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12346.6259765625
tensor(12346.6338, grad_fn=<NegBackward0>) tensor(12346.6260, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12346.6171875
tensor(12346.6260, grad_fn=<NegBackward0>) tensor(12346.6172, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12346.6103515625
tensor(12346.6172, grad_fn=<NegBackward0>) tensor(12346.6104, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12346.6044921875
tensor(12346.6104, grad_fn=<NegBackward0>) tensor(12346.6045, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12346.59765625
tensor(12346.6045, grad_fn=<NegBackward0>) tensor(12346.5977, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12346.5927734375
tensor(12346.5977, grad_fn=<NegBackward0>) tensor(12346.5928, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12346.5859375
tensor(12346.5928, grad_fn=<NegBackward0>) tensor(12346.5859, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12346.5830078125
tensor(12346.5859, grad_fn=<NegBackward0>) tensor(12346.5830, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12346.5791015625
tensor(12346.5830, grad_fn=<NegBackward0>) tensor(12346.5791, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12346.5732421875
tensor(12346.5791, grad_fn=<NegBackward0>) tensor(12346.5732, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12346.5693359375
tensor(12346.5732, grad_fn=<NegBackward0>) tensor(12346.5693, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12346.5625
tensor(12346.5693, grad_fn=<NegBackward0>) tensor(12346.5625, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12346.556640625
tensor(12346.5625, grad_fn=<NegBackward0>) tensor(12346.5566, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12346.552734375
tensor(12346.5566, grad_fn=<NegBackward0>) tensor(12346.5527, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12346.546875
tensor(12346.5527, grad_fn=<NegBackward0>) tensor(12346.5469, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12346.5419921875
tensor(12346.5469, grad_fn=<NegBackward0>) tensor(12346.5420, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12346.537109375
tensor(12346.5420, grad_fn=<NegBackward0>) tensor(12346.5371, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12346.5361328125
tensor(12346.5371, grad_fn=<NegBackward0>) tensor(12346.5361, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12346.529296875
tensor(12346.5361, grad_fn=<NegBackward0>) tensor(12346.5293, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12346.5283203125
tensor(12346.5293, grad_fn=<NegBackward0>) tensor(12346.5283, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12346.5234375
tensor(12346.5283, grad_fn=<NegBackward0>) tensor(12346.5234, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12346.5234375
tensor(12346.5234, grad_fn=<NegBackward0>) tensor(12346.5234, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12346.517578125
tensor(12346.5234, grad_fn=<NegBackward0>) tensor(12346.5176, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12346.5166015625
tensor(12346.5176, grad_fn=<NegBackward0>) tensor(12346.5166, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12346.5146484375
tensor(12346.5166, grad_fn=<NegBackward0>) tensor(12346.5146, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12346.5126953125
tensor(12346.5146, grad_fn=<NegBackward0>) tensor(12346.5127, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12346.51171875
tensor(12346.5127, grad_fn=<NegBackward0>) tensor(12346.5117, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12346.509765625
tensor(12346.5117, grad_fn=<NegBackward0>) tensor(12346.5098, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12346.5068359375
tensor(12346.5098, grad_fn=<NegBackward0>) tensor(12346.5068, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12346.505859375
tensor(12346.5068, grad_fn=<NegBackward0>) tensor(12346.5059, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12346.5068359375
tensor(12346.5059, grad_fn=<NegBackward0>) tensor(12346.5068, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12346.5029296875
tensor(12346.5059, grad_fn=<NegBackward0>) tensor(12346.5029, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12346.50390625
tensor(12346.5029, grad_fn=<NegBackward0>) tensor(12346.5039, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12346.5029296875
tensor(12346.5029, grad_fn=<NegBackward0>) tensor(12346.5029, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12346.5009765625
tensor(12346.5029, grad_fn=<NegBackward0>) tensor(12346.5010, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12346.5009765625
tensor(12346.5010, grad_fn=<NegBackward0>) tensor(12346.5010, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12346.5009765625
tensor(12346.5010, grad_fn=<NegBackward0>) tensor(12346.5010, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12346.4990234375
tensor(12346.5010, grad_fn=<NegBackward0>) tensor(12346.4990, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12346.4970703125
tensor(12346.4990, grad_fn=<NegBackward0>) tensor(12346.4971, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12346.4970703125
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.4971, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12346.4970703125
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.4971, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12346.4970703125
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.4971, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12346.5029296875
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.5029, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12346.501953125
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.5020, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12346.4951171875
tensor(12346.4971, grad_fn=<NegBackward0>) tensor(12346.4951, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12346.494140625
tensor(12346.4951, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12346.501953125
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.5020, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12346.494140625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12346.494140625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12346.494140625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12346.4931640625
tensor(12346.4941, grad_fn=<NegBackward0>) tensor(12346.4932, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12346.4931640625
tensor(12346.4932, grad_fn=<NegBackward0>) tensor(12346.4932, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12346.4912109375
tensor(12346.4932, grad_fn=<NegBackward0>) tensor(12346.4912, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12346.5068359375
tensor(12346.4912, grad_fn=<NegBackward0>) tensor(12346.5068, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12346.494140625
tensor(12346.4912, grad_fn=<NegBackward0>) tensor(12346.4941, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12346.51953125
tensor(12346.4912, grad_fn=<NegBackward0>) tensor(12346.5195, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12346.4970703125
tensor(12346.4912, grad_fn=<NegBackward0>) tensor(12346.4971, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -12346.556640625
tensor(12346.4912, grad_fn=<NegBackward0>) tensor(12346.5566, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.0040, 0.9960],
        [0.2085, 0.7915]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5123, 0.4877], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2083, 0.2021],
         [0.6638, 0.1968]],

        [[0.6201, 0.1984],
         [0.6029, 0.6584]],

        [[0.5428, 0.2066],
         [0.6407, 0.7181]],

        [[0.5871, 0.2114],
         [0.6375, 0.5771]],

        [[0.5597, 0.1893],
         [0.6755, 0.5676]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017131861369067227
Average Adjusted Rand Index: 0.0019393939393939393
[-0.0018599660773026315, -0.0017131861369067227] [0.0, 0.0019393939393939393] [12346.4912109375, 12346.556640625]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11807.822284900833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21465.71484375
inf tensor(21465.7148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12336.505859375
tensor(21465.7148, grad_fn=<NegBackward0>) tensor(12336.5059, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12335.9521484375
tensor(12336.5059, grad_fn=<NegBackward0>) tensor(12335.9521, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12334.9013671875
tensor(12335.9521, grad_fn=<NegBackward0>) tensor(12334.9014, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12332.955078125
tensor(12334.9014, grad_fn=<NegBackward0>) tensor(12332.9551, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12332.1376953125
tensor(12332.9551, grad_fn=<NegBackward0>) tensor(12332.1377, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12331.765625
tensor(12332.1377, grad_fn=<NegBackward0>) tensor(12331.7656, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12331.337890625
tensor(12331.7656, grad_fn=<NegBackward0>) tensor(12331.3379, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12327.625
tensor(12331.3379, grad_fn=<NegBackward0>) tensor(12327.6250, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12323.0927734375
tensor(12327.6250, grad_fn=<NegBackward0>) tensor(12323.0928, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12048.5166015625
tensor(12323.0928, grad_fn=<NegBackward0>) tensor(12048.5166, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11837.6982421875
tensor(12048.5166, grad_fn=<NegBackward0>) tensor(11837.6982, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11827.494140625
tensor(11837.6982, grad_fn=<NegBackward0>) tensor(11827.4941, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11820.4150390625
tensor(11827.4941, grad_fn=<NegBackward0>) tensor(11820.4150, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11813.576171875
tensor(11820.4150, grad_fn=<NegBackward0>) tensor(11813.5762, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11810.673828125
tensor(11813.5762, grad_fn=<NegBackward0>) tensor(11810.6738, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11810.576171875
tensor(11810.6738, grad_fn=<NegBackward0>) tensor(11810.5762, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11810.5263671875
tensor(11810.5762, grad_fn=<NegBackward0>) tensor(11810.5264, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11810.4794921875
tensor(11810.5264, grad_fn=<NegBackward0>) tensor(11810.4795, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11810.4609375
tensor(11810.4795, grad_fn=<NegBackward0>) tensor(11810.4609, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11810.4384765625
tensor(11810.4609, grad_fn=<NegBackward0>) tensor(11810.4385, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11810.4287109375
tensor(11810.4385, grad_fn=<NegBackward0>) tensor(11810.4287, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11810.4189453125
tensor(11810.4287, grad_fn=<NegBackward0>) tensor(11810.4189, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11810.4111328125
tensor(11810.4189, grad_fn=<NegBackward0>) tensor(11810.4111, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11810.4052734375
tensor(11810.4111, grad_fn=<NegBackward0>) tensor(11810.4053, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11810.3984375
tensor(11810.4053, grad_fn=<NegBackward0>) tensor(11810.3984, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11810.3916015625
tensor(11810.3984, grad_fn=<NegBackward0>) tensor(11810.3916, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11810.3876953125
tensor(11810.3916, grad_fn=<NegBackward0>) tensor(11810.3877, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11810.384765625
tensor(11810.3877, grad_fn=<NegBackward0>) tensor(11810.3848, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11810.3818359375
tensor(11810.3848, grad_fn=<NegBackward0>) tensor(11810.3818, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11810.37890625
tensor(11810.3818, grad_fn=<NegBackward0>) tensor(11810.3789, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11810.3779296875
tensor(11810.3789, grad_fn=<NegBackward0>) tensor(11810.3779, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11810.373046875
tensor(11810.3779, grad_fn=<NegBackward0>) tensor(11810.3730, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11796.4287109375
tensor(11810.3730, grad_fn=<NegBackward0>) tensor(11796.4287, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11796.421875
tensor(11796.4287, grad_fn=<NegBackward0>) tensor(11796.4219, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11796.419921875
tensor(11796.4219, grad_fn=<NegBackward0>) tensor(11796.4199, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11796.41796875
tensor(11796.4199, grad_fn=<NegBackward0>) tensor(11796.4180, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11796.4169921875
tensor(11796.4180, grad_fn=<NegBackward0>) tensor(11796.4170, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11796.4287109375
tensor(11796.4170, grad_fn=<NegBackward0>) tensor(11796.4287, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11796.4140625
tensor(11796.4170, grad_fn=<NegBackward0>) tensor(11796.4141, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11796.4130859375
tensor(11796.4141, grad_fn=<NegBackward0>) tensor(11796.4131, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11796.412109375
tensor(11796.4131, grad_fn=<NegBackward0>) tensor(11796.4121, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11796.4130859375
tensor(11796.4121, grad_fn=<NegBackward0>) tensor(11796.4131, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11796.41015625
tensor(11796.4121, grad_fn=<NegBackward0>) tensor(11796.4102, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11796.41015625
tensor(11796.4102, grad_fn=<NegBackward0>) tensor(11796.4102, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11796.41015625
tensor(11796.4102, grad_fn=<NegBackward0>) tensor(11796.4102, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11796.41015625
tensor(11796.4102, grad_fn=<NegBackward0>) tensor(11796.4102, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11796.408203125
tensor(11796.4102, grad_fn=<NegBackward0>) tensor(11796.4082, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11796.4072265625
tensor(11796.4082, grad_fn=<NegBackward0>) tensor(11796.4072, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11796.4052734375
tensor(11796.4072, grad_fn=<NegBackward0>) tensor(11796.4053, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11796.408203125
tensor(11796.4053, grad_fn=<NegBackward0>) tensor(11796.4082, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11796.4072265625
tensor(11796.4053, grad_fn=<NegBackward0>) tensor(11796.4072, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11796.4052734375
tensor(11796.4053, grad_fn=<NegBackward0>) tensor(11796.4053, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11796.404296875
tensor(11796.4053, grad_fn=<NegBackward0>) tensor(11796.4043, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11796.404296875
tensor(11796.4043, grad_fn=<NegBackward0>) tensor(11796.4043, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11796.4033203125
tensor(11796.4043, grad_fn=<NegBackward0>) tensor(11796.4033, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11796.4033203125
tensor(11796.4033, grad_fn=<NegBackward0>) tensor(11796.4033, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11796.40234375
tensor(11796.4033, grad_fn=<NegBackward0>) tensor(11796.4023, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11796.40234375
tensor(11796.4023, grad_fn=<NegBackward0>) tensor(11796.4023, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11796.4033203125
tensor(11796.4023, grad_fn=<NegBackward0>) tensor(11796.4033, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11796.400390625
tensor(11796.4023, grad_fn=<NegBackward0>) tensor(11796.4004, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11796.4013671875
tensor(11796.4004, grad_fn=<NegBackward0>) tensor(11796.4014, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11796.3994140625
tensor(11796.4004, grad_fn=<NegBackward0>) tensor(11796.3994, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11796.400390625
tensor(11796.3994, grad_fn=<NegBackward0>) tensor(11796.4004, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11796.4013671875
tensor(11796.3994, grad_fn=<NegBackward0>) tensor(11796.4014, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11796.3994140625
tensor(11796.3994, grad_fn=<NegBackward0>) tensor(11796.3994, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11795.6982421875
tensor(11796.3994, grad_fn=<NegBackward0>) tensor(11795.6982, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11795.697265625
tensor(11795.6982, grad_fn=<NegBackward0>) tensor(11795.6973, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11795.697265625
tensor(11795.6973, grad_fn=<NegBackward0>) tensor(11795.6973, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11795.6943359375
tensor(11795.6973, grad_fn=<NegBackward0>) tensor(11795.6943, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11795.697265625
tensor(11795.6943, grad_fn=<NegBackward0>) tensor(11795.6973, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11795.6865234375
tensor(11795.6943, grad_fn=<NegBackward0>) tensor(11795.6865, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11795.6875
tensor(11795.6865, grad_fn=<NegBackward0>) tensor(11795.6875, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11795.6875
tensor(11795.6865, grad_fn=<NegBackward0>) tensor(11795.6875, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11795.6875
tensor(11795.6865, grad_fn=<NegBackward0>) tensor(11795.6875, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11795.6865234375
tensor(11795.6865, grad_fn=<NegBackward0>) tensor(11795.6865, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11795.6865234375
tensor(11795.6865, grad_fn=<NegBackward0>) tensor(11795.6865, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11795.685546875
tensor(11795.6865, grad_fn=<NegBackward0>) tensor(11795.6855, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11795.6875
tensor(11795.6855, grad_fn=<NegBackward0>) tensor(11795.6875, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11795.6865234375
tensor(11795.6855, grad_fn=<NegBackward0>) tensor(11795.6865, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -11795.6865234375
tensor(11795.6855, grad_fn=<NegBackward0>) tensor(11795.6865, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -11795.6865234375
tensor(11795.6855, grad_fn=<NegBackward0>) tensor(11795.6865, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -11795.693359375
tensor(11795.6855, grad_fn=<NegBackward0>) tensor(11795.6934, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.1783, 0.8217],
        [0.7598, 0.2402]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5102, 0.4898], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.0913],
         [0.6149, 0.2973]],

        [[0.5940, 0.1103],
         [0.5235, 0.5357]],

        [[0.5212, 0.1063],
         [0.6732, 0.5164]],

        [[0.6301, 0.0934],
         [0.5707, 0.6923]],

        [[0.7251, 0.0902],
         [0.6478, 0.5484]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.036502623190869034
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23735.16796875
inf tensor(23735.1680, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12339.021484375
tensor(23735.1680, grad_fn=<NegBackward0>) tensor(12339.0215, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12336.5185546875
tensor(12339.0215, grad_fn=<NegBackward0>) tensor(12336.5186, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12336.4150390625
tensor(12336.5186, grad_fn=<NegBackward0>) tensor(12336.4150, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12336.3623046875
tensor(12336.4150, grad_fn=<NegBackward0>) tensor(12336.3623, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12336.3271484375
tensor(12336.3623, grad_fn=<NegBackward0>) tensor(12336.3271, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12336.296875
tensor(12336.3271, grad_fn=<NegBackward0>) tensor(12336.2969, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12336.2607421875
tensor(12336.2969, grad_fn=<NegBackward0>) tensor(12336.2607, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12336.197265625
tensor(12336.2607, grad_fn=<NegBackward0>) tensor(12336.1973, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12336.029296875
tensor(12336.1973, grad_fn=<NegBackward0>) tensor(12336.0293, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12335.681640625
tensor(12336.0293, grad_fn=<NegBackward0>) tensor(12335.6816, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12335.51953125
tensor(12335.6816, grad_fn=<NegBackward0>) tensor(12335.5195, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12335.40234375
tensor(12335.5195, grad_fn=<NegBackward0>) tensor(12335.4023, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12335.2890625
tensor(12335.4023, grad_fn=<NegBackward0>) tensor(12335.2891, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12335.1630859375
tensor(12335.2891, grad_fn=<NegBackward0>) tensor(12335.1631, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12335.0205078125
tensor(12335.1631, grad_fn=<NegBackward0>) tensor(12335.0205, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12334.880859375
tensor(12335.0205, grad_fn=<NegBackward0>) tensor(12334.8809, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12334.765625
tensor(12334.8809, grad_fn=<NegBackward0>) tensor(12334.7656, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12334.7001953125
tensor(12334.7656, grad_fn=<NegBackward0>) tensor(12334.7002, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12334.671875
tensor(12334.7002, grad_fn=<NegBackward0>) tensor(12334.6719, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12334.6689453125
tensor(12334.6719, grad_fn=<NegBackward0>) tensor(12334.6689, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12334.662109375
tensor(12334.6689, grad_fn=<NegBackward0>) tensor(12334.6621, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12334.6689453125
tensor(12334.6621, grad_fn=<NegBackward0>) tensor(12334.6689, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -12334.658203125
tensor(12334.6621, grad_fn=<NegBackward0>) tensor(12334.6582, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12334.658203125
tensor(12334.6582, grad_fn=<NegBackward0>) tensor(12334.6582, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12334.65625
tensor(12334.6582, grad_fn=<NegBackward0>) tensor(12334.6562, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12334.65625
tensor(12334.6562, grad_fn=<NegBackward0>) tensor(12334.6562, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12334.6533203125
tensor(12334.6562, grad_fn=<NegBackward0>) tensor(12334.6533, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12334.6640625
tensor(12334.6533, grad_fn=<NegBackward0>) tensor(12334.6641, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12334.6484375
tensor(12334.6533, grad_fn=<NegBackward0>) tensor(12334.6484, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12334.64453125
tensor(12334.6484, grad_fn=<NegBackward0>) tensor(12334.6445, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12334.6357421875
tensor(12334.6445, grad_fn=<NegBackward0>) tensor(12334.6357, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12334.6201171875
tensor(12334.6357, grad_fn=<NegBackward0>) tensor(12334.6201, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12334.564453125
tensor(12334.6201, grad_fn=<NegBackward0>) tensor(12334.5645, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11874.8251953125
tensor(12334.5645, grad_fn=<NegBackward0>) tensor(11874.8252, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11795.2138671875
tensor(11874.8252, grad_fn=<NegBackward0>) tensor(11795.2139, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11795.203125
tensor(11795.2139, grad_fn=<NegBackward0>) tensor(11795.2031, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11795.1943359375
tensor(11795.2031, grad_fn=<NegBackward0>) tensor(11795.1943, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11795.1845703125
tensor(11795.1943, grad_fn=<NegBackward0>) tensor(11795.1846, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11795.171875
tensor(11795.1846, grad_fn=<NegBackward0>) tensor(11795.1719, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11795.1689453125
tensor(11795.1719, grad_fn=<NegBackward0>) tensor(11795.1689, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11795.1640625
tensor(11795.1689, grad_fn=<NegBackward0>) tensor(11795.1641, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11795.1591796875
tensor(11795.1641, grad_fn=<NegBackward0>) tensor(11795.1592, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11795.1513671875
tensor(11795.1592, grad_fn=<NegBackward0>) tensor(11795.1514, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11795.111328125
tensor(11795.1514, grad_fn=<NegBackward0>) tensor(11795.1113, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11795.1103515625
tensor(11795.1113, grad_fn=<NegBackward0>) tensor(11795.1104, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11795.109375
tensor(11795.1104, grad_fn=<NegBackward0>) tensor(11795.1094, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11795.1083984375
tensor(11795.1094, grad_fn=<NegBackward0>) tensor(11795.1084, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11795.1083984375
tensor(11795.1084, grad_fn=<NegBackward0>) tensor(11795.1084, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11795.1064453125
tensor(11795.1084, grad_fn=<NegBackward0>) tensor(11795.1064, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11795.1025390625
tensor(11795.1064, grad_fn=<NegBackward0>) tensor(11795.1025, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11795.1015625
tensor(11795.1025, grad_fn=<NegBackward0>) tensor(11795.1016, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11795.1005859375
tensor(11795.1016, grad_fn=<NegBackward0>) tensor(11795.1006, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11795.099609375
tensor(11795.1006, grad_fn=<NegBackward0>) tensor(11795.0996, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11795.0986328125
tensor(11795.0996, grad_fn=<NegBackward0>) tensor(11795.0986, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11795.0986328125
tensor(11795.0986, grad_fn=<NegBackward0>) tensor(11795.0986, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11795.0986328125
tensor(11795.0986, grad_fn=<NegBackward0>) tensor(11795.0986, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11795.09765625
tensor(11795.0986, grad_fn=<NegBackward0>) tensor(11795.0977, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11795.1083984375
tensor(11795.0977, grad_fn=<NegBackward0>) tensor(11795.1084, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11795.09765625
tensor(11795.0977, grad_fn=<NegBackward0>) tensor(11795.0977, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11795.0966796875
tensor(11795.0977, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11795.099609375
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.0996, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11795.0966796875
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11795.109375
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.1094, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11795.0966796875
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11795.0966796875
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11795.0966796875
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11795.095703125
tensor(11795.0967, grad_fn=<NegBackward0>) tensor(11795.0957, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11795.095703125
tensor(11795.0957, grad_fn=<NegBackward0>) tensor(11795.0957, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11795.0966796875
tensor(11795.0957, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11795.0986328125
tensor(11795.0957, grad_fn=<NegBackward0>) tensor(11795.0986, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11795.095703125
tensor(11795.0957, grad_fn=<NegBackward0>) tensor(11795.0957, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11795.0947265625
tensor(11795.0957, grad_fn=<NegBackward0>) tensor(11795.0947, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11795.0947265625
tensor(11795.0947, grad_fn=<NegBackward0>) tensor(11795.0947, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11795.0966796875
tensor(11795.0947, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11795.095703125
tensor(11795.0947, grad_fn=<NegBackward0>) tensor(11795.0957, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11795.095703125
tensor(11795.0947, grad_fn=<NegBackward0>) tensor(11795.0957, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11795.15625
tensor(11795.0947, grad_fn=<NegBackward0>) tensor(11795.1562, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11795.0966796875
tensor(11795.0947, grad_fn=<NegBackward0>) tensor(11795.0967, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.8152, 0.1848],
        [0.2461, 0.7539]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5101, 0.4899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3011, 0.0912],
         [0.5585, 0.2931]],

        [[0.6592, 0.1103],
         [0.7219, 0.6827]],

        [[0.5785, 0.1062],
         [0.5108, 0.7231]],

        [[0.6041, 0.0934],
         [0.5855, 0.6523]],

        [[0.6210, 0.0902],
         [0.7103, 0.6232]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.992
[0.036502623190869034, 0.9919994334721989] [0.992, 0.992] [11795.693359375, 11795.0966796875]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11905.542073034829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20757.201171875
inf tensor(20757.2012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12451.5283203125
tensor(20757.2012, grad_fn=<NegBackward0>) tensor(12451.5283, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12451.333984375
tensor(12451.5283, grad_fn=<NegBackward0>) tensor(12451.3340, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12451.2724609375
tensor(12451.3340, grad_fn=<NegBackward0>) tensor(12451.2725, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12451.23046875
tensor(12451.2725, grad_fn=<NegBackward0>) tensor(12451.2305, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12451.1943359375
tensor(12451.2305, grad_fn=<NegBackward0>) tensor(12451.1943, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12451.1572265625
tensor(12451.1943, grad_fn=<NegBackward0>) tensor(12451.1572, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12451.1279296875
tensor(12451.1572, grad_fn=<NegBackward0>) tensor(12451.1279, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12451.09765625
tensor(12451.1279, grad_fn=<NegBackward0>) tensor(12451.0977, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12451.0595703125
tensor(12451.0977, grad_fn=<NegBackward0>) tensor(12451.0596, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12451.0078125
tensor(12451.0596, grad_fn=<NegBackward0>) tensor(12451.0078, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12450.9345703125
tensor(12451.0078, grad_fn=<NegBackward0>) tensor(12450.9346, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12450.8564453125
tensor(12450.9346, grad_fn=<NegBackward0>) tensor(12450.8564, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12450.7900390625
tensor(12450.8564, grad_fn=<NegBackward0>) tensor(12450.7900, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12450.736328125
tensor(12450.7900, grad_fn=<NegBackward0>) tensor(12450.7363, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12450.7197265625
tensor(12450.7363, grad_fn=<NegBackward0>) tensor(12450.7197, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12450.673828125
tensor(12450.7197, grad_fn=<NegBackward0>) tensor(12450.6738, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12450.6552734375
tensor(12450.6738, grad_fn=<NegBackward0>) tensor(12450.6553, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12450.640625
tensor(12450.6553, grad_fn=<NegBackward0>) tensor(12450.6406, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12450.6279296875
tensor(12450.6406, grad_fn=<NegBackward0>) tensor(12450.6279, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12450.6181640625
tensor(12450.6279, grad_fn=<NegBackward0>) tensor(12450.6182, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12450.6103515625
tensor(12450.6182, grad_fn=<NegBackward0>) tensor(12450.6104, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12450.603515625
tensor(12450.6104, grad_fn=<NegBackward0>) tensor(12450.6035, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12450.595703125
tensor(12450.6035, grad_fn=<NegBackward0>) tensor(12450.5957, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12450.591796875
tensor(12450.5957, grad_fn=<NegBackward0>) tensor(12450.5918, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12450.587890625
tensor(12450.5918, grad_fn=<NegBackward0>) tensor(12450.5879, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12450.5830078125
tensor(12450.5879, grad_fn=<NegBackward0>) tensor(12450.5830, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12450.580078125
tensor(12450.5830, grad_fn=<NegBackward0>) tensor(12450.5801, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12450.5771484375
tensor(12450.5801, grad_fn=<NegBackward0>) tensor(12450.5771, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12450.57421875
tensor(12450.5771, grad_fn=<NegBackward0>) tensor(12450.5742, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12450.572265625
tensor(12450.5742, grad_fn=<NegBackward0>) tensor(12450.5723, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12450.5703125
tensor(12450.5723, grad_fn=<NegBackward0>) tensor(12450.5703, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12450.5673828125
tensor(12450.5703, grad_fn=<NegBackward0>) tensor(12450.5674, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12450.5673828125
tensor(12450.5674, grad_fn=<NegBackward0>) tensor(12450.5674, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12450.5634765625
tensor(12450.5674, grad_fn=<NegBackward0>) tensor(12450.5635, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12450.5634765625
tensor(12450.5635, grad_fn=<NegBackward0>) tensor(12450.5635, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12450.560546875
tensor(12450.5635, grad_fn=<NegBackward0>) tensor(12450.5605, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12450.5576171875
tensor(12450.5605, grad_fn=<NegBackward0>) tensor(12450.5576, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12450.5546875
tensor(12450.5576, grad_fn=<NegBackward0>) tensor(12450.5547, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12450.552734375
tensor(12450.5547, grad_fn=<NegBackward0>) tensor(12450.5527, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12450.5498046875
tensor(12450.5527, grad_fn=<NegBackward0>) tensor(12450.5498, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12450.546875
tensor(12450.5498, grad_fn=<NegBackward0>) tensor(12450.5469, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12450.5458984375
tensor(12450.5469, grad_fn=<NegBackward0>) tensor(12450.5459, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12450.54296875
tensor(12450.5459, grad_fn=<NegBackward0>) tensor(12450.5430, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12450.541015625
tensor(12450.5430, grad_fn=<NegBackward0>) tensor(12450.5410, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12450.5390625
tensor(12450.5410, grad_fn=<NegBackward0>) tensor(12450.5391, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12450.5380859375
tensor(12450.5391, grad_fn=<NegBackward0>) tensor(12450.5381, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12450.53515625
tensor(12450.5381, grad_fn=<NegBackward0>) tensor(12450.5352, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12450.5361328125
tensor(12450.5352, grad_fn=<NegBackward0>) tensor(12450.5361, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12450.53125
tensor(12450.5352, grad_fn=<NegBackward0>) tensor(12450.5312, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12450.5302734375
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5303, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12450.5302734375
tensor(12450.5303, grad_fn=<NegBackward0>) tensor(12450.5303, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12450.5283203125
tensor(12450.5303, grad_fn=<NegBackward0>) tensor(12450.5283, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12450.52734375
tensor(12450.5283, grad_fn=<NegBackward0>) tensor(12450.5273, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12450.52734375
tensor(12450.5273, grad_fn=<NegBackward0>) tensor(12450.5273, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12450.525390625
tensor(12450.5273, grad_fn=<NegBackward0>) tensor(12450.5254, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12450.5244140625
tensor(12450.5254, grad_fn=<NegBackward0>) tensor(12450.5244, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12450.525390625
tensor(12450.5244, grad_fn=<NegBackward0>) tensor(12450.5254, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12450.5234375
tensor(12450.5244, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12450.5244140625
tensor(12450.5234, grad_fn=<NegBackward0>) tensor(12450.5244, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12450.5234375
tensor(12450.5234, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12450.5224609375
tensor(12450.5234, grad_fn=<NegBackward0>) tensor(12450.5225, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12450.5234375
tensor(12450.5225, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12450.5224609375
tensor(12450.5225, grad_fn=<NegBackward0>) tensor(12450.5225, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12450.521484375
tensor(12450.5225, grad_fn=<NegBackward0>) tensor(12450.5215, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12450.5244140625
tensor(12450.5215, grad_fn=<NegBackward0>) tensor(12450.5244, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12450.5234375
tensor(12450.5215, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12450.5234375
tensor(12450.5215, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12450.5234375
tensor(12450.5215, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -12450.5234375
tensor(12450.5215, grad_fn=<NegBackward0>) tensor(12450.5234, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.0015, 0.9985],
        [0.1258, 0.8742]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9164, 0.0836], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2069, 0.2022],
         [0.6126, 0.2002]],

        [[0.6001, 0.2913],
         [0.7256, 0.7037]],

        [[0.6239, 0.1967],
         [0.7205, 0.5034]],

        [[0.6309, 0.1991],
         [0.5735, 0.5276]],

        [[0.5647, 0.2074],
         [0.6930, 0.6223]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00029486190291748903
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20159.875
inf tensor(20159.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12451.2265625
tensor(20159.8750, grad_fn=<NegBackward0>) tensor(12451.2266, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12450.9755859375
tensor(12451.2266, grad_fn=<NegBackward0>) tensor(12450.9756, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12450.916015625
tensor(12450.9756, grad_fn=<NegBackward0>) tensor(12450.9160, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12450.8876953125
tensor(12450.9160, grad_fn=<NegBackward0>) tensor(12450.8877, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12450.87109375
tensor(12450.8877, grad_fn=<NegBackward0>) tensor(12450.8711, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12450.85546875
tensor(12450.8711, grad_fn=<NegBackward0>) tensor(12450.8555, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12450.8447265625
tensor(12450.8555, grad_fn=<NegBackward0>) tensor(12450.8447, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12450.8359375
tensor(12450.8447, grad_fn=<NegBackward0>) tensor(12450.8359, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12450.826171875
tensor(12450.8359, grad_fn=<NegBackward0>) tensor(12450.8262, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12450.8154296875
tensor(12450.8262, grad_fn=<NegBackward0>) tensor(12450.8154, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12450.802734375
tensor(12450.8154, grad_fn=<NegBackward0>) tensor(12450.8027, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12450.7744140625
tensor(12450.8027, grad_fn=<NegBackward0>) tensor(12450.7744, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12450.7099609375
tensor(12450.7744, grad_fn=<NegBackward0>) tensor(12450.7100, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12450.6357421875
tensor(12450.7100, grad_fn=<NegBackward0>) tensor(12450.6357, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12450.5986328125
tensor(12450.6357, grad_fn=<NegBackward0>) tensor(12450.5986, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12450.5859375
tensor(12450.5986, grad_fn=<NegBackward0>) tensor(12450.5859, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12450.578125
tensor(12450.5859, grad_fn=<NegBackward0>) tensor(12450.5781, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12450.576171875
tensor(12450.5781, grad_fn=<NegBackward0>) tensor(12450.5762, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12450.57421875
tensor(12450.5762, grad_fn=<NegBackward0>) tensor(12450.5742, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12450.5712890625
tensor(12450.5742, grad_fn=<NegBackward0>) tensor(12450.5713, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12450.568359375
tensor(12450.5713, grad_fn=<NegBackward0>) tensor(12450.5684, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12450.5654296875
tensor(12450.5684, grad_fn=<NegBackward0>) tensor(12450.5654, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12450.5625
tensor(12450.5654, grad_fn=<NegBackward0>) tensor(12450.5625, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12450.5595703125
tensor(12450.5625, grad_fn=<NegBackward0>) tensor(12450.5596, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12450.5576171875
tensor(12450.5596, grad_fn=<NegBackward0>) tensor(12450.5576, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12450.5546875
tensor(12450.5576, grad_fn=<NegBackward0>) tensor(12450.5547, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12450.55078125
tensor(12450.5547, grad_fn=<NegBackward0>) tensor(12450.5508, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12450.5478515625
tensor(12450.5508, grad_fn=<NegBackward0>) tensor(12450.5479, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12450.544921875
tensor(12450.5479, grad_fn=<NegBackward0>) tensor(12450.5449, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12450.544921875
tensor(12450.5449, grad_fn=<NegBackward0>) tensor(12450.5449, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12450.541015625
tensor(12450.5449, grad_fn=<NegBackward0>) tensor(12450.5410, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12450.5400390625
tensor(12450.5410, grad_fn=<NegBackward0>) tensor(12450.5400, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12450.537109375
tensor(12450.5400, grad_fn=<NegBackward0>) tensor(12450.5371, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12450.5361328125
tensor(12450.5371, grad_fn=<NegBackward0>) tensor(12450.5361, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12450.53515625
tensor(12450.5361, grad_fn=<NegBackward0>) tensor(12450.5352, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12450.53515625
tensor(12450.5352, grad_fn=<NegBackward0>) tensor(12450.5352, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12450.533203125
tensor(12450.5352, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12450.533203125
tensor(12450.5332, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12450.533203125
tensor(12450.5332, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12450.533203125
tensor(12450.5332, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12450.533203125
tensor(12450.5332, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12450.5322265625
tensor(12450.5332, grad_fn=<NegBackward0>) tensor(12450.5322, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12450.5322265625
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5322, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12450.533203125
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12450.5322265625
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5322, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12450.533203125
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12450.533203125
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -12450.5322265625
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5322, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12450.5322265625
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5322, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12450.53125
tensor(12450.5322, grad_fn=<NegBackward0>) tensor(12450.5312, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12450.53125
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5312, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12450.533203125
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12450.53125
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5312, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12450.5400390625
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5400, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12450.533203125
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12450.533203125
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5332, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12450.5322265625
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5322, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12450.53515625
tensor(12450.5312, grad_fn=<NegBackward0>) tensor(12450.5352, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.0111, 0.9889],
        [0.1105, 0.8895]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9925, 0.0075], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2062, 0.2016],
         [0.5063, 0.2003]],

        [[0.5970, 0.2901],
         [0.7214, 0.6686]],

        [[0.5570, 0.1961],
         [0.5855, 0.6851]],

        [[0.7004, 0.1986],
         [0.7088, 0.6554]],

        [[0.5795, 0.2075],
         [0.6574, 0.6299]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00029486190291748903
Average Adjusted Rand Index: 0.0
[0.00029486190291748903, 0.00029486190291748903] [0.0, 0.0] [12450.5234375, 12450.53515625]
-------------------------------------
This iteration is 11
True Objective function: Loss = -11822.074886207021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20150.69140625
inf tensor(20150.6914, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12307.2255859375
tensor(20150.6914, grad_fn=<NegBackward0>) tensor(12307.2256, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12306.9677734375
tensor(12307.2256, grad_fn=<NegBackward0>) tensor(12306.9678, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12306.8603515625
tensor(12306.9678, grad_fn=<NegBackward0>) tensor(12306.8604, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12306.7197265625
tensor(12306.8604, grad_fn=<NegBackward0>) tensor(12306.7197, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12306.345703125
tensor(12306.7197, grad_fn=<NegBackward0>) tensor(12306.3457, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12306.025390625
tensor(12306.3457, grad_fn=<NegBackward0>) tensor(12306.0254, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12305.98046875
tensor(12306.0254, grad_fn=<NegBackward0>) tensor(12305.9805, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12305.9375
tensor(12305.9805, grad_fn=<NegBackward0>) tensor(12305.9375, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12305.880859375
tensor(12305.9375, grad_fn=<NegBackward0>) tensor(12305.8809, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12305.7939453125
tensor(12305.8809, grad_fn=<NegBackward0>) tensor(12305.7939, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12305.626953125
tensor(12305.7939, grad_fn=<NegBackward0>) tensor(12305.6270, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12305.267578125
tensor(12305.6270, grad_fn=<NegBackward0>) tensor(12305.2676, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12304.8115234375
tensor(12305.2676, grad_fn=<NegBackward0>) tensor(12304.8115, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12304.6630859375
tensor(12304.8115, grad_fn=<NegBackward0>) tensor(12304.6631, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12304.5693359375
tensor(12304.6631, grad_fn=<NegBackward0>) tensor(12304.5693, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12304.3076171875
tensor(12304.5693, grad_fn=<NegBackward0>) tensor(12304.3076, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12303.927734375
tensor(12304.3076, grad_fn=<NegBackward0>) tensor(12303.9277, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12303.7763671875
tensor(12303.9277, grad_fn=<NegBackward0>) tensor(12303.7764, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12303.6796875
tensor(12303.7764, grad_fn=<NegBackward0>) tensor(12303.6797, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12303.3994140625
tensor(12303.6797, grad_fn=<NegBackward0>) tensor(12303.3994, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12050.591796875
tensor(12303.3994, grad_fn=<NegBackward0>) tensor(12050.5918, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11871.2392578125
tensor(12050.5918, grad_fn=<NegBackward0>) tensor(11871.2393, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11862.212890625
tensor(11871.2393, grad_fn=<NegBackward0>) tensor(11862.2129, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11861.9423828125
tensor(11862.2129, grad_fn=<NegBackward0>) tensor(11861.9424, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11861.923828125
tensor(11861.9424, grad_fn=<NegBackward0>) tensor(11861.9238, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11861.9140625
tensor(11861.9238, grad_fn=<NegBackward0>) tensor(11861.9141, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11861.873046875
tensor(11861.9141, grad_fn=<NegBackward0>) tensor(11861.8730, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11861.8671875
tensor(11861.8730, grad_fn=<NegBackward0>) tensor(11861.8672, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11861.8642578125
tensor(11861.8672, grad_fn=<NegBackward0>) tensor(11861.8643, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11861.861328125
tensor(11861.8643, grad_fn=<NegBackward0>) tensor(11861.8613, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11861.859375
tensor(11861.8613, grad_fn=<NegBackward0>) tensor(11861.8594, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11861.857421875
tensor(11861.8594, grad_fn=<NegBackward0>) tensor(11861.8574, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11861.8544921875
tensor(11861.8574, grad_fn=<NegBackward0>) tensor(11861.8545, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11861.8515625
tensor(11861.8545, grad_fn=<NegBackward0>) tensor(11861.8516, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11861.8427734375
tensor(11861.8516, grad_fn=<NegBackward0>) tensor(11861.8428, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11861.83984375
tensor(11861.8428, grad_fn=<NegBackward0>) tensor(11861.8398, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11861.8359375
tensor(11861.8398, grad_fn=<NegBackward0>) tensor(11861.8359, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11861.8271484375
tensor(11861.8359, grad_fn=<NegBackward0>) tensor(11861.8271, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11861.8271484375
tensor(11861.8271, grad_fn=<NegBackward0>) tensor(11861.8271, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11861.826171875
tensor(11861.8271, grad_fn=<NegBackward0>) tensor(11861.8262, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11861.826171875
tensor(11861.8262, grad_fn=<NegBackward0>) tensor(11861.8262, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11861.8251953125
tensor(11861.8262, grad_fn=<NegBackward0>) tensor(11861.8252, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11861.8251953125
tensor(11861.8252, grad_fn=<NegBackward0>) tensor(11861.8252, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11861.826171875
tensor(11861.8252, grad_fn=<NegBackward0>) tensor(11861.8262, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11861.82421875
tensor(11861.8252, grad_fn=<NegBackward0>) tensor(11861.8242, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11861.8232421875
tensor(11861.8242, grad_fn=<NegBackward0>) tensor(11861.8232, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11861.8271484375
tensor(11861.8232, grad_fn=<NegBackward0>) tensor(11861.8271, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11861.8232421875
tensor(11861.8232, grad_fn=<NegBackward0>) tensor(11861.8232, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11861.822265625
tensor(11861.8232, grad_fn=<NegBackward0>) tensor(11861.8223, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11861.8232421875
tensor(11861.8223, grad_fn=<NegBackward0>) tensor(11861.8232, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11861.8212890625
tensor(11861.8223, grad_fn=<NegBackward0>) tensor(11861.8213, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11861.8212890625
tensor(11861.8213, grad_fn=<NegBackward0>) tensor(11861.8213, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11861.822265625
tensor(11861.8213, grad_fn=<NegBackward0>) tensor(11861.8223, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11861.818359375
tensor(11861.8213, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11861.818359375
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11861.8193359375
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8193, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11861.822265625
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8223, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11861.818359375
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11861.8203125
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8203, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11861.818359375
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11861.818359375
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11861.818359375
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11861.8173828125
tensor(11861.8184, grad_fn=<NegBackward0>) tensor(11861.8174, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11861.814453125
tensor(11861.8174, grad_fn=<NegBackward0>) tensor(11861.8145, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11861.81640625
tensor(11861.8145, grad_fn=<NegBackward0>) tensor(11861.8164, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11861.818359375
tensor(11861.8145, grad_fn=<NegBackward0>) tensor(11861.8184, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11861.8134765625
tensor(11861.8145, grad_fn=<NegBackward0>) tensor(11861.8135, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11861.822265625
tensor(11861.8135, grad_fn=<NegBackward0>) tensor(11861.8223, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11861.8125
tensor(11861.8135, grad_fn=<NegBackward0>) tensor(11861.8125, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11861.8125
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8125, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11861.8134765625
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8135, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11861.814453125
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8145, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11861.8173828125
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8174, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11861.81640625
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8164, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11861.8125
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8125, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11861.8134765625
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8135, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11861.814453125
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8145, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11861.8466796875
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8467, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11861.8125
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8125, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11861.822265625
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8223, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11861.8115234375
tensor(11861.8125, grad_fn=<NegBackward0>) tensor(11861.8115, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11861.8955078125
tensor(11861.8115, grad_fn=<NegBackward0>) tensor(11861.8955, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11861.8125
tensor(11861.8115, grad_fn=<NegBackward0>) tensor(11861.8125, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11861.833984375
tensor(11861.8115, grad_fn=<NegBackward0>) tensor(11861.8340, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11861.8125
tensor(11861.8115, grad_fn=<NegBackward0>) tensor(11861.8125, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11861.8203125
tensor(11861.8115, grad_fn=<NegBackward0>) tensor(11861.8203, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.5456, 0.4544],
        [0.5211, 0.4789]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4483, 0.5517], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2960, 0.0924],
         [0.6571, 0.2960]],

        [[0.7034, 0.1067],
         [0.5414, 0.6171]],

        [[0.6710, 0.1015],
         [0.6497, 0.5852]],

        [[0.6446, 0.1021],
         [0.6201, 0.7280]],

        [[0.6000, 0.0940],
         [0.7131, 0.7247]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.34443167343386316
Average Adjusted Rand Index: 0.976161616161616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21254.169921875
inf tensor(21254.1699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12307.2294921875
tensor(21254.1699, grad_fn=<NegBackward0>) tensor(12307.2295, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12306.970703125
tensor(12307.2295, grad_fn=<NegBackward0>) tensor(12306.9707, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12306.876953125
tensor(12306.9707, grad_fn=<NegBackward0>) tensor(12306.8770, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12306.7841796875
tensor(12306.8770, grad_fn=<NegBackward0>) tensor(12306.7842, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12306.6650390625
tensor(12306.7842, grad_fn=<NegBackward0>) tensor(12306.6650, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12306.4765625
tensor(12306.6650, grad_fn=<NegBackward0>) tensor(12306.4766, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12306.1611328125
tensor(12306.4766, grad_fn=<NegBackward0>) tensor(12306.1611, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12305.9765625
tensor(12306.1611, grad_fn=<NegBackward0>) tensor(12305.9766, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12305.9091796875
tensor(12305.9766, grad_fn=<NegBackward0>) tensor(12305.9092, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12305.8701171875
tensor(12305.9092, grad_fn=<NegBackward0>) tensor(12305.8701, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12305.8427734375
tensor(12305.8701, grad_fn=<NegBackward0>) tensor(12305.8428, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12305.8212890625
tensor(12305.8428, grad_fn=<NegBackward0>) tensor(12305.8213, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12305.8017578125
tensor(12305.8213, grad_fn=<NegBackward0>) tensor(12305.8018, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12305.7841796875
tensor(12305.8018, grad_fn=<NegBackward0>) tensor(12305.7842, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12305.7626953125
tensor(12305.7842, grad_fn=<NegBackward0>) tensor(12305.7627, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12305.7373046875
tensor(12305.7627, grad_fn=<NegBackward0>) tensor(12305.7373, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12305.693359375
tensor(12305.7373, grad_fn=<NegBackward0>) tensor(12305.6934, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12305.6142578125
tensor(12305.6934, grad_fn=<NegBackward0>) tensor(12305.6143, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12305.416015625
tensor(12305.6143, grad_fn=<NegBackward0>) tensor(12305.4160, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12304.7783203125
tensor(12305.4160, grad_fn=<NegBackward0>) tensor(12304.7783, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12303.966796875
tensor(12304.7783, grad_fn=<NegBackward0>) tensor(12303.9668, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12303.6640625
tensor(12303.9668, grad_fn=<NegBackward0>) tensor(12303.6641, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12303.533203125
tensor(12303.6641, grad_fn=<NegBackward0>) tensor(12303.5332, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12303.462890625
tensor(12303.5332, grad_fn=<NegBackward0>) tensor(12303.4629, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12303.427734375
tensor(12303.4629, grad_fn=<NegBackward0>) tensor(12303.4277, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12303.40234375
tensor(12303.4277, grad_fn=<NegBackward0>) tensor(12303.4023, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12303.3876953125
tensor(12303.4023, grad_fn=<NegBackward0>) tensor(12303.3877, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12303.375
tensor(12303.3877, grad_fn=<NegBackward0>) tensor(12303.3750, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12303.365234375
tensor(12303.3750, grad_fn=<NegBackward0>) tensor(12303.3652, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12303.3583984375
tensor(12303.3652, grad_fn=<NegBackward0>) tensor(12303.3584, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12303.353515625
tensor(12303.3584, grad_fn=<NegBackward0>) tensor(12303.3535, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12303.3486328125
tensor(12303.3535, grad_fn=<NegBackward0>) tensor(12303.3486, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12303.345703125
tensor(12303.3486, grad_fn=<NegBackward0>) tensor(12303.3457, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12303.3408203125
tensor(12303.3457, grad_fn=<NegBackward0>) tensor(12303.3408, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12303.337890625
tensor(12303.3408, grad_fn=<NegBackward0>) tensor(12303.3379, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12303.3369140625
tensor(12303.3379, grad_fn=<NegBackward0>) tensor(12303.3369, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12303.333984375
tensor(12303.3369, grad_fn=<NegBackward0>) tensor(12303.3340, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12303.3310546875
tensor(12303.3340, grad_fn=<NegBackward0>) tensor(12303.3311, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12303.3310546875
tensor(12303.3311, grad_fn=<NegBackward0>) tensor(12303.3311, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12303.330078125
tensor(12303.3311, grad_fn=<NegBackward0>) tensor(12303.3301, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12303.328125
tensor(12303.3301, grad_fn=<NegBackward0>) tensor(12303.3281, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12303.3271484375
tensor(12303.3281, grad_fn=<NegBackward0>) tensor(12303.3271, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12303.3271484375
tensor(12303.3271, grad_fn=<NegBackward0>) tensor(12303.3271, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12303.326171875
tensor(12303.3271, grad_fn=<NegBackward0>) tensor(12303.3262, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12303.3251953125
tensor(12303.3262, grad_fn=<NegBackward0>) tensor(12303.3252, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12303.3251953125
tensor(12303.3252, grad_fn=<NegBackward0>) tensor(12303.3252, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12303.322265625
tensor(12303.3252, grad_fn=<NegBackward0>) tensor(12303.3223, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12303.3232421875
tensor(12303.3223, grad_fn=<NegBackward0>) tensor(12303.3232, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12303.3232421875
tensor(12303.3223, grad_fn=<NegBackward0>) tensor(12303.3232, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -12303.322265625
tensor(12303.3223, grad_fn=<NegBackward0>) tensor(12303.3223, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12303.322265625
tensor(12303.3223, grad_fn=<NegBackward0>) tensor(12303.3223, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12303.322265625
tensor(12303.3223, grad_fn=<NegBackward0>) tensor(12303.3223, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12303.3212890625
tensor(12303.3223, grad_fn=<NegBackward0>) tensor(12303.3213, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12303.3203125
tensor(12303.3213, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12303.322265625
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3223, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12303.3212890625
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3213, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12303.3203125
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12303.3203125
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12303.3203125
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12303.3203125
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12303.3203125
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12303.318359375
tensor(12303.3203, grad_fn=<NegBackward0>) tensor(12303.3184, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12303.318359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3184, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12303.3193359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3193, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12303.3193359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3193, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12303.3193359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3193, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12303.318359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3184, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12303.3193359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3193, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12303.3203125
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3203, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12303.322265625
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3223, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -12303.353515625
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3535, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -12303.3193359375
tensor(12303.3184, grad_fn=<NegBackward0>) tensor(12303.3193, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.9942, 0.0058],
        [0.9912, 0.0088]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0146, 0.9854], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1863],
         [0.6163, 0.1850]],

        [[0.6166, 0.3253],
         [0.5263, 0.6335]],

        [[0.6230, 0.2025],
         [0.6089, 0.6943]],

        [[0.6643, 0.1988],
         [0.5190, 0.5021]],

        [[0.5114, 0.2583],
         [0.6028, 0.5816]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00031954652715849544
Average Adjusted Rand Index: -0.0004529465619428516
[0.34443167343386316, -0.00031954652715849544] [0.976161616161616, -0.0004529465619428516] [11861.8203125, 12303.3193359375]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11959.961666528518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22426.955078125
inf tensor(22426.9551, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12440.4423828125
tensor(22426.9551, grad_fn=<NegBackward0>) tensor(12440.4424, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12440.197265625
tensor(12440.4424, grad_fn=<NegBackward0>) tensor(12440.1973, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12440.1318359375
tensor(12440.1973, grad_fn=<NegBackward0>) tensor(12440.1318, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12440.0732421875
tensor(12440.1318, grad_fn=<NegBackward0>) tensor(12440.0732, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12440.015625
tensor(12440.0732, grad_fn=<NegBackward0>) tensor(12440.0156, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12439.9638671875
tensor(12440.0156, grad_fn=<NegBackward0>) tensor(12439.9639, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12439.91796875
tensor(12439.9639, grad_fn=<NegBackward0>) tensor(12439.9180, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12439.876953125
tensor(12439.9180, grad_fn=<NegBackward0>) tensor(12439.8770, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12439.83984375
tensor(12439.8770, grad_fn=<NegBackward0>) tensor(12439.8398, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12439.8125
tensor(12439.8398, grad_fn=<NegBackward0>) tensor(12439.8125, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12439.787109375
tensor(12439.8125, grad_fn=<NegBackward0>) tensor(12439.7871, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12439.763671875
tensor(12439.7871, grad_fn=<NegBackward0>) tensor(12439.7637, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12439.740234375
tensor(12439.7637, grad_fn=<NegBackward0>) tensor(12439.7402, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12439.712890625
tensor(12439.7402, grad_fn=<NegBackward0>) tensor(12439.7129, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12439.6689453125
tensor(12439.7129, grad_fn=<NegBackward0>) tensor(12439.6689, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12439.57421875
tensor(12439.6689, grad_fn=<NegBackward0>) tensor(12439.5742, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12439.4169921875
tensor(12439.5742, grad_fn=<NegBackward0>) tensor(12439.4170, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12439.3408203125
tensor(12439.4170, grad_fn=<NegBackward0>) tensor(12439.3408, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12439.2978515625
tensor(12439.3408, grad_fn=<NegBackward0>) tensor(12439.2979, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12439.265625
tensor(12439.2979, grad_fn=<NegBackward0>) tensor(12439.2656, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12439.2421875
tensor(12439.2656, grad_fn=<NegBackward0>) tensor(12439.2422, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12439.224609375
tensor(12439.2422, grad_fn=<NegBackward0>) tensor(12439.2246, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12439.212890625
tensor(12439.2246, grad_fn=<NegBackward0>) tensor(12439.2129, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12439.205078125
tensor(12439.2129, grad_fn=<NegBackward0>) tensor(12439.2051, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12439.1982421875
tensor(12439.2051, grad_fn=<NegBackward0>) tensor(12439.1982, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12439.1923828125
tensor(12439.1982, grad_fn=<NegBackward0>) tensor(12439.1924, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12439.1884765625
tensor(12439.1924, grad_fn=<NegBackward0>) tensor(12439.1885, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12439.1845703125
tensor(12439.1885, grad_fn=<NegBackward0>) tensor(12439.1846, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12439.1806640625
tensor(12439.1846, grad_fn=<NegBackward0>) tensor(12439.1807, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12439.1787109375
tensor(12439.1807, grad_fn=<NegBackward0>) tensor(12439.1787, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12439.1748046875
tensor(12439.1787, grad_fn=<NegBackward0>) tensor(12439.1748, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12439.1689453125
tensor(12439.1748, grad_fn=<NegBackward0>) tensor(12439.1689, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12439.1669921875
tensor(12439.1689, grad_fn=<NegBackward0>) tensor(12439.1670, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12439.16015625
tensor(12439.1670, grad_fn=<NegBackward0>) tensor(12439.1602, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12439.1513671875
tensor(12439.1602, grad_fn=<NegBackward0>) tensor(12439.1514, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12439.1416015625
tensor(12439.1514, grad_fn=<NegBackward0>) tensor(12439.1416, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12439.125
tensor(12439.1416, grad_fn=<NegBackward0>) tensor(12439.1250, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12439.1025390625
tensor(12439.1250, grad_fn=<NegBackward0>) tensor(12439.1025, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12439.0791015625
tensor(12439.1025, grad_fn=<NegBackward0>) tensor(12439.0791, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12439.0546875
tensor(12439.0791, grad_fn=<NegBackward0>) tensor(12439.0547, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12439.0390625
tensor(12439.0547, grad_fn=<NegBackward0>) tensor(12439.0391, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12439.0244140625
tensor(12439.0391, grad_fn=<NegBackward0>) tensor(12439.0244, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12439.0146484375
tensor(12439.0244, grad_fn=<NegBackward0>) tensor(12439.0146, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12439.0087890625
tensor(12439.0146, grad_fn=<NegBackward0>) tensor(12439.0088, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12439.001953125
tensor(12439.0088, grad_fn=<NegBackward0>) tensor(12439.0020, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12438.9990234375
tensor(12439.0020, grad_fn=<NegBackward0>) tensor(12438.9990, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12438.99609375
tensor(12438.9990, grad_fn=<NegBackward0>) tensor(12438.9961, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12438.9921875
tensor(12438.9961, grad_fn=<NegBackward0>) tensor(12438.9922, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12438.9892578125
tensor(12438.9922, grad_fn=<NegBackward0>) tensor(12438.9893, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12438.990234375
tensor(12438.9893, grad_fn=<NegBackward0>) tensor(12438.9902, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12438.9873046875
tensor(12438.9893, grad_fn=<NegBackward0>) tensor(12438.9873, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12438.9853515625
tensor(12438.9873, grad_fn=<NegBackward0>) tensor(12438.9854, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12438.9853515625
tensor(12438.9854, grad_fn=<NegBackward0>) tensor(12438.9854, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12438.9853515625
tensor(12438.9854, grad_fn=<NegBackward0>) tensor(12438.9854, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12438.982421875
tensor(12438.9854, grad_fn=<NegBackward0>) tensor(12438.9824, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12438.982421875
tensor(12438.9824, grad_fn=<NegBackward0>) tensor(12438.9824, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12438.982421875
tensor(12438.9824, grad_fn=<NegBackward0>) tensor(12438.9824, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12438.9814453125
tensor(12438.9824, grad_fn=<NegBackward0>) tensor(12438.9814, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12438.9814453125
tensor(12438.9814, grad_fn=<NegBackward0>) tensor(12438.9814, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12438.9794921875
tensor(12438.9814, grad_fn=<NegBackward0>) tensor(12438.9795, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12438.9794921875
tensor(12438.9795, grad_fn=<NegBackward0>) tensor(12438.9795, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12438.9794921875
tensor(12438.9795, grad_fn=<NegBackward0>) tensor(12438.9795, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12438.9794921875
tensor(12438.9795, grad_fn=<NegBackward0>) tensor(12438.9795, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12438.978515625
tensor(12438.9795, grad_fn=<NegBackward0>) tensor(12438.9785, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12438.9765625
tensor(12438.9785, grad_fn=<NegBackward0>) tensor(12438.9766, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12438.978515625
tensor(12438.9766, grad_fn=<NegBackward0>) tensor(12438.9785, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12438.9755859375
tensor(12438.9766, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12438.9765625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9766, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12438.9775390625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12438.9775390625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -12438.9765625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9766, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -12438.9765625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9766, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.9984, 0.0016],
        [0.0143, 0.9857]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0162, 0.9838], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2785, 0.2298],
         [0.6093, 0.1993]],

        [[0.5843, 0.2451],
         [0.5230, 0.5095]],

        [[0.6283, 0.2757],
         [0.6666, 0.5937]],

        [[0.6430, 0.1841],
         [0.5513, 0.6042]],

        [[0.5211, 0.2142],
         [0.7003, 0.5597]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.002462564867228772
Average Adjusted Rand Index: -0.004064819297642753
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22066.130859375
inf tensor(22066.1309, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12440.9912109375
tensor(22066.1309, grad_fn=<NegBackward0>) tensor(12440.9912, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12440.47265625
tensor(12440.9912, grad_fn=<NegBackward0>) tensor(12440.4727, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12440.34375
tensor(12440.4727, grad_fn=<NegBackward0>) tensor(12440.3438, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12440.251953125
tensor(12440.3438, grad_fn=<NegBackward0>) tensor(12440.2520, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12440.1689453125
tensor(12440.2520, grad_fn=<NegBackward0>) tensor(12440.1689, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12440.09375
tensor(12440.1689, grad_fn=<NegBackward0>) tensor(12440.0938, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12440.02734375
tensor(12440.0938, grad_fn=<NegBackward0>) tensor(12440.0273, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12439.9677734375
tensor(12440.0273, grad_fn=<NegBackward0>) tensor(12439.9678, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12439.9130859375
tensor(12439.9678, grad_fn=<NegBackward0>) tensor(12439.9131, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12439.865234375
tensor(12439.9131, grad_fn=<NegBackward0>) tensor(12439.8652, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12439.8134765625
tensor(12439.8652, grad_fn=<NegBackward0>) tensor(12439.8135, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12439.7607421875
tensor(12439.8135, grad_fn=<NegBackward0>) tensor(12439.7607, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12439.693359375
tensor(12439.7607, grad_fn=<NegBackward0>) tensor(12439.6934, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12439.6103515625
tensor(12439.6934, grad_fn=<NegBackward0>) tensor(12439.6104, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12439.5380859375
tensor(12439.6104, grad_fn=<NegBackward0>) tensor(12439.5381, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12439.4951171875
tensor(12439.5381, grad_fn=<NegBackward0>) tensor(12439.4951, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12439.46484375
tensor(12439.4951, grad_fn=<NegBackward0>) tensor(12439.4648, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12439.443359375
tensor(12439.4648, grad_fn=<NegBackward0>) tensor(12439.4434, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12439.4208984375
tensor(12439.4434, grad_fn=<NegBackward0>) tensor(12439.4209, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12439.40234375
tensor(12439.4209, grad_fn=<NegBackward0>) tensor(12439.4023, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12439.3818359375
tensor(12439.4023, grad_fn=<NegBackward0>) tensor(12439.3818, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12439.361328125
tensor(12439.3818, grad_fn=<NegBackward0>) tensor(12439.3613, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12439.33984375
tensor(12439.3613, grad_fn=<NegBackward0>) tensor(12439.3398, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12439.3193359375
tensor(12439.3398, grad_fn=<NegBackward0>) tensor(12439.3193, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12439.298828125
tensor(12439.3193, grad_fn=<NegBackward0>) tensor(12439.2988, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12439.2802734375
tensor(12439.2988, grad_fn=<NegBackward0>) tensor(12439.2803, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12439.263671875
tensor(12439.2803, grad_fn=<NegBackward0>) tensor(12439.2637, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12439.25
tensor(12439.2637, grad_fn=<NegBackward0>) tensor(12439.2500, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12439.236328125
tensor(12439.2500, grad_fn=<NegBackward0>) tensor(12439.2363, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12439.2294921875
tensor(12439.2363, grad_fn=<NegBackward0>) tensor(12439.2295, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12439.21875
tensor(12439.2295, grad_fn=<NegBackward0>) tensor(12439.2188, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12439.2138671875
tensor(12439.2188, grad_fn=<NegBackward0>) tensor(12439.2139, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12439.2060546875
tensor(12439.2139, grad_fn=<NegBackward0>) tensor(12439.2061, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12439.2021484375
tensor(12439.2061, grad_fn=<NegBackward0>) tensor(12439.2021, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12439.197265625
tensor(12439.2021, grad_fn=<NegBackward0>) tensor(12439.1973, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12439.1943359375
tensor(12439.1973, grad_fn=<NegBackward0>) tensor(12439.1943, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12439.1923828125
tensor(12439.1943, grad_fn=<NegBackward0>) tensor(12439.1924, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12439.1884765625
tensor(12439.1924, grad_fn=<NegBackward0>) tensor(12439.1885, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12439.1845703125
tensor(12439.1885, grad_fn=<NegBackward0>) tensor(12439.1846, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12439.1845703125
tensor(12439.1846, grad_fn=<NegBackward0>) tensor(12439.1846, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12439.1806640625
tensor(12439.1846, grad_fn=<NegBackward0>) tensor(12439.1807, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12439.177734375
tensor(12439.1807, grad_fn=<NegBackward0>) tensor(12439.1777, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12439.1748046875
tensor(12439.1777, grad_fn=<NegBackward0>) tensor(12439.1748, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12439.1708984375
tensor(12439.1748, grad_fn=<NegBackward0>) tensor(12439.1709, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12439.1669921875
tensor(12439.1709, grad_fn=<NegBackward0>) tensor(12439.1670, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12439.1630859375
tensor(12439.1670, grad_fn=<NegBackward0>) tensor(12439.1631, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12439.1611328125
tensor(12439.1631, grad_fn=<NegBackward0>) tensor(12439.1611, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12439.154296875
tensor(12439.1611, grad_fn=<NegBackward0>) tensor(12439.1543, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12439.14453125
tensor(12439.1543, grad_fn=<NegBackward0>) tensor(12439.1445, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12439.1318359375
tensor(12439.1445, grad_fn=<NegBackward0>) tensor(12439.1318, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12439.115234375
tensor(12439.1318, grad_fn=<NegBackward0>) tensor(12439.1152, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12439.095703125
tensor(12439.1152, grad_fn=<NegBackward0>) tensor(12439.0957, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12439.0751953125
tensor(12439.0957, grad_fn=<NegBackward0>) tensor(12439.0752, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12439.0576171875
tensor(12439.0752, grad_fn=<NegBackward0>) tensor(12439.0576, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12439.04296875
tensor(12439.0576, grad_fn=<NegBackward0>) tensor(12439.0430, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12439.0302734375
tensor(12439.0430, grad_fn=<NegBackward0>) tensor(12439.0303, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12439.01953125
tensor(12439.0303, grad_fn=<NegBackward0>) tensor(12439.0195, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12439.013671875
tensor(12439.0195, grad_fn=<NegBackward0>) tensor(12439.0137, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12439.0078125
tensor(12439.0137, grad_fn=<NegBackward0>) tensor(12439.0078, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12439.0009765625
tensor(12439.0078, grad_fn=<NegBackward0>) tensor(12439.0010, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12438.9990234375
tensor(12439.0010, grad_fn=<NegBackward0>) tensor(12438.9990, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12438.994140625
tensor(12438.9990, grad_fn=<NegBackward0>) tensor(12438.9941, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12438.994140625
tensor(12438.9941, grad_fn=<NegBackward0>) tensor(12438.9941, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12438.9912109375
tensor(12438.9941, grad_fn=<NegBackward0>) tensor(12438.9912, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12438.99609375
tensor(12438.9912, grad_fn=<NegBackward0>) tensor(12438.9961, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12438.9892578125
tensor(12438.9912, grad_fn=<NegBackward0>) tensor(12438.9893, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12438.9873046875
tensor(12438.9893, grad_fn=<NegBackward0>) tensor(12438.9873, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12438.98828125
tensor(12438.9873, grad_fn=<NegBackward0>) tensor(12438.9883, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12438.984375
tensor(12438.9873, grad_fn=<NegBackward0>) tensor(12438.9844, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12438.9833984375
tensor(12438.9844, grad_fn=<NegBackward0>) tensor(12438.9834, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12438.982421875
tensor(12438.9834, grad_fn=<NegBackward0>) tensor(12438.9824, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12438.982421875
tensor(12438.9824, grad_fn=<NegBackward0>) tensor(12438.9824, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12438.98046875
tensor(12438.9824, grad_fn=<NegBackward0>) tensor(12438.9805, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12438.9814453125
tensor(12438.9805, grad_fn=<NegBackward0>) tensor(12438.9814, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12438.98046875
tensor(12438.9805, grad_fn=<NegBackward0>) tensor(12438.9805, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12438.9931640625
tensor(12438.9805, grad_fn=<NegBackward0>) tensor(12438.9932, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12438.9794921875
tensor(12438.9805, grad_fn=<NegBackward0>) tensor(12438.9795, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12438.9794921875
tensor(12438.9795, grad_fn=<NegBackward0>) tensor(12438.9795, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12438.9775390625
tensor(12438.9795, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12438.9775390625
tensor(12438.9775, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12438.9775390625
tensor(12438.9775, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12438.9775390625
tensor(12438.9775, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12438.9765625
tensor(12438.9775, grad_fn=<NegBackward0>) tensor(12438.9766, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12438.9775390625
tensor(12438.9766, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12438.9775390625
tensor(12438.9766, grad_fn=<NegBackward0>) tensor(12438.9775, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12438.9755859375
tensor(12438.9766, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12438.9931640625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9932, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12438.9765625
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9766, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12438.9755859375
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12438.9755859375
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12438.9755859375
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12438.9755859375
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12438.9755859375
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12438.9814453125
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9814, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12438.974609375
tensor(12438.9756, grad_fn=<NegBackward0>) tensor(12438.9746, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12438.974609375
tensor(12438.9746, grad_fn=<NegBackward0>) tensor(12438.9746, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12438.9755859375
tensor(12438.9746, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12438.9755859375
tensor(12438.9746, grad_fn=<NegBackward0>) tensor(12438.9756, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12439.3388671875
tensor(12438.9746, grad_fn=<NegBackward0>) tensor(12439.3389, grad_fn=<NegBackward0>)
3
pi: tensor([[9.9921e-01, 7.9477e-04],
        [1.4559e-02, 9.8544e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0160, 0.9840], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2783, 0.2305],
         [0.5308, 0.1993]],

        [[0.6748, 0.2450],
         [0.6758, 0.6466]],

        [[0.7044, 0.2756],
         [0.6816, 0.7131]],

        [[0.6359, 0.1841],
         [0.7132, 0.6543]],

        [[0.5078, 0.2143],
         [0.7013, 0.5691]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.002462564867228772
Average Adjusted Rand Index: -0.004064819297642753
[-0.002462564867228772, -0.002462564867228772] [-0.004064819297642753, -0.004064819297642753] [12438.9765625, 12438.9765625]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11928.138670627
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21781.716796875
inf tensor(21781.7168, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12448.845703125
tensor(21781.7168, grad_fn=<NegBackward0>) tensor(12448.8457, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12448.6083984375
tensor(12448.8457, grad_fn=<NegBackward0>) tensor(12448.6084, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12448.5400390625
tensor(12448.6084, grad_fn=<NegBackward0>) tensor(12448.5400, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12448.501953125
tensor(12448.5400, grad_fn=<NegBackward0>) tensor(12448.5020, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12448.4794921875
tensor(12448.5020, grad_fn=<NegBackward0>) tensor(12448.4795, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12448.4619140625
tensor(12448.4795, grad_fn=<NegBackward0>) tensor(12448.4619, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12448.4501953125
tensor(12448.4619, grad_fn=<NegBackward0>) tensor(12448.4502, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12448.439453125
tensor(12448.4502, grad_fn=<NegBackward0>) tensor(12448.4395, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12448.4326171875
tensor(12448.4395, grad_fn=<NegBackward0>) tensor(12448.4326, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12448.4248046875
tensor(12448.4326, grad_fn=<NegBackward0>) tensor(12448.4248, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12448.4169921875
tensor(12448.4248, grad_fn=<NegBackward0>) tensor(12448.4170, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12448.4111328125
tensor(12448.4170, grad_fn=<NegBackward0>) tensor(12448.4111, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12448.404296875
tensor(12448.4111, grad_fn=<NegBackward0>) tensor(12448.4043, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12448.3974609375
tensor(12448.4043, grad_fn=<NegBackward0>) tensor(12448.3975, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12448.3896484375
tensor(12448.3975, grad_fn=<NegBackward0>) tensor(12448.3896, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12448.380859375
tensor(12448.3896, grad_fn=<NegBackward0>) tensor(12448.3809, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12448.37109375
tensor(12448.3809, grad_fn=<NegBackward0>) tensor(12448.3711, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12448.3603515625
tensor(12448.3711, grad_fn=<NegBackward0>) tensor(12448.3604, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12448.3447265625
tensor(12448.3604, grad_fn=<NegBackward0>) tensor(12448.3447, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12448.3291015625
tensor(12448.3447, grad_fn=<NegBackward0>) tensor(12448.3291, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12448.3095703125
tensor(12448.3291, grad_fn=<NegBackward0>) tensor(12448.3096, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12448.2861328125
tensor(12448.3096, grad_fn=<NegBackward0>) tensor(12448.2861, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12448.259765625
tensor(12448.2861, grad_fn=<NegBackward0>) tensor(12448.2598, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12448.2314453125
tensor(12448.2598, grad_fn=<NegBackward0>) tensor(12448.2314, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12448.1982421875
tensor(12448.2314, grad_fn=<NegBackward0>) tensor(12448.1982, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12448.162109375
tensor(12448.1982, grad_fn=<NegBackward0>) tensor(12448.1621, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12448.12109375
tensor(12448.1621, grad_fn=<NegBackward0>) tensor(12448.1211, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12448.0810546875
tensor(12448.1211, grad_fn=<NegBackward0>) tensor(12448.0811, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12448.05078125
tensor(12448.0811, grad_fn=<NegBackward0>) tensor(12448.0508, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12448.0322265625
tensor(12448.0508, grad_fn=<NegBackward0>) tensor(12448.0322, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12448.0263671875
tensor(12448.0322, grad_fn=<NegBackward0>) tensor(12448.0264, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12448.0234375
tensor(12448.0264, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12448.0234375
tensor(12448.0234, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12448.0244140625
tensor(12448.0234, grad_fn=<NegBackward0>) tensor(12448.0244, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12448.0234375
tensor(12448.0234, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12448.0234375
tensor(12448.0234, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12448.0224609375
tensor(12448.0234, grad_fn=<NegBackward0>) tensor(12448.0225, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12448.0234375
tensor(12448.0225, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12448.0234375
tensor(12448.0225, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -12448.0244140625
tensor(12448.0225, grad_fn=<NegBackward0>) tensor(12448.0244, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -12448.02734375
tensor(12448.0225, grad_fn=<NegBackward0>) tensor(12448.0273, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -12448.0234375
tensor(12448.0225, grad_fn=<NegBackward0>) tensor(12448.0234, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4200 due to no improvement.
pi: tensor([[0.4512, 0.5488],
        [0.9753, 0.0247]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9933, 0.0067], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1990, 0.1881],
         [0.5849, 0.2081]],

        [[0.7097, 0.2059],
         [0.6645, 0.5828]],

        [[0.6626, 0.2068],
         [0.6555, 0.6138]],

        [[0.6377, 0.2004],
         [0.7277, 0.6552]],

        [[0.6897, 0.2021],
         [0.6457, 0.6764]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.012674371685496887
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0022695224896134574
Average Adjusted Rand Index: 0.0025348743370993774
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23711.03515625
inf tensor(23711.0352, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12449.0478515625
tensor(23711.0352, grad_fn=<NegBackward0>) tensor(12449.0479, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12448.560546875
tensor(12449.0479, grad_fn=<NegBackward0>) tensor(12448.5605, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12448.4921875
tensor(12448.5605, grad_fn=<NegBackward0>) tensor(12448.4922, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12448.4580078125
tensor(12448.4922, grad_fn=<NegBackward0>) tensor(12448.4580, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12448.4375
tensor(12448.4580, grad_fn=<NegBackward0>) tensor(12448.4375, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12448.423828125
tensor(12448.4375, grad_fn=<NegBackward0>) tensor(12448.4238, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12448.416015625
tensor(12448.4238, grad_fn=<NegBackward0>) tensor(12448.4160, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12448.408203125
tensor(12448.4160, grad_fn=<NegBackward0>) tensor(12448.4082, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12448.4013671875
tensor(12448.4082, grad_fn=<NegBackward0>) tensor(12448.4014, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12448.3994140625
tensor(12448.4014, grad_fn=<NegBackward0>) tensor(12448.3994, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12448.3935546875
tensor(12448.3994, grad_fn=<NegBackward0>) tensor(12448.3936, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12448.390625
tensor(12448.3936, grad_fn=<NegBackward0>) tensor(12448.3906, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12448.38671875
tensor(12448.3906, grad_fn=<NegBackward0>) tensor(12448.3867, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12448.3837890625
tensor(12448.3867, grad_fn=<NegBackward0>) tensor(12448.3838, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12448.37890625
tensor(12448.3838, grad_fn=<NegBackward0>) tensor(12448.3789, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12448.373046875
tensor(12448.3789, grad_fn=<NegBackward0>) tensor(12448.3730, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12448.3681640625
tensor(12448.3730, grad_fn=<NegBackward0>) tensor(12448.3682, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12448.3603515625
tensor(12448.3682, grad_fn=<NegBackward0>) tensor(12448.3604, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12448.3505859375
tensor(12448.3604, grad_fn=<NegBackward0>) tensor(12448.3506, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12448.33984375
tensor(12448.3506, grad_fn=<NegBackward0>) tensor(12448.3398, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12448.3251953125
tensor(12448.3398, grad_fn=<NegBackward0>) tensor(12448.3252, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12448.3076171875
tensor(12448.3252, grad_fn=<NegBackward0>) tensor(12448.3076, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12448.2861328125
tensor(12448.3076, grad_fn=<NegBackward0>) tensor(12448.2861, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12448.2646484375
tensor(12448.2861, grad_fn=<NegBackward0>) tensor(12448.2646, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12448.224609375
tensor(12448.2646, grad_fn=<NegBackward0>) tensor(12448.2246, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12448.1533203125
tensor(12448.2246, grad_fn=<NegBackward0>) tensor(12448.1533, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12448.123046875
tensor(12448.1533, grad_fn=<NegBackward0>) tensor(12448.1230, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12448.109375
tensor(12448.1230, grad_fn=<NegBackward0>) tensor(12448.1094, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12448.1015625
tensor(12448.1094, grad_fn=<NegBackward0>) tensor(12448.1016, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12448.0947265625
tensor(12448.1016, grad_fn=<NegBackward0>) tensor(12448.0947, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12448.0849609375
tensor(12448.0947, grad_fn=<NegBackward0>) tensor(12448.0850, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12448.0771484375
tensor(12448.0850, grad_fn=<NegBackward0>) tensor(12448.0771, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12448.0673828125
tensor(12448.0771, grad_fn=<NegBackward0>) tensor(12448.0674, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12448.060546875
tensor(12448.0674, grad_fn=<NegBackward0>) tensor(12448.0605, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12448.0517578125
tensor(12448.0605, grad_fn=<NegBackward0>) tensor(12448.0518, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12448.0439453125
tensor(12448.0518, grad_fn=<NegBackward0>) tensor(12448.0439, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12448.037109375
tensor(12448.0439, grad_fn=<NegBackward0>) tensor(12448.0371, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12448.0302734375
tensor(12448.0371, grad_fn=<NegBackward0>) tensor(12448.0303, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12448.0263671875
tensor(12448.0303, grad_fn=<NegBackward0>) tensor(12448.0264, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12448.0205078125
tensor(12448.0264, grad_fn=<NegBackward0>) tensor(12448.0205, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12448.0185546875
tensor(12448.0205, grad_fn=<NegBackward0>) tensor(12448.0186, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12448.0166015625
tensor(12448.0186, grad_fn=<NegBackward0>) tensor(12448.0166, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12448.0146484375
tensor(12448.0166, grad_fn=<NegBackward0>) tensor(12448.0146, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12448.0146484375
tensor(12448.0146, grad_fn=<NegBackward0>) tensor(12448.0146, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12448.0126953125
tensor(12448.0146, grad_fn=<NegBackward0>) tensor(12448.0127, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12448.0126953125
tensor(12448.0127, grad_fn=<NegBackward0>) tensor(12448.0127, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12448.0302734375
tensor(12448.0127, grad_fn=<NegBackward0>) tensor(12448.0303, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12448.0107421875
tensor(12448.0127, grad_fn=<NegBackward0>) tensor(12448.0107, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12448.046875
tensor(12448.0107, grad_fn=<NegBackward0>) tensor(12448.0469, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12448.009765625
tensor(12448.0107, grad_fn=<NegBackward0>) tensor(12448.0098, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12448.025390625
tensor(12448.0098, grad_fn=<NegBackward0>) tensor(12448.0254, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12448.0078125
tensor(12448.0098, grad_fn=<NegBackward0>) tensor(12448.0078, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12448.0068359375
tensor(12448.0078, grad_fn=<NegBackward0>) tensor(12448.0068, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12448.0068359375
tensor(12448.0068, grad_fn=<NegBackward0>) tensor(12448.0068, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12448.0048828125
tensor(12448.0068, grad_fn=<NegBackward0>) tensor(12448.0049, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12448.0126953125
tensor(12448.0049, grad_fn=<NegBackward0>) tensor(12448.0127, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12448.0048828125
tensor(12448.0049, grad_fn=<NegBackward0>) tensor(12448.0049, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12448.0068359375
tensor(12448.0049, grad_fn=<NegBackward0>) tensor(12448.0068, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12448.0048828125
tensor(12448.0049, grad_fn=<NegBackward0>) tensor(12448.0049, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12448.00390625
tensor(12448.0049, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12448.0048828125
tensor(12448.0039, grad_fn=<NegBackward0>) tensor(12448.0049, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12448.00390625
tensor(12448.0039, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12448.0029296875
tensor(12448.0039, grad_fn=<NegBackward0>) tensor(12448.0029, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12448.00390625
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12448.00390625
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12448.00390625
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12448.0048828125
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0049, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -12448.0029296875
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0029, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12448.005859375
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0059, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12448.00390625
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12448.0830078125
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0830, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12448.0029296875
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0029, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12448.005859375
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0059, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12448.00390625
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12448.0029296875
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0029, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12448.001953125
tensor(12448.0029, grad_fn=<NegBackward0>) tensor(12448.0020, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12448.0029296875
tensor(12448.0020, grad_fn=<NegBackward0>) tensor(12448.0029, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12448.0146484375
tensor(12448.0020, grad_fn=<NegBackward0>) tensor(12448.0146, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12448.091796875
tensor(12448.0020, grad_fn=<NegBackward0>) tensor(12448.0918, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -12448.0048828125
tensor(12448.0020, grad_fn=<NegBackward0>) tensor(12448.0049, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -12448.00390625
tensor(12448.0020, grad_fn=<NegBackward0>) tensor(12448.0039, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.1107, 0.8893],
        [0.4447, 0.5553]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9941, 0.0059], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.1938],
         [0.5802, 0.2044]],

        [[0.6930, 0.2073],
         [0.5265, 0.5146]],

        [[0.6574, 0.2036],
         [0.6596, 0.5835]],

        [[0.5853, 0.1985],
         [0.5262, 0.6262]],

        [[0.5683, 0.2001],
         [0.6836, 0.5405]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018567194034408375
Average Adjusted Rand Index: 0.0
[-0.0022695224896134574, -0.0018567194034408375] [0.0025348743370993774, 0.0] [12448.0234375, 12448.00390625]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11804.124268475483
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21199.04296875
inf tensor(21199.0430, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12314.5517578125
tensor(21199.0430, grad_fn=<NegBackward0>) tensor(12314.5518, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12314.0556640625
tensor(12314.5518, grad_fn=<NegBackward0>) tensor(12314.0557, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12313.8056640625
tensor(12314.0557, grad_fn=<NegBackward0>) tensor(12313.8057, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12313.55859375
tensor(12313.8057, grad_fn=<NegBackward0>) tensor(12313.5586, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12313.330078125
tensor(12313.5586, grad_fn=<NegBackward0>) tensor(12313.3301, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12313.1337890625
tensor(12313.3301, grad_fn=<NegBackward0>) tensor(12313.1338, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12312.9853515625
tensor(12313.1338, grad_fn=<NegBackward0>) tensor(12312.9854, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12312.8662109375
tensor(12312.9854, grad_fn=<NegBackward0>) tensor(12312.8662, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12312.7587890625
tensor(12312.8662, grad_fn=<NegBackward0>) tensor(12312.7588, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12312.6572265625
tensor(12312.7588, grad_fn=<NegBackward0>) tensor(12312.6572, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12312.55859375
tensor(12312.6572, grad_fn=<NegBackward0>) tensor(12312.5586, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12312.4609375
tensor(12312.5586, grad_fn=<NegBackward0>) tensor(12312.4609, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12312.3642578125
tensor(12312.4609, grad_fn=<NegBackward0>) tensor(12312.3643, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12312.2646484375
tensor(12312.3643, grad_fn=<NegBackward0>) tensor(12312.2646, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12312.1630859375
tensor(12312.2646, grad_fn=<NegBackward0>) tensor(12312.1631, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12312.0673828125
tensor(12312.1631, grad_fn=<NegBackward0>) tensor(12312.0674, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12311.982421875
tensor(12312.0674, grad_fn=<NegBackward0>) tensor(12311.9824, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12311.9072265625
tensor(12311.9824, grad_fn=<NegBackward0>) tensor(12311.9072, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12311.8408203125
tensor(12311.9072, grad_fn=<NegBackward0>) tensor(12311.8408, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12311.787109375
tensor(12311.8408, grad_fn=<NegBackward0>) tensor(12311.7871, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12311.74609375
tensor(12311.7871, grad_fn=<NegBackward0>) tensor(12311.7461, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12311.7158203125
tensor(12311.7461, grad_fn=<NegBackward0>) tensor(12311.7158, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12311.693359375
tensor(12311.7158, grad_fn=<NegBackward0>) tensor(12311.6934, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12311.67578125
tensor(12311.6934, grad_fn=<NegBackward0>) tensor(12311.6758, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12311.6650390625
tensor(12311.6758, grad_fn=<NegBackward0>) tensor(12311.6650, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12311.6552734375
tensor(12311.6650, grad_fn=<NegBackward0>) tensor(12311.6553, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12311.6474609375
tensor(12311.6553, grad_fn=<NegBackward0>) tensor(12311.6475, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12311.6396484375
tensor(12311.6475, grad_fn=<NegBackward0>) tensor(12311.6396, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12311.6318359375
tensor(12311.6396, grad_fn=<NegBackward0>) tensor(12311.6318, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12311.6259765625
tensor(12311.6318, grad_fn=<NegBackward0>) tensor(12311.6260, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12311.619140625
tensor(12311.6260, grad_fn=<NegBackward0>) tensor(12311.6191, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12311.611328125
tensor(12311.6191, grad_fn=<NegBackward0>) tensor(12311.6113, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12311.6044921875
tensor(12311.6113, grad_fn=<NegBackward0>) tensor(12311.6045, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12311.59765625
tensor(12311.6045, grad_fn=<NegBackward0>) tensor(12311.5977, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12311.5888671875
tensor(12311.5977, grad_fn=<NegBackward0>) tensor(12311.5889, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12311.583984375
tensor(12311.5889, grad_fn=<NegBackward0>) tensor(12311.5840, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12311.578125
tensor(12311.5840, grad_fn=<NegBackward0>) tensor(12311.5781, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12311.5732421875
tensor(12311.5781, grad_fn=<NegBackward0>) tensor(12311.5732, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12311.568359375
tensor(12311.5732, grad_fn=<NegBackward0>) tensor(12311.5684, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12311.5693359375
tensor(12311.5684, grad_fn=<NegBackward0>) tensor(12311.5693, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12311.5625
tensor(12311.5684, grad_fn=<NegBackward0>) tensor(12311.5625, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12311.5595703125
tensor(12311.5625, grad_fn=<NegBackward0>) tensor(12311.5596, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12311.55859375
tensor(12311.5596, grad_fn=<NegBackward0>) tensor(12311.5586, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12311.5576171875
tensor(12311.5586, grad_fn=<NegBackward0>) tensor(12311.5576, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12311.55859375
tensor(12311.5576, grad_fn=<NegBackward0>) tensor(12311.5586, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12311.5537109375
tensor(12311.5576, grad_fn=<NegBackward0>) tensor(12311.5537, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12311.5556640625
tensor(12311.5537, grad_fn=<NegBackward0>) tensor(12311.5557, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12311.55078125
tensor(12311.5537, grad_fn=<NegBackward0>) tensor(12311.5508, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12311.548828125
tensor(12311.5508, grad_fn=<NegBackward0>) tensor(12311.5488, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12311.5439453125
tensor(12311.5488, grad_fn=<NegBackward0>) tensor(12311.5439, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12311.533203125
tensor(12311.5439, grad_fn=<NegBackward0>) tensor(12311.5332, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12311.4931640625
tensor(12311.5332, grad_fn=<NegBackward0>) tensor(12311.4932, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12311.490234375
tensor(12311.4932, grad_fn=<NegBackward0>) tensor(12311.4902, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12311.4892578125
tensor(12311.4902, grad_fn=<NegBackward0>) tensor(12311.4893, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12311.486328125
tensor(12311.4893, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12311.4873046875
tensor(12311.4863, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12311.4853515625
tensor(12311.4863, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12311.486328125
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12311.4873046875
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12311.4873046875
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12311.484375
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12311.484375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12311.4873046875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12311.4853515625
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12311.4853515625
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12311.484375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12311.505859375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.5059, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12311.5234375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.5234, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12311.484375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12311.4921875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4922, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12311.55078125
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.5508, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12311.486328125
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -12311.4990234375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4990, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -12311.484375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12311.5185546875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.5186, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12311.484375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12311.4873046875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12311.486328125
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12311.4873046875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12311.5185546875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.5186, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12311.4892578125
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4893, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.5869, 0.4131],
        [0.8157, 0.1843]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0175, 0.9825], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.2526],
         [0.6685, 0.1909]],

        [[0.6662, 0.2019],
         [0.6866, 0.6008]],

        [[0.5020, 0.2078],
         [0.5864, 0.7259]],

        [[0.6127, 0.1917],
         [0.6904, 0.7185]],

        [[0.5866, 0.1851],
         [0.6831, 0.7100]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016961241602102017
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22022.21875
inf tensor(22022.2188, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12314.052734375
tensor(22022.2188, grad_fn=<NegBackward0>) tensor(12314.0527, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12313.8447265625
tensor(12314.0527, grad_fn=<NegBackward0>) tensor(12313.8447, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12313.765625
tensor(12313.8447, grad_fn=<NegBackward0>) tensor(12313.7656, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12313.6875
tensor(12313.7656, grad_fn=<NegBackward0>) tensor(12313.6875, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12313.5908203125
tensor(12313.6875, grad_fn=<NegBackward0>) tensor(12313.5908, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12313.4638671875
tensor(12313.5908, grad_fn=<NegBackward0>) tensor(12313.4639, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12313.2841796875
tensor(12313.4639, grad_fn=<NegBackward0>) tensor(12313.2842, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12313.0576171875
tensor(12313.2842, grad_fn=<NegBackward0>) tensor(12313.0576, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12312.86328125
tensor(12313.0576, grad_fn=<NegBackward0>) tensor(12312.8633, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12312.716796875
tensor(12312.8633, grad_fn=<NegBackward0>) tensor(12312.7168, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12312.591796875
tensor(12312.7168, grad_fn=<NegBackward0>) tensor(12312.5918, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12312.46875
tensor(12312.5918, grad_fn=<NegBackward0>) tensor(12312.4688, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12312.3427734375
tensor(12312.4688, grad_fn=<NegBackward0>) tensor(12312.3428, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12312.2158203125
tensor(12312.3428, grad_fn=<NegBackward0>) tensor(12312.2158, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12312.0908203125
tensor(12312.2158, grad_fn=<NegBackward0>) tensor(12312.0908, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12311.9814453125
tensor(12312.0908, grad_fn=<NegBackward0>) tensor(12311.9814, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12311.8916015625
tensor(12311.9814, grad_fn=<NegBackward0>) tensor(12311.8916, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12311.8203125
tensor(12311.8916, grad_fn=<NegBackward0>) tensor(12311.8203, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12311.7666015625
tensor(12311.8203, grad_fn=<NegBackward0>) tensor(12311.7666, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12311.732421875
tensor(12311.7666, grad_fn=<NegBackward0>) tensor(12311.7324, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12311.708984375
tensor(12311.7324, grad_fn=<NegBackward0>) tensor(12311.7090, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12311.693359375
tensor(12311.7090, grad_fn=<NegBackward0>) tensor(12311.6934, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12311.68359375
tensor(12311.6934, grad_fn=<NegBackward0>) tensor(12311.6836, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12311.6767578125
tensor(12311.6836, grad_fn=<NegBackward0>) tensor(12311.6768, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12311.6708984375
tensor(12311.6768, grad_fn=<NegBackward0>) tensor(12311.6709, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12311.6630859375
tensor(12311.6709, grad_fn=<NegBackward0>) tensor(12311.6631, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12311.65625
tensor(12311.6631, grad_fn=<NegBackward0>) tensor(12311.6562, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12311.6455078125
tensor(12311.6562, grad_fn=<NegBackward0>) tensor(12311.6455, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12311.634765625
tensor(12311.6455, grad_fn=<NegBackward0>) tensor(12311.6348, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12311.62109375
tensor(12311.6348, grad_fn=<NegBackward0>) tensor(12311.6211, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12311.6064453125
tensor(12311.6211, grad_fn=<NegBackward0>) tensor(12311.6064, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12311.591796875
tensor(12311.6064, grad_fn=<NegBackward0>) tensor(12311.5918, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12311.58203125
tensor(12311.5918, grad_fn=<NegBackward0>) tensor(12311.5820, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12311.5732421875
tensor(12311.5820, grad_fn=<NegBackward0>) tensor(12311.5732, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12311.5693359375
tensor(12311.5732, grad_fn=<NegBackward0>) tensor(12311.5693, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12311.5634765625
tensor(12311.5693, grad_fn=<NegBackward0>) tensor(12311.5635, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12311.560546875
tensor(12311.5635, grad_fn=<NegBackward0>) tensor(12311.5605, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12311.55859375
tensor(12311.5605, grad_fn=<NegBackward0>) tensor(12311.5586, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12311.5576171875
tensor(12311.5586, grad_fn=<NegBackward0>) tensor(12311.5576, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12311.556640625
tensor(12311.5576, grad_fn=<NegBackward0>) tensor(12311.5566, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12311.5537109375
tensor(12311.5566, grad_fn=<NegBackward0>) tensor(12311.5537, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12311.552734375
tensor(12311.5537, grad_fn=<NegBackward0>) tensor(12311.5527, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12311.5537109375
tensor(12311.5527, grad_fn=<NegBackward0>) tensor(12311.5537, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12311.55078125
tensor(12311.5527, grad_fn=<NegBackward0>) tensor(12311.5508, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12311.552734375
tensor(12311.5508, grad_fn=<NegBackward0>) tensor(12311.5527, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12311.546875
tensor(12311.5508, grad_fn=<NegBackward0>) tensor(12311.5469, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12311.54296875
tensor(12311.5469, grad_fn=<NegBackward0>) tensor(12311.5430, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12311.4931640625
tensor(12311.5430, grad_fn=<NegBackward0>) tensor(12311.4932, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12311.48828125
tensor(12311.4932, grad_fn=<NegBackward0>) tensor(12311.4883, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12311.486328125
tensor(12311.4883, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12311.486328125
tensor(12311.4863, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12311.4873046875
tensor(12311.4863, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12311.4853515625
tensor(12311.4863, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12311.4873046875
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12311.4853515625
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12311.486328125
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4863, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12311.484375
tensor(12311.4854, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12311.4873046875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12311.484375
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4844, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12311.4853515625
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12311.4853515625
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4854, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12311.4873046875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12311.4873046875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4873, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -12311.4921875
tensor(12311.4844, grad_fn=<NegBackward0>) tensor(12311.4922, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.5836, 0.4164],
        [0.8149, 0.1851]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0175, 0.9825], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2030, 0.2526],
         [0.5590, 0.1909]],

        [[0.6789, 0.2021],
         [0.7140, 0.5404]],

        [[0.6444, 0.2080],
         [0.5012, 0.5657]],

        [[0.7251, 0.1919],
         [0.6190, 0.6404]],

        [[0.5456, 0.1852],
         [0.6467, 0.6846]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016961241602102017
Average Adjusted Rand Index: 0.0
[-0.0016961241602102017, -0.0016961241602102017] [0.0, 0.0] [12311.4892578125, 12311.4921875]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11864.218564037514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22930.453125
inf tensor(22930.4531, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12359.04296875
tensor(22930.4531, grad_fn=<NegBackward0>) tensor(12359.0430, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12358.5205078125
tensor(12359.0430, grad_fn=<NegBackward0>) tensor(12358.5205, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12358.46875
tensor(12358.5205, grad_fn=<NegBackward0>) tensor(12358.4688, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12358.44921875
tensor(12358.4688, grad_fn=<NegBackward0>) tensor(12358.4492, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12358.4345703125
tensor(12358.4492, grad_fn=<NegBackward0>) tensor(12358.4346, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12358.419921875
tensor(12358.4346, grad_fn=<NegBackward0>) tensor(12358.4199, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12358.408203125
tensor(12358.4199, grad_fn=<NegBackward0>) tensor(12358.4082, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12358.3935546875
tensor(12358.4082, grad_fn=<NegBackward0>) tensor(12358.3936, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12358.3779296875
tensor(12358.3936, grad_fn=<NegBackward0>) tensor(12358.3779, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12358.3603515625
tensor(12358.3779, grad_fn=<NegBackward0>) tensor(12358.3604, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12358.341796875
tensor(12358.3604, grad_fn=<NegBackward0>) tensor(12358.3418, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12358.3251953125
tensor(12358.3418, grad_fn=<NegBackward0>) tensor(12358.3252, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12358.310546875
tensor(12358.3252, grad_fn=<NegBackward0>) tensor(12358.3105, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12358.2958984375
tensor(12358.3105, grad_fn=<NegBackward0>) tensor(12358.2959, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12358.2822265625
tensor(12358.2959, grad_fn=<NegBackward0>) tensor(12358.2822, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12358.2685546875
tensor(12358.2822, grad_fn=<NegBackward0>) tensor(12358.2686, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12358.2548828125
tensor(12358.2686, grad_fn=<NegBackward0>) tensor(12358.2549, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12358.240234375
tensor(12358.2549, grad_fn=<NegBackward0>) tensor(12358.2402, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12358.224609375
tensor(12358.2402, grad_fn=<NegBackward0>) tensor(12358.2246, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12358.2080078125
tensor(12358.2246, grad_fn=<NegBackward0>) tensor(12358.2080, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12358.1884765625
tensor(12358.2080, grad_fn=<NegBackward0>) tensor(12358.1885, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12358.1708984375
tensor(12358.1885, grad_fn=<NegBackward0>) tensor(12358.1709, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12358.15234375
tensor(12358.1709, grad_fn=<NegBackward0>) tensor(12358.1523, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12358.1328125
tensor(12358.1523, grad_fn=<NegBackward0>) tensor(12358.1328, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12358.11328125
tensor(12358.1328, grad_fn=<NegBackward0>) tensor(12358.1133, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12358.0927734375
tensor(12358.1133, grad_fn=<NegBackward0>) tensor(12358.0928, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12358.0634765625
tensor(12358.0928, grad_fn=<NegBackward0>) tensor(12358.0635, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12358.025390625
tensor(12358.0635, grad_fn=<NegBackward0>) tensor(12358.0254, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12357.966796875
tensor(12358.0254, grad_fn=<NegBackward0>) tensor(12357.9668, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12357.869140625
tensor(12357.9668, grad_fn=<NegBackward0>) tensor(12357.8691, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12357.7333984375
tensor(12357.8691, grad_fn=<NegBackward0>) tensor(12357.7334, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12357.6826171875
tensor(12357.7334, grad_fn=<NegBackward0>) tensor(12357.6826, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12357.67578125
tensor(12357.6826, grad_fn=<NegBackward0>) tensor(12357.6758, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12357.6748046875
tensor(12357.6758, grad_fn=<NegBackward0>) tensor(12357.6748, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12357.6748046875
tensor(12357.6748, grad_fn=<NegBackward0>) tensor(12357.6748, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12357.6748046875
tensor(12357.6748, grad_fn=<NegBackward0>) tensor(12357.6748, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12357.67578125
tensor(12357.6748, grad_fn=<NegBackward0>) tensor(12357.6758, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12357.6748046875
tensor(12357.6748, grad_fn=<NegBackward0>) tensor(12357.6748, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12357.6748046875
tensor(12357.6748, grad_fn=<NegBackward0>) tensor(12357.6748, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12357.6728515625
tensor(12357.6748, grad_fn=<NegBackward0>) tensor(12357.6729, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12357.671875
tensor(12357.6729, grad_fn=<NegBackward0>) tensor(12357.6719, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12357.673828125
tensor(12357.6719, grad_fn=<NegBackward0>) tensor(12357.6738, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12357.6728515625
tensor(12357.6719, grad_fn=<NegBackward0>) tensor(12357.6729, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -12357.673828125
tensor(12357.6719, grad_fn=<NegBackward0>) tensor(12357.6738, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -12357.6728515625
tensor(12357.6719, grad_fn=<NegBackward0>) tensor(12357.6729, grad_fn=<NegBackward0>)
4
Iteration 4600: Loss = -12357.669921875
tensor(12357.6719, grad_fn=<NegBackward0>) tensor(12357.6699, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12357.6728515625
tensor(12357.6699, grad_fn=<NegBackward0>) tensor(12357.6729, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12357.66796875
tensor(12357.6699, grad_fn=<NegBackward0>) tensor(12357.6680, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12357.7021484375
tensor(12357.6680, grad_fn=<NegBackward0>) tensor(12357.7021, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12357.6669921875
tensor(12357.6680, grad_fn=<NegBackward0>) tensor(12357.6670, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12357.6669921875
tensor(12357.6670, grad_fn=<NegBackward0>) tensor(12357.6670, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12357.6669921875
tensor(12357.6670, grad_fn=<NegBackward0>) tensor(12357.6670, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12357.6669921875
tensor(12357.6670, grad_fn=<NegBackward0>) tensor(12357.6670, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12357.666015625
tensor(12357.6670, grad_fn=<NegBackward0>) tensor(12357.6660, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12357.666015625
tensor(12357.6660, grad_fn=<NegBackward0>) tensor(12357.6660, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12357.6728515625
tensor(12357.6660, grad_fn=<NegBackward0>) tensor(12357.6729, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12357.666015625
tensor(12357.6660, grad_fn=<NegBackward0>) tensor(12357.6660, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12357.6689453125
tensor(12357.6660, grad_fn=<NegBackward0>) tensor(12357.6689, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12357.6650390625
tensor(12357.6660, grad_fn=<NegBackward0>) tensor(12357.6650, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12357.6748046875
tensor(12357.6650, grad_fn=<NegBackward0>) tensor(12357.6748, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12357.6650390625
tensor(12357.6650, grad_fn=<NegBackward0>) tensor(12357.6650, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12357.666015625
tensor(12357.6650, grad_fn=<NegBackward0>) tensor(12357.6660, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12357.6640625
tensor(12357.6650, grad_fn=<NegBackward0>) tensor(12357.6641, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12357.66796875
tensor(12357.6641, grad_fn=<NegBackward0>) tensor(12357.6680, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12357.666015625
tensor(12357.6641, grad_fn=<NegBackward0>) tensor(12357.6660, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12357.6669921875
tensor(12357.6641, grad_fn=<NegBackward0>) tensor(12357.6670, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12357.6650390625
tensor(12357.6641, grad_fn=<NegBackward0>) tensor(12357.6650, grad_fn=<NegBackward0>)
4
Iteration 6800: Loss = -12357.6650390625
tensor(12357.6641, grad_fn=<NegBackward0>) tensor(12357.6650, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.3004, 0.6996],
        [0.9661, 0.0339]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0030, 0.9970], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.2037],
         [0.5127, 0.2033]],

        [[0.6200, 0.2445],
         [0.6114, 0.7255]],

        [[0.7059, 0.2027],
         [0.6457, 0.5240]],

        [[0.6526, 0.2023],
         [0.5929, 0.5685]],

        [[0.6926, 0.1947],
         [0.5244, 0.7110]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.012894826403332019
Global Adjusted Rand Index: -0.0007481349591924137
Average Adjusted Rand Index: -0.0025789652806664036
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21739.93359375
inf tensor(21739.9336, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12359.083984375
tensor(21739.9336, grad_fn=<NegBackward0>) tensor(12359.0840, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12358.74609375
tensor(12359.0840, grad_fn=<NegBackward0>) tensor(12358.7461, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12358.6474609375
tensor(12358.7461, grad_fn=<NegBackward0>) tensor(12358.6475, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12358.5830078125
tensor(12358.6475, grad_fn=<NegBackward0>) tensor(12358.5830, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12358.5322265625
tensor(12358.5830, grad_fn=<NegBackward0>) tensor(12358.5322, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12358.4873046875
tensor(12358.5322, grad_fn=<NegBackward0>) tensor(12358.4873, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12358.4423828125
tensor(12358.4873, grad_fn=<NegBackward0>) tensor(12358.4424, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12358.4033203125
tensor(12358.4424, grad_fn=<NegBackward0>) tensor(12358.4033, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12358.3671875
tensor(12358.4033, grad_fn=<NegBackward0>) tensor(12358.3672, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12358.3310546875
tensor(12358.3672, grad_fn=<NegBackward0>) tensor(12358.3311, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12358.2958984375
tensor(12358.3311, grad_fn=<NegBackward0>) tensor(12358.2959, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12358.2548828125
tensor(12358.2959, grad_fn=<NegBackward0>) tensor(12358.2549, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12358.2060546875
tensor(12358.2549, grad_fn=<NegBackward0>) tensor(12358.2061, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12358.1484375
tensor(12358.2061, grad_fn=<NegBackward0>) tensor(12358.1484, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12358.07421875
tensor(12358.1484, grad_fn=<NegBackward0>) tensor(12358.0742, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12357.97265625
tensor(12358.0742, grad_fn=<NegBackward0>) tensor(12357.9727, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12357.8330078125
tensor(12357.9727, grad_fn=<NegBackward0>) tensor(12357.8330, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12357.6494140625
tensor(12357.8330, grad_fn=<NegBackward0>) tensor(12357.6494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12357.48046875
tensor(12357.6494, grad_fn=<NegBackward0>) tensor(12357.4805, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12357.41015625
tensor(12357.4805, grad_fn=<NegBackward0>) tensor(12357.4102, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12357.3935546875
tensor(12357.4102, grad_fn=<NegBackward0>) tensor(12357.3936, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12357.3896484375
tensor(12357.3936, grad_fn=<NegBackward0>) tensor(12357.3896, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12357.388671875
tensor(12357.3896, grad_fn=<NegBackward0>) tensor(12357.3887, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12357.3876953125
tensor(12357.3887, grad_fn=<NegBackward0>) tensor(12357.3877, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12357.3857421875
tensor(12357.3877, grad_fn=<NegBackward0>) tensor(12357.3857, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12357.3857421875
tensor(12357.3857, grad_fn=<NegBackward0>) tensor(12357.3857, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12357.3837890625
tensor(12357.3857, grad_fn=<NegBackward0>) tensor(12357.3838, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12357.384765625
tensor(12357.3838, grad_fn=<NegBackward0>) tensor(12357.3848, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12357.380859375
tensor(12357.3838, grad_fn=<NegBackward0>) tensor(12357.3809, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12357.37890625
tensor(12357.3809, grad_fn=<NegBackward0>) tensor(12357.3789, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12357.376953125
tensor(12357.3789, grad_fn=<NegBackward0>) tensor(12357.3770, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12357.375
tensor(12357.3770, grad_fn=<NegBackward0>) tensor(12357.3750, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12357.37109375
tensor(12357.3750, grad_fn=<NegBackward0>) tensor(12357.3711, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12357.3681640625
tensor(12357.3711, grad_fn=<NegBackward0>) tensor(12357.3682, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12357.3603515625
tensor(12357.3682, grad_fn=<NegBackward0>) tensor(12357.3604, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12357.353515625
tensor(12357.3604, grad_fn=<NegBackward0>) tensor(12357.3535, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12357.3369140625
tensor(12357.3535, grad_fn=<NegBackward0>) tensor(12357.3369, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12357.310546875
tensor(12357.3369, grad_fn=<NegBackward0>) tensor(12357.3105, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12357.2626953125
tensor(12357.3105, grad_fn=<NegBackward0>) tensor(12357.2627, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12357.1748046875
tensor(12357.2627, grad_fn=<NegBackward0>) tensor(12357.1748, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12067.8017578125
tensor(12357.1748, grad_fn=<NegBackward0>) tensor(12067.8018, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11857.3759765625
tensor(12067.8018, grad_fn=<NegBackward0>) tensor(11857.3760, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11857.2724609375
tensor(11857.3760, grad_fn=<NegBackward0>) tensor(11857.2725, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11857.26171875
tensor(11857.2725, grad_fn=<NegBackward0>) tensor(11857.2617, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11857.2548828125
tensor(11857.2617, grad_fn=<NegBackward0>) tensor(11857.2549, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11857.037109375
tensor(11857.2549, grad_fn=<NegBackward0>) tensor(11857.0371, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11857.0263671875
tensor(11857.0371, grad_fn=<NegBackward0>) tensor(11857.0264, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11857.0263671875
tensor(11857.0264, grad_fn=<NegBackward0>) tensor(11857.0264, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11857.0234375
tensor(11857.0264, grad_fn=<NegBackward0>) tensor(11857.0234, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11857.0224609375
tensor(11857.0234, grad_fn=<NegBackward0>) tensor(11857.0225, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11857.021484375
tensor(11857.0225, grad_fn=<NegBackward0>) tensor(11857.0215, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11857.017578125
tensor(11857.0215, grad_fn=<NegBackward0>) tensor(11857.0176, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11857.0166015625
tensor(11857.0176, grad_fn=<NegBackward0>) tensor(11857.0166, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11857.015625
tensor(11857.0166, grad_fn=<NegBackward0>) tensor(11857.0156, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11857.015625
tensor(11857.0156, grad_fn=<NegBackward0>) tensor(11857.0156, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11857.01171875
tensor(11857.0156, grad_fn=<NegBackward0>) tensor(11857.0117, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11856.994140625
tensor(11857.0117, grad_fn=<NegBackward0>) tensor(11856.9941, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11857.0
tensor(11856.9941, grad_fn=<NegBackward0>) tensor(11857., grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11856.986328125
tensor(11856.9941, grad_fn=<NegBackward0>) tensor(11856.9863, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11856.9365234375
tensor(11856.9863, grad_fn=<NegBackward0>) tensor(11856.9365, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11856.9345703125
tensor(11856.9365, grad_fn=<NegBackward0>) tensor(11856.9346, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11856.9345703125
tensor(11856.9346, grad_fn=<NegBackward0>) tensor(11856.9346, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11856.94140625
tensor(11856.9346, grad_fn=<NegBackward0>) tensor(11856.9414, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11856.9306640625
tensor(11856.9346, grad_fn=<NegBackward0>) tensor(11856.9307, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11856.9208984375
tensor(11856.9307, grad_fn=<NegBackward0>) tensor(11856.9209, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11856.923828125
tensor(11856.9209, grad_fn=<NegBackward0>) tensor(11856.9238, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11856.9189453125
tensor(11856.9209, grad_fn=<NegBackward0>) tensor(11856.9189, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11856.91796875
tensor(11856.9189, grad_fn=<NegBackward0>) tensor(11856.9180, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11856.9130859375
tensor(11856.9180, grad_fn=<NegBackward0>) tensor(11856.9131, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11856.9140625
tensor(11856.9131, grad_fn=<NegBackward0>) tensor(11856.9141, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11856.9140625
tensor(11856.9131, grad_fn=<NegBackward0>) tensor(11856.9141, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11856.912109375
tensor(11856.9131, grad_fn=<NegBackward0>) tensor(11856.9121, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11856.912109375
tensor(11856.9121, grad_fn=<NegBackward0>) tensor(11856.9121, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11856.912109375
tensor(11856.9121, grad_fn=<NegBackward0>) tensor(11856.9121, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11856.908203125
tensor(11856.9121, grad_fn=<NegBackward0>) tensor(11856.9082, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11856.90625
tensor(11856.9082, grad_fn=<NegBackward0>) tensor(11856.9062, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11856.912109375
tensor(11856.9062, grad_fn=<NegBackward0>) tensor(11856.9121, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11856.9052734375
tensor(11856.9062, grad_fn=<NegBackward0>) tensor(11856.9053, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11856.9052734375
tensor(11856.9053, grad_fn=<NegBackward0>) tensor(11856.9053, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11856.904296875
tensor(11856.9053, grad_fn=<NegBackward0>) tensor(11856.9043, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11856.9052734375
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9053, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11856.90625
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9062, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11856.953125
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9531, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11856.904296875
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9043, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11856.9052734375
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9053, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11856.9052734375
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9053, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11856.904296875
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9043, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11856.919921875
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9199, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11856.904296875
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9043, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11856.9130859375
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9131, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11856.904296875
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9043, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11856.904296875
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9043, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11856.9072265625
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9072, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11856.9033203125
tensor(11856.9043, grad_fn=<NegBackward0>) tensor(11856.9033, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11856.8916015625
tensor(11856.9033, grad_fn=<NegBackward0>) tensor(11856.8916, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11856.88671875
tensor(11856.8916, grad_fn=<NegBackward0>) tensor(11856.8867, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11856.8818359375
tensor(11856.8867, grad_fn=<NegBackward0>) tensor(11856.8818, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11856.8896484375
tensor(11856.8818, grad_fn=<NegBackward0>) tensor(11856.8896, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11856.8828125
tensor(11856.8818, grad_fn=<NegBackward0>) tensor(11856.8828, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7907, 0.2093],
        [0.2658, 0.7342]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5201, 0.4799], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2979, 0.0991],
         [0.6788, 0.2974]],

        [[0.6961, 0.0987],
         [0.6982, 0.5609]],

        [[0.7154, 0.1045],
         [0.6440, 0.6942]],

        [[0.6400, 0.1007],
         [0.6097, 0.6564]],

        [[0.6151, 0.0996],
         [0.5632, 0.6155]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.882389682918764
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9603197549983711
Average Adjusted Rand Index: 0.960639552745369
[-0.0007481349591924137, 0.9603197549983711] [-0.0025789652806664036, 0.960639552745369] [12357.6650390625, 11856.8837890625]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11984.311193808191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21646.890625
inf tensor(21646.8906, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12506.279296875
tensor(21646.8906, grad_fn=<NegBackward0>) tensor(12506.2793, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12506.0703125
tensor(12506.2793, grad_fn=<NegBackward0>) tensor(12506.0703, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12506.0185546875
tensor(12506.0703, grad_fn=<NegBackward0>) tensor(12506.0186, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12505.990234375
tensor(12506.0186, grad_fn=<NegBackward0>) tensor(12505.9902, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12505.9736328125
tensor(12505.9902, grad_fn=<NegBackward0>) tensor(12505.9736, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12505.9599609375
tensor(12505.9736, grad_fn=<NegBackward0>) tensor(12505.9600, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12505.9501953125
tensor(12505.9600, grad_fn=<NegBackward0>) tensor(12505.9502, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12505.939453125
tensor(12505.9502, grad_fn=<NegBackward0>) tensor(12505.9395, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12505.923828125
tensor(12505.9395, grad_fn=<NegBackward0>) tensor(12505.9238, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12505.8876953125
tensor(12505.9238, grad_fn=<NegBackward0>) tensor(12505.8877, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12505.7177734375
tensor(12505.8877, grad_fn=<NegBackward0>) tensor(12505.7178, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12505.294921875
tensor(12505.7178, grad_fn=<NegBackward0>) tensor(12505.2949, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12505.1875
tensor(12505.2949, grad_fn=<NegBackward0>) tensor(12505.1875, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12505.1572265625
tensor(12505.1875, grad_fn=<NegBackward0>) tensor(12505.1572, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12505.146484375
tensor(12505.1572, grad_fn=<NegBackward0>) tensor(12505.1465, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12505.140625
tensor(12505.1465, grad_fn=<NegBackward0>) tensor(12505.1406, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12505.13671875
tensor(12505.1406, grad_fn=<NegBackward0>) tensor(12505.1367, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12505.134765625
tensor(12505.1367, grad_fn=<NegBackward0>) tensor(12505.1348, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12505.1328125
tensor(12505.1348, grad_fn=<NegBackward0>) tensor(12505.1328, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12505.1328125
tensor(12505.1328, grad_fn=<NegBackward0>) tensor(12505.1328, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12505.130859375
tensor(12505.1328, grad_fn=<NegBackward0>) tensor(12505.1309, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12505.1298828125
tensor(12505.1309, grad_fn=<NegBackward0>) tensor(12505.1299, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12505.1279296875
tensor(12505.1299, grad_fn=<NegBackward0>) tensor(12505.1279, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12505.1259765625
tensor(12505.1279, grad_fn=<NegBackward0>) tensor(12505.1260, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12505.1240234375
tensor(12505.1260, grad_fn=<NegBackward0>) tensor(12505.1240, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12505.119140625
tensor(12505.1240, grad_fn=<NegBackward0>) tensor(12505.1191, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12505.1044921875
tensor(12505.1191, grad_fn=<NegBackward0>) tensor(12505.1045, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12503.681640625
tensor(12505.1045, grad_fn=<NegBackward0>) tensor(12503.6816, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12502.2353515625
tensor(12503.6816, grad_fn=<NegBackward0>) tensor(12502.2354, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12502.109375
tensor(12502.2354, grad_fn=<NegBackward0>) tensor(12502.1094, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12502.06640625
tensor(12502.1094, grad_fn=<NegBackward0>) tensor(12502.0664, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12502.044921875
tensor(12502.0664, grad_fn=<NegBackward0>) tensor(12502.0449, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12502.0302734375
tensor(12502.0449, grad_fn=<NegBackward0>) tensor(12502.0303, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12502.0224609375
tensor(12502.0303, grad_fn=<NegBackward0>) tensor(12502.0225, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12502.015625
tensor(12502.0225, grad_fn=<NegBackward0>) tensor(12502.0156, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12502.009765625
tensor(12502.0156, grad_fn=<NegBackward0>) tensor(12502.0098, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12502.0068359375
tensor(12502.0098, grad_fn=<NegBackward0>) tensor(12502.0068, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12502.0029296875
tensor(12502.0068, grad_fn=<NegBackward0>) tensor(12502.0029, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12502.0009765625
tensor(12502.0029, grad_fn=<NegBackward0>) tensor(12502.0010, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12501.9990234375
tensor(12502.0010, grad_fn=<NegBackward0>) tensor(12501.9990, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12501.9970703125
tensor(12501.9990, grad_fn=<NegBackward0>) tensor(12501.9971, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12501.9951171875
tensor(12501.9971, grad_fn=<NegBackward0>) tensor(12501.9951, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12501.9970703125
tensor(12501.9951, grad_fn=<NegBackward0>) tensor(12501.9971, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12501.994140625
tensor(12501.9951, grad_fn=<NegBackward0>) tensor(12501.9941, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12501.9931640625
tensor(12501.9941, grad_fn=<NegBackward0>) tensor(12501.9932, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12501.9921875
tensor(12501.9932, grad_fn=<NegBackward0>) tensor(12501.9922, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12501.9921875
tensor(12501.9922, grad_fn=<NegBackward0>) tensor(12501.9922, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12501.990234375
tensor(12501.9922, grad_fn=<NegBackward0>) tensor(12501.9902, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12501.98828125
tensor(12501.9902, grad_fn=<NegBackward0>) tensor(12501.9883, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12501.9892578125
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9893, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12501.98828125
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9883, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12501.9892578125
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9893, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12501.98828125
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9883, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12501.98828125
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9883, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12501.98828125
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9883, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12501.9873046875
tensor(12501.9883, grad_fn=<NegBackward0>) tensor(12501.9873, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12501.9873046875
tensor(12501.9873, grad_fn=<NegBackward0>) tensor(12501.9873, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12501.986328125
tensor(12501.9873, grad_fn=<NegBackward0>) tensor(12501.9863, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12501.986328125
tensor(12501.9863, grad_fn=<NegBackward0>) tensor(12501.9863, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12501.9853515625
tensor(12501.9863, grad_fn=<NegBackward0>) tensor(12501.9854, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12501.986328125
tensor(12501.9854, grad_fn=<NegBackward0>) tensor(12501.9863, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12501.9853515625
tensor(12501.9854, grad_fn=<NegBackward0>) tensor(12501.9854, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12501.986328125
tensor(12501.9854, grad_fn=<NegBackward0>) tensor(12501.9863, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12501.986328125
tensor(12501.9854, grad_fn=<NegBackward0>) tensor(12501.9863, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12501.984375
tensor(12501.9854, grad_fn=<NegBackward0>) tensor(12501.9844, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12501.9853515625
tensor(12501.9844, grad_fn=<NegBackward0>) tensor(12501.9854, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12501.9853515625
tensor(12501.9844, grad_fn=<NegBackward0>) tensor(12501.9854, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12501.9853515625
tensor(12501.9844, grad_fn=<NegBackward0>) tensor(12501.9854, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12501.984375
tensor(12501.9844, grad_fn=<NegBackward0>) tensor(12501.9844, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12501.9833984375
tensor(12501.9844, grad_fn=<NegBackward0>) tensor(12501.9834, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12501.984375
tensor(12501.9834, grad_fn=<NegBackward0>) tensor(12501.9844, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12501.984375
tensor(12501.9834, grad_fn=<NegBackward0>) tensor(12501.9844, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12501.986328125
tensor(12501.9834, grad_fn=<NegBackward0>) tensor(12501.9863, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -12501.984375
tensor(12501.9834, grad_fn=<NegBackward0>) tensor(12501.9844, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -12501.9833984375
tensor(12501.9834, grad_fn=<NegBackward0>) tensor(12501.9834, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12501.982421875
tensor(12501.9834, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12501.990234375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9902, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12502.291015625
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12502.2910, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12501.9833984375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9834, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12501.9833984375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9834, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12501.990234375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9902, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12501.9833984375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9834, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12501.9833984375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9834, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12502.21484375
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12502.2148, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12501.982421875
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9824, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12501.9814453125
tensor(12501.9824, grad_fn=<NegBackward0>) tensor(12501.9814, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12502.05078125
tensor(12501.9814, grad_fn=<NegBackward0>) tensor(12502.0508, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9976e-01, 2.4028e-04],
        [3.3920e-03, 9.9661e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0071, 0.9929], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2825, 0.2219],
         [0.6068, 0.2048]],

        [[0.5417, 0.0816],
         [0.5456, 0.6200]],

        [[0.5859, 0.2380],
         [0.7121, 0.6788]],

        [[0.5024, 0.1547],
         [0.6164, 0.6742]],

        [[0.7273, 0.1222],
         [0.6337, 0.7081]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.00015159247594385454
Average Adjusted Rand Index: -0.0005185674284992829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21933.990234375
inf tensor(21933.9902, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12506.328125
tensor(21933.9902, grad_fn=<NegBackward0>) tensor(12506.3281, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12506.0712890625
tensor(12506.3281, grad_fn=<NegBackward0>) tensor(12506.0713, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12506.0166015625
tensor(12506.0713, grad_fn=<NegBackward0>) tensor(12506.0166, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12505.990234375
tensor(12506.0166, grad_fn=<NegBackward0>) tensor(12505.9902, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12505.9765625
tensor(12505.9902, grad_fn=<NegBackward0>) tensor(12505.9766, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12505.9677734375
tensor(12505.9766, grad_fn=<NegBackward0>) tensor(12505.9678, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12505.9580078125
tensor(12505.9678, grad_fn=<NegBackward0>) tensor(12505.9580, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12505.9501953125
tensor(12505.9580, grad_fn=<NegBackward0>) tensor(12505.9502, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12505.9462890625
tensor(12505.9502, grad_fn=<NegBackward0>) tensor(12505.9463, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12505.9404296875
tensor(12505.9463, grad_fn=<NegBackward0>) tensor(12505.9404, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12505.9375
tensor(12505.9404, grad_fn=<NegBackward0>) tensor(12505.9375, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12505.931640625
tensor(12505.9375, grad_fn=<NegBackward0>) tensor(12505.9316, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12505.927734375
tensor(12505.9316, grad_fn=<NegBackward0>) tensor(12505.9277, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12505.9248046875
tensor(12505.9277, grad_fn=<NegBackward0>) tensor(12505.9248, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12505.9189453125
tensor(12505.9248, grad_fn=<NegBackward0>) tensor(12505.9189, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12505.9130859375
tensor(12505.9189, grad_fn=<NegBackward0>) tensor(12505.9131, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12505.908203125
tensor(12505.9131, grad_fn=<NegBackward0>) tensor(12505.9082, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12505.8984375
tensor(12505.9082, grad_fn=<NegBackward0>) tensor(12505.8984, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12505.888671875
tensor(12505.8984, grad_fn=<NegBackward0>) tensor(12505.8887, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12505.8740234375
tensor(12505.8887, grad_fn=<NegBackward0>) tensor(12505.8740, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12505.8603515625
tensor(12505.8740, grad_fn=<NegBackward0>) tensor(12505.8604, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12505.845703125
tensor(12505.8604, grad_fn=<NegBackward0>) tensor(12505.8457, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12505.8349609375
tensor(12505.8457, grad_fn=<NegBackward0>) tensor(12505.8350, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12505.828125
tensor(12505.8350, grad_fn=<NegBackward0>) tensor(12505.8281, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12505.8212890625
tensor(12505.8281, grad_fn=<NegBackward0>) tensor(12505.8213, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12505.81640625
tensor(12505.8213, grad_fn=<NegBackward0>) tensor(12505.8164, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12505.8125
tensor(12505.8164, grad_fn=<NegBackward0>) tensor(12505.8125, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12505.8095703125
tensor(12505.8125, grad_fn=<NegBackward0>) tensor(12505.8096, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12505.8037109375
tensor(12505.8096, grad_fn=<NegBackward0>) tensor(12505.8037, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12505.7998046875
tensor(12505.8037, grad_fn=<NegBackward0>) tensor(12505.7998, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12505.7958984375
tensor(12505.7998, grad_fn=<NegBackward0>) tensor(12505.7959, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12505.7939453125
tensor(12505.7959, grad_fn=<NegBackward0>) tensor(12505.7939, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12505.7880859375
tensor(12505.7939, grad_fn=<NegBackward0>) tensor(12505.7881, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12505.7841796875
tensor(12505.7881, grad_fn=<NegBackward0>) tensor(12505.7842, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12505.779296875
tensor(12505.7842, grad_fn=<NegBackward0>) tensor(12505.7793, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12505.775390625
tensor(12505.7793, grad_fn=<NegBackward0>) tensor(12505.7754, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12505.771484375
tensor(12505.7754, grad_fn=<NegBackward0>) tensor(12505.7715, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12505.7666015625
tensor(12505.7715, grad_fn=<NegBackward0>) tensor(12505.7666, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12505.763671875
tensor(12505.7666, grad_fn=<NegBackward0>) tensor(12505.7637, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12505.759765625
tensor(12505.7637, grad_fn=<NegBackward0>) tensor(12505.7598, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12505.7568359375
tensor(12505.7598, grad_fn=<NegBackward0>) tensor(12505.7568, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12505.7529296875
tensor(12505.7568, grad_fn=<NegBackward0>) tensor(12505.7529, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12505.7509765625
tensor(12505.7529, grad_fn=<NegBackward0>) tensor(12505.7510, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12505.7490234375
tensor(12505.7510, grad_fn=<NegBackward0>) tensor(12505.7490, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12505.7470703125
tensor(12505.7490, grad_fn=<NegBackward0>) tensor(12505.7471, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12505.74609375
tensor(12505.7471, grad_fn=<NegBackward0>) tensor(12505.7461, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12505.7451171875
tensor(12505.7461, grad_fn=<NegBackward0>) tensor(12505.7451, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12505.7431640625
tensor(12505.7451, grad_fn=<NegBackward0>) tensor(12505.7432, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12505.7412109375
tensor(12505.7432, grad_fn=<NegBackward0>) tensor(12505.7412, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12505.740234375
tensor(12505.7412, grad_fn=<NegBackward0>) tensor(12505.7402, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12505.740234375
tensor(12505.7402, grad_fn=<NegBackward0>) tensor(12505.7402, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12505.7421875
tensor(12505.7402, grad_fn=<NegBackward0>) tensor(12505.7422, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12505.7373046875
tensor(12505.7402, grad_fn=<NegBackward0>) tensor(12505.7373, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12505.7373046875
tensor(12505.7373, grad_fn=<NegBackward0>) tensor(12505.7373, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12505.7373046875
tensor(12505.7373, grad_fn=<NegBackward0>) tensor(12505.7373, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12505.736328125
tensor(12505.7373, grad_fn=<NegBackward0>) tensor(12505.7363, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12505.7421875
tensor(12505.7363, grad_fn=<NegBackward0>) tensor(12505.7422, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12505.7353515625
tensor(12505.7363, grad_fn=<NegBackward0>) tensor(12505.7354, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12505.736328125
tensor(12505.7354, grad_fn=<NegBackward0>) tensor(12505.7363, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12505.744140625
tensor(12505.7354, grad_fn=<NegBackward0>) tensor(12505.7441, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -12505.7353515625
tensor(12505.7354, grad_fn=<NegBackward0>) tensor(12505.7354, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12505.734375
tensor(12505.7354, grad_fn=<NegBackward0>) tensor(12505.7344, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12505.732421875
tensor(12505.7344, grad_fn=<NegBackward0>) tensor(12505.7324, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12505.7333984375
tensor(12505.7324, grad_fn=<NegBackward0>) tensor(12505.7334, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12505.736328125
tensor(12505.7324, grad_fn=<NegBackward0>) tensor(12505.7363, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12505.732421875
tensor(12505.7324, grad_fn=<NegBackward0>) tensor(12505.7324, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12505.732421875
tensor(12505.7324, grad_fn=<NegBackward0>) tensor(12505.7324, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12505.734375
tensor(12505.7324, grad_fn=<NegBackward0>) tensor(12505.7344, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12505.7314453125
tensor(12505.7324, grad_fn=<NegBackward0>) tensor(12505.7314, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12505.736328125
tensor(12505.7314, grad_fn=<NegBackward0>) tensor(12505.7363, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12505.7314453125
tensor(12505.7314, grad_fn=<NegBackward0>) tensor(12505.7314, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12505.73046875
tensor(12505.7314, grad_fn=<NegBackward0>) tensor(12505.7305, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12505.73046875
tensor(12505.7305, grad_fn=<NegBackward0>) tensor(12505.7305, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12505.7294921875
tensor(12505.7305, grad_fn=<NegBackward0>) tensor(12505.7295, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12505.751953125
tensor(12505.7295, grad_fn=<NegBackward0>) tensor(12505.7520, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12505.728515625
tensor(12505.7295, grad_fn=<NegBackward0>) tensor(12505.7285, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12505.7314453125
tensor(12505.7285, grad_fn=<NegBackward0>) tensor(12505.7314, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12505.728515625
tensor(12505.7285, grad_fn=<NegBackward0>) tensor(12505.7285, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12505.728515625
tensor(12505.7285, grad_fn=<NegBackward0>) tensor(12505.7285, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12505.79296875
tensor(12505.7285, grad_fn=<NegBackward0>) tensor(12505.7930, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12505.716796875
tensor(12505.7285, grad_fn=<NegBackward0>) tensor(12505.7168, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12505.7529296875
tensor(12505.7168, grad_fn=<NegBackward0>) tensor(12505.7529, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12505.6943359375
tensor(12505.7168, grad_fn=<NegBackward0>) tensor(12505.6943, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12505.6796875
tensor(12505.6943, grad_fn=<NegBackward0>) tensor(12505.6797, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12505.6728515625
tensor(12505.6797, grad_fn=<NegBackward0>) tensor(12505.6729, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12505.6943359375
tensor(12505.6729, grad_fn=<NegBackward0>) tensor(12505.6943, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12505.625
tensor(12505.6729, grad_fn=<NegBackward0>) tensor(12505.6250, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12501.748046875
tensor(12505.6250, grad_fn=<NegBackward0>) tensor(12501.7480, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12501.1630859375
tensor(12501.7480, grad_fn=<NegBackward0>) tensor(12501.1631, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12501.1337890625
tensor(12501.1631, grad_fn=<NegBackward0>) tensor(12501.1338, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12501.078125
tensor(12501.1338, grad_fn=<NegBackward0>) tensor(12501.0781, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12501.064453125
tensor(12501.0781, grad_fn=<NegBackward0>) tensor(12501.0645, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12501.0576171875
tensor(12501.0645, grad_fn=<NegBackward0>) tensor(12501.0576, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12501.0517578125
tensor(12501.0576, grad_fn=<NegBackward0>) tensor(12501.0518, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12501.0458984375
tensor(12501.0518, grad_fn=<NegBackward0>) tensor(12501.0459, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12501.0439453125
tensor(12501.0459, grad_fn=<NegBackward0>) tensor(12501.0439, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12501.0400390625
tensor(12501.0439, grad_fn=<NegBackward0>) tensor(12501.0400, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12501.041015625
tensor(12501.0400, grad_fn=<NegBackward0>) tensor(12501.0410, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12501.037109375
tensor(12501.0400, grad_fn=<NegBackward0>) tensor(12501.0371, grad_fn=<NegBackward0>)
pi: tensor([[9.9980e-01, 1.9615e-04],
        [5.6302e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0293, 0.9707], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0020, 0.2431],
         [0.7096, 0.2023]],

        [[0.5190, 0.2458],
         [0.6703, 0.5391]],

        [[0.6746, 0.2692],
         [0.5246, 0.6043]],

        [[0.7275, 0.2377],
         [0.6296, 0.6166]],

        [[0.7184, 0.1206],
         [0.6631, 0.5802]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
Global Adjusted Rand Index: -0.0007266249896091228
Average Adjusted Rand Index: -0.0017110129804768289
[0.00015159247594385454, -0.0007266249896091228] [-0.0005185674284992829, -0.0017110129804768289] [12501.9814453125, 12501.0361328125]
-------------------------------------
This iteration is 17
True Objective function: Loss = -11862.393307153518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21109.005859375
inf tensor(21109.0059, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12325.6767578125
tensor(21109.0059, grad_fn=<NegBackward0>) tensor(12325.6768, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12325.1748046875
tensor(12325.6768, grad_fn=<NegBackward0>) tensor(12325.1748, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12325.0791015625
tensor(12325.1748, grad_fn=<NegBackward0>) tensor(12325.0791, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12325.01171875
tensor(12325.0791, grad_fn=<NegBackward0>) tensor(12325.0117, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12324.955078125
tensor(12325.0117, grad_fn=<NegBackward0>) tensor(12324.9551, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12324.900390625
tensor(12324.9551, grad_fn=<NegBackward0>) tensor(12324.9004, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12324.849609375
tensor(12324.9004, grad_fn=<NegBackward0>) tensor(12324.8496, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12324.7958984375
tensor(12324.8496, grad_fn=<NegBackward0>) tensor(12324.7959, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12324.7470703125
tensor(12324.7959, grad_fn=<NegBackward0>) tensor(12324.7471, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12324.69140625
tensor(12324.7471, grad_fn=<NegBackward0>) tensor(12324.6914, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12324.6357421875
tensor(12324.6914, grad_fn=<NegBackward0>) tensor(12324.6357, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12324.5751953125
tensor(12324.6357, grad_fn=<NegBackward0>) tensor(12324.5752, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12324.509765625
tensor(12324.5752, grad_fn=<NegBackward0>) tensor(12324.5098, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12324.4384765625
tensor(12324.5098, grad_fn=<NegBackward0>) tensor(12324.4385, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12324.3623046875
tensor(12324.4385, grad_fn=<NegBackward0>) tensor(12324.3623, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12324.2841796875
tensor(12324.3623, grad_fn=<NegBackward0>) tensor(12324.2842, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12324.21484375
tensor(12324.2842, grad_fn=<NegBackward0>) tensor(12324.2148, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12324.1572265625
tensor(12324.2148, grad_fn=<NegBackward0>) tensor(12324.1572, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12324.1142578125
tensor(12324.1572, grad_fn=<NegBackward0>) tensor(12324.1143, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12324.0830078125
tensor(12324.1143, grad_fn=<NegBackward0>) tensor(12324.0830, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12324.0625
tensor(12324.0830, grad_fn=<NegBackward0>) tensor(12324.0625, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12324.044921875
tensor(12324.0625, grad_fn=<NegBackward0>) tensor(12324.0449, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12324.033203125
tensor(12324.0449, grad_fn=<NegBackward0>) tensor(12324.0332, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12324.0263671875
tensor(12324.0332, grad_fn=<NegBackward0>) tensor(12324.0264, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12324.01953125
tensor(12324.0264, grad_fn=<NegBackward0>) tensor(12324.0195, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12324.0146484375
tensor(12324.0195, grad_fn=<NegBackward0>) tensor(12324.0146, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12324.01171875
tensor(12324.0146, grad_fn=<NegBackward0>) tensor(12324.0117, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12324.0087890625
tensor(12324.0117, grad_fn=<NegBackward0>) tensor(12324.0088, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12324.0087890625
tensor(12324.0088, grad_fn=<NegBackward0>) tensor(12324.0088, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12324.0068359375
tensor(12324.0088, grad_fn=<NegBackward0>) tensor(12324.0068, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12324.0048828125
tensor(12324.0068, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12324.0048828125
tensor(12324.0049, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12324.0078125
tensor(12324.0049, grad_fn=<NegBackward0>) tensor(12324.0078, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12324.00390625
tensor(12324.0049, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12324.005859375
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0059, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12324.0048828125
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -12324.0048828125
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -12324.0048828125
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
4
Iteration 3900: Loss = -12324.00390625
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12324.00390625
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12324.00390625
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12324.0048828125
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12324.0048828125
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -12324.0048828125
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -12324.0029296875
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12324.0048828125
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0049, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12324.0029296875
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12324.00390625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12324.00390625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -12324.00390625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -12324.009765625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0098, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -12324.001953125
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0020, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12324.0029296875
tensor(12324.0020, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12324.0068359375
tensor(12324.0020, grad_fn=<NegBackward0>) tensor(12324.0068, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -12324.0029296875
tensor(12324.0020, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -12324.00390625
tensor(12324.0020, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -12324.00390625
tensor(12324.0020, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5700 due to no improvement.
pi: tensor([[0.0278, 0.9722],
        [0.1332, 0.8668]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0169, 0.9831], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1805, 0.1948],
         [0.5924, 0.2003]],

        [[0.6112, 0.1900],
         [0.5718, 0.7281]],

        [[0.6343, 0.2119],
         [0.5597, 0.6952]],

        [[0.7120, 0.1853],
         [0.5036, 0.6901]],

        [[0.7195, 0.1743],
         [0.6835, 0.5338]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21093.705078125
inf tensor(21093.7051, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12325.4599609375
tensor(21093.7051, grad_fn=<NegBackward0>) tensor(12325.4600, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12325.1328125
tensor(12325.4600, grad_fn=<NegBackward0>) tensor(12325.1328, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12325.076171875
tensor(12325.1328, grad_fn=<NegBackward0>) tensor(12325.0762, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12325.0478515625
tensor(12325.0762, grad_fn=<NegBackward0>) tensor(12325.0479, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12325.01953125
tensor(12325.0479, grad_fn=<NegBackward0>) tensor(12325.0195, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12324.98828125
tensor(12325.0195, grad_fn=<NegBackward0>) tensor(12324.9883, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12324.947265625
tensor(12324.9883, grad_fn=<NegBackward0>) tensor(12324.9473, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12324.890625
tensor(12324.9473, grad_fn=<NegBackward0>) tensor(12324.8906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12324.81640625
tensor(12324.8906, grad_fn=<NegBackward0>) tensor(12324.8164, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12324.728515625
tensor(12324.8164, grad_fn=<NegBackward0>) tensor(12324.7285, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12324.638671875
tensor(12324.7285, grad_fn=<NegBackward0>) tensor(12324.6387, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12324.55078125
tensor(12324.6387, grad_fn=<NegBackward0>) tensor(12324.5508, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12324.4658203125
tensor(12324.5508, grad_fn=<NegBackward0>) tensor(12324.4658, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12324.375
tensor(12324.4658, grad_fn=<NegBackward0>) tensor(12324.3750, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12324.28515625
tensor(12324.3750, grad_fn=<NegBackward0>) tensor(12324.2852, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12324.2021484375
tensor(12324.2852, grad_fn=<NegBackward0>) tensor(12324.2021, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12324.13671875
tensor(12324.2021, grad_fn=<NegBackward0>) tensor(12324.1367, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12324.0927734375
tensor(12324.1367, grad_fn=<NegBackward0>) tensor(12324.0928, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12324.0634765625
tensor(12324.0928, grad_fn=<NegBackward0>) tensor(12324.0635, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12324.0439453125
tensor(12324.0635, grad_fn=<NegBackward0>) tensor(12324.0439, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12324.0302734375
tensor(12324.0439, grad_fn=<NegBackward0>) tensor(12324.0303, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12324.0205078125
tensor(12324.0303, grad_fn=<NegBackward0>) tensor(12324.0205, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12324.0146484375
tensor(12324.0205, grad_fn=<NegBackward0>) tensor(12324.0146, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12324.0107421875
tensor(12324.0146, grad_fn=<NegBackward0>) tensor(12324.0107, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12324.0087890625
tensor(12324.0107, grad_fn=<NegBackward0>) tensor(12324.0088, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12324.0068359375
tensor(12324.0088, grad_fn=<NegBackward0>) tensor(12324.0068, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12324.00390625
tensor(12324.0068, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12324.0029296875
tensor(12324.0039, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12324.00390625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -12324.00390625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
2
Iteration 3100: Loss = -12324.0029296875
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12324.0029296875
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12324.00390625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0039, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12324.0029296875
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12324.0029296875
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12324.0009765625
tensor(12324.0029, grad_fn=<NegBackward0>) tensor(12324.0010, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12324.001953125
tensor(12324.0010, grad_fn=<NegBackward0>) tensor(12324.0020, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12324.001953125
tensor(12324.0010, grad_fn=<NegBackward0>) tensor(12324.0020, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -12324.0029296875
tensor(12324.0010, grad_fn=<NegBackward0>) tensor(12324.0029, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -12324.001953125
tensor(12324.0010, grad_fn=<NegBackward0>) tensor(12324.0020, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -12324.001953125
tensor(12324.0010, grad_fn=<NegBackward0>) tensor(12324.0020, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4100 due to no improvement.
pi: tensor([[0.8650, 0.1350],
        [0.9798, 0.0202]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9847, 0.0153], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.1947],
         [0.6637, 0.1805]],

        [[0.5076, 0.1901],
         [0.5214, 0.5500]],

        [[0.6704, 0.2119],
         [0.6731, 0.5288]],

        [[0.6902, 0.1854],
         [0.6698, 0.7006]],

        [[0.7199, 0.1743],
         [0.5551, 0.6492]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [12324.00390625, 12324.001953125]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11811.369077805504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20348.150390625
inf tensor(20348.1504, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12291.5771484375
tensor(20348.1504, grad_fn=<NegBackward0>) tensor(12291.5771, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12291.32421875
tensor(12291.5771, grad_fn=<NegBackward0>) tensor(12291.3242, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12291.0888671875
tensor(12291.3242, grad_fn=<NegBackward0>) tensor(12291.0889, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12290.859375
tensor(12291.0889, grad_fn=<NegBackward0>) tensor(12290.8594, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12290.740234375
tensor(12290.8594, grad_fn=<NegBackward0>) tensor(12290.7402, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12290.6591796875
tensor(12290.7402, grad_fn=<NegBackward0>) tensor(12290.6592, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12290.56640625
tensor(12290.6592, grad_fn=<NegBackward0>) tensor(12290.5664, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12290.4775390625
tensor(12290.5664, grad_fn=<NegBackward0>) tensor(12290.4775, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12290.4111328125
tensor(12290.4775, grad_fn=<NegBackward0>) tensor(12290.4111, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12290.35546875
tensor(12290.4111, grad_fn=<NegBackward0>) tensor(12290.3555, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12290.29296875
tensor(12290.3555, grad_fn=<NegBackward0>) tensor(12290.2930, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12290.2099609375
tensor(12290.2930, grad_fn=<NegBackward0>) tensor(12290.2100, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12290.1162109375
tensor(12290.2100, grad_fn=<NegBackward0>) tensor(12290.1162, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12290.0517578125
tensor(12290.1162, grad_fn=<NegBackward0>) tensor(12290.0518, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12290.0185546875
tensor(12290.0518, grad_fn=<NegBackward0>) tensor(12290.0186, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12290.0009765625
tensor(12290.0186, grad_fn=<NegBackward0>) tensor(12290.0010, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12289.9873046875
tensor(12290.0010, grad_fn=<NegBackward0>) tensor(12289.9873, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12289.98046875
tensor(12289.9873, grad_fn=<NegBackward0>) tensor(12289.9805, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12289.9755859375
tensor(12289.9805, grad_fn=<NegBackward0>) tensor(12289.9756, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12289.9716796875
tensor(12289.9756, grad_fn=<NegBackward0>) tensor(12289.9717, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12289.96875
tensor(12289.9717, grad_fn=<NegBackward0>) tensor(12289.9688, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12289.966796875
tensor(12289.9688, grad_fn=<NegBackward0>) tensor(12289.9668, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12289.966796875
tensor(12289.9668, grad_fn=<NegBackward0>) tensor(12289.9668, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12289.9658203125
tensor(12289.9668, grad_fn=<NegBackward0>) tensor(12289.9658, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12289.966796875
tensor(12289.9658, grad_fn=<NegBackward0>) tensor(12289.9668, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -12289.9658203125
tensor(12289.9658, grad_fn=<NegBackward0>) tensor(12289.9658, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12289.9658203125
tensor(12289.9658, grad_fn=<NegBackward0>) tensor(12289.9658, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12289.9658203125
tensor(12289.9658, grad_fn=<NegBackward0>) tensor(12289.9658, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12289.9638671875
tensor(12289.9658, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12289.96484375
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9648, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -12289.9638671875
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12289.96484375
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9648, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12289.9638671875
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12289.96484375
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9648, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12289.9638671875
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12289.962890625
tensor(12289.9639, grad_fn=<NegBackward0>) tensor(12289.9629, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12289.9638671875
tensor(12289.9629, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12289.96484375
tensor(12289.9629, grad_fn=<NegBackward0>) tensor(12289.9648, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -12289.9638671875
tensor(12289.9629, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
3
Iteration 4000: Loss = -12289.9638671875
tensor(12289.9629, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
4
Iteration 4100: Loss = -12289.9638671875
tensor(12289.9629, grad_fn=<NegBackward0>) tensor(12289.9639, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4100 due to no improvement.
pi: tensor([[0.3648, 0.6352],
        [0.0051, 0.9949]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0403, 0.9597], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1550, 0.1754],
         [0.6621, 0.1986]],

        [[0.6379, 0.1024],
         [0.7171, 0.6973]],

        [[0.6335, 0.2650],
         [0.7293, 0.6953]],

        [[0.5292, 0.1644],
         [0.6037, 0.7028]],

        [[0.6964, 0.0923],
         [0.5432, 0.6417]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.018335442091456804
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0024810091574889704
Average Adjusted Rand Index: 0.005399824978035971
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21101.416015625
inf tensor(21101.4160, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12291.8173828125
tensor(21101.4160, grad_fn=<NegBackward0>) tensor(12291.8174, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12291.5361328125
tensor(12291.8174, grad_fn=<NegBackward0>) tensor(12291.5361, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12291.4580078125
tensor(12291.5361, grad_fn=<NegBackward0>) tensor(12291.4580, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12291.39453125
tensor(12291.4580, grad_fn=<NegBackward0>) tensor(12291.3945, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12291.322265625
tensor(12291.3945, grad_fn=<NegBackward0>) tensor(12291.3223, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12291.2490234375
tensor(12291.3223, grad_fn=<NegBackward0>) tensor(12291.2490, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12291.173828125
tensor(12291.2490, grad_fn=<NegBackward0>) tensor(12291.1738, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12291.109375
tensor(12291.1738, grad_fn=<NegBackward0>) tensor(12291.1094, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12291.0634765625
tensor(12291.1094, grad_fn=<NegBackward0>) tensor(12291.0635, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12291.037109375
tensor(12291.0635, grad_fn=<NegBackward0>) tensor(12291.0371, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12291.0185546875
tensor(12291.0371, grad_fn=<NegBackward0>) tensor(12291.0186, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12291.0029296875
tensor(12291.0186, grad_fn=<NegBackward0>) tensor(12291.0029, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12290.98828125
tensor(12291.0029, grad_fn=<NegBackward0>) tensor(12290.9883, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12290.9697265625
tensor(12290.9883, grad_fn=<NegBackward0>) tensor(12290.9697, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12290.9443359375
tensor(12290.9697, grad_fn=<NegBackward0>) tensor(12290.9443, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12290.9169921875
tensor(12290.9443, grad_fn=<NegBackward0>) tensor(12290.9170, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12290.90234375
tensor(12290.9170, grad_fn=<NegBackward0>) tensor(12290.9023, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12290.8818359375
tensor(12290.9023, grad_fn=<NegBackward0>) tensor(12290.8818, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12290.61328125
tensor(12290.8818, grad_fn=<NegBackward0>) tensor(12290.6133, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12289.4794921875
tensor(12290.6133, grad_fn=<NegBackward0>) tensor(12289.4795, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12289.37890625
tensor(12289.4795, grad_fn=<NegBackward0>) tensor(12289.3789, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12289.3466796875
tensor(12289.3789, grad_fn=<NegBackward0>) tensor(12289.3467, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12289.328125
tensor(12289.3467, grad_fn=<NegBackward0>) tensor(12289.3281, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12289.318359375
tensor(12289.3281, grad_fn=<NegBackward0>) tensor(12289.3184, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12289.3115234375
tensor(12289.3184, grad_fn=<NegBackward0>) tensor(12289.3115, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12289.306640625
tensor(12289.3115, grad_fn=<NegBackward0>) tensor(12289.3066, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12289.302734375
tensor(12289.3066, grad_fn=<NegBackward0>) tensor(12289.3027, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12289.298828125
tensor(12289.3027, grad_fn=<NegBackward0>) tensor(12289.2988, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12289.296875
tensor(12289.2988, grad_fn=<NegBackward0>) tensor(12289.2969, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12289.291015625
tensor(12289.2969, grad_fn=<NegBackward0>) tensor(12289.2910, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12289.2861328125
tensor(12289.2910, grad_fn=<NegBackward0>) tensor(12289.2861, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12289.275390625
tensor(12289.2861, grad_fn=<NegBackward0>) tensor(12289.2754, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12289.2685546875
tensor(12289.2754, grad_fn=<NegBackward0>) tensor(12289.2686, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12289.2626953125
tensor(12289.2686, grad_fn=<NegBackward0>) tensor(12289.2627, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12289.259765625
tensor(12289.2627, grad_fn=<NegBackward0>) tensor(12289.2598, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12289.25390625
tensor(12289.2598, grad_fn=<NegBackward0>) tensor(12289.2539, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12289.25390625
tensor(12289.2539, grad_fn=<NegBackward0>) tensor(12289.2539, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12289.251953125
tensor(12289.2539, grad_fn=<NegBackward0>) tensor(12289.2520, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12289.2490234375
tensor(12289.2520, grad_fn=<NegBackward0>) tensor(12289.2490, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12289.248046875
tensor(12289.2490, grad_fn=<NegBackward0>) tensor(12289.2480, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12289.24609375
tensor(12289.2480, grad_fn=<NegBackward0>) tensor(12289.2461, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12289.244140625
tensor(12289.2461, grad_fn=<NegBackward0>) tensor(12289.2441, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12289.2451171875
tensor(12289.2441, grad_fn=<NegBackward0>) tensor(12289.2451, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12289.244140625
tensor(12289.2441, grad_fn=<NegBackward0>) tensor(12289.2441, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12289.2421875
tensor(12289.2441, grad_fn=<NegBackward0>) tensor(12289.2422, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12289.2421875
tensor(12289.2422, grad_fn=<NegBackward0>) tensor(12289.2422, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12289.2421875
tensor(12289.2422, grad_fn=<NegBackward0>) tensor(12289.2422, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12289.2412109375
tensor(12289.2422, grad_fn=<NegBackward0>) tensor(12289.2412, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12289.2392578125
tensor(12289.2412, grad_fn=<NegBackward0>) tensor(12289.2393, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12289.23828125
tensor(12289.2393, grad_fn=<NegBackward0>) tensor(12289.2383, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12289.23828125
tensor(12289.2383, grad_fn=<NegBackward0>) tensor(12289.2383, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12289.23828125
tensor(12289.2383, grad_fn=<NegBackward0>) tensor(12289.2383, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12289.2373046875
tensor(12289.2383, grad_fn=<NegBackward0>) tensor(12289.2373, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12289.23828125
tensor(12289.2373, grad_fn=<NegBackward0>) tensor(12289.2383, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12289.2373046875
tensor(12289.2373, grad_fn=<NegBackward0>) tensor(12289.2373, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12289.236328125
tensor(12289.2373, grad_fn=<NegBackward0>) tensor(12289.2363, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12289.23828125
tensor(12289.2363, grad_fn=<NegBackward0>) tensor(12289.2383, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12289.2353515625
tensor(12289.2363, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12289.2373046875
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2373, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12289.2373046875
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2373, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -12289.2353515625
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12289.236328125
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2363, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12289.236328125
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2363, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12289.2353515625
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12289.2353515625
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12289.236328125
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2363, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12289.2470703125
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2471, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12289.2353515625
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12289.236328125
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2363, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12289.234375
tensor(12289.2354, grad_fn=<NegBackward0>) tensor(12289.2344, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12289.2353515625
tensor(12289.2344, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12289.2421875
tensor(12289.2344, grad_fn=<NegBackward0>) tensor(12289.2422, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12289.2353515625
tensor(12289.2344, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -12289.240234375
tensor(12289.2344, grad_fn=<NegBackward0>) tensor(12289.2402, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -12289.2353515625
tensor(12289.2344, grad_fn=<NegBackward0>) tensor(12289.2354, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[1.0000e+00, 2.2128e-06],
        [1.4914e-03, 9.9851e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9894, 0.0106], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1928],
         [0.5459, 0.2661]],

        [[0.7217, 0.2500],
         [0.6944, 0.7077]],

        [[0.5577, 0.2420],
         [0.5990, 0.5818]],

        [[0.6377, 0.1438],
         [0.5837, 0.5307]],

        [[0.6388, 0.0857],
         [0.6734, 0.5938]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0010898893779624022
Average Adjusted Rand Index: 0.00013069047420880025
[0.0024810091574889704, 0.0010898893779624022] [0.005399824978035971, 0.00013069047420880025] [12289.9638671875, 12289.2353515625]
-------------------------------------
This iteration is 19
True Objective function: Loss = -11855.938516362165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22394.619140625
inf tensor(22394.6191, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12374.28515625
tensor(22394.6191, grad_fn=<NegBackward0>) tensor(12374.2852, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12362.95703125
tensor(12374.2852, grad_fn=<NegBackward0>) tensor(12362.9570, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12362.5517578125
tensor(12362.9570, grad_fn=<NegBackward0>) tensor(12362.5518, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.400390625
tensor(12362.5518, grad_fn=<NegBackward0>) tensor(12362.4004, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.3251953125
tensor(12362.4004, grad_fn=<NegBackward0>) tensor(12362.3252, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.279296875
tensor(12362.3252, grad_fn=<NegBackward0>) tensor(12362.2793, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.2373046875
tensor(12362.2793, grad_fn=<NegBackward0>) tensor(12362.2373, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.1943359375
tensor(12362.2373, grad_fn=<NegBackward0>) tensor(12362.1943, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.1484375
tensor(12362.1943, grad_fn=<NegBackward0>) tensor(12362.1484, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.0966796875
tensor(12362.1484, grad_fn=<NegBackward0>) tensor(12362.0967, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.048828125
tensor(12362.0967, grad_fn=<NegBackward0>) tensor(12362.0488, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.005859375
tensor(12362.0488, grad_fn=<NegBackward0>) tensor(12362.0059, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12361.9765625
tensor(12362.0059, grad_fn=<NegBackward0>) tensor(12361.9766, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12361.955078125
tensor(12361.9766, grad_fn=<NegBackward0>) tensor(12361.9551, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12361.935546875
tensor(12361.9551, grad_fn=<NegBackward0>) tensor(12361.9355, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12361.9130859375
tensor(12361.9355, grad_fn=<NegBackward0>) tensor(12361.9131, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12361.892578125
tensor(12361.9131, grad_fn=<NegBackward0>) tensor(12361.8926, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12361.8701171875
tensor(12361.8926, grad_fn=<NegBackward0>) tensor(12361.8701, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12361.8525390625
tensor(12361.8701, grad_fn=<NegBackward0>) tensor(12361.8525, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12361.8388671875
tensor(12361.8525, grad_fn=<NegBackward0>) tensor(12361.8389, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12361.8310546875
tensor(12361.8389, grad_fn=<NegBackward0>) tensor(12361.8311, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12361.8349609375
tensor(12361.8311, grad_fn=<NegBackward0>) tensor(12361.8350, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -12361.8193359375
tensor(12361.8311, grad_fn=<NegBackward0>) tensor(12361.8193, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12361.81640625
tensor(12361.8193, grad_fn=<NegBackward0>) tensor(12361.8164, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12361.8154296875
tensor(12361.8164, grad_fn=<NegBackward0>) tensor(12361.8154, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12361.814453125
tensor(12361.8154, grad_fn=<NegBackward0>) tensor(12361.8145, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12361.8115234375
tensor(12361.8145, grad_fn=<NegBackward0>) tensor(12361.8115, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12361.810546875
tensor(12361.8115, grad_fn=<NegBackward0>) tensor(12361.8105, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12361.810546875
tensor(12361.8105, grad_fn=<NegBackward0>) tensor(12361.8105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12361.8095703125
tensor(12361.8105, grad_fn=<NegBackward0>) tensor(12361.8096, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12361.8076171875
tensor(12361.8096, grad_fn=<NegBackward0>) tensor(12361.8076, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12361.84375
tensor(12361.8076, grad_fn=<NegBackward0>) tensor(12361.8438, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12361.7998046875
tensor(12361.8076, grad_fn=<NegBackward0>) tensor(12361.7998, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12361.79296875
tensor(12361.7998, grad_fn=<NegBackward0>) tensor(12361.7930, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12361.779296875
tensor(12361.7930, grad_fn=<NegBackward0>) tensor(12361.7793, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12361.7509765625
tensor(12361.7793, grad_fn=<NegBackward0>) tensor(12361.7510, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12361.7578125
tensor(12361.7510, grad_fn=<NegBackward0>) tensor(12361.7578, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12361.7236328125
tensor(12361.7510, grad_fn=<NegBackward0>) tensor(12361.7236, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12361.7177734375
tensor(12361.7236, grad_fn=<NegBackward0>) tensor(12361.7178, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12361.712890625
tensor(12361.7178, grad_fn=<NegBackward0>) tensor(12361.7129, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12361.708984375
tensor(12361.7129, grad_fn=<NegBackward0>) tensor(12361.7090, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12361.708984375
tensor(12361.7090, grad_fn=<NegBackward0>) tensor(12361.7090, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12361.70703125
tensor(12361.7090, grad_fn=<NegBackward0>) tensor(12361.7070, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12361.70703125
tensor(12361.7070, grad_fn=<NegBackward0>) tensor(12361.7070, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12361.7041015625
tensor(12361.7070, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12361.7060546875
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7061, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12361.7080078125
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7080, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -12361.7060546875
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7061, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -12361.705078125
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7051, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -12361.7041015625
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12361.7041015625
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12361.7119140625
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7119, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12361.7041015625
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12361.7041015625
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12361.703125
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7031, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12361.7021484375
tensor(12361.7031, grad_fn=<NegBackward0>) tensor(12361.7021, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12361.705078125
tensor(12361.7021, grad_fn=<NegBackward0>) tensor(12361.7051, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12361.701171875
tensor(12361.7021, grad_fn=<NegBackward0>) tensor(12361.7012, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12361.7080078125
tensor(12361.7012, grad_fn=<NegBackward0>) tensor(12361.7080, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12361.69921875
tensor(12361.7012, grad_fn=<NegBackward0>) tensor(12361.6992, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12361.697265625
tensor(12361.6992, grad_fn=<NegBackward0>) tensor(12361.6973, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12361.7060546875
tensor(12361.6973, grad_fn=<NegBackward0>) tensor(12361.7061, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12361.693359375
tensor(12361.6973, grad_fn=<NegBackward0>) tensor(12361.6934, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12361.6904296875
tensor(12361.6934, grad_fn=<NegBackward0>) tensor(12361.6904, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12361.6787109375
tensor(12361.6904, grad_fn=<NegBackward0>) tensor(12361.6787, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12361.6767578125
tensor(12361.6787, grad_fn=<NegBackward0>) tensor(12361.6768, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12361.6728515625
tensor(12361.6768, grad_fn=<NegBackward0>) tensor(12361.6729, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12361.671875
tensor(12361.6729, grad_fn=<NegBackward0>) tensor(12361.6719, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12361.6689453125
tensor(12361.6719, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12361.669921875
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12361.6689453125
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12361.669921875
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12361.66796875
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12361.6689453125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12361.6728515625
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6729, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12361.79296875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.7930, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -12361.6689453125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -12361.6748046875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6748, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.4786, 0.5214],
        [0.4569, 0.5431]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0093, 0.9907], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.1176],
         [0.6987, 0.1977]],

        [[0.5674, 0.1993],
         [0.5774, 0.6943]],

        [[0.7174, 0.2082],
         [0.6143, 0.6939]],

        [[0.5485, 0.1999],
         [0.5084, 0.5632]],

        [[0.6647, 0.1936],
         [0.6628, 0.7098]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00014229232547417533
Average Adjusted Rand Index: 8.260569873033385e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21975.2109375
inf tensor(21975.2109, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12362.974609375
tensor(21975.2109, grad_fn=<NegBackward0>) tensor(12362.9746, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12362.685546875
tensor(12362.9746, grad_fn=<NegBackward0>) tensor(12362.6855, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12362.6025390625
tensor(12362.6855, grad_fn=<NegBackward0>) tensor(12362.6025, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.548828125
tensor(12362.6025, grad_fn=<NegBackward0>) tensor(12362.5488, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.5029296875
tensor(12362.5488, grad_fn=<NegBackward0>) tensor(12362.5029, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.462890625
tensor(12362.5029, grad_fn=<NegBackward0>) tensor(12362.4629, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.4208984375
tensor(12362.4629, grad_fn=<NegBackward0>) tensor(12362.4209, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.37890625
tensor(12362.4209, grad_fn=<NegBackward0>) tensor(12362.3789, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.32421875
tensor(12362.3789, grad_fn=<NegBackward0>) tensor(12362.3242, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.25390625
tensor(12362.3242, grad_fn=<NegBackward0>) tensor(12362.2539, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.16796875
tensor(12362.2539, grad_fn=<NegBackward0>) tensor(12362.1680, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.0625
tensor(12362.1680, grad_fn=<NegBackward0>) tensor(12362.0625, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12361.9755859375
tensor(12362.0625, grad_fn=<NegBackward0>) tensor(12361.9756, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12361.921875
tensor(12361.9756, grad_fn=<NegBackward0>) tensor(12361.9219, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12361.8828125
tensor(12361.9219, grad_fn=<NegBackward0>) tensor(12361.8828, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12361.8466796875
tensor(12361.8828, grad_fn=<NegBackward0>) tensor(12361.8467, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12361.814453125
tensor(12361.8467, grad_fn=<NegBackward0>) tensor(12361.8145, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12361.78515625
tensor(12361.8145, grad_fn=<NegBackward0>) tensor(12361.7852, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12361.763671875
tensor(12361.7852, grad_fn=<NegBackward0>) tensor(12361.7637, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12361.7470703125
tensor(12361.7637, grad_fn=<NegBackward0>) tensor(12361.7471, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12361.7333984375
tensor(12361.7471, grad_fn=<NegBackward0>) tensor(12361.7334, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12361.724609375
tensor(12361.7334, grad_fn=<NegBackward0>) tensor(12361.7246, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12361.7177734375
tensor(12361.7246, grad_fn=<NegBackward0>) tensor(12361.7178, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12361.712890625
tensor(12361.7178, grad_fn=<NegBackward0>) tensor(12361.7129, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12361.7099609375
tensor(12361.7129, grad_fn=<NegBackward0>) tensor(12361.7100, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12361.7080078125
tensor(12361.7100, grad_fn=<NegBackward0>) tensor(12361.7080, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12361.7060546875
tensor(12361.7080, grad_fn=<NegBackward0>) tensor(12361.7061, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12361.7041015625
tensor(12361.7061, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12361.7041015625
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7041, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12361.701171875
tensor(12361.7041, grad_fn=<NegBackward0>) tensor(12361.7012, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12361.703125
tensor(12361.7012, grad_fn=<NegBackward0>) tensor(12361.7031, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -12361.7001953125
tensor(12361.7012, grad_fn=<NegBackward0>) tensor(12361.7002, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12361.7021484375
tensor(12361.7002, grad_fn=<NegBackward0>) tensor(12361.7021, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12361.69921875
tensor(12361.7002, grad_fn=<NegBackward0>) tensor(12361.6992, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12361.701171875
tensor(12361.6992, grad_fn=<NegBackward0>) tensor(12361.7012, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12361.69921875
tensor(12361.6992, grad_fn=<NegBackward0>) tensor(12361.6992, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12361.69921875
tensor(12361.6992, grad_fn=<NegBackward0>) tensor(12361.6992, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12361.6982421875
tensor(12361.6992, grad_fn=<NegBackward0>) tensor(12361.6982, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12361.697265625
tensor(12361.6982, grad_fn=<NegBackward0>) tensor(12361.6973, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12361.697265625
tensor(12361.6973, grad_fn=<NegBackward0>) tensor(12361.6973, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12361.6943359375
tensor(12361.6973, grad_fn=<NegBackward0>) tensor(12361.6943, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12361.693359375
tensor(12361.6943, grad_fn=<NegBackward0>) tensor(12361.6934, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12361.6923828125
tensor(12361.6934, grad_fn=<NegBackward0>) tensor(12361.6924, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12361.689453125
tensor(12361.6924, grad_fn=<NegBackward0>) tensor(12361.6895, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12361.6904296875
tensor(12361.6895, grad_fn=<NegBackward0>) tensor(12361.6904, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12361.6845703125
tensor(12361.6895, grad_fn=<NegBackward0>) tensor(12361.6846, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12361.6826171875
tensor(12361.6846, grad_fn=<NegBackward0>) tensor(12361.6826, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12361.6787109375
tensor(12361.6826, grad_fn=<NegBackward0>) tensor(12361.6787, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12361.677734375
tensor(12361.6787, grad_fn=<NegBackward0>) tensor(12361.6777, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12361.67578125
tensor(12361.6777, grad_fn=<NegBackward0>) tensor(12361.6758, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12361.673828125
tensor(12361.6758, grad_fn=<NegBackward0>) tensor(12361.6738, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12361.681640625
tensor(12361.6738, grad_fn=<NegBackward0>) tensor(12361.6816, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12361.6708984375
tensor(12361.6738, grad_fn=<NegBackward0>) tensor(12361.6709, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12361.6708984375
tensor(12361.6709, grad_fn=<NegBackward0>) tensor(12361.6709, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12361.6708984375
tensor(12361.6709, grad_fn=<NegBackward0>) tensor(12361.6709, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12361.669921875
tensor(12361.6709, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12361.669921875
tensor(12361.6699, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12361.6689453125
tensor(12361.6699, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12361.68359375
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6836, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12361.6689453125
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12361.6689453125
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12361.669921875
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12361.669921875
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12361.66796875
tensor(12361.6689, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12361.6689453125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12361.6689453125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6689, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12361.6708984375
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6709, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12361.669921875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6699, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12361.6767578125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6768, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12361.720703125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.7207, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12361.6748046875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6748, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12361.6748046875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6748, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12361.66796875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6680, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12361.7197265625
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.7197, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12361.67578125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6758, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12361.6953125
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6953, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12361.6943359375
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6943, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -12361.6748046875
tensor(12361.6680, grad_fn=<NegBackward0>) tensor(12361.6748, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.4780, 0.5220],
        [0.4540, 0.5460]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0093, 0.9907], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2025, 0.1177],
         [0.7088, 0.1978]],

        [[0.5302, 0.1994],
         [0.5298, 0.6607]],

        [[0.6387, 0.2083],
         [0.7226, 0.7228]],

        [[0.5298, 0.2001],
         [0.6326, 0.5416]],

        [[0.6570, 0.1939],
         [0.5762, 0.7037]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007535694595274068
Average Adjusted Rand Index: -0.001219209991574565
[0.00014229232547417533, -0.0007535694595274068] [8.260569873033385e-05, -0.001219209991574565] [12361.6748046875, 12361.6748046875]
-------------------------------------
This iteration is 20
True Objective function: Loss = -12036.627784772678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21180.919921875
inf tensor(21180.9199, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12521.865234375
tensor(21180.9199, grad_fn=<NegBackward0>) tensor(12521.8652, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12521.357421875
tensor(12521.8652, grad_fn=<NegBackward0>) tensor(12521.3574, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12521.224609375
tensor(12521.3574, grad_fn=<NegBackward0>) tensor(12521.2246, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12521.1484375
tensor(12521.2246, grad_fn=<NegBackward0>) tensor(12521.1484, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12521.09765625
tensor(12521.1484, grad_fn=<NegBackward0>) tensor(12521.0977, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12521.0576171875
tensor(12521.0977, grad_fn=<NegBackward0>) tensor(12521.0576, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12521.0302734375
tensor(12521.0576, grad_fn=<NegBackward0>) tensor(12521.0303, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12521.0078125
tensor(12521.0303, grad_fn=<NegBackward0>) tensor(12521.0078, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12520.98828125
tensor(12521.0078, grad_fn=<NegBackward0>) tensor(12520.9883, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12520.97265625
tensor(12520.9883, grad_fn=<NegBackward0>) tensor(12520.9727, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12520.9599609375
tensor(12520.9727, grad_fn=<NegBackward0>) tensor(12520.9600, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12520.9462890625
tensor(12520.9600, grad_fn=<NegBackward0>) tensor(12520.9463, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12520.9375
tensor(12520.9463, grad_fn=<NegBackward0>) tensor(12520.9375, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12520.9306640625
tensor(12520.9375, grad_fn=<NegBackward0>) tensor(12520.9307, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12520.9267578125
tensor(12520.9307, grad_fn=<NegBackward0>) tensor(12520.9268, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12520.921875
tensor(12520.9268, grad_fn=<NegBackward0>) tensor(12520.9219, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12520.9189453125
tensor(12520.9219, grad_fn=<NegBackward0>) tensor(12520.9189, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12520.9169921875
tensor(12520.9189, grad_fn=<NegBackward0>) tensor(12520.9170, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12520.9140625
tensor(12520.9170, grad_fn=<NegBackward0>) tensor(12520.9141, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12520.912109375
tensor(12520.9141, grad_fn=<NegBackward0>) tensor(12520.9121, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12520.912109375
tensor(12520.9121, grad_fn=<NegBackward0>) tensor(12520.9121, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12520.908203125
tensor(12520.9121, grad_fn=<NegBackward0>) tensor(12520.9082, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12520.90625
tensor(12520.9082, grad_fn=<NegBackward0>) tensor(12520.9062, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12520.90625
tensor(12520.9062, grad_fn=<NegBackward0>) tensor(12520.9062, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12520.9052734375
tensor(12520.9062, grad_fn=<NegBackward0>) tensor(12520.9053, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12520.90234375
tensor(12520.9053, grad_fn=<NegBackward0>) tensor(12520.9023, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12520.8984375
tensor(12520.9023, grad_fn=<NegBackward0>) tensor(12520.8984, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12520.8974609375
tensor(12520.8984, grad_fn=<NegBackward0>) tensor(12520.8975, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12520.896484375
tensor(12520.8975, grad_fn=<NegBackward0>) tensor(12520.8965, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12520.896484375
tensor(12520.8965, grad_fn=<NegBackward0>) tensor(12520.8965, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12520.892578125
tensor(12520.8965, grad_fn=<NegBackward0>) tensor(12520.8926, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12520.8916015625
tensor(12520.8926, grad_fn=<NegBackward0>) tensor(12520.8916, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12520.890625
tensor(12520.8916, grad_fn=<NegBackward0>) tensor(12520.8906, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12520.88671875
tensor(12520.8906, grad_fn=<NegBackward0>) tensor(12520.8867, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12520.88671875
tensor(12520.8867, grad_fn=<NegBackward0>) tensor(12520.8867, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12520.8837890625
tensor(12520.8867, grad_fn=<NegBackward0>) tensor(12520.8838, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12520.8828125
tensor(12520.8838, grad_fn=<NegBackward0>) tensor(12520.8828, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12520.8798828125
tensor(12520.8828, grad_fn=<NegBackward0>) tensor(12520.8799, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12520.8818359375
tensor(12520.8799, grad_fn=<NegBackward0>) tensor(12520.8818, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12520.876953125
tensor(12520.8799, grad_fn=<NegBackward0>) tensor(12520.8770, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12520.875
tensor(12520.8770, grad_fn=<NegBackward0>) tensor(12520.8750, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12520.8720703125
tensor(12520.8750, grad_fn=<NegBackward0>) tensor(12520.8721, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12520.87109375
tensor(12520.8721, grad_fn=<NegBackward0>) tensor(12520.8711, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12520.8701171875
tensor(12520.8711, grad_fn=<NegBackward0>) tensor(12520.8701, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12520.8662109375
tensor(12520.8701, grad_fn=<NegBackward0>) tensor(12520.8662, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12520.86328125
tensor(12520.8662, grad_fn=<NegBackward0>) tensor(12520.8633, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12520.8603515625
tensor(12520.8633, grad_fn=<NegBackward0>) tensor(12520.8604, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12520.859375
tensor(12520.8604, grad_fn=<NegBackward0>) tensor(12520.8594, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12520.8583984375
tensor(12520.8594, grad_fn=<NegBackward0>) tensor(12520.8584, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12520.8564453125
tensor(12520.8584, grad_fn=<NegBackward0>) tensor(12520.8564, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12520.859375
tensor(12520.8564, grad_fn=<NegBackward0>) tensor(12520.8594, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12520.8515625
tensor(12520.8564, grad_fn=<NegBackward0>) tensor(12520.8516, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12520.8505859375
tensor(12520.8516, grad_fn=<NegBackward0>) tensor(12520.8506, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12520.8515625
tensor(12520.8506, grad_fn=<NegBackward0>) tensor(12520.8516, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12520.849609375
tensor(12520.8506, grad_fn=<NegBackward0>) tensor(12520.8496, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12520.849609375
tensor(12520.8496, grad_fn=<NegBackward0>) tensor(12520.8496, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12520.84765625
tensor(12520.8496, grad_fn=<NegBackward0>) tensor(12520.8477, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12520.8466796875
tensor(12520.8477, grad_fn=<NegBackward0>) tensor(12520.8467, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12520.8447265625
tensor(12520.8467, grad_fn=<NegBackward0>) tensor(12520.8447, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12520.8623046875
tensor(12520.8447, grad_fn=<NegBackward0>) tensor(12520.8623, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12520.84375
tensor(12520.8447, grad_fn=<NegBackward0>) tensor(12520.8438, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12520.8515625
tensor(12520.8438, grad_fn=<NegBackward0>) tensor(12520.8516, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12520.8427734375
tensor(12520.8438, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12520.8427734375
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12520.8427734375
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12520.841796875
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8418, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12520.841796875
tensor(12520.8418, grad_fn=<NegBackward0>) tensor(12520.8418, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12520.8466796875
tensor(12520.8418, grad_fn=<NegBackward0>) tensor(12520.8467, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12520.8408203125
tensor(12520.8418, grad_fn=<NegBackward0>) tensor(12520.8408, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12520.8408203125
tensor(12520.8408, grad_fn=<NegBackward0>) tensor(12520.8408, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12520.83984375
tensor(12520.8408, grad_fn=<NegBackward0>) tensor(12520.8398, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12520.8388671875
tensor(12520.8398, grad_fn=<NegBackward0>) tensor(12520.8389, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12520.88671875
tensor(12520.8389, grad_fn=<NegBackward0>) tensor(12520.8867, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12520.8388671875
tensor(12520.8389, grad_fn=<NegBackward0>) tensor(12520.8389, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12520.861328125
tensor(12520.8389, grad_fn=<NegBackward0>) tensor(12520.8613, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12520.95703125
tensor(12520.8389, grad_fn=<NegBackward0>) tensor(12520.9570, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12520.8388671875
tensor(12520.8389, grad_fn=<NegBackward0>) tensor(12520.8389, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12520.83203125
tensor(12520.8389, grad_fn=<NegBackward0>) tensor(12520.8320, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12520.833984375
tensor(12520.8320, grad_fn=<NegBackward0>) tensor(12520.8340, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12520.8310546875
tensor(12520.8320, grad_fn=<NegBackward0>) tensor(12520.8311, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12520.8408203125
tensor(12520.8311, grad_fn=<NegBackward0>) tensor(12520.8408, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12520.8701171875
tensor(12520.8311, grad_fn=<NegBackward0>) tensor(12520.8701, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12520.8603515625
tensor(12520.8311, grad_fn=<NegBackward0>) tensor(12520.8604, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12520.8291015625
tensor(12520.8311, grad_fn=<NegBackward0>) tensor(12520.8291, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12520.837890625
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8379, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12520.83203125
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8320, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12520.830078125
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8301, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12520.8310546875
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8311, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -12520.83203125
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8320, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.8984, 0.1016],
        [0.5784, 0.4216]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6757, 0.3243], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.2059],
         [0.5388, 0.2103]],

        [[0.7031, 0.2095],
         [0.7153, 0.5437]],

        [[0.5563, 0.2013],
         [0.5846, 0.5111]],

        [[0.5814, 0.2088],
         [0.6856, 0.5405]],

        [[0.6774, 0.2043],
         [0.6013, 0.7190]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21000.693359375
inf tensor(21000.6934, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12521.4658203125
tensor(21000.6934, grad_fn=<NegBackward0>) tensor(12521.4658, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12521.083984375
tensor(12521.4658, grad_fn=<NegBackward0>) tensor(12521.0840, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12521.037109375
tensor(12521.0840, grad_fn=<NegBackward0>) tensor(12521.0371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12521.01171875
tensor(12521.0371, grad_fn=<NegBackward0>) tensor(12521.0117, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12520.9921875
tensor(12521.0117, grad_fn=<NegBackward0>) tensor(12520.9922, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12520.9755859375
tensor(12520.9922, grad_fn=<NegBackward0>) tensor(12520.9756, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12520.962890625
tensor(12520.9756, grad_fn=<NegBackward0>) tensor(12520.9629, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12520.94921875
tensor(12520.9629, grad_fn=<NegBackward0>) tensor(12520.9492, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12520.9384765625
tensor(12520.9492, grad_fn=<NegBackward0>) tensor(12520.9385, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12520.9287109375
tensor(12520.9385, grad_fn=<NegBackward0>) tensor(12520.9287, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12520.921875
tensor(12520.9287, grad_fn=<NegBackward0>) tensor(12520.9219, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12520.91796875
tensor(12520.9219, grad_fn=<NegBackward0>) tensor(12520.9180, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12520.916015625
tensor(12520.9180, grad_fn=<NegBackward0>) tensor(12520.9160, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12520.9140625
tensor(12520.9160, grad_fn=<NegBackward0>) tensor(12520.9141, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12520.9111328125
tensor(12520.9141, grad_fn=<NegBackward0>) tensor(12520.9111, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12520.91015625
tensor(12520.9111, grad_fn=<NegBackward0>) tensor(12520.9102, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12520.91015625
tensor(12520.9102, grad_fn=<NegBackward0>) tensor(12520.9102, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12520.9072265625
tensor(12520.9102, grad_fn=<NegBackward0>) tensor(12520.9072, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12520.9052734375
tensor(12520.9072, grad_fn=<NegBackward0>) tensor(12520.9053, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12520.90234375
tensor(12520.9053, grad_fn=<NegBackward0>) tensor(12520.9023, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12520.9013671875
tensor(12520.9023, grad_fn=<NegBackward0>) tensor(12520.9014, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12520.8984375
tensor(12520.9014, grad_fn=<NegBackward0>) tensor(12520.8984, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12520.8974609375
tensor(12520.8984, grad_fn=<NegBackward0>) tensor(12520.8975, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12520.8955078125
tensor(12520.8975, grad_fn=<NegBackward0>) tensor(12520.8955, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12520.8935546875
tensor(12520.8955, grad_fn=<NegBackward0>) tensor(12520.8936, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12520.892578125
tensor(12520.8936, grad_fn=<NegBackward0>) tensor(12520.8926, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12520.8896484375
tensor(12520.8926, grad_fn=<NegBackward0>) tensor(12520.8896, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12520.8876953125
tensor(12520.8896, grad_fn=<NegBackward0>) tensor(12520.8877, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12520.8857421875
tensor(12520.8877, grad_fn=<NegBackward0>) tensor(12520.8857, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12520.8828125
tensor(12520.8857, grad_fn=<NegBackward0>) tensor(12520.8828, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12520.880859375
tensor(12520.8828, grad_fn=<NegBackward0>) tensor(12520.8809, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12520.8779296875
tensor(12520.8809, grad_fn=<NegBackward0>) tensor(12520.8779, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12520.875
tensor(12520.8779, grad_fn=<NegBackward0>) tensor(12520.8750, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12520.873046875
tensor(12520.8750, grad_fn=<NegBackward0>) tensor(12520.8730, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12520.869140625
tensor(12520.8730, grad_fn=<NegBackward0>) tensor(12520.8691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12520.8671875
tensor(12520.8691, grad_fn=<NegBackward0>) tensor(12520.8672, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12520.8642578125
tensor(12520.8672, grad_fn=<NegBackward0>) tensor(12520.8643, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12520.8603515625
tensor(12520.8643, grad_fn=<NegBackward0>) tensor(12520.8604, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12520.859375
tensor(12520.8604, grad_fn=<NegBackward0>) tensor(12520.8594, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12520.8564453125
tensor(12520.8594, grad_fn=<NegBackward0>) tensor(12520.8564, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12520.85546875
tensor(12520.8564, grad_fn=<NegBackward0>) tensor(12520.8555, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12520.853515625
tensor(12520.8555, grad_fn=<NegBackward0>) tensor(12520.8535, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12520.849609375
tensor(12520.8535, grad_fn=<NegBackward0>) tensor(12520.8496, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12520.849609375
tensor(12520.8496, grad_fn=<NegBackward0>) tensor(12520.8496, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12520.849609375
tensor(12520.8496, grad_fn=<NegBackward0>) tensor(12520.8496, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12520.8466796875
tensor(12520.8496, grad_fn=<NegBackward0>) tensor(12520.8467, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12520.8466796875
tensor(12520.8467, grad_fn=<NegBackward0>) tensor(12520.8467, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12520.8447265625
tensor(12520.8467, grad_fn=<NegBackward0>) tensor(12520.8447, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12520.8447265625
tensor(12520.8447, grad_fn=<NegBackward0>) tensor(12520.8447, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12520.8447265625
tensor(12520.8447, grad_fn=<NegBackward0>) tensor(12520.8447, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12520.84375
tensor(12520.8447, grad_fn=<NegBackward0>) tensor(12520.8438, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12520.8427734375
tensor(12520.8438, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12520.8427734375
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12520.8427734375
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12520.8427734375
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12520.841796875
tensor(12520.8428, grad_fn=<NegBackward0>) tensor(12520.8418, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12520.8427734375
tensor(12520.8418, grad_fn=<NegBackward0>) tensor(12520.8428, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12520.8408203125
tensor(12520.8418, grad_fn=<NegBackward0>) tensor(12520.8408, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12520.8564453125
tensor(12520.8408, grad_fn=<NegBackward0>) tensor(12520.8564, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12520.837890625
tensor(12520.8408, grad_fn=<NegBackward0>) tensor(12520.8379, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12520.8388671875
tensor(12520.8379, grad_fn=<NegBackward0>) tensor(12520.8389, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12520.833984375
tensor(12520.8379, grad_fn=<NegBackward0>) tensor(12520.8340, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12520.833984375
tensor(12520.8340, grad_fn=<NegBackward0>) tensor(12520.8340, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12520.830078125
tensor(12520.8340, grad_fn=<NegBackward0>) tensor(12520.8301, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12520.830078125
tensor(12520.8301, grad_fn=<NegBackward0>) tensor(12520.8301, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12520.8291015625
tensor(12520.8301, grad_fn=<NegBackward0>) tensor(12520.8291, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12520.8291015625
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8291, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12520.828125
tensor(12520.8291, grad_fn=<NegBackward0>) tensor(12520.8281, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12520.82421875
tensor(12520.8281, grad_fn=<NegBackward0>) tensor(12520.8242, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12520.822265625
tensor(12520.8242, grad_fn=<NegBackward0>) tensor(12520.8223, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12520.8193359375
tensor(12520.8223, grad_fn=<NegBackward0>) tensor(12520.8193, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12520.826171875
tensor(12520.8193, grad_fn=<NegBackward0>) tensor(12520.8262, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12520.8125
tensor(12520.8193, grad_fn=<NegBackward0>) tensor(12520.8125, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12520.8037109375
tensor(12520.8125, grad_fn=<NegBackward0>) tensor(12520.8037, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12520.7890625
tensor(12520.8037, grad_fn=<NegBackward0>) tensor(12520.7891, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12520.763671875
tensor(12520.7891, grad_fn=<NegBackward0>) tensor(12520.7637, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12520.7421875
tensor(12520.7637, grad_fn=<NegBackward0>) tensor(12520.7422, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12520.7265625
tensor(12520.7422, grad_fn=<NegBackward0>) tensor(12520.7266, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12520.71484375
tensor(12520.7266, grad_fn=<NegBackward0>) tensor(12520.7148, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12520.6982421875
tensor(12520.7148, grad_fn=<NegBackward0>) tensor(12520.6982, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12520.666015625
tensor(12520.6982, grad_fn=<NegBackward0>) tensor(12520.6660, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12519.349609375
tensor(12520.6660, grad_fn=<NegBackward0>) tensor(12519.3496, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12519.294921875
tensor(12519.3496, grad_fn=<NegBackward0>) tensor(12519.2949, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12519.2841796875
tensor(12519.2949, grad_fn=<NegBackward0>) tensor(12519.2842, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12519.2822265625
tensor(12519.2842, grad_fn=<NegBackward0>) tensor(12519.2822, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12519.2744140625
tensor(12519.2822, grad_fn=<NegBackward0>) tensor(12519.2744, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12519.2724609375
tensor(12519.2744, grad_fn=<NegBackward0>) tensor(12519.2725, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12519.349609375
tensor(12519.2725, grad_fn=<NegBackward0>) tensor(12519.3496, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12519.271484375
tensor(12519.2725, grad_fn=<NegBackward0>) tensor(12519.2715, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12519.28125
tensor(12519.2715, grad_fn=<NegBackward0>) tensor(12519.2812, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12519.2685546875
tensor(12519.2715, grad_fn=<NegBackward0>) tensor(12519.2686, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12519.2724609375
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2725, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12519.314453125
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.3145, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -12519.2685546875
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2686, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12519.2685546875
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2686, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12519.2783203125
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2783, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12519.271484375
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2715, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12519.296875
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2969, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -12519.2646484375
tensor(12519.2686, grad_fn=<NegBackward0>) tensor(12519.2646, grad_fn=<NegBackward0>)
pi: tensor([[3.2065e-05, 9.9997e-01],
        [9.9997e-01, 3.4950e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0287, 0.9713], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2081, 0.2694],
         [0.5115, 0.2019]],

        [[0.7169, 0.2020],
         [0.5832, 0.6604]],

        [[0.6785, 0.1650],
         [0.6236, 0.7010]],

        [[0.6761, 0.1384],
         [0.6580, 0.5650]],

        [[0.7198, 0.2192],
         [0.6314, 0.5324]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: -0.0014450288844263077
Average Adjusted Rand Index: -0.0015781552766075274
[0.0, -0.0014450288844263077] [0.0, -0.0015781552766075274] [12520.83203125, 12519.2646484375]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11993.732235912514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21626.86328125
inf tensor(21626.8633, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12607.5888671875
tensor(21626.8633, grad_fn=<NegBackward0>) tensor(12607.5889, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12607.3212890625
tensor(12607.5889, grad_fn=<NegBackward0>) tensor(12607.3213, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12607.14453125
tensor(12607.3213, grad_fn=<NegBackward0>) tensor(12607.1445, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12606.8154296875
tensor(12607.1445, grad_fn=<NegBackward0>) tensor(12606.8154, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12606.529296875
tensor(12606.8154, grad_fn=<NegBackward0>) tensor(12606.5293, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12606.3427734375
tensor(12606.5293, grad_fn=<NegBackward0>) tensor(12606.3428, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12606.1640625
tensor(12606.3428, grad_fn=<NegBackward0>) tensor(12606.1641, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12605.9326171875
tensor(12606.1641, grad_fn=<NegBackward0>) tensor(12605.9326, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12605.2412109375
tensor(12605.9326, grad_fn=<NegBackward0>) tensor(12605.2412, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12599.8056640625
tensor(12605.2412, grad_fn=<NegBackward0>) tensor(12599.8057, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12254.8525390625
tensor(12599.8057, grad_fn=<NegBackward0>) tensor(12254.8525, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12171.8486328125
tensor(12254.8525, grad_fn=<NegBackward0>) tensor(12171.8486, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12171.6923828125
tensor(12171.8486, grad_fn=<NegBackward0>) tensor(12171.6924, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12171.62890625
tensor(12171.6924, grad_fn=<NegBackward0>) tensor(12171.6289, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12171.58203125
tensor(12171.6289, grad_fn=<NegBackward0>) tensor(12171.5820, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12171.4130859375
tensor(12171.5820, grad_fn=<NegBackward0>) tensor(12171.4131, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12171.3603515625
tensor(12171.4131, grad_fn=<NegBackward0>) tensor(12171.3604, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12171.20703125
tensor(12171.3604, grad_fn=<NegBackward0>) tensor(12171.2070, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12162.826171875
tensor(12171.2070, grad_fn=<NegBackward0>) tensor(12162.8262, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12157.2373046875
tensor(12162.8262, grad_fn=<NegBackward0>) tensor(12157.2373, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12137.0400390625
tensor(12157.2373, grad_fn=<NegBackward0>) tensor(12137.0400, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12113.478515625
tensor(12137.0400, grad_fn=<NegBackward0>) tensor(12113.4785, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12097.09375
tensor(12113.4785, grad_fn=<NegBackward0>) tensor(12097.0938, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12087.28515625
tensor(12097.0938, grad_fn=<NegBackward0>) tensor(12087.2852, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12058.734375
tensor(12087.2852, grad_fn=<NegBackward0>) tensor(12058.7344, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12056.3916015625
tensor(12058.7344, grad_fn=<NegBackward0>) tensor(12056.3916, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12025.9326171875
tensor(12056.3916, grad_fn=<NegBackward0>) tensor(12025.9326, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12025.9111328125
tensor(12025.9326, grad_fn=<NegBackward0>) tensor(12025.9111, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12025.890625
tensor(12025.9111, grad_fn=<NegBackward0>) tensor(12025.8906, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12013.3388671875
tensor(12025.8906, grad_fn=<NegBackward0>) tensor(12013.3389, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12004.2177734375
tensor(12013.3389, grad_fn=<NegBackward0>) tensor(12004.2178, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11992.4755859375
tensor(12004.2178, grad_fn=<NegBackward0>) tensor(11992.4756, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11992.447265625
tensor(11992.4756, grad_fn=<NegBackward0>) tensor(11992.4473, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11982.265625
tensor(11992.4473, grad_fn=<NegBackward0>) tensor(11982.2656, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11982.2626953125
tensor(11982.2656, grad_fn=<NegBackward0>) tensor(11982.2627, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11982.259765625
tensor(11982.2627, grad_fn=<NegBackward0>) tensor(11982.2598, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11982.26171875
tensor(11982.2598, grad_fn=<NegBackward0>) tensor(11982.2617, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11982.2578125
tensor(11982.2598, grad_fn=<NegBackward0>) tensor(11982.2578, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11982.2568359375
tensor(11982.2578, grad_fn=<NegBackward0>) tensor(11982.2568, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11982.25390625
tensor(11982.2568, grad_fn=<NegBackward0>) tensor(11982.2539, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11982.2548828125
tensor(11982.2539, grad_fn=<NegBackward0>) tensor(11982.2549, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11982.2548828125
tensor(11982.2539, grad_fn=<NegBackward0>) tensor(11982.2549, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11982.251953125
tensor(11982.2539, grad_fn=<NegBackward0>) tensor(11982.2520, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11982.2607421875
tensor(11982.2520, grad_fn=<NegBackward0>) tensor(11982.2607, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11982.2529296875
tensor(11982.2520, grad_fn=<NegBackward0>) tensor(11982.2529, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11982.2529296875
tensor(11982.2520, grad_fn=<NegBackward0>) tensor(11982.2529, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11982.25
tensor(11982.2520, grad_fn=<NegBackward0>) tensor(11982.2500, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11982.2509765625
tensor(11982.2500, grad_fn=<NegBackward0>) tensor(11982.2510, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11982.2509765625
tensor(11982.2500, grad_fn=<NegBackward0>) tensor(11982.2510, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11982.251953125
tensor(11982.2500, grad_fn=<NegBackward0>) tensor(11982.2520, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11982.25
tensor(11982.2500, grad_fn=<NegBackward0>) tensor(11982.2500, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11982.259765625
tensor(11982.2500, grad_fn=<NegBackward0>) tensor(11982.2598, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11982.2490234375
tensor(11982.2500, grad_fn=<NegBackward0>) tensor(11982.2490, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11982.25390625
tensor(11982.2490, grad_fn=<NegBackward0>) tensor(11982.2539, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11982.2412109375
tensor(11982.2490, grad_fn=<NegBackward0>) tensor(11982.2412, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11982.2392578125
tensor(11982.2412, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11982.2490234375
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2490, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11982.2392578125
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11982.2392578125
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11982.244140625
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2441, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11982.23828125
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11982.244140625
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2441, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11982.2392578125
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11982.25
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2500, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11982.2431640625
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2432, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11982.23828125
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11982.23828125
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11982.2392578125
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11982.23828125
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11982.2412109375
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2412, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11982.244140625
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2441, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11982.2373046875
tensor(11982.2383, grad_fn=<NegBackward0>) tensor(11982.2373, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11982.2421875
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2422, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11982.2373046875
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2373, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11982.2373046875
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2373, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11982.2373046875
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2373, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11982.2392578125
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11982.2421875
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2422, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11982.23828125
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11982.2412109375
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2412, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -11982.2373046875
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2373, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11982.23828125
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11982.2666015625
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2666, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11982.2626953125
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2627, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -11982.240234375
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -11982.23828125
tensor(11982.2373, grad_fn=<NegBackward0>) tensor(11982.2383, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7812, 0.2188],
        [0.2787, 0.7213]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6014, 0.3986], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3078, 0.1034],
         [0.5804, 0.3184]],

        [[0.6233, 0.0911],
         [0.5400, 0.6581]],

        [[0.6842, 0.0942],
         [0.6987, 0.5610]],

        [[0.5962, 0.0960],
         [0.7149, 0.6817]],

        [[0.6924, 0.1001],
         [0.6988, 0.5166]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9840231441209258
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21386.978515625
inf tensor(21386.9785, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12607.7021484375
tensor(21386.9785, grad_fn=<NegBackward0>) tensor(12607.7021, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12607.4072265625
tensor(12607.7021, grad_fn=<NegBackward0>) tensor(12607.4072, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12607.2705078125
tensor(12607.4072, grad_fn=<NegBackward0>) tensor(12607.2705, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12607.0341796875
tensor(12607.2705, grad_fn=<NegBackward0>) tensor(12607.0342, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12606.6865234375
tensor(12607.0342, grad_fn=<NegBackward0>) tensor(12606.6865, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12606.388671875
tensor(12606.6865, grad_fn=<NegBackward0>) tensor(12606.3887, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12606.1171875
tensor(12606.3887, grad_fn=<NegBackward0>) tensor(12606.1172, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12605.5068359375
tensor(12606.1172, grad_fn=<NegBackward0>) tensor(12605.5068, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12409.8427734375
tensor(12605.5068, grad_fn=<NegBackward0>) tensor(12409.8428, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12033.560546875
tensor(12409.8428, grad_fn=<NegBackward0>) tensor(12033.5605, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12011.99609375
tensor(12033.5605, grad_fn=<NegBackward0>) tensor(12011.9961, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12005.232421875
tensor(12011.9961, grad_fn=<NegBackward0>) tensor(12005.2324, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12005.1884765625
tensor(12005.2324, grad_fn=<NegBackward0>) tensor(12005.1885, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12005.146484375
tensor(12005.1885, grad_fn=<NegBackward0>) tensor(12005.1465, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11991.6904296875
tensor(12005.1465, grad_fn=<NegBackward0>) tensor(11991.6904, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11991.666015625
tensor(11991.6904, grad_fn=<NegBackward0>) tensor(11991.6660, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11991.6552734375
tensor(11991.6660, grad_fn=<NegBackward0>) tensor(11991.6553, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11991.6474609375
tensor(11991.6553, grad_fn=<NegBackward0>) tensor(11991.6475, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11991.642578125
tensor(11991.6475, grad_fn=<NegBackward0>) tensor(11991.6426, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11991.6376953125
tensor(11991.6426, grad_fn=<NegBackward0>) tensor(11991.6377, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11991.6318359375
tensor(11991.6377, grad_fn=<NegBackward0>) tensor(11991.6318, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11991.6103515625
tensor(11991.6318, grad_fn=<NegBackward0>) tensor(11991.6104, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11991.587890625
tensor(11991.6104, grad_fn=<NegBackward0>) tensor(11991.5879, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11991.5869140625
tensor(11991.5879, grad_fn=<NegBackward0>) tensor(11991.5869, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11991.5859375
tensor(11991.5869, grad_fn=<NegBackward0>) tensor(11991.5859, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11991.58203125
tensor(11991.5859, grad_fn=<NegBackward0>) tensor(11991.5820, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11991.5810546875
tensor(11991.5820, grad_fn=<NegBackward0>) tensor(11991.5811, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11991.580078125
tensor(11991.5811, grad_fn=<NegBackward0>) tensor(11991.5801, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11991.578125
tensor(11991.5801, grad_fn=<NegBackward0>) tensor(11991.5781, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11991.5771484375
tensor(11991.5781, grad_fn=<NegBackward0>) tensor(11991.5771, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11991.5751953125
tensor(11991.5771, grad_fn=<NegBackward0>) tensor(11991.5752, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11991.5751953125
tensor(11991.5752, grad_fn=<NegBackward0>) tensor(11991.5752, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11991.57421875
tensor(11991.5752, grad_fn=<NegBackward0>) tensor(11991.5742, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11991.572265625
tensor(11991.5742, grad_fn=<NegBackward0>) tensor(11991.5723, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11991.5703125
tensor(11991.5723, grad_fn=<NegBackward0>) tensor(11991.5703, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11991.576171875
tensor(11991.5703, grad_fn=<NegBackward0>) tensor(11991.5762, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11991.5634765625
tensor(11991.5703, grad_fn=<NegBackward0>) tensor(11991.5635, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11991.5546875
tensor(11991.5635, grad_fn=<NegBackward0>) tensor(11991.5547, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11991.55078125
tensor(11991.5547, grad_fn=<NegBackward0>) tensor(11991.5508, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11991.55078125
tensor(11991.5508, grad_fn=<NegBackward0>) tensor(11991.5508, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11991.5498046875
tensor(11991.5508, grad_fn=<NegBackward0>) tensor(11991.5498, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11991.55078125
tensor(11991.5498, grad_fn=<NegBackward0>) tensor(11991.5508, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11991.55078125
tensor(11991.5498, grad_fn=<NegBackward0>) tensor(11991.5508, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11991.5498046875
tensor(11991.5498, grad_fn=<NegBackward0>) tensor(11991.5498, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11991.5498046875
tensor(11991.5498, grad_fn=<NegBackward0>) tensor(11991.5498, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11991.5478515625
tensor(11991.5498, grad_fn=<NegBackward0>) tensor(11991.5479, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11991.5498046875
tensor(11991.5479, grad_fn=<NegBackward0>) tensor(11991.5498, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11991.5478515625
tensor(11991.5479, grad_fn=<NegBackward0>) tensor(11991.5479, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11991.5478515625
tensor(11991.5479, grad_fn=<NegBackward0>) tensor(11991.5479, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11991.544921875
tensor(11991.5479, grad_fn=<NegBackward0>) tensor(11991.5449, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11991.5439453125
tensor(11991.5449, grad_fn=<NegBackward0>) tensor(11991.5439, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11991.544921875
tensor(11991.5439, grad_fn=<NegBackward0>) tensor(11991.5449, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11991.5439453125
tensor(11991.5439, grad_fn=<NegBackward0>) tensor(11991.5439, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11991.544921875
tensor(11991.5439, grad_fn=<NegBackward0>) tensor(11991.5449, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11991.5478515625
tensor(11991.5439, grad_fn=<NegBackward0>) tensor(11991.5479, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11991.54296875
tensor(11991.5439, grad_fn=<NegBackward0>) tensor(11991.5430, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11991.5419921875
tensor(11991.5430, grad_fn=<NegBackward0>) tensor(11991.5420, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11991.54296875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5430, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11991.54296875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5430, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11991.5419921875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5420, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11991.54296875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5430, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11991.54296875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5430, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11991.5439453125
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5439, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11991.5419921875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5420, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11991.5419921875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5420, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11991.5478515625
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5479, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11991.5419921875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5420, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11991.5419921875
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5420, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11991.5009765625
tensor(11991.5420, grad_fn=<NegBackward0>) tensor(11991.5010, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11982.2490234375
tensor(11991.5010, grad_fn=<NegBackward0>) tensor(11982.2490, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11982.2490234375
tensor(11982.2490, grad_fn=<NegBackward0>) tensor(11982.2490, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11982.2578125
tensor(11982.2490, grad_fn=<NegBackward0>) tensor(11982.2578, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11982.240234375
tensor(11982.2490, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11982.2412109375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2412, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11982.244140625
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2441, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11982.2509765625
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2510, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11982.240234375
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11982.2392578125
tensor(11982.2402, grad_fn=<NegBackward0>) tensor(11982.2393, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11982.240234375
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11982.240234375
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2402, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11982.244140625
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2441, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -11982.3544921875
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.3545, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -11982.2412109375
tensor(11982.2393, grad_fn=<NegBackward0>) tensor(11982.2412, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.7809, 0.2191],
        [0.2783, 0.7217]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6017, 0.3983], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3078, 0.1034],
         [0.6830, 0.3185]],

        [[0.6025, 0.0911],
         [0.7233, 0.7301]],

        [[0.7172, 0.0942],
         [0.7052, 0.5060]],

        [[0.5454, 0.0957],
         [0.5363, 0.5132]],

        [[0.5976, 0.1001],
         [0.5064, 0.7147]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9840231441209258
Average Adjusted Rand Index: 0.9839998119331363
[0.9840231441209258, 0.9840231441209258] [0.9839998119331363, 0.9839998119331363] [11982.23828125, 11982.2412109375]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11869.560032531203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22306.1953125
inf tensor(22306.1953, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11859.548828125
tensor(22306.1953, grad_fn=<NegBackward0>) tensor(11859.5488, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11857.2119140625
tensor(11859.5488, grad_fn=<NegBackward0>) tensor(11857.2119, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11856.9814453125
tensor(11857.2119, grad_fn=<NegBackward0>) tensor(11856.9814, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11856.880859375
tensor(11856.9814, grad_fn=<NegBackward0>) tensor(11856.8809, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11856.8271484375
tensor(11856.8809, grad_fn=<NegBackward0>) tensor(11856.8271, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11856.79296875
tensor(11856.8271, grad_fn=<NegBackward0>) tensor(11856.7930, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11856.771484375
tensor(11856.7930, grad_fn=<NegBackward0>) tensor(11856.7715, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11856.7578125
tensor(11856.7715, grad_fn=<NegBackward0>) tensor(11856.7578, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11856.74609375
tensor(11856.7578, grad_fn=<NegBackward0>) tensor(11856.7461, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11856.7392578125
tensor(11856.7461, grad_fn=<NegBackward0>) tensor(11856.7393, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11856.73046875
tensor(11856.7393, grad_fn=<NegBackward0>) tensor(11856.7305, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11856.7265625
tensor(11856.7305, grad_fn=<NegBackward0>) tensor(11856.7266, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11856.7216796875
tensor(11856.7266, grad_fn=<NegBackward0>) tensor(11856.7217, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11856.720703125
tensor(11856.7217, grad_fn=<NegBackward0>) tensor(11856.7207, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11856.716796875
tensor(11856.7207, grad_fn=<NegBackward0>) tensor(11856.7168, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11856.716796875
tensor(11856.7168, grad_fn=<NegBackward0>) tensor(11856.7168, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11856.7138671875
tensor(11856.7168, grad_fn=<NegBackward0>) tensor(11856.7139, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11856.7119140625
tensor(11856.7139, grad_fn=<NegBackward0>) tensor(11856.7119, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11856.7099609375
tensor(11856.7119, grad_fn=<NegBackward0>) tensor(11856.7100, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11856.7080078125
tensor(11856.7100, grad_fn=<NegBackward0>) tensor(11856.7080, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11856.7080078125
tensor(11856.7080, grad_fn=<NegBackward0>) tensor(11856.7080, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11856.7060546875
tensor(11856.7080, grad_fn=<NegBackward0>) tensor(11856.7061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11856.705078125
tensor(11856.7061, grad_fn=<NegBackward0>) tensor(11856.7051, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11856.7060546875
tensor(11856.7051, grad_fn=<NegBackward0>) tensor(11856.7061, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11856.7041015625
tensor(11856.7051, grad_fn=<NegBackward0>) tensor(11856.7041, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11856.705078125
tensor(11856.7041, grad_fn=<NegBackward0>) tensor(11856.7051, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -11856.703125
tensor(11856.7041, grad_fn=<NegBackward0>) tensor(11856.7031, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11856.7021484375
tensor(11856.7031, grad_fn=<NegBackward0>) tensor(11856.7021, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11856.7021484375
tensor(11856.7021, grad_fn=<NegBackward0>) tensor(11856.7021, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11856.701171875
tensor(11856.7021, grad_fn=<NegBackward0>) tensor(11856.7012, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11856.720703125
tensor(11856.7012, grad_fn=<NegBackward0>) tensor(11856.7207, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11856.7001953125
tensor(11856.7012, grad_fn=<NegBackward0>) tensor(11856.7002, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11856.7001953125
tensor(11856.7002, grad_fn=<NegBackward0>) tensor(11856.7002, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11856.7021484375
tensor(11856.7002, grad_fn=<NegBackward0>) tensor(11856.7021, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11856.7001953125
tensor(11856.7002, grad_fn=<NegBackward0>) tensor(11856.7002, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11856.705078125
tensor(11856.7002, grad_fn=<NegBackward0>) tensor(11856.7051, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11856.69921875
tensor(11856.7002, grad_fn=<NegBackward0>) tensor(11856.6992, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11856.7041015625
tensor(11856.6992, grad_fn=<NegBackward0>) tensor(11856.7041, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11856.7001953125
tensor(11856.6992, grad_fn=<NegBackward0>) tensor(11856.7002, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11856.7021484375
tensor(11856.6992, grad_fn=<NegBackward0>) tensor(11856.7021, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -11856.703125
tensor(11856.6992, grad_fn=<NegBackward0>) tensor(11856.7031, grad_fn=<NegBackward0>)
4
Iteration 4200: Loss = -11856.7041015625
tensor(11856.6992, grad_fn=<NegBackward0>) tensor(11856.7041, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4200 due to no improvement.
pi: tensor([[0.7312, 0.2688],
        [0.3041, 0.6959]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5300, 0.4700], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2987, 0.1000],
         [0.6839, 0.3119]],

        [[0.5719, 0.0973],
         [0.6819, 0.6991]],

        [[0.7054, 0.0901],
         [0.7146, 0.5063]],

        [[0.5494, 0.1029],
         [0.7205, 0.5563]],

        [[0.5742, 0.1002],
         [0.6774, 0.5525]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
Global Adjusted Rand Index: 0.9760961731911297
Average Adjusted Rand Index: 0.9761612186588084
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22490.1640625
inf tensor(22490.1641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12409.0810546875
tensor(22490.1641, grad_fn=<NegBackward0>) tensor(12409.0811, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12408.6787109375
tensor(12409.0811, grad_fn=<NegBackward0>) tensor(12408.6787, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12408.537109375
tensor(12408.6787, grad_fn=<NegBackward0>) tensor(12408.5371, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12408.4306640625
tensor(12408.5371, grad_fn=<NegBackward0>) tensor(12408.4307, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12408.33203125
tensor(12408.4307, grad_fn=<NegBackward0>) tensor(12408.3320, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12408.23828125
tensor(12408.3320, grad_fn=<NegBackward0>) tensor(12408.2383, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12408.15234375
tensor(12408.2383, grad_fn=<NegBackward0>) tensor(12408.1523, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12408.0751953125
tensor(12408.1523, grad_fn=<NegBackward0>) tensor(12408.0752, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12408.001953125
tensor(12408.0752, grad_fn=<NegBackward0>) tensor(12408.0020, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12407.9384765625
tensor(12408.0020, grad_fn=<NegBackward0>) tensor(12407.9385, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12407.8779296875
tensor(12407.9385, grad_fn=<NegBackward0>) tensor(12407.8779, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12407.8203125
tensor(12407.8779, grad_fn=<NegBackward0>) tensor(12407.8203, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12407.76171875
tensor(12407.8203, grad_fn=<NegBackward0>) tensor(12407.7617, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12407.7041015625
tensor(12407.7617, grad_fn=<NegBackward0>) tensor(12407.7041, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12407.650390625
tensor(12407.7041, grad_fn=<NegBackward0>) tensor(12407.6504, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12407.6025390625
tensor(12407.6504, grad_fn=<NegBackward0>) tensor(12407.6025, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12407.5576171875
tensor(12407.6025, grad_fn=<NegBackward0>) tensor(12407.5576, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12407.5205078125
tensor(12407.5576, grad_fn=<NegBackward0>) tensor(12407.5205, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12407.4873046875
tensor(12407.5205, grad_fn=<NegBackward0>) tensor(12407.4873, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12407.4541015625
tensor(12407.4873, grad_fn=<NegBackward0>) tensor(12407.4541, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12407.4208984375
tensor(12407.4541, grad_fn=<NegBackward0>) tensor(12407.4209, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12407.384765625
tensor(12407.4209, grad_fn=<NegBackward0>) tensor(12407.3848, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12407.34765625
tensor(12407.3848, grad_fn=<NegBackward0>) tensor(12407.3477, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12407.310546875
tensor(12407.3477, grad_fn=<NegBackward0>) tensor(12407.3105, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12407.271484375
tensor(12407.3105, grad_fn=<NegBackward0>) tensor(12407.2715, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12407.240234375
tensor(12407.2715, grad_fn=<NegBackward0>) tensor(12407.2402, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12407.193359375
tensor(12407.2402, grad_fn=<NegBackward0>) tensor(12407.1934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12407.1591796875
tensor(12407.1934, grad_fn=<NegBackward0>) tensor(12407.1592, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12407.12890625
tensor(12407.1592, grad_fn=<NegBackward0>) tensor(12407.1289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12407.0888671875
tensor(12407.1289, grad_fn=<NegBackward0>) tensor(12407.0889, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12407.056640625
tensor(12407.0889, grad_fn=<NegBackward0>) tensor(12407.0566, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12407.0849609375
tensor(12407.0566, grad_fn=<NegBackward0>) tensor(12407.0850, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12406.9921875
tensor(12407.0566, grad_fn=<NegBackward0>) tensor(12406.9922, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12406.9716796875
tensor(12406.9922, grad_fn=<NegBackward0>) tensor(12406.9717, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12406.908203125
tensor(12406.9717, grad_fn=<NegBackward0>) tensor(12406.9082, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12406.83203125
tensor(12406.9082, grad_fn=<NegBackward0>) tensor(12406.8320, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12406.685546875
tensor(12406.8320, grad_fn=<NegBackward0>) tensor(12406.6855, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12406.4306640625
tensor(12406.6855, grad_fn=<NegBackward0>) tensor(12406.4307, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12406.2119140625
tensor(12406.4307, grad_fn=<NegBackward0>) tensor(12406.2119, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12406.1142578125
tensor(12406.2119, grad_fn=<NegBackward0>) tensor(12406.1143, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12406.072265625
tensor(12406.1143, grad_fn=<NegBackward0>) tensor(12406.0723, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12406.052734375
tensor(12406.0723, grad_fn=<NegBackward0>) tensor(12406.0527, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12406.04296875
tensor(12406.0527, grad_fn=<NegBackward0>) tensor(12406.0430, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12406.03515625
tensor(12406.0430, grad_fn=<NegBackward0>) tensor(12406.0352, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12406.033203125
tensor(12406.0352, grad_fn=<NegBackward0>) tensor(12406.0332, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12406.029296875
tensor(12406.0332, grad_fn=<NegBackward0>) tensor(12406.0293, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12406.0283203125
tensor(12406.0293, grad_fn=<NegBackward0>) tensor(12406.0283, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12406.025390625
tensor(12406.0283, grad_fn=<NegBackward0>) tensor(12406.0254, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12406.025390625
tensor(12406.0254, grad_fn=<NegBackward0>) tensor(12406.0254, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12406.0244140625
tensor(12406.0254, grad_fn=<NegBackward0>) tensor(12406.0244, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12406.0234375
tensor(12406.0244, grad_fn=<NegBackward0>) tensor(12406.0234, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12406.0244140625
tensor(12406.0234, grad_fn=<NegBackward0>) tensor(12406.0244, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12406.021484375
tensor(12406.0234, grad_fn=<NegBackward0>) tensor(12406.0215, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12406.021484375
tensor(12406.0215, grad_fn=<NegBackward0>) tensor(12406.0215, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12406.021484375
tensor(12406.0215, grad_fn=<NegBackward0>) tensor(12406.0215, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12406.021484375
tensor(12406.0215, grad_fn=<NegBackward0>) tensor(12406.0215, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12406.01953125
tensor(12406.0215, grad_fn=<NegBackward0>) tensor(12406.0195, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12406.01953125
tensor(12406.0195, grad_fn=<NegBackward0>) tensor(12406.0195, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12406.01953125
tensor(12406.0195, grad_fn=<NegBackward0>) tensor(12406.0195, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12406.017578125
tensor(12406.0195, grad_fn=<NegBackward0>) tensor(12406.0176, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12406.017578125
tensor(12406.0176, grad_fn=<NegBackward0>) tensor(12406.0176, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12406.0166015625
tensor(12406.0176, grad_fn=<NegBackward0>) tensor(12406.0166, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12406.013671875
tensor(12406.0166, grad_fn=<NegBackward0>) tensor(12406.0137, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12406.01171875
tensor(12406.0137, grad_fn=<NegBackward0>) tensor(12406.0117, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12406.0068359375
tensor(12406.0117, grad_fn=<NegBackward0>) tensor(12406.0068, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12405.998046875
tensor(12406.0068, grad_fn=<NegBackward0>) tensor(12405.9980, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12405.9873046875
tensor(12405.9980, grad_fn=<NegBackward0>) tensor(12405.9873, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12405.9697265625
tensor(12405.9873, grad_fn=<NegBackward0>) tensor(12405.9697, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12405.955078125
tensor(12405.9697, grad_fn=<NegBackward0>) tensor(12405.9551, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12405.9462890625
tensor(12405.9551, grad_fn=<NegBackward0>) tensor(12405.9463, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12405.9384765625
tensor(12405.9463, grad_fn=<NegBackward0>) tensor(12405.9385, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12405.9345703125
tensor(12405.9385, grad_fn=<NegBackward0>) tensor(12405.9346, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12405.9306640625
tensor(12405.9346, grad_fn=<NegBackward0>) tensor(12405.9307, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12405.927734375
tensor(12405.9307, grad_fn=<NegBackward0>) tensor(12405.9277, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12405.9248046875
tensor(12405.9277, grad_fn=<NegBackward0>) tensor(12405.9248, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12405.9228515625
tensor(12405.9248, grad_fn=<NegBackward0>) tensor(12405.9229, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12405.9228515625
tensor(12405.9229, grad_fn=<NegBackward0>) tensor(12405.9229, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12405.9248046875
tensor(12405.9229, grad_fn=<NegBackward0>) tensor(12405.9248, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12405.919921875
tensor(12405.9229, grad_fn=<NegBackward0>) tensor(12405.9199, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12405.919921875
tensor(12405.9199, grad_fn=<NegBackward0>) tensor(12405.9199, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12405.9169921875
tensor(12405.9199, grad_fn=<NegBackward0>) tensor(12405.9170, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12405.9169921875
tensor(12405.9170, grad_fn=<NegBackward0>) tensor(12405.9170, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12405.9169921875
tensor(12405.9170, grad_fn=<NegBackward0>) tensor(12405.9170, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12405.9169921875
tensor(12405.9170, grad_fn=<NegBackward0>) tensor(12405.9170, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12405.916015625
tensor(12405.9170, grad_fn=<NegBackward0>) tensor(12405.9160, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12405.9228515625
tensor(12405.9160, grad_fn=<NegBackward0>) tensor(12405.9229, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12405.9140625
tensor(12405.9160, grad_fn=<NegBackward0>) tensor(12405.9141, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12406.01953125
tensor(12405.9141, grad_fn=<NegBackward0>) tensor(12406.0195, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12405.912109375
tensor(12405.9141, grad_fn=<NegBackward0>) tensor(12405.9121, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12405.9130859375
tensor(12405.9121, grad_fn=<NegBackward0>) tensor(12405.9131, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12405.9130859375
tensor(12405.9121, grad_fn=<NegBackward0>) tensor(12405.9131, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12405.9130859375
tensor(12405.9121, grad_fn=<NegBackward0>) tensor(12405.9131, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12405.9130859375
tensor(12405.9121, grad_fn=<NegBackward0>) tensor(12405.9131, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -12405.91015625
tensor(12405.9121, grad_fn=<NegBackward0>) tensor(12405.9102, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12405.912109375
tensor(12405.9102, grad_fn=<NegBackward0>) tensor(12405.9121, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12405.912109375
tensor(12405.9102, grad_fn=<NegBackward0>) tensor(12405.9121, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12405.916015625
tensor(12405.9102, grad_fn=<NegBackward0>) tensor(12405.9160, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -12405.9150390625
tensor(12405.9102, grad_fn=<NegBackward0>) tensor(12405.9150, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -12405.93359375
tensor(12405.9102, grad_fn=<NegBackward0>) tensor(12405.9336, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[6.3852e-04, 9.9936e-01],
        [9.9899e-01, 1.0099e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1285, 0.8715], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2076, 0.2041],
         [0.6111, 0.1936]],

        [[0.5260, 0.1963],
         [0.6861, 0.6680]],

        [[0.6989, 0.1969],
         [0.5600, 0.6792]],

        [[0.5755, 0.2137],
         [0.6886, 0.5172]],

        [[0.5591, 0.2092],
         [0.7146, 0.5279]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001470258621575994
Average Adjusted Rand Index: 0.0
[0.9760961731911297, -0.001470258621575994] [0.9761612186588084, 0.0] [11856.7041015625, 12405.93359375]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11896.31422611485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21445.181640625
inf tensor(21445.1816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12364.26953125
tensor(21445.1816, grad_fn=<NegBackward0>) tensor(12364.2695, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12364.1083984375
tensor(12364.2695, grad_fn=<NegBackward0>) tensor(12364.1084, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12364.0380859375
tensor(12364.1084, grad_fn=<NegBackward0>) tensor(12364.0381, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12363.9853515625
tensor(12364.0381, grad_fn=<NegBackward0>) tensor(12363.9854, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12363.9287109375
tensor(12363.9854, grad_fn=<NegBackward0>) tensor(12363.9287, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12363.8642578125
tensor(12363.9287, grad_fn=<NegBackward0>) tensor(12363.8643, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12363.7763671875
tensor(12363.8643, grad_fn=<NegBackward0>) tensor(12363.7764, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12363.6455078125
tensor(12363.7764, grad_fn=<NegBackward0>) tensor(12363.6455, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12363.4482421875
tensor(12363.6455, grad_fn=<NegBackward0>) tensor(12363.4482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12363.232421875
tensor(12363.4482, grad_fn=<NegBackward0>) tensor(12363.2324, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12363.09375
tensor(12363.2324, grad_fn=<NegBackward0>) tensor(12363.0938, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12363.017578125
tensor(12363.0938, grad_fn=<NegBackward0>) tensor(12363.0176, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12362.9619140625
tensor(12363.0176, grad_fn=<NegBackward0>) tensor(12362.9619, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12362.9208984375
tensor(12362.9619, grad_fn=<NegBackward0>) tensor(12362.9209, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12362.884765625
tensor(12362.9209, grad_fn=<NegBackward0>) tensor(12362.8848, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12362.8544921875
tensor(12362.8848, grad_fn=<NegBackward0>) tensor(12362.8545, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12362.8251953125
tensor(12362.8545, grad_fn=<NegBackward0>) tensor(12362.8252, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12362.796875
tensor(12362.8252, grad_fn=<NegBackward0>) tensor(12362.7969, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12362.7666015625
tensor(12362.7969, grad_fn=<NegBackward0>) tensor(12362.7666, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12362.736328125
tensor(12362.7666, grad_fn=<NegBackward0>) tensor(12362.7363, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12362.7060546875
tensor(12362.7363, grad_fn=<NegBackward0>) tensor(12362.7061, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12362.673828125
tensor(12362.7061, grad_fn=<NegBackward0>) tensor(12362.6738, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12362.646484375
tensor(12362.6738, grad_fn=<NegBackward0>) tensor(12362.6465, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12362.619140625
tensor(12362.6465, grad_fn=<NegBackward0>) tensor(12362.6191, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12362.5947265625
tensor(12362.6191, grad_fn=<NegBackward0>) tensor(12362.5947, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12362.5771484375
tensor(12362.5947, grad_fn=<NegBackward0>) tensor(12362.5771, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12362.560546875
tensor(12362.5771, grad_fn=<NegBackward0>) tensor(12362.5605, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12362.5458984375
tensor(12362.5605, grad_fn=<NegBackward0>) tensor(12362.5459, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12362.533203125
tensor(12362.5459, grad_fn=<NegBackward0>) tensor(12362.5332, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12362.521484375
tensor(12362.5332, grad_fn=<NegBackward0>) tensor(12362.5215, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12362.509765625
tensor(12362.5215, grad_fn=<NegBackward0>) tensor(12362.5098, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12362.4990234375
tensor(12362.5098, grad_fn=<NegBackward0>) tensor(12362.4990, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12362.48828125
tensor(12362.4990, grad_fn=<NegBackward0>) tensor(12362.4883, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12362.478515625
tensor(12362.4883, grad_fn=<NegBackward0>) tensor(12362.4785, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12362.46484375
tensor(12362.4785, grad_fn=<NegBackward0>) tensor(12362.4648, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12362.4462890625
tensor(12362.4648, grad_fn=<NegBackward0>) tensor(12362.4463, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12362.416015625
tensor(12362.4463, grad_fn=<NegBackward0>) tensor(12362.4160, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12362.3486328125
tensor(12362.4160, grad_fn=<NegBackward0>) tensor(12362.3486, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12362.21484375
tensor(12362.3486, grad_fn=<NegBackward0>) tensor(12362.2148, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12362.0673828125
tensor(12362.2148, grad_fn=<NegBackward0>) tensor(12362.0674, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12361.9677734375
tensor(12362.0674, grad_fn=<NegBackward0>) tensor(12361.9678, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12361.90234375
tensor(12361.9678, grad_fn=<NegBackward0>) tensor(12361.9023, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12361.8505859375
tensor(12361.9023, grad_fn=<NegBackward0>) tensor(12361.8506, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12361.7958984375
tensor(12361.8506, grad_fn=<NegBackward0>) tensor(12361.7959, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12361.5888671875
tensor(12361.7959, grad_fn=<NegBackward0>) tensor(12361.5889, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12360.765625
tensor(12361.5889, grad_fn=<NegBackward0>) tensor(12360.7656, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12360.6552734375
tensor(12360.7656, grad_fn=<NegBackward0>) tensor(12360.6553, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12360.62109375
tensor(12360.6553, grad_fn=<NegBackward0>) tensor(12360.6211, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12360.6025390625
tensor(12360.6211, grad_fn=<NegBackward0>) tensor(12360.6025, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12360.59375
tensor(12360.6025, grad_fn=<NegBackward0>) tensor(12360.5938, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12360.5859375
tensor(12360.5938, grad_fn=<NegBackward0>) tensor(12360.5859, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12360.58203125
tensor(12360.5859, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12360.578125
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5781, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12360.5732421875
tensor(12360.5781, grad_fn=<NegBackward0>) tensor(12360.5732, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12360.572265625
tensor(12360.5732, grad_fn=<NegBackward0>) tensor(12360.5723, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12360.5693359375
tensor(12360.5723, grad_fn=<NegBackward0>) tensor(12360.5693, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12360.5673828125
tensor(12360.5693, grad_fn=<NegBackward0>) tensor(12360.5674, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12360.56640625
tensor(12360.5674, grad_fn=<NegBackward0>) tensor(12360.5664, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12360.5654296875
tensor(12360.5664, grad_fn=<NegBackward0>) tensor(12360.5654, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12360.564453125
tensor(12360.5654, grad_fn=<NegBackward0>) tensor(12360.5645, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12360.5634765625
tensor(12360.5645, grad_fn=<NegBackward0>) tensor(12360.5635, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12360.5615234375
tensor(12360.5635, grad_fn=<NegBackward0>) tensor(12360.5615, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12360.5615234375
tensor(12360.5615, grad_fn=<NegBackward0>) tensor(12360.5615, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12360.5595703125
tensor(12360.5615, grad_fn=<NegBackward0>) tensor(12360.5596, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12360.5595703125
tensor(12360.5596, grad_fn=<NegBackward0>) tensor(12360.5596, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12360.560546875
tensor(12360.5596, grad_fn=<NegBackward0>) tensor(12360.5605, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12360.5576171875
tensor(12360.5596, grad_fn=<NegBackward0>) tensor(12360.5576, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12360.5634765625
tensor(12360.5576, grad_fn=<NegBackward0>) tensor(12360.5635, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12360.5576171875
tensor(12360.5576, grad_fn=<NegBackward0>) tensor(12360.5576, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12360.5625
tensor(12360.5576, grad_fn=<NegBackward0>) tensor(12360.5625, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12360.5556640625
tensor(12360.5576, grad_fn=<NegBackward0>) tensor(12360.5557, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12360.556640625
tensor(12360.5557, grad_fn=<NegBackward0>) tensor(12360.5566, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12360.556640625
tensor(12360.5557, grad_fn=<NegBackward0>) tensor(12360.5566, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12360.6513671875
tensor(12360.5557, grad_fn=<NegBackward0>) tensor(12360.6514, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12360.5556640625
tensor(12360.5557, grad_fn=<NegBackward0>) tensor(12360.5557, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12360.5546875
tensor(12360.5557, grad_fn=<NegBackward0>) tensor(12360.5547, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12360.5546875
tensor(12360.5547, grad_fn=<NegBackward0>) tensor(12360.5547, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12360.5654296875
tensor(12360.5547, grad_fn=<NegBackward0>) tensor(12360.5654, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12360.599609375
tensor(12360.5547, grad_fn=<NegBackward0>) tensor(12360.5996, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12360.5537109375
tensor(12360.5547, grad_fn=<NegBackward0>) tensor(12360.5537, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12360.5537109375
tensor(12360.5537, grad_fn=<NegBackward0>) tensor(12360.5537, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12360.5537109375
tensor(12360.5537, grad_fn=<NegBackward0>) tensor(12360.5537, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12360.552734375
tensor(12360.5537, grad_fn=<NegBackward0>) tensor(12360.5527, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12360.5537109375
tensor(12360.5527, grad_fn=<NegBackward0>) tensor(12360.5537, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12360.5537109375
tensor(12360.5527, grad_fn=<NegBackward0>) tensor(12360.5537, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12360.552734375
tensor(12360.5527, grad_fn=<NegBackward0>) tensor(12360.5527, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12360.5546875
tensor(12360.5527, grad_fn=<NegBackward0>) tensor(12360.5547, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12360.5517578125
tensor(12360.5527, grad_fn=<NegBackward0>) tensor(12360.5518, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12360.5576171875
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.5576, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12360.5537109375
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.5537, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12360.666015625
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.6660, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12360.552734375
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.5527, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -12360.5517578125
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.5518, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12360.552734375
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.5527, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12360.55078125
tensor(12360.5518, grad_fn=<NegBackward0>) tensor(12360.5508, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12360.583984375
tensor(12360.5508, grad_fn=<NegBackward0>) tensor(12360.5840, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12360.552734375
tensor(12360.5508, grad_fn=<NegBackward0>) tensor(12360.5527, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12360.55078125
tensor(12360.5508, grad_fn=<NegBackward0>) tensor(12360.5508, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12360.5517578125
tensor(12360.5508, grad_fn=<NegBackward0>) tensor(12360.5518, grad_fn=<NegBackward0>)
1
pi: tensor([[5.1893e-05, 9.9995e-01],
        [9.9990e-01, 1.0325e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9538, 0.0462], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.2520],
         [0.6210, 0.2016]],

        [[0.6764, 0.2357],
         [0.5457, 0.5526]],

        [[0.6467, 0.1425],
         [0.5971, 0.7287]],

        [[0.6928, 0.2268],
         [0.5463, 0.5671]],

        [[0.5937, 0.2313],
         [0.5691, 0.5058]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.012864505300896736
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: -0.004267232452421997
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: -0.004063414211148451
Global Adjusted Rand Index: -0.0012565154148738286
Average Adjusted Rand Index: 0.0007657139471404908
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24073.650390625
inf tensor(24073.6504, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12363.9833984375
tensor(24073.6504, grad_fn=<NegBackward0>) tensor(12363.9834, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12363.4033203125
tensor(12363.9834, grad_fn=<NegBackward0>) tensor(12363.4033, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12363.0087890625
tensor(12363.4033, grad_fn=<NegBackward0>) tensor(12363.0088, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.822265625
tensor(12363.0088, grad_fn=<NegBackward0>) tensor(12362.8223, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.76953125
tensor(12362.8223, grad_fn=<NegBackward0>) tensor(12362.7695, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.7138671875
tensor(12362.7695, grad_fn=<NegBackward0>) tensor(12362.7139, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.6591796875
tensor(12362.7139, grad_fn=<NegBackward0>) tensor(12362.6592, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.615234375
tensor(12362.6592, grad_fn=<NegBackward0>) tensor(12362.6152, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.580078125
tensor(12362.6152, grad_fn=<NegBackward0>) tensor(12362.5801, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.55078125
tensor(12362.5801, grad_fn=<NegBackward0>) tensor(12362.5508, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.5244140625
tensor(12362.5508, grad_fn=<NegBackward0>) tensor(12362.5244, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.50390625
tensor(12362.5244, grad_fn=<NegBackward0>) tensor(12362.5039, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12362.48828125
tensor(12362.5039, grad_fn=<NegBackward0>) tensor(12362.4883, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12362.4765625
tensor(12362.4883, grad_fn=<NegBackward0>) tensor(12362.4766, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12362.4658203125
tensor(12362.4766, grad_fn=<NegBackward0>) tensor(12362.4658, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12362.451171875
tensor(12362.4658, grad_fn=<NegBackward0>) tensor(12362.4512, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12362.431640625
tensor(12362.4512, grad_fn=<NegBackward0>) tensor(12362.4316, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12362.41015625
tensor(12362.4316, grad_fn=<NegBackward0>) tensor(12362.4102, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12362.3984375
tensor(12362.4102, grad_fn=<NegBackward0>) tensor(12362.3984, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12362.3955078125
tensor(12362.3984, grad_fn=<NegBackward0>) tensor(12362.3955, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12362.3935546875
tensor(12362.3955, grad_fn=<NegBackward0>) tensor(12362.3936, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12362.392578125
tensor(12362.3936, grad_fn=<NegBackward0>) tensor(12362.3926, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12362.392578125
tensor(12362.3926, grad_fn=<NegBackward0>) tensor(12362.3926, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12362.3916015625
tensor(12362.3926, grad_fn=<NegBackward0>) tensor(12362.3916, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12362.3916015625
tensor(12362.3916, grad_fn=<NegBackward0>) tensor(12362.3916, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12362.390625
tensor(12362.3916, grad_fn=<NegBackward0>) tensor(12362.3906, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12362.392578125
tensor(12362.3906, grad_fn=<NegBackward0>) tensor(12362.3926, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12362.3916015625
tensor(12362.3906, grad_fn=<NegBackward0>) tensor(12362.3916, grad_fn=<NegBackward0>)
2
Iteration 2900: Loss = -12362.392578125
tensor(12362.3906, grad_fn=<NegBackward0>) tensor(12362.3926, grad_fn=<NegBackward0>)
3
Iteration 3000: Loss = -12362.3916015625
tensor(12362.3906, grad_fn=<NegBackward0>) tensor(12362.3916, grad_fn=<NegBackward0>)
4
Iteration 3100: Loss = -12362.3916015625
tensor(12362.3906, grad_fn=<NegBackward0>) tensor(12362.3916, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3100 due to no improvement.
pi: tensor([[0.6791, 0.3209],
        [0.5245, 0.4755]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0055, 0.9945], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1972, 0.1994],
         [0.5732, 0.2013]],

        [[0.5155, 0.2015],
         [0.6257, 0.5894]],

        [[0.7070, 0.1869],
         [0.7103, 0.7215]],

        [[0.5269, 0.2081],
         [0.6375, 0.5702]],

        [[0.5855, 0.2010],
         [0.6497, 0.6640]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00031347386698087374
Average Adjusted Rand Index: 0.0006397103728861219
[-0.0012565154148738286, 0.00031347386698087374] [0.0007657139471404908, 0.0006397103728861219] [12360.5517578125, 12362.3916015625]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11903.273415728168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21086.53125
inf tensor(21086.5312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12439.06640625
tensor(21086.5312, grad_fn=<NegBackward0>) tensor(12439.0664, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12438.8544921875
tensor(12439.0664, grad_fn=<NegBackward0>) tensor(12438.8545, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12438.8017578125
tensor(12438.8545, grad_fn=<NegBackward0>) tensor(12438.8018, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12438.7705078125
tensor(12438.8018, grad_fn=<NegBackward0>) tensor(12438.7705, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12438.7451171875
tensor(12438.7705, grad_fn=<NegBackward0>) tensor(12438.7451, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12438.7294921875
tensor(12438.7451, grad_fn=<NegBackward0>) tensor(12438.7295, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12438.71484375
tensor(12438.7295, grad_fn=<NegBackward0>) tensor(12438.7148, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12438.701171875
tensor(12438.7148, grad_fn=<NegBackward0>) tensor(12438.7012, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12438.6923828125
tensor(12438.7012, grad_fn=<NegBackward0>) tensor(12438.6924, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12438.6806640625
tensor(12438.6924, grad_fn=<NegBackward0>) tensor(12438.6807, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12438.6689453125
tensor(12438.6807, grad_fn=<NegBackward0>) tensor(12438.6689, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12438.6552734375
tensor(12438.6689, grad_fn=<NegBackward0>) tensor(12438.6553, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12438.6396484375
tensor(12438.6553, grad_fn=<NegBackward0>) tensor(12438.6396, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12438.62109375
tensor(12438.6396, grad_fn=<NegBackward0>) tensor(12438.6211, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12438.5947265625
tensor(12438.6211, grad_fn=<NegBackward0>) tensor(12438.5947, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12438.5517578125
tensor(12438.5947, grad_fn=<NegBackward0>) tensor(12438.5518, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12438.4580078125
tensor(12438.5518, grad_fn=<NegBackward0>) tensor(12438.4580, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12438.2041015625
tensor(12438.4580, grad_fn=<NegBackward0>) tensor(12438.2041, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12437.958984375
tensor(12438.2041, grad_fn=<NegBackward0>) tensor(12437.9590, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12437.83984375
tensor(12437.9590, grad_fn=<NegBackward0>) tensor(12437.8398, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12437.7900390625
tensor(12437.8398, grad_fn=<NegBackward0>) tensor(12437.7900, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12437.763671875
tensor(12437.7900, grad_fn=<NegBackward0>) tensor(12437.7637, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12437.7470703125
tensor(12437.7637, grad_fn=<NegBackward0>) tensor(12437.7471, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12437.7333984375
tensor(12437.7471, grad_fn=<NegBackward0>) tensor(12437.7334, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12437.7255859375
tensor(12437.7334, grad_fn=<NegBackward0>) tensor(12437.7256, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12437.716796875
tensor(12437.7256, grad_fn=<NegBackward0>) tensor(12437.7168, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12437.703125
tensor(12437.7168, grad_fn=<NegBackward0>) tensor(12437.7031, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12437.6728515625
tensor(12437.7031, grad_fn=<NegBackward0>) tensor(12437.6729, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12437.390625
tensor(12437.6729, grad_fn=<NegBackward0>) tensor(12437.3906, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12437.013671875
tensor(12437.3906, grad_fn=<NegBackward0>) tensor(12437.0137, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12436.9853515625
tensor(12437.0137, grad_fn=<NegBackward0>) tensor(12436.9854, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12436.9775390625
tensor(12436.9854, grad_fn=<NegBackward0>) tensor(12436.9775, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12436.9716796875
tensor(12436.9775, grad_fn=<NegBackward0>) tensor(12436.9717, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12436.96484375
tensor(12436.9717, grad_fn=<NegBackward0>) tensor(12436.9648, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12436.9619140625
tensor(12436.9648, grad_fn=<NegBackward0>) tensor(12436.9619, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12436.95703125
tensor(12436.9619, grad_fn=<NegBackward0>) tensor(12436.9570, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12436.9541015625
tensor(12436.9570, grad_fn=<NegBackward0>) tensor(12436.9541, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12436.953125
tensor(12436.9541, grad_fn=<NegBackward0>) tensor(12436.9531, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12436.9521484375
tensor(12436.9531, grad_fn=<NegBackward0>) tensor(12436.9521, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12436.94921875
tensor(12436.9521, grad_fn=<NegBackward0>) tensor(12436.9492, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12436.947265625
tensor(12436.9492, grad_fn=<NegBackward0>) tensor(12436.9473, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12436.9453125
tensor(12436.9473, grad_fn=<NegBackward0>) tensor(12436.9453, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12436.9462890625
tensor(12436.9453, grad_fn=<NegBackward0>) tensor(12436.9463, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12436.943359375
tensor(12436.9453, grad_fn=<NegBackward0>) tensor(12436.9434, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12436.943359375
tensor(12436.9434, grad_fn=<NegBackward0>) tensor(12436.9434, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12436.943359375
tensor(12436.9434, grad_fn=<NegBackward0>) tensor(12436.9434, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12436.9423828125
tensor(12436.9434, grad_fn=<NegBackward0>) tensor(12436.9424, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12436.94140625
tensor(12436.9424, grad_fn=<NegBackward0>) tensor(12436.9414, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12436.939453125
tensor(12436.9414, grad_fn=<NegBackward0>) tensor(12436.9395, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12436.939453125
tensor(12436.9395, grad_fn=<NegBackward0>) tensor(12436.9395, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12436.9384765625
tensor(12436.9395, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12436.9384765625
tensor(12436.9385, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12436.9384765625
tensor(12436.9385, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12436.9375
tensor(12436.9385, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12436.9384765625
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12436.935546875
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12436.9365234375
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9365, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12436.9365234375
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9365, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12436.935546875
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12436.9365234375
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9365, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12436.935546875
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12436.9345703125
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12436.9345703125
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12436.9345703125
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12436.9345703125
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12436.9423828125
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9424, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12436.93359375
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12436.9560546875
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9561, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12436.935546875
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[5.1301e-04, 9.9949e-01],
        [3.1954e-02, 9.6805e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9909, 0.0091], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1940, 0.1947],
         [0.6520, 0.2056]],

        [[0.7273, 0.2024],
         [0.5361, 0.5544]],

        [[0.6236, 0.1366],
         [0.5536, 0.6945]],

        [[0.5758, 0.1352],
         [0.5353, 0.6267]],

        [[0.5682, 0.2073],
         [0.5474, 0.6588]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00031954652715849544
Average Adjusted Rand Index: 0.00033344555058593435
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21255.04296875
inf tensor(21255.0430, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12439.060546875
tensor(21255.0430, grad_fn=<NegBackward0>) tensor(12439.0605, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12438.78515625
tensor(12439.0605, grad_fn=<NegBackward0>) tensor(12438.7852, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12438.7421875
tensor(12438.7852, grad_fn=<NegBackward0>) tensor(12438.7422, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12438.720703125
tensor(12438.7422, grad_fn=<NegBackward0>) tensor(12438.7207, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12438.705078125
tensor(12438.7207, grad_fn=<NegBackward0>) tensor(12438.7051, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12438.689453125
tensor(12438.7051, grad_fn=<NegBackward0>) tensor(12438.6895, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12438.6708984375
tensor(12438.6895, grad_fn=<NegBackward0>) tensor(12438.6709, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12438.6474609375
tensor(12438.6709, grad_fn=<NegBackward0>) tensor(12438.6475, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12438.6044921875
tensor(12438.6475, grad_fn=<NegBackward0>) tensor(12438.6045, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12438.513671875
tensor(12438.6045, grad_fn=<NegBackward0>) tensor(12438.5137, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12438.1435546875
tensor(12438.5137, grad_fn=<NegBackward0>) tensor(12438.1436, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12437.7841796875
tensor(12438.1436, grad_fn=<NegBackward0>) tensor(12437.7842, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12437.658203125
tensor(12437.7842, grad_fn=<NegBackward0>) tensor(12437.6582, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12437.6025390625
tensor(12437.6582, grad_fn=<NegBackward0>) tensor(12437.6025, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12437.5751953125
tensor(12437.6025, grad_fn=<NegBackward0>) tensor(12437.5752, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12437.5634765625
tensor(12437.5752, grad_fn=<NegBackward0>) tensor(12437.5635, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12437.5556640625
tensor(12437.5635, grad_fn=<NegBackward0>) tensor(12437.5557, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12437.55078125
tensor(12437.5557, grad_fn=<NegBackward0>) tensor(12437.5508, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12437.5478515625
tensor(12437.5508, grad_fn=<NegBackward0>) tensor(12437.5479, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12437.544921875
tensor(12437.5479, grad_fn=<NegBackward0>) tensor(12437.5449, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12437.54296875
tensor(12437.5449, grad_fn=<NegBackward0>) tensor(12437.5430, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12437.541015625
tensor(12437.5430, grad_fn=<NegBackward0>) tensor(12437.5410, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12437.5380859375
tensor(12437.5410, grad_fn=<NegBackward0>) tensor(12437.5381, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12437.5322265625
tensor(12437.5381, grad_fn=<NegBackward0>) tensor(12437.5322, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12437.505859375
tensor(12437.5322, grad_fn=<NegBackward0>) tensor(12437.5059, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12437.228515625
tensor(12437.5059, grad_fn=<NegBackward0>) tensor(12437.2285, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12437.005859375
tensor(12437.2285, grad_fn=<NegBackward0>) tensor(12437.0059, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12436.9794921875
tensor(12437.0059, grad_fn=<NegBackward0>) tensor(12436.9795, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12436.966796875
tensor(12436.9795, grad_fn=<NegBackward0>) tensor(12436.9668, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12436.9580078125
tensor(12436.9668, grad_fn=<NegBackward0>) tensor(12436.9580, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12436.953125
tensor(12436.9580, grad_fn=<NegBackward0>) tensor(12436.9531, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12436.9501953125
tensor(12436.9531, grad_fn=<NegBackward0>) tensor(12436.9502, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12436.947265625
tensor(12436.9502, grad_fn=<NegBackward0>) tensor(12436.9473, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12436.9453125
tensor(12436.9473, grad_fn=<NegBackward0>) tensor(12436.9453, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12436.943359375
tensor(12436.9453, grad_fn=<NegBackward0>) tensor(12436.9434, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12436.9423828125
tensor(12436.9434, grad_fn=<NegBackward0>) tensor(12436.9424, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12436.9423828125
tensor(12436.9424, grad_fn=<NegBackward0>) tensor(12436.9424, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12436.94140625
tensor(12436.9424, grad_fn=<NegBackward0>) tensor(12436.9414, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12436.939453125
tensor(12436.9414, grad_fn=<NegBackward0>) tensor(12436.9395, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12436.9404296875
tensor(12436.9395, grad_fn=<NegBackward0>) tensor(12436.9404, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12436.9404296875
tensor(12436.9395, grad_fn=<NegBackward0>) tensor(12436.9404, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12436.9384765625
tensor(12436.9395, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12436.9375
tensor(12436.9385, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12436.9384765625
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12436.9375
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9375, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12436.935546875
tensor(12436.9375, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12436.935546875
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12436.935546875
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12436.9345703125
tensor(12436.9355, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12436.935546875
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12436.93359375
tensor(12436.9346, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12436.93359375
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12436.93359375
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12436.93359375
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12436.93359375
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12436.93359375
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12436.9345703125
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9346, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12436.9326171875
tensor(12436.9336, grad_fn=<NegBackward0>) tensor(12436.9326, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12436.93359375
tensor(12436.9326, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12436.93359375
tensor(12436.9326, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12436.935546875
tensor(12436.9326, grad_fn=<NegBackward0>) tensor(12436.9355, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12436.93359375
tensor(12436.9326, grad_fn=<NegBackward0>) tensor(12436.9336, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -12436.9384765625
tensor(12436.9326, grad_fn=<NegBackward0>) tensor(12436.9385, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[4.5980e-04, 9.9954e-01],
        [3.2023e-02, 9.6798e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9940, 0.0060], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1940, 0.1946],
         [0.6843, 0.2059]],

        [[0.6166, 0.2024],
         [0.7130, 0.7207]],

        [[0.5246, 0.1366],
         [0.5869, 0.6342]],

        [[0.5617, 0.1351],
         [0.5461, 0.6090]],

        [[0.5453, 0.2073],
         [0.6210, 0.6266]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00031954652715849544
Average Adjusted Rand Index: 0.00033344555058593435
[-0.00031954652715849544, -0.00031954652715849544] [0.00033344555058593435, 0.00033344555058593435] [12436.9345703125, 12436.9384765625]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11765.463763320342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20485.03125
inf tensor(20485.0312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12265.40234375
tensor(20485.0312, grad_fn=<NegBackward0>) tensor(12265.4023, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12264.962890625
tensor(12265.4023, grad_fn=<NegBackward0>) tensor(12264.9629, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12264.8623046875
tensor(12264.9629, grad_fn=<NegBackward0>) tensor(12264.8623, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12264.8134765625
tensor(12264.8623, grad_fn=<NegBackward0>) tensor(12264.8135, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12264.78515625
tensor(12264.8135, grad_fn=<NegBackward0>) tensor(12264.7852, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12264.765625
tensor(12264.7852, grad_fn=<NegBackward0>) tensor(12264.7656, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12264.748046875
tensor(12264.7656, grad_fn=<NegBackward0>) tensor(12264.7480, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12264.7333984375
tensor(12264.7480, grad_fn=<NegBackward0>) tensor(12264.7334, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12264.7216796875
tensor(12264.7334, grad_fn=<NegBackward0>) tensor(12264.7217, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12264.708984375
tensor(12264.7217, grad_fn=<NegBackward0>) tensor(12264.7090, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12264.7001953125
tensor(12264.7090, grad_fn=<NegBackward0>) tensor(12264.7002, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12264.69140625
tensor(12264.7002, grad_fn=<NegBackward0>) tensor(12264.6914, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12264.68359375
tensor(12264.6914, grad_fn=<NegBackward0>) tensor(12264.6836, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12264.677734375
tensor(12264.6836, grad_fn=<NegBackward0>) tensor(12264.6777, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12264.6708984375
tensor(12264.6777, grad_fn=<NegBackward0>) tensor(12264.6709, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12264.6669921875
tensor(12264.6709, grad_fn=<NegBackward0>) tensor(12264.6670, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12264.662109375
tensor(12264.6670, grad_fn=<NegBackward0>) tensor(12264.6621, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12264.66015625
tensor(12264.6621, grad_fn=<NegBackward0>) tensor(12264.6602, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12264.66015625
tensor(12264.6602, grad_fn=<NegBackward0>) tensor(12264.6602, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12264.6572265625
tensor(12264.6602, grad_fn=<NegBackward0>) tensor(12264.6572, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12264.658203125
tensor(12264.6572, grad_fn=<NegBackward0>) tensor(12264.6582, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -12264.6572265625
tensor(12264.6572, grad_fn=<NegBackward0>) tensor(12264.6572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12264.6572265625
tensor(12264.6572, grad_fn=<NegBackward0>) tensor(12264.6572, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12264.654296875
tensor(12264.6572, grad_fn=<NegBackward0>) tensor(12264.6543, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12264.654296875
tensor(12264.6543, grad_fn=<NegBackward0>) tensor(12264.6543, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12264.6513671875
tensor(12264.6543, grad_fn=<NegBackward0>) tensor(12264.6514, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12264.6513671875
tensor(12264.6514, grad_fn=<NegBackward0>) tensor(12264.6514, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12264.6494140625
tensor(12264.6514, grad_fn=<NegBackward0>) tensor(12264.6494, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12264.646484375
tensor(12264.6494, grad_fn=<NegBackward0>) tensor(12264.6465, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12264.646484375
tensor(12264.6465, grad_fn=<NegBackward0>) tensor(12264.6465, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12264.6455078125
tensor(12264.6465, grad_fn=<NegBackward0>) tensor(12264.6455, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12264.642578125
tensor(12264.6455, grad_fn=<NegBackward0>) tensor(12264.6426, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12264.6416015625
tensor(12264.6426, grad_fn=<NegBackward0>) tensor(12264.6416, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12264.640625
tensor(12264.6416, grad_fn=<NegBackward0>) tensor(12264.6406, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12264.640625
tensor(12264.6406, grad_fn=<NegBackward0>) tensor(12264.6406, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12264.638671875
tensor(12264.6406, grad_fn=<NegBackward0>) tensor(12264.6387, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12264.638671875
tensor(12264.6387, grad_fn=<NegBackward0>) tensor(12264.6387, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12264.638671875
tensor(12264.6387, grad_fn=<NegBackward0>) tensor(12264.6387, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12264.6396484375
tensor(12264.6387, grad_fn=<NegBackward0>) tensor(12264.6396, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12264.6357421875
tensor(12264.6387, grad_fn=<NegBackward0>) tensor(12264.6357, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12264.63671875
tensor(12264.6357, grad_fn=<NegBackward0>) tensor(12264.6367, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12264.6357421875
tensor(12264.6357, grad_fn=<NegBackward0>) tensor(12264.6357, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12264.6357421875
tensor(12264.6357, grad_fn=<NegBackward0>) tensor(12264.6357, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12264.634765625
tensor(12264.6357, grad_fn=<NegBackward0>) tensor(12264.6348, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12264.6279296875
tensor(12264.6348, grad_fn=<NegBackward0>) tensor(12264.6279, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12264.6044921875
tensor(12264.6279, grad_fn=<NegBackward0>) tensor(12264.6045, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12264.5810546875
tensor(12264.6045, grad_fn=<NegBackward0>) tensor(12264.5811, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12264.5732421875
tensor(12264.5811, grad_fn=<NegBackward0>) tensor(12264.5732, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12264.5712890625
tensor(12264.5732, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12264.5732421875
tensor(12264.5713, grad_fn=<NegBackward0>) tensor(12264.5732, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12264.5703125
tensor(12264.5713, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12264.5712890625
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12264.5712890625
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12264.5703125
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12264.5693359375
tensor(12264.5703, grad_fn=<NegBackward0>) tensor(12264.5693, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12264.5703125
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12264.5712890625
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12264.5712890625
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12264.5703125
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -12264.5693359375
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5693, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12264.5703125
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12264.568359375
tensor(12264.5693, grad_fn=<NegBackward0>) tensor(12264.5684, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12264.5703125
tensor(12264.5684, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12264.5703125
tensor(12264.5684, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12264.5703125
tensor(12264.5684, grad_fn=<NegBackward0>) tensor(12264.5703, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12264.5712890625
tensor(12264.5684, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -12264.5712890625
tensor(12264.5684, grad_fn=<NegBackward0>) tensor(12264.5713, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.9810, 0.0190],
        [0.9942, 0.0058]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0593, 0.9407], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.1978],
         [0.6158, 0.1980]],

        [[0.6318, 0.2817],
         [0.6971, 0.6461]],

        [[0.7013, 0.1953],
         [0.5342, 0.5182]],

        [[0.5318, 0.2014],
         [0.5497, 0.7294]],

        [[0.6029, 0.1485],
         [0.6993, 0.7075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 7.705317577713987e-06
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21925.724609375
inf tensor(21925.7246, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12265.4814453125
tensor(21925.7246, grad_fn=<NegBackward0>) tensor(12265.4814, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12265.0693359375
tensor(12265.4814, grad_fn=<NegBackward0>) tensor(12265.0693, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12264.9814453125
tensor(12265.0693, grad_fn=<NegBackward0>) tensor(12264.9814, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12264.9375
tensor(12264.9814, grad_fn=<NegBackward0>) tensor(12264.9375, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12264.9072265625
tensor(12264.9375, grad_fn=<NegBackward0>) tensor(12264.9072, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12264.8828125
tensor(12264.9072, grad_fn=<NegBackward0>) tensor(12264.8828, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12264.8671875
tensor(12264.8828, grad_fn=<NegBackward0>) tensor(12264.8672, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12264.853515625
tensor(12264.8672, grad_fn=<NegBackward0>) tensor(12264.8535, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12264.8408203125
tensor(12264.8535, grad_fn=<NegBackward0>) tensor(12264.8408, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12264.830078125
tensor(12264.8408, grad_fn=<NegBackward0>) tensor(12264.8301, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12264.82421875
tensor(12264.8301, grad_fn=<NegBackward0>) tensor(12264.8242, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12264.8134765625
tensor(12264.8242, grad_fn=<NegBackward0>) tensor(12264.8135, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12264.806640625
tensor(12264.8135, grad_fn=<NegBackward0>) tensor(12264.8066, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12264.7978515625
tensor(12264.8066, grad_fn=<NegBackward0>) tensor(12264.7979, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12264.791015625
tensor(12264.7979, grad_fn=<NegBackward0>) tensor(12264.7910, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12264.77734375
tensor(12264.7910, grad_fn=<NegBackward0>) tensor(12264.7773, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12264.759765625
tensor(12264.7773, grad_fn=<NegBackward0>) tensor(12264.7598, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12264.7353515625
tensor(12264.7598, grad_fn=<NegBackward0>) tensor(12264.7354, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12264.71484375
tensor(12264.7354, grad_fn=<NegBackward0>) tensor(12264.7148, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12264.7021484375
tensor(12264.7148, grad_fn=<NegBackward0>) tensor(12264.7021, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12264.69140625
tensor(12264.7021, grad_fn=<NegBackward0>) tensor(12264.6914, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12264.6845703125
tensor(12264.6914, grad_fn=<NegBackward0>) tensor(12264.6846, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12264.6767578125
tensor(12264.6846, grad_fn=<NegBackward0>) tensor(12264.6768, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12264.6708984375
tensor(12264.6768, grad_fn=<NegBackward0>) tensor(12264.6709, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12264.6650390625
tensor(12264.6709, grad_fn=<NegBackward0>) tensor(12264.6650, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12264.6591796875
tensor(12264.6650, grad_fn=<NegBackward0>) tensor(12264.6592, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12264.65234375
tensor(12264.6592, grad_fn=<NegBackward0>) tensor(12264.6523, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12264.646484375
tensor(12264.6523, grad_fn=<NegBackward0>) tensor(12264.6465, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12264.64453125
tensor(12264.6465, grad_fn=<NegBackward0>) tensor(12264.6445, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12264.6416015625
tensor(12264.6445, grad_fn=<NegBackward0>) tensor(12264.6416, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12264.638671875
tensor(12264.6416, grad_fn=<NegBackward0>) tensor(12264.6387, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12264.63671875
tensor(12264.6387, grad_fn=<NegBackward0>) tensor(12264.6367, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12264.634765625
tensor(12264.6367, grad_fn=<NegBackward0>) tensor(12264.6348, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12264.634765625
tensor(12264.6348, grad_fn=<NegBackward0>) tensor(12264.6348, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12264.6337890625
tensor(12264.6348, grad_fn=<NegBackward0>) tensor(12264.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12264.6328125
tensor(12264.6338, grad_fn=<NegBackward0>) tensor(12264.6328, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12264.634765625
tensor(12264.6328, grad_fn=<NegBackward0>) tensor(12264.6348, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12264.6328125
tensor(12264.6328, grad_fn=<NegBackward0>) tensor(12264.6328, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12264.6328125
tensor(12264.6328, grad_fn=<NegBackward0>) tensor(12264.6328, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12264.630859375
tensor(12264.6328, grad_fn=<NegBackward0>) tensor(12264.6309, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12264.6328125
tensor(12264.6309, grad_fn=<NegBackward0>) tensor(12264.6328, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12264.6318359375
tensor(12264.6309, grad_fn=<NegBackward0>) tensor(12264.6318, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -12264.6328125
tensor(12264.6309, grad_fn=<NegBackward0>) tensor(12264.6328, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -12264.6328125
tensor(12264.6309, grad_fn=<NegBackward0>) tensor(12264.6328, grad_fn=<NegBackward0>)
4
Iteration 4500: Loss = -12264.6318359375
tensor(12264.6309, grad_fn=<NegBackward0>) tensor(12264.6318, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4500 due to no improvement.
pi: tensor([[0.9906, 0.0094],
        [0.9409, 0.0591]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9917, 0.0083], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.2029],
         [0.5952, 0.2269]],

        [[0.7169, 0.2730],
         [0.5623, 0.5560]],

        [[0.6377, 0.1973],
         [0.5922, 0.5503]],

        [[0.6835, 0.2035],
         [0.5360, 0.6079]],

        [[0.5970, 0.1407],
         [0.5882, 0.6085]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[7.705317577713987e-06, 0.0] [0.0, 0.0] [12264.5712890625, 12264.6318359375]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11897.771236841018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23039.09375
inf tensor(23039.0938, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12335.0625
tensor(23039.0938, grad_fn=<NegBackward0>) tensor(12335.0625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12334.8369140625
tensor(12335.0625, grad_fn=<NegBackward0>) tensor(12334.8369, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12334.7431640625
tensor(12334.8369, grad_fn=<NegBackward0>) tensor(12334.7432, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12334.4326171875
tensor(12334.7432, grad_fn=<NegBackward0>) tensor(12334.4326, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12334.224609375
tensor(12334.4326, grad_fn=<NegBackward0>) tensor(12334.2246, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12334.162109375
tensor(12334.2246, grad_fn=<NegBackward0>) tensor(12334.1621, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12334.119140625
tensor(12334.1621, grad_fn=<NegBackward0>) tensor(12334.1191, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12334.0869140625
tensor(12334.1191, grad_fn=<NegBackward0>) tensor(12334.0869, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12334.0517578125
tensor(12334.0869, grad_fn=<NegBackward0>) tensor(12334.0518, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12334.0107421875
tensor(12334.0518, grad_fn=<NegBackward0>) tensor(12334.0107, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12333.9609375
tensor(12334.0107, grad_fn=<NegBackward0>) tensor(12333.9609, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12333.8994140625
tensor(12333.9609, grad_fn=<NegBackward0>) tensor(12333.8994, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12333.8193359375
tensor(12333.8994, grad_fn=<NegBackward0>) tensor(12333.8193, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12333.712890625
tensor(12333.8193, grad_fn=<NegBackward0>) tensor(12333.7129, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12333.5556640625
tensor(12333.7129, grad_fn=<NegBackward0>) tensor(12333.5557, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12333.4072265625
tensor(12333.5557, grad_fn=<NegBackward0>) tensor(12333.4072, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12333.3447265625
tensor(12333.4072, grad_fn=<NegBackward0>) tensor(12333.3447, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12333.3251953125
tensor(12333.3447, grad_fn=<NegBackward0>) tensor(12333.3252, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12333.3193359375
tensor(12333.3252, grad_fn=<NegBackward0>) tensor(12333.3193, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12333.314453125
tensor(12333.3193, grad_fn=<NegBackward0>) tensor(12333.3145, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12333.3125
tensor(12333.3145, grad_fn=<NegBackward0>) tensor(12333.3125, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12333.3115234375
tensor(12333.3125, grad_fn=<NegBackward0>) tensor(12333.3115, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12333.3095703125
tensor(12333.3115, grad_fn=<NegBackward0>) tensor(12333.3096, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12333.3095703125
tensor(12333.3096, grad_fn=<NegBackward0>) tensor(12333.3096, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12333.30859375
tensor(12333.3096, grad_fn=<NegBackward0>) tensor(12333.3086, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12333.30859375
tensor(12333.3086, grad_fn=<NegBackward0>) tensor(12333.3086, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12333.3095703125
tensor(12333.3086, grad_fn=<NegBackward0>) tensor(12333.3096, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12333.3076171875
tensor(12333.3086, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12333.3076171875
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12333.306640625
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12333.306640625
tensor(12333.3066, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12333.3076171875
tensor(12333.3066, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12333.306640625
tensor(12333.3066, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12333.3056640625
tensor(12333.3066, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12333.3056640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12333.3046875
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12333.3037109375
tensor(12333.3047, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12333.3037109375
tensor(12333.3037, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12333.3056640625
tensor(12333.3037, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12333.3037109375
tensor(12333.3037, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12333.302734375
tensor(12333.3037, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12333.3017578125
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12333.3037109375
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12333.30078125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12333.3076171875
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12333.3037109375
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12333.3046875
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12333.298828125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.2988, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12333.3017578125
tensor(12333.2988, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12333.3076171875
tensor(12333.2988, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12333.30078125
tensor(12333.2988, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -12333.30078125
tensor(12333.2988, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -12333.3017578125
tensor(12333.2988, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[9.6646e-01, 3.3542e-02],
        [9.9946e-01, 5.4361e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9229, 0.0771], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.2326],
         [0.7199, 0.2477]],

        [[0.6447, 0.1930],
         [0.5288, 0.5003]],

        [[0.7262, 0.1983],
         [0.7269, 0.6311]],

        [[0.6860, 0.1231],
         [0.6214, 0.6151]],

        [[0.7294, 0.2451],
         [0.5210, 0.5206]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000333334401712826
Average Adjusted Rand Index: 0.0001633280491905654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21232.828125
inf tensor(21232.8281, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12335.0537109375
tensor(21232.8281, grad_fn=<NegBackward0>) tensor(12335.0537, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12334.734375
tensor(12335.0537, grad_fn=<NegBackward0>) tensor(12334.7344, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12334.568359375
tensor(12334.7344, grad_fn=<NegBackward0>) tensor(12334.5684, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12334.3193359375
tensor(12334.5684, grad_fn=<NegBackward0>) tensor(12334.3193, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12333.8486328125
tensor(12334.3193, grad_fn=<NegBackward0>) tensor(12333.8486, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12333.556640625
tensor(12333.8486, grad_fn=<NegBackward0>) tensor(12333.5566, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12333.44921875
tensor(12333.5566, grad_fn=<NegBackward0>) tensor(12333.4492, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12333.3984375
tensor(12333.4492, grad_fn=<NegBackward0>) tensor(12333.3984, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12333.3720703125
tensor(12333.3984, grad_fn=<NegBackward0>) tensor(12333.3721, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12333.3564453125
tensor(12333.3721, grad_fn=<NegBackward0>) tensor(12333.3564, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12333.345703125
tensor(12333.3564, grad_fn=<NegBackward0>) tensor(12333.3457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12333.337890625
tensor(12333.3457, grad_fn=<NegBackward0>) tensor(12333.3379, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12333.333984375
tensor(12333.3379, grad_fn=<NegBackward0>) tensor(12333.3340, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12333.3310546875
tensor(12333.3340, grad_fn=<NegBackward0>) tensor(12333.3311, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12333.3291015625
tensor(12333.3311, grad_fn=<NegBackward0>) tensor(12333.3291, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12333.326171875
tensor(12333.3291, grad_fn=<NegBackward0>) tensor(12333.3262, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12333.32421875
tensor(12333.3262, grad_fn=<NegBackward0>) tensor(12333.3242, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12333.322265625
tensor(12333.3242, grad_fn=<NegBackward0>) tensor(12333.3223, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12333.322265625
tensor(12333.3223, grad_fn=<NegBackward0>) tensor(12333.3223, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12333.3203125
tensor(12333.3223, grad_fn=<NegBackward0>) tensor(12333.3203, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12333.3203125
tensor(12333.3203, grad_fn=<NegBackward0>) tensor(12333.3203, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12333.3173828125
tensor(12333.3203, grad_fn=<NegBackward0>) tensor(12333.3174, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12333.31640625
tensor(12333.3174, grad_fn=<NegBackward0>) tensor(12333.3164, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12333.31640625
tensor(12333.3164, grad_fn=<NegBackward0>) tensor(12333.3164, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12333.3154296875
tensor(12333.3164, grad_fn=<NegBackward0>) tensor(12333.3154, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12333.3154296875
tensor(12333.3154, grad_fn=<NegBackward0>) tensor(12333.3154, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12333.3154296875
tensor(12333.3154, grad_fn=<NegBackward0>) tensor(12333.3154, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12333.3134765625
tensor(12333.3154, grad_fn=<NegBackward0>) tensor(12333.3135, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12333.3134765625
tensor(12333.3135, grad_fn=<NegBackward0>) tensor(12333.3135, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12333.3115234375
tensor(12333.3135, grad_fn=<NegBackward0>) tensor(12333.3115, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12333.3125
tensor(12333.3115, grad_fn=<NegBackward0>) tensor(12333.3125, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -12333.3125
tensor(12333.3115, grad_fn=<NegBackward0>) tensor(12333.3125, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -12333.3115234375
tensor(12333.3115, grad_fn=<NegBackward0>) tensor(12333.3115, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12333.30859375
tensor(12333.3115, grad_fn=<NegBackward0>) tensor(12333.3086, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12333.3115234375
tensor(12333.3086, grad_fn=<NegBackward0>) tensor(12333.3115, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12333.3076171875
tensor(12333.3086, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12333.30859375
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3086, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12333.30859375
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3086, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -12333.3076171875
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12333.3076171875
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12333.3056640625
tensor(12333.3076, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12333.3076171875
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12333.3076171875
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3076, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -12333.3056640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12333.306640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12333.306640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12333.306640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -12333.3056640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12333.306640625
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12333.3046875
tensor(12333.3057, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12333.3046875
tensor(12333.3047, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12333.3046875
tensor(12333.3047, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12333.302734375
tensor(12333.3047, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12333.3046875
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12333.306640625
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3066, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12333.302734375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12333.3037109375
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12333.3017578125
tensor(12333.3027, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12333.302734375
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12333.302734375
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12333.3046875
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12333.328125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3281, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12333.34765625
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3477, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12333.302734375
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12333.3046875
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3047, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12333.3017578125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12333.30078125
tensor(12333.3018, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12333.3056640625
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3057, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12333.302734375
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3027, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12333.3037109375
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3037, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12333.3203125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3203, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -12333.3271484375
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3271, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -12333.30078125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3008, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12333.3017578125
tensor(12333.3008, grad_fn=<NegBackward0>) tensor(12333.3018, grad_fn=<NegBackward0>)
1
pi: tensor([[4.3147e-04, 9.9957e-01],
        [3.3631e-02, 9.6637e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0765, 0.9235], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2474, 0.2329],
         [0.5664, 0.1976]],

        [[0.5233, 0.1930],
         [0.5870, 0.6336]],

        [[0.6914, 0.1983],
         [0.6302, 0.7166]],

        [[0.6125, 0.1231],
         [0.5626, 0.7056]],

        [[0.6619, 0.2450],
         [0.6613, 0.6276]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000333334401712826
Average Adjusted Rand Index: 0.0001633280491905654
[0.000333334401712826, 0.000333334401712826] [0.0001633280491905654, 0.0001633280491905654] [12333.3017578125, 12333.30078125]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11899.646277888682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21401.685546875
inf tensor(21401.6855, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12400.2802734375
tensor(21401.6855, grad_fn=<NegBackward0>) tensor(12400.2803, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12398.52734375
tensor(12400.2803, grad_fn=<NegBackward0>) tensor(12398.5273, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12396.9443359375
tensor(12398.5273, grad_fn=<NegBackward0>) tensor(12396.9443, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12396.6689453125
tensor(12396.9443, grad_fn=<NegBackward0>) tensor(12396.6689, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12396.5341796875
tensor(12396.6689, grad_fn=<NegBackward0>) tensor(12396.5342, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12396.455078125
tensor(12396.5342, grad_fn=<NegBackward0>) tensor(12396.4551, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12396.4033203125
tensor(12396.4551, grad_fn=<NegBackward0>) tensor(12396.4033, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12396.365234375
tensor(12396.4033, grad_fn=<NegBackward0>) tensor(12396.3652, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12396.3291015625
tensor(12396.3652, grad_fn=<NegBackward0>) tensor(12396.3291, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12396.28125
tensor(12396.3291, grad_fn=<NegBackward0>) tensor(12396.2812, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12396.20703125
tensor(12396.2812, grad_fn=<NegBackward0>) tensor(12396.2070, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12396.0419921875
tensor(12396.2070, grad_fn=<NegBackward0>) tensor(12396.0420, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12395.52734375
tensor(12396.0420, grad_fn=<NegBackward0>) tensor(12395.5273, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12394.759765625
tensor(12395.5273, grad_fn=<NegBackward0>) tensor(12394.7598, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12394.521484375
tensor(12394.7598, grad_fn=<NegBackward0>) tensor(12394.5215, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12393.6474609375
tensor(12394.5215, grad_fn=<NegBackward0>) tensor(12393.6475, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11956.8896484375
tensor(12393.6475, grad_fn=<NegBackward0>) tensor(11956.8896, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11906.6337890625
tensor(11956.8896, grad_fn=<NegBackward0>) tensor(11906.6338, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11891.9228515625
tensor(11906.6338, grad_fn=<NegBackward0>) tensor(11891.9229, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11891.865234375
tensor(11891.9229, grad_fn=<NegBackward0>) tensor(11891.8652, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11891.849609375
tensor(11891.8652, grad_fn=<NegBackward0>) tensor(11891.8496, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11891.8408203125
tensor(11891.8496, grad_fn=<NegBackward0>) tensor(11891.8408, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11891.83203125
tensor(11891.8408, grad_fn=<NegBackward0>) tensor(11891.8320, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11890.92578125
tensor(11891.8320, grad_fn=<NegBackward0>) tensor(11890.9258, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11890.9208984375
tensor(11890.9258, grad_fn=<NegBackward0>) tensor(11890.9209, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11890.9091796875
tensor(11890.9209, grad_fn=<NegBackward0>) tensor(11890.9092, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11890.890625
tensor(11890.9092, grad_fn=<NegBackward0>) tensor(11890.8906, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11890.861328125
tensor(11890.8906, grad_fn=<NegBackward0>) tensor(11890.8613, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11890.8583984375
tensor(11890.8613, grad_fn=<NegBackward0>) tensor(11890.8584, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11890.85546875
tensor(11890.8584, grad_fn=<NegBackward0>) tensor(11890.8555, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11890.845703125
tensor(11890.8555, grad_fn=<NegBackward0>) tensor(11890.8457, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11890.84375
tensor(11890.8457, grad_fn=<NegBackward0>) tensor(11890.8438, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11890.84375
tensor(11890.8438, grad_fn=<NegBackward0>) tensor(11890.8438, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11890.841796875
tensor(11890.8438, grad_fn=<NegBackward0>) tensor(11890.8418, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11890.8408203125
tensor(11890.8418, grad_fn=<NegBackward0>) tensor(11890.8408, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11890.8505859375
tensor(11890.8408, grad_fn=<NegBackward0>) tensor(11890.8506, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11890.841796875
tensor(11890.8408, grad_fn=<NegBackward0>) tensor(11890.8418, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11890.8408203125
tensor(11890.8408, grad_fn=<NegBackward0>) tensor(11890.8408, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11890.8388671875
tensor(11890.8408, grad_fn=<NegBackward0>) tensor(11890.8389, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11890.8388671875
tensor(11890.8389, grad_fn=<NegBackward0>) tensor(11890.8389, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11890.8447265625
tensor(11890.8389, grad_fn=<NegBackward0>) tensor(11890.8447, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11890.8388671875
tensor(11890.8389, grad_fn=<NegBackward0>) tensor(11890.8389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11890.837890625
tensor(11890.8389, grad_fn=<NegBackward0>) tensor(11890.8379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11890.837890625
tensor(11890.8379, grad_fn=<NegBackward0>) tensor(11890.8379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11890.837890625
tensor(11890.8379, grad_fn=<NegBackward0>) tensor(11890.8379, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11890.8369140625
tensor(11890.8379, grad_fn=<NegBackward0>) tensor(11890.8369, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11890.8359375
tensor(11890.8369, grad_fn=<NegBackward0>) tensor(11890.8359, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11890.8359375
tensor(11890.8359, grad_fn=<NegBackward0>) tensor(11890.8359, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11890.822265625
tensor(11890.8359, grad_fn=<NegBackward0>) tensor(11890.8223, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11890.7734375
tensor(11890.8223, grad_fn=<NegBackward0>) tensor(11890.7734, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11890.779296875
tensor(11890.7734, grad_fn=<NegBackward0>) tensor(11890.7793, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11890.7734375
tensor(11890.7734, grad_fn=<NegBackward0>) tensor(11890.7734, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11890.7744140625
tensor(11890.7734, grad_fn=<NegBackward0>) tensor(11890.7744, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11890.7763671875
tensor(11890.7734, grad_fn=<NegBackward0>) tensor(11890.7764, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11890.7724609375
tensor(11890.7734, grad_fn=<NegBackward0>) tensor(11890.7725, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11890.7724609375
tensor(11890.7725, grad_fn=<NegBackward0>) tensor(11890.7725, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11890.771484375
tensor(11890.7725, grad_fn=<NegBackward0>) tensor(11890.7715, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11890.771484375
tensor(11890.7715, grad_fn=<NegBackward0>) tensor(11890.7715, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11890.7705078125
tensor(11890.7715, grad_fn=<NegBackward0>) tensor(11890.7705, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11890.7705078125
tensor(11890.7705, grad_fn=<NegBackward0>) tensor(11890.7705, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11890.7705078125
tensor(11890.7705, grad_fn=<NegBackward0>) tensor(11890.7705, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11890.765625
tensor(11890.7705, grad_fn=<NegBackward0>) tensor(11890.7656, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11890.7587890625
tensor(11890.7656, grad_fn=<NegBackward0>) tensor(11890.7588, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11890.7607421875
tensor(11890.7588, grad_fn=<NegBackward0>) tensor(11890.7607, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11890.7568359375
tensor(11890.7588, grad_fn=<NegBackward0>) tensor(11890.7568, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11890.7578125
tensor(11890.7568, grad_fn=<NegBackward0>) tensor(11890.7578, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11890.7568359375
tensor(11890.7568, grad_fn=<NegBackward0>) tensor(11890.7568, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11890.755859375
tensor(11890.7568, grad_fn=<NegBackward0>) tensor(11890.7559, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11890.7568359375
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7568, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11890.755859375
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7559, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11890.7568359375
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7568, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11890.755859375
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7559, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11890.755859375
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7559, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11890.759765625
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7598, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11890.7421875
tensor(11890.7559, grad_fn=<NegBackward0>) tensor(11890.7422, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11890.794921875
tensor(11890.7422, grad_fn=<NegBackward0>) tensor(11890.7949, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11890.7412109375
tensor(11890.7422, grad_fn=<NegBackward0>) tensor(11890.7412, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11890.7421875
tensor(11890.7412, grad_fn=<NegBackward0>) tensor(11890.7422, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11890.7412109375
tensor(11890.7412, grad_fn=<NegBackward0>) tensor(11890.7412, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11890.7509765625
tensor(11890.7412, grad_fn=<NegBackward0>) tensor(11890.7510, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11890.7431640625
tensor(11890.7412, grad_fn=<NegBackward0>) tensor(11890.7432, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11890.7470703125
tensor(11890.7412, grad_fn=<NegBackward0>) tensor(11890.7471, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -11890.7294921875
tensor(11890.7412, grad_fn=<NegBackward0>) tensor(11890.7295, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11890.728515625
tensor(11890.7295, grad_fn=<NegBackward0>) tensor(11890.7285, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11890.7294921875
tensor(11890.7285, grad_fn=<NegBackward0>) tensor(11890.7295, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11890.7294921875
tensor(11890.7285, grad_fn=<NegBackward0>) tensor(11890.7295, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11890.7275390625
tensor(11890.7285, grad_fn=<NegBackward0>) tensor(11890.7275, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11890.728515625
tensor(11890.7275, grad_fn=<NegBackward0>) tensor(11890.7285, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11890.7294921875
tensor(11890.7275, grad_fn=<NegBackward0>) tensor(11890.7295, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11890.728515625
tensor(11890.7275, grad_fn=<NegBackward0>) tensor(11890.7285, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -11890.7294921875
tensor(11890.7275, grad_fn=<NegBackward0>) tensor(11890.7295, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -11890.734375
tensor(11890.7275, grad_fn=<NegBackward0>) tensor(11890.7344, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.2246, 0.7754],
        [0.7460, 0.2540]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4676, 0.5324], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2982, 0.0982],
         [0.6938, 0.2995]],

        [[0.5536, 0.0957],
         [0.5505, 0.7120]],

        [[0.5303, 0.1066],
         [0.5097, 0.6187]],

        [[0.7197, 0.0998],
         [0.5786, 0.6432]],

        [[0.6284, 0.0991],
         [0.5938, 0.5231]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.039714653550038004
Average Adjusted Rand Index: 0.9759942934046396
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24592.630859375
inf tensor(24592.6309, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12127.53125
tensor(24592.6309, grad_fn=<NegBackward0>) tensor(12127.5312, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12013.8642578125
tensor(12127.5312, grad_fn=<NegBackward0>) tensor(12013.8643, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11948.359375
tensor(12013.8643, grad_fn=<NegBackward0>) tensor(11948.3594, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11937.2998046875
tensor(11948.3594, grad_fn=<NegBackward0>) tensor(11937.2998, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11937.2138671875
tensor(11937.2998, grad_fn=<NegBackward0>) tensor(11937.2139, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11932.1982421875
tensor(11937.2139, grad_fn=<NegBackward0>) tensor(11932.1982, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11932.1708984375
tensor(11932.1982, grad_fn=<NegBackward0>) tensor(11932.1709, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11932.1552734375
tensor(11932.1709, grad_fn=<NegBackward0>) tensor(11932.1553, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11932.1435546875
tensor(11932.1553, grad_fn=<NegBackward0>) tensor(11932.1436, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11932.1337890625
tensor(11932.1436, grad_fn=<NegBackward0>) tensor(11932.1338, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11932.1279296875
tensor(11932.1338, grad_fn=<NegBackward0>) tensor(11932.1279, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11932.123046875
tensor(11932.1279, grad_fn=<NegBackward0>) tensor(11932.1230, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11932.1181640625
tensor(11932.1230, grad_fn=<NegBackward0>) tensor(11932.1182, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11932.1162109375
tensor(11932.1182, grad_fn=<NegBackward0>) tensor(11932.1162, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11932.11328125
tensor(11932.1162, grad_fn=<NegBackward0>) tensor(11932.1133, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11932.109375
tensor(11932.1133, grad_fn=<NegBackward0>) tensor(11932.1094, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11932.1123046875
tensor(11932.1094, grad_fn=<NegBackward0>) tensor(11932.1123, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -11932.1162109375
tensor(11932.1094, grad_fn=<NegBackward0>) tensor(11932.1162, grad_fn=<NegBackward0>)
2
Iteration 1900: Loss = -11932.1044921875
tensor(11932.1094, grad_fn=<NegBackward0>) tensor(11932.1045, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11932.1025390625
tensor(11932.1045, grad_fn=<NegBackward0>) tensor(11932.1025, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11932.1015625
tensor(11932.1025, grad_fn=<NegBackward0>) tensor(11932.1016, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11932.099609375
tensor(11932.1016, grad_fn=<NegBackward0>) tensor(11932.0996, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11932.0966796875
tensor(11932.0996, grad_fn=<NegBackward0>) tensor(11932.0967, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11932.0947265625
tensor(11932.0967, grad_fn=<NegBackward0>) tensor(11932.0947, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11932.0947265625
tensor(11932.0947, grad_fn=<NegBackward0>) tensor(11932.0947, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11932.0927734375
tensor(11932.0947, grad_fn=<NegBackward0>) tensor(11932.0928, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11932.095703125
tensor(11932.0928, grad_fn=<NegBackward0>) tensor(11932.0957, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11932.091796875
tensor(11932.0928, grad_fn=<NegBackward0>) tensor(11932.0918, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11932.091796875
tensor(11932.0918, grad_fn=<NegBackward0>) tensor(11932.0918, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11932.095703125
tensor(11932.0918, grad_fn=<NegBackward0>) tensor(11932.0957, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11932.0908203125
tensor(11932.0918, grad_fn=<NegBackward0>) tensor(11932.0908, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11932.0888671875
tensor(11932.0908, grad_fn=<NegBackward0>) tensor(11932.0889, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11932.08984375
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0898, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11932.0888671875
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0889, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11932.0888671875
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0889, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11932.08984375
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0898, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11932.0888671875
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0889, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11932.091796875
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0918, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11932.08984375
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0898, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11932.0869140625
tensor(11932.0889, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11932.0888671875
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0889, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11932.0908203125
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0908, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11932.087890625
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0879, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -11932.08984375
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0898, grad_fn=<NegBackward0>)
4
Iteration 4500: Loss = -11932.0869140625
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11932.0869140625
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11932.0927734375
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0928, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11932.087890625
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0879, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11932.0908203125
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0908, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11932.0869140625
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11932.087890625
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0879, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11932.0859375
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11932.0859375
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11932.0869140625
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11932.0869140625
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11932.0888671875
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0889, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11932.0869140625
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -11932.0859375
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11932.0859375
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11932.0859375
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11932.0859375
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11932.083984375
tensor(11932.0859, grad_fn=<NegBackward0>) tensor(11932.0840, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11932.0849609375
tensor(11932.0840, grad_fn=<NegBackward0>) tensor(11932.0850, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11932.0859375
tensor(11932.0840, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11932.0859375
tensor(11932.0840, grad_fn=<NegBackward0>) tensor(11932.0859, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11932.0849609375
tensor(11932.0840, grad_fn=<NegBackward0>) tensor(11932.0850, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11932.0849609375
tensor(11932.0840, grad_fn=<NegBackward0>) tensor(11932.0850, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.4244, 0.5756],
        [0.6766, 0.3234]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5351, 0.4649], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2961, 0.0982],
         [0.7117, 0.3018]],

        [[0.6551, 0.0952],
         [0.7199, 0.6704]],

        [[0.7187, 0.1068],
         [0.7257, 0.6258]],

        [[0.5471, 0.0998],
         [0.5572, 0.5328]],

        [[0.5835, 0.0991],
         [0.7181, 0.6739]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.03644021351317708
Average Adjusted Rand Index: 0.9759942934046396
[0.039714653550038004, 0.03644021351317708] [0.9759942934046396, 0.9759942934046396] [11890.734375, 11932.0849609375]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11933.77418705235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22024.82421875
inf tensor(22024.8242, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12495.2578125
tensor(22024.8242, grad_fn=<NegBackward0>) tensor(12495.2578, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12494.6611328125
tensor(12495.2578, grad_fn=<NegBackward0>) tensor(12494.6611, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12494.1220703125
tensor(12494.6611, grad_fn=<NegBackward0>) tensor(12494.1221, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12493.048828125
tensor(12494.1221, grad_fn=<NegBackward0>) tensor(12493.0488, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12491.783203125
tensor(12493.0488, grad_fn=<NegBackward0>) tensor(12491.7832, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12491.2431640625
tensor(12491.7832, grad_fn=<NegBackward0>) tensor(12491.2432, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12490.9306640625
tensor(12491.2432, grad_fn=<NegBackward0>) tensor(12490.9307, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12490.7314453125
tensor(12490.9307, grad_fn=<NegBackward0>) tensor(12490.7314, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12490.5595703125
tensor(12490.7314, grad_fn=<NegBackward0>) tensor(12490.5596, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12490.3740234375
tensor(12490.5596, grad_fn=<NegBackward0>) tensor(12490.3740, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12490.1240234375
tensor(12490.3740, grad_fn=<NegBackward0>) tensor(12490.1240, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12489.5380859375
tensor(12490.1240, grad_fn=<NegBackward0>) tensor(12489.5381, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12143.517578125
tensor(12489.5381, grad_fn=<NegBackward0>) tensor(12143.5176, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12124.8291015625
tensor(12143.5176, grad_fn=<NegBackward0>) tensor(12124.8291, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12101.5048828125
tensor(12124.8291, grad_fn=<NegBackward0>) tensor(12101.5049, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12052.591796875
tensor(12101.5049, grad_fn=<NegBackward0>) tensor(12052.5918, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12023.8388671875
tensor(12052.5918, grad_fn=<NegBackward0>) tensor(12023.8389, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12023.609375
tensor(12023.8389, grad_fn=<NegBackward0>) tensor(12023.6094, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12019.46875
tensor(12023.6094, grad_fn=<NegBackward0>) tensor(12019.4688, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12008.9658203125
tensor(12019.4688, grad_fn=<NegBackward0>) tensor(12008.9658, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11983.380859375
tensor(12008.9658, grad_fn=<NegBackward0>) tensor(11983.3809, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11975.7060546875
tensor(11983.3809, grad_fn=<NegBackward0>) tensor(11975.7061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11975.6923828125
tensor(11975.7061, grad_fn=<NegBackward0>) tensor(11975.6924, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11975.6640625
tensor(11975.6924, grad_fn=<NegBackward0>) tensor(11975.6641, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11975.64453125
tensor(11975.6641, grad_fn=<NegBackward0>) tensor(11975.6445, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11962.859375
tensor(11975.6445, grad_fn=<NegBackward0>) tensor(11962.8594, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11962.84765625
tensor(11962.8594, grad_fn=<NegBackward0>) tensor(11962.8477, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11962.841796875
tensor(11962.8477, grad_fn=<NegBackward0>) tensor(11962.8418, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11962.8359375
tensor(11962.8418, grad_fn=<NegBackward0>) tensor(11962.8359, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11962.83203125
tensor(11962.8359, grad_fn=<NegBackward0>) tensor(11962.8320, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11962.826171875
tensor(11962.8320, grad_fn=<NegBackward0>) tensor(11962.8262, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11962.8212890625
tensor(11962.8262, grad_fn=<NegBackward0>) tensor(11962.8213, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11962.818359375
tensor(11962.8213, grad_fn=<NegBackward0>) tensor(11962.8184, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11962.81640625
tensor(11962.8184, grad_fn=<NegBackward0>) tensor(11962.8164, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11962.822265625
tensor(11962.8164, grad_fn=<NegBackward0>) tensor(11962.8223, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11962.810546875
tensor(11962.8164, grad_fn=<NegBackward0>) tensor(11962.8105, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11962.8115234375
tensor(11962.8105, grad_fn=<NegBackward0>) tensor(11962.8115, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11962.806640625
tensor(11962.8105, grad_fn=<NegBackward0>) tensor(11962.8066, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11962.8046875
tensor(11962.8066, grad_fn=<NegBackward0>) tensor(11962.8047, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11962.8017578125
tensor(11962.8047, grad_fn=<NegBackward0>) tensor(11962.8018, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11962.7958984375
tensor(11962.8018, grad_fn=<NegBackward0>) tensor(11962.7959, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11962.794921875
tensor(11962.7959, grad_fn=<NegBackward0>) tensor(11962.7949, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11962.787109375
tensor(11962.7949, grad_fn=<NegBackward0>) tensor(11962.7871, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11962.7861328125
tensor(11962.7871, grad_fn=<NegBackward0>) tensor(11962.7861, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11962.78515625
tensor(11962.7861, grad_fn=<NegBackward0>) tensor(11962.7852, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11962.7841796875
tensor(11962.7852, grad_fn=<NegBackward0>) tensor(11962.7842, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11962.783203125
tensor(11962.7842, grad_fn=<NegBackward0>) tensor(11962.7832, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11962.7822265625
tensor(11962.7832, grad_fn=<NegBackward0>) tensor(11962.7822, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11962.78125
tensor(11962.7822, grad_fn=<NegBackward0>) tensor(11962.7812, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11962.7861328125
tensor(11962.7812, grad_fn=<NegBackward0>) tensor(11962.7861, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11962.7802734375
tensor(11962.7812, grad_fn=<NegBackward0>) tensor(11962.7803, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11962.7861328125
tensor(11962.7803, grad_fn=<NegBackward0>) tensor(11962.7861, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11962.779296875
tensor(11962.7803, grad_fn=<NegBackward0>) tensor(11962.7793, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11962.7841796875
tensor(11962.7793, grad_fn=<NegBackward0>) tensor(11962.7842, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11962.779296875
tensor(11962.7793, grad_fn=<NegBackward0>) tensor(11962.7793, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11962.78515625
tensor(11962.7793, grad_fn=<NegBackward0>) tensor(11962.7852, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11962.77734375
tensor(11962.7793, grad_fn=<NegBackward0>) tensor(11962.7773, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11962.77734375
tensor(11962.7773, grad_fn=<NegBackward0>) tensor(11962.7773, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11962.77734375
tensor(11962.7773, grad_fn=<NegBackward0>) tensor(11962.7773, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11962.7763671875
tensor(11962.7773, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11962.7763671875
tensor(11962.7764, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11962.7763671875
tensor(11962.7764, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11962.7763671875
tensor(11962.7764, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11962.7744140625
tensor(11962.7764, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11962.791015625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7910, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11962.7744140625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11962.7744140625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11962.775390625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11962.7734375
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11962.77734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7773, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11962.7744140625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11962.77734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7773, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11962.7734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11962.775390625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11962.7724609375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7725, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11962.7734375
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11962.8662109375
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.8662, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11962.771484375
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11962.7734375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11962.84375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.8438, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11962.7744140625
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11962.7763671875
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11962.79296875
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7930, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11962.7705078125
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7705, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11962.7734375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11962.7724609375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7725, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11962.779296875
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7793, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11962.7744140625
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -11962.7958984375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7959, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.5489, 0.4511],
        [0.3000, 0.7000]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5724, 0.4276], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3026, 0.0903],
         [0.6623, 0.3053]],

        [[0.7282, 0.1004],
         [0.6492, 0.5471]],

        [[0.6895, 0.0967],
         [0.7203, 0.6177]],

        [[0.6466, 0.0989],
         [0.5243, 0.5881]],

        [[0.5010, 0.0997],
         [0.6365, 0.6361]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.35851391050410975
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19880.943359375
inf tensor(19880.9434, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12494.658203125
tensor(19880.9434, grad_fn=<NegBackward0>) tensor(12494.6582, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12493.8173828125
tensor(12494.6582, grad_fn=<NegBackward0>) tensor(12493.8174, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12493.0498046875
tensor(12493.8174, grad_fn=<NegBackward0>) tensor(12493.0498, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12492.21484375
tensor(12493.0498, grad_fn=<NegBackward0>) tensor(12492.2148, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12491.255859375
tensor(12492.2148, grad_fn=<NegBackward0>) tensor(12491.2559, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12490.7451171875
tensor(12491.2559, grad_fn=<NegBackward0>) tensor(12490.7451, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12490.474609375
tensor(12490.7451, grad_fn=<NegBackward0>) tensor(12490.4746, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12490.15625
tensor(12490.4746, grad_fn=<NegBackward0>) tensor(12490.1562, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12488.900390625
tensor(12490.1562, grad_fn=<NegBackward0>) tensor(12488.9004, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12125.6455078125
tensor(12488.9004, grad_fn=<NegBackward0>) tensor(12125.6455, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12122.0986328125
tensor(12125.6455, grad_fn=<NegBackward0>) tensor(12122.0986, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12102.62109375
tensor(12122.0986, grad_fn=<NegBackward0>) tensor(12102.6211, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12040.9150390625
tensor(12102.6211, grad_fn=<NegBackward0>) tensor(12040.9150, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11963.2060546875
tensor(12040.9150, grad_fn=<NegBackward0>) tensor(11963.2061, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11962.986328125
tensor(11963.2061, grad_fn=<NegBackward0>) tensor(11962.9863, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11962.9267578125
tensor(11962.9863, grad_fn=<NegBackward0>) tensor(11962.9268, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11962.89453125
tensor(11962.9268, grad_fn=<NegBackward0>) tensor(11962.8945, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11962.8720703125
tensor(11962.8945, grad_fn=<NegBackward0>) tensor(11962.8721, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11962.857421875
tensor(11962.8721, grad_fn=<NegBackward0>) tensor(11962.8574, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11962.8447265625
tensor(11962.8574, grad_fn=<NegBackward0>) tensor(11962.8447, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11962.8359375
tensor(11962.8447, grad_fn=<NegBackward0>) tensor(11962.8359, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11962.8291015625
tensor(11962.8359, grad_fn=<NegBackward0>) tensor(11962.8291, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11962.8203125
tensor(11962.8291, grad_fn=<NegBackward0>) tensor(11962.8203, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11962.81640625
tensor(11962.8203, grad_fn=<NegBackward0>) tensor(11962.8164, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11962.8115234375
tensor(11962.8164, grad_fn=<NegBackward0>) tensor(11962.8115, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11962.80859375
tensor(11962.8115, grad_fn=<NegBackward0>) tensor(11962.8086, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11962.8037109375
tensor(11962.8086, grad_fn=<NegBackward0>) tensor(11962.8037, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11962.8017578125
tensor(11962.8037, grad_fn=<NegBackward0>) tensor(11962.8018, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11962.798828125
tensor(11962.8018, grad_fn=<NegBackward0>) tensor(11962.7988, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11962.796875
tensor(11962.7988, grad_fn=<NegBackward0>) tensor(11962.7969, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11962.7958984375
tensor(11962.7969, grad_fn=<NegBackward0>) tensor(11962.7959, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11962.7919921875
tensor(11962.7959, grad_fn=<NegBackward0>) tensor(11962.7920, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11962.791015625
tensor(11962.7920, grad_fn=<NegBackward0>) tensor(11962.7910, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11962.7900390625
tensor(11962.7910, grad_fn=<NegBackward0>) tensor(11962.7900, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11962.791015625
tensor(11962.7900, grad_fn=<NegBackward0>) tensor(11962.7910, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11962.787109375
tensor(11962.7900, grad_fn=<NegBackward0>) tensor(11962.7871, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11962.787109375
tensor(11962.7871, grad_fn=<NegBackward0>) tensor(11962.7871, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11962.7861328125
tensor(11962.7871, grad_fn=<NegBackward0>) tensor(11962.7861, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11962.78515625
tensor(11962.7861, grad_fn=<NegBackward0>) tensor(11962.7852, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11962.7958984375
tensor(11962.7852, grad_fn=<NegBackward0>) tensor(11962.7959, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11962.7841796875
tensor(11962.7852, grad_fn=<NegBackward0>) tensor(11962.7842, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11962.783203125
tensor(11962.7842, grad_fn=<NegBackward0>) tensor(11962.7832, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11962.783203125
tensor(11962.7832, grad_fn=<NegBackward0>) tensor(11962.7832, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11962.7822265625
tensor(11962.7832, grad_fn=<NegBackward0>) tensor(11962.7822, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11962.78125
tensor(11962.7822, grad_fn=<NegBackward0>) tensor(11962.7812, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11962.7802734375
tensor(11962.7812, grad_fn=<NegBackward0>) tensor(11962.7803, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11962.7802734375
tensor(11962.7803, grad_fn=<NegBackward0>) tensor(11962.7803, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11962.7783203125
tensor(11962.7803, grad_fn=<NegBackward0>) tensor(11962.7783, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11962.783203125
tensor(11962.7783, grad_fn=<NegBackward0>) tensor(11962.7832, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11962.7763671875
tensor(11962.7783, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11962.7763671875
tensor(11962.7764, grad_fn=<NegBackward0>) tensor(11962.7764, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11962.775390625
tensor(11962.7764, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11962.775390625
tensor(11962.7754, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11962.77734375
tensor(11962.7754, grad_fn=<NegBackward0>) tensor(11962.7773, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11962.775390625
tensor(11962.7754, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11962.7744140625
tensor(11962.7754, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11962.775390625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11962.775390625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -11962.775390625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -11962.7744140625
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11962.7734375
tensor(11962.7744, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11962.7822265625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7822, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11962.7734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11962.7734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11962.7822265625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7822, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11962.775390625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7754, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11962.7822265625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7822, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11962.7734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11962.7734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11962.8583984375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.8584, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11962.7734375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7734, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11962.7744140625
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7744, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11962.7724609375
tensor(11962.7734, grad_fn=<NegBackward0>) tensor(11962.7725, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11962.80859375
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.8086, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11962.7724609375
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.7725, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11962.779296875
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.7793, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11962.771484375
tensor(11962.7725, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11962.7724609375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7725, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11962.771484375
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11962.7705078125
tensor(11962.7715, grad_fn=<NegBackward0>) tensor(11962.7705, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11962.771484375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11962.7705078125
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7705, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11962.8466796875
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.8467, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11962.771484375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11962.771484375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11962.771484375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7715, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11962.7724609375
tensor(11962.7705, grad_fn=<NegBackward0>) tensor(11962.7725, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.6986, 0.3014],
        [0.4535, 0.5465]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4293, 0.5707], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3042, 0.0903],
         [0.7217, 0.3035]],

        [[0.6170, 0.1010],
         [0.5706, 0.5946]],

        [[0.5275, 0.0967],
         [0.5187, 0.5153]],

        [[0.6753, 0.0989],
         [0.5825, 0.5247]],

        [[0.7295, 0.0997],
         [0.7212, 0.6020]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.35851391050410975
Average Adjusted Rand Index: 1.0
[0.35851391050410975, 0.35851391050410975] [1.0, 1.0] [11962.7958984375, 11962.7724609375]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11769.134415471859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20835.03125
inf tensor(20835.0312, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12276.6865234375
tensor(20835.0312, grad_fn=<NegBackward0>) tensor(12276.6865, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12276.30078125
tensor(12276.6865, grad_fn=<NegBackward0>) tensor(12276.3008, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12276.189453125
tensor(12276.3008, grad_fn=<NegBackward0>) tensor(12276.1895, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12276.1279296875
tensor(12276.1895, grad_fn=<NegBackward0>) tensor(12276.1279, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12276.0859375
tensor(12276.1279, grad_fn=<NegBackward0>) tensor(12276.0859, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12276.0576171875
tensor(12276.0859, grad_fn=<NegBackward0>) tensor(12276.0576, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12276.0380859375
tensor(12276.0576, grad_fn=<NegBackward0>) tensor(12276.0381, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12276.01953125
tensor(12276.0381, grad_fn=<NegBackward0>) tensor(12276.0195, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12276.0029296875
tensor(12276.0195, grad_fn=<NegBackward0>) tensor(12276.0029, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12275.98828125
tensor(12276.0029, grad_fn=<NegBackward0>) tensor(12275.9883, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12275.9697265625
tensor(12275.9883, grad_fn=<NegBackward0>) tensor(12275.9697, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12275.94921875
tensor(12275.9697, grad_fn=<NegBackward0>) tensor(12275.9492, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12275.921875
tensor(12275.9492, grad_fn=<NegBackward0>) tensor(12275.9219, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12275.88671875
tensor(12275.9219, grad_fn=<NegBackward0>) tensor(12275.8867, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12275.8447265625
tensor(12275.8867, grad_fn=<NegBackward0>) tensor(12275.8447, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12275.791015625
tensor(12275.8447, grad_fn=<NegBackward0>) tensor(12275.7910, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12275.728515625
tensor(12275.7910, grad_fn=<NegBackward0>) tensor(12275.7285, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12275.6708984375
tensor(12275.7285, grad_fn=<NegBackward0>) tensor(12275.6709, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12275.626953125
tensor(12275.6709, grad_fn=<NegBackward0>) tensor(12275.6270, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12275.595703125
tensor(12275.6270, grad_fn=<NegBackward0>) tensor(12275.5957, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12275.5751953125
tensor(12275.5957, grad_fn=<NegBackward0>) tensor(12275.5752, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12275.5615234375
tensor(12275.5752, grad_fn=<NegBackward0>) tensor(12275.5615, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12275.5537109375
tensor(12275.5615, grad_fn=<NegBackward0>) tensor(12275.5537, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12275.5478515625
tensor(12275.5537, grad_fn=<NegBackward0>) tensor(12275.5479, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12275.5439453125
tensor(12275.5479, grad_fn=<NegBackward0>) tensor(12275.5439, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12275.541015625
tensor(12275.5439, grad_fn=<NegBackward0>) tensor(12275.5410, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12275.537109375
tensor(12275.5410, grad_fn=<NegBackward0>) tensor(12275.5371, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12275.53515625
tensor(12275.5371, grad_fn=<NegBackward0>) tensor(12275.5352, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12275.5322265625
tensor(12275.5352, grad_fn=<NegBackward0>) tensor(12275.5322, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12275.5302734375
tensor(12275.5322, grad_fn=<NegBackward0>) tensor(12275.5303, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12275.529296875
tensor(12275.5303, grad_fn=<NegBackward0>) tensor(12275.5293, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12275.525390625
tensor(12275.5293, grad_fn=<NegBackward0>) tensor(12275.5254, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12275.521484375
tensor(12275.5254, grad_fn=<NegBackward0>) tensor(12275.5215, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12275.5185546875
tensor(12275.5215, grad_fn=<NegBackward0>) tensor(12275.5186, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12275.5126953125
tensor(12275.5186, grad_fn=<NegBackward0>) tensor(12275.5127, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12275.5048828125
tensor(12275.5127, grad_fn=<NegBackward0>) tensor(12275.5049, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12275.49609375
tensor(12275.5049, grad_fn=<NegBackward0>) tensor(12275.4961, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12275.486328125
tensor(12275.4961, grad_fn=<NegBackward0>) tensor(12275.4863, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12275.4775390625
tensor(12275.4863, grad_fn=<NegBackward0>) tensor(12275.4775, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12275.46875
tensor(12275.4775, grad_fn=<NegBackward0>) tensor(12275.4688, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12275.4599609375
tensor(12275.4688, grad_fn=<NegBackward0>) tensor(12275.4600, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12275.455078125
tensor(12275.4600, grad_fn=<NegBackward0>) tensor(12275.4551, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12275.4736328125
tensor(12275.4551, grad_fn=<NegBackward0>) tensor(12275.4736, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12275.4453125
tensor(12275.4551, grad_fn=<NegBackward0>) tensor(12275.4453, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12275.4423828125
tensor(12275.4453, grad_fn=<NegBackward0>) tensor(12275.4424, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12275.4404296875
tensor(12275.4424, grad_fn=<NegBackward0>) tensor(12275.4404, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12275.4384765625
tensor(12275.4404, grad_fn=<NegBackward0>) tensor(12275.4385, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12275.4462890625
tensor(12275.4385, grad_fn=<NegBackward0>) tensor(12275.4463, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12275.435546875
tensor(12275.4385, grad_fn=<NegBackward0>) tensor(12275.4355, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12275.435546875
tensor(12275.4355, grad_fn=<NegBackward0>) tensor(12275.4355, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12275.4345703125
tensor(12275.4355, grad_fn=<NegBackward0>) tensor(12275.4346, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12275.43359375
tensor(12275.4346, grad_fn=<NegBackward0>) tensor(12275.4336, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12275.46484375
tensor(12275.4336, grad_fn=<NegBackward0>) tensor(12275.4648, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12275.4326171875
tensor(12275.4336, grad_fn=<NegBackward0>) tensor(12275.4326, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12275.4326171875
tensor(12275.4326, grad_fn=<NegBackward0>) tensor(12275.4326, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12275.4375
tensor(12275.4326, grad_fn=<NegBackward0>) tensor(12275.4375, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12275.431640625
tensor(12275.4326, grad_fn=<NegBackward0>) tensor(12275.4316, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12275.4306640625
tensor(12275.4316, grad_fn=<NegBackward0>) tensor(12275.4307, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12275.4326171875
tensor(12275.4307, grad_fn=<NegBackward0>) tensor(12275.4326, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12275.4296875
tensor(12275.4307, grad_fn=<NegBackward0>) tensor(12275.4297, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12275.427734375
tensor(12275.4297, grad_fn=<NegBackward0>) tensor(12275.4277, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12275.4287109375
tensor(12275.4277, grad_fn=<NegBackward0>) tensor(12275.4287, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12275.4267578125
tensor(12275.4277, grad_fn=<NegBackward0>) tensor(12275.4268, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12275.4248046875
tensor(12275.4268, grad_fn=<NegBackward0>) tensor(12275.4248, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12275.421875
tensor(12275.4248, grad_fn=<NegBackward0>) tensor(12275.4219, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12275.4140625
tensor(12275.4219, grad_fn=<NegBackward0>) tensor(12275.4141, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12275.3984375
tensor(12275.4141, grad_fn=<NegBackward0>) tensor(12275.3984, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12275.376953125
tensor(12275.3984, grad_fn=<NegBackward0>) tensor(12275.3770, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12275.353515625
tensor(12275.3770, grad_fn=<NegBackward0>) tensor(12275.3535, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12274.2705078125
tensor(12275.3535, grad_fn=<NegBackward0>) tensor(12274.2705, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12273.833984375
tensor(12274.2705, grad_fn=<NegBackward0>) tensor(12273.8340, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12273.8427734375
tensor(12273.8340, grad_fn=<NegBackward0>) tensor(12273.8428, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12273.7998046875
tensor(12273.8340, grad_fn=<NegBackward0>) tensor(12273.7998, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12273.814453125
tensor(12273.7998, grad_fn=<NegBackward0>) tensor(12273.8145, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12273.791015625
tensor(12273.7998, grad_fn=<NegBackward0>) tensor(12273.7910, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12273.7880859375
tensor(12273.7910, grad_fn=<NegBackward0>) tensor(12273.7881, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12273.7958984375
tensor(12273.7881, grad_fn=<NegBackward0>) tensor(12273.7959, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12273.7861328125
tensor(12273.7881, grad_fn=<NegBackward0>) tensor(12273.7861, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12273.78515625
tensor(12273.7861, grad_fn=<NegBackward0>) tensor(12273.7852, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12273.78515625
tensor(12273.7852, grad_fn=<NegBackward0>) tensor(12273.7852, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12273.7841796875
tensor(12273.7852, grad_fn=<NegBackward0>) tensor(12273.7842, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12273.7939453125
tensor(12273.7842, grad_fn=<NegBackward0>) tensor(12273.7939, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12273.7822265625
tensor(12273.7842, grad_fn=<NegBackward0>) tensor(12273.7822, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12273.8037109375
tensor(12273.7822, grad_fn=<NegBackward0>) tensor(12273.8037, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12273.78125
tensor(12273.7822, grad_fn=<NegBackward0>) tensor(12273.7812, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12273.78125
tensor(12273.7812, grad_fn=<NegBackward0>) tensor(12273.7812, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12273.78125
tensor(12273.7812, grad_fn=<NegBackward0>) tensor(12273.7812, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12273.78125
tensor(12273.7812, grad_fn=<NegBackward0>) tensor(12273.7812, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12273.9365234375
tensor(12273.7812, grad_fn=<NegBackward0>) tensor(12273.9365, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12273.7802734375
tensor(12273.7812, grad_fn=<NegBackward0>) tensor(12273.7803, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12273.7841796875
tensor(12273.7803, grad_fn=<NegBackward0>) tensor(12273.7842, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12273.794921875
tensor(12273.7803, grad_fn=<NegBackward0>) tensor(12273.7949, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -12273.7880859375
tensor(12273.7803, grad_fn=<NegBackward0>) tensor(12273.7881, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -12273.7841796875
tensor(12273.7803, grad_fn=<NegBackward0>) tensor(12273.7842, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -12273.7802734375
tensor(12273.7803, grad_fn=<NegBackward0>) tensor(12273.7803, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12273.779296875
tensor(12273.7803, grad_fn=<NegBackward0>) tensor(12273.7793, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12273.9736328125
tensor(12273.7793, grad_fn=<NegBackward0>) tensor(12273.9736, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12273.7802734375
tensor(12273.7793, grad_fn=<NegBackward0>) tensor(12273.7803, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12273.81640625
tensor(12273.7793, grad_fn=<NegBackward0>) tensor(12273.8164, grad_fn=<NegBackward0>)
3
pi: tensor([[1.1948e-05, 9.9999e-01],
        [9.9999e-01, 9.3632e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9565, 0.0435], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.1449],
         [0.7272, 0.1943]],

        [[0.5844, 0.2432],
         [0.6437, 0.5426]],

        [[0.6863, 0.1797],
         [0.5358, 0.6318]],

        [[0.7227, 0.1595],
         [0.6450, 0.5233]],

        [[0.7309, 0.1639],
         [0.5543, 0.7007]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.008260141115681973
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015539260913902783
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
Global Adjusted Rand Index: -0.0009467101632148221
Average Adjusted Rand Index: -0.0018717581893426948
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23087.28515625
inf tensor(23087.2852, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12276.7509765625
tensor(23087.2852, grad_fn=<NegBackward0>) tensor(12276.7510, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12276.24609375
tensor(12276.7510, grad_fn=<NegBackward0>) tensor(12276.2461, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12276.1533203125
tensor(12276.2461, grad_fn=<NegBackward0>) tensor(12276.1533, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12276.1015625
tensor(12276.1533, grad_fn=<NegBackward0>) tensor(12276.1016, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12276.0673828125
tensor(12276.1016, grad_fn=<NegBackward0>) tensor(12276.0674, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12276.0361328125
tensor(12276.0674, grad_fn=<NegBackward0>) tensor(12276.0361, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12276.0068359375
tensor(12276.0361, grad_fn=<NegBackward0>) tensor(12276.0068, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12275.97265625
tensor(12276.0068, grad_fn=<NegBackward0>) tensor(12275.9727, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12275.9267578125
tensor(12275.9727, grad_fn=<NegBackward0>) tensor(12275.9268, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12275.89453125
tensor(12275.9268, grad_fn=<NegBackward0>) tensor(12275.8945, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12275.8779296875
tensor(12275.8945, grad_fn=<NegBackward0>) tensor(12275.8779, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12275.8623046875
tensor(12275.8779, grad_fn=<NegBackward0>) tensor(12275.8623, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12275.8505859375
tensor(12275.8623, grad_fn=<NegBackward0>) tensor(12275.8506, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12275.8349609375
tensor(12275.8506, grad_fn=<NegBackward0>) tensor(12275.8350, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12275.8212890625
tensor(12275.8350, grad_fn=<NegBackward0>) tensor(12275.8213, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12275.8095703125
tensor(12275.8213, grad_fn=<NegBackward0>) tensor(12275.8096, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12275.796875
tensor(12275.8096, grad_fn=<NegBackward0>) tensor(12275.7969, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12275.783203125
tensor(12275.7969, grad_fn=<NegBackward0>) tensor(12275.7832, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12275.771484375
tensor(12275.7832, grad_fn=<NegBackward0>) tensor(12275.7715, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12275.7607421875
tensor(12275.7715, grad_fn=<NegBackward0>) tensor(12275.7607, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12275.7470703125
tensor(12275.7607, grad_fn=<NegBackward0>) tensor(12275.7471, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12275.7353515625
tensor(12275.7471, grad_fn=<NegBackward0>) tensor(12275.7354, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12275.7197265625
tensor(12275.7354, grad_fn=<NegBackward0>) tensor(12275.7197, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12275.7060546875
tensor(12275.7197, grad_fn=<NegBackward0>) tensor(12275.7061, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12275.689453125
tensor(12275.7061, grad_fn=<NegBackward0>) tensor(12275.6895, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12275.671875
tensor(12275.6895, grad_fn=<NegBackward0>) tensor(12275.6719, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12275.658203125
tensor(12275.6719, grad_fn=<NegBackward0>) tensor(12275.6582, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12275.640625
tensor(12275.6582, grad_fn=<NegBackward0>) tensor(12275.6406, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12275.6240234375
tensor(12275.6406, grad_fn=<NegBackward0>) tensor(12275.6240, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12275.611328125
tensor(12275.6240, grad_fn=<NegBackward0>) tensor(12275.6113, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12275.6005859375
tensor(12275.6113, grad_fn=<NegBackward0>) tensor(12275.6006, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12275.587890625
tensor(12275.6006, grad_fn=<NegBackward0>) tensor(12275.5879, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12275.5771484375
tensor(12275.5879, grad_fn=<NegBackward0>) tensor(12275.5771, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12275.560546875
tensor(12275.5771, grad_fn=<NegBackward0>) tensor(12275.5605, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12275.55078125
tensor(12275.5605, grad_fn=<NegBackward0>) tensor(12275.5508, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12275.5029296875
tensor(12275.5508, grad_fn=<NegBackward0>) tensor(12275.5029, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12275.462890625
tensor(12275.5029, grad_fn=<NegBackward0>) tensor(12275.4629, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12275.421875
tensor(12275.4629, grad_fn=<NegBackward0>) tensor(12275.4219, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12275.380859375
tensor(12275.4219, grad_fn=<NegBackward0>) tensor(12275.3809, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12275.3447265625
tensor(12275.3809, grad_fn=<NegBackward0>) tensor(12275.3447, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12275.31640625
tensor(12275.3447, grad_fn=<NegBackward0>) tensor(12275.3164, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12275.296875
tensor(12275.3164, grad_fn=<NegBackward0>) tensor(12275.2969, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12275.2900390625
tensor(12275.2969, grad_fn=<NegBackward0>) tensor(12275.2900, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12275.287109375
tensor(12275.2900, grad_fn=<NegBackward0>) tensor(12275.2871, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12275.2841796875
tensor(12275.2871, grad_fn=<NegBackward0>) tensor(12275.2842, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12275.279296875
tensor(12275.2842, grad_fn=<NegBackward0>) tensor(12275.2793, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12275.2744140625
tensor(12275.2793, grad_fn=<NegBackward0>) tensor(12275.2744, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12275.26953125
tensor(12275.2744, grad_fn=<NegBackward0>) tensor(12275.2695, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12275.28515625
tensor(12275.2695, grad_fn=<NegBackward0>) tensor(12275.2852, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12275.25390625
tensor(12275.2695, grad_fn=<NegBackward0>) tensor(12275.2539, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12275.2431640625
tensor(12275.2539, grad_fn=<NegBackward0>) tensor(12275.2432, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12275.2353515625
tensor(12275.2432, grad_fn=<NegBackward0>) tensor(12275.2354, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12275.29296875
tensor(12275.2354, grad_fn=<NegBackward0>) tensor(12275.2930, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12275.220703125
tensor(12275.2354, grad_fn=<NegBackward0>) tensor(12275.2207, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12275.2177734375
tensor(12275.2207, grad_fn=<NegBackward0>) tensor(12275.2178, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12275.2119140625
tensor(12275.2178, grad_fn=<NegBackward0>) tensor(12275.2119, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12275.22265625
tensor(12275.2119, grad_fn=<NegBackward0>) tensor(12275.2227, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12275.2080078125
tensor(12275.2119, grad_fn=<NegBackward0>) tensor(12275.2080, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12275.2060546875
tensor(12275.2080, grad_fn=<NegBackward0>) tensor(12275.2061, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12275.205078125
tensor(12275.2061, grad_fn=<NegBackward0>) tensor(12275.2051, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12275.2041015625
tensor(12275.2051, grad_fn=<NegBackward0>) tensor(12275.2041, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12275.205078125
tensor(12275.2041, grad_fn=<NegBackward0>) tensor(12275.2051, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12275.203125
tensor(12275.2041, grad_fn=<NegBackward0>) tensor(12275.2031, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12275.2041015625
tensor(12275.2031, grad_fn=<NegBackward0>) tensor(12275.2041, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12275.2021484375
tensor(12275.2031, grad_fn=<NegBackward0>) tensor(12275.2021, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12275.2177734375
tensor(12275.2021, grad_fn=<NegBackward0>) tensor(12275.2178, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12275.2041015625
tensor(12275.2021, grad_fn=<NegBackward0>) tensor(12275.2041, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12275.2041015625
tensor(12275.2021, grad_fn=<NegBackward0>) tensor(12275.2041, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12275.203125
tensor(12275.2021, grad_fn=<NegBackward0>) tensor(12275.2031, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -12275.205078125
tensor(12275.2021, grad_fn=<NegBackward0>) tensor(12275.2051, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.1343, 0.8657],
        [0.3428, 0.6572]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9823, 0.0177], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1924, 0.2752],
         [0.5349, 0.1986]],

        [[0.6509, 0.1968],
         [0.6213, 0.5469]],

        [[0.5761, 0.1949],
         [0.7161, 0.5695]],

        [[0.5920, 0.1892],
         [0.5697, 0.6945]],

        [[0.6857, 0.2033],
         [0.5927, 0.7251]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006269261438607936
Average Adjusted Rand Index: 0.0
[-0.0009467101632148221, -0.0006269261438607936] [-0.0018717581893426948, 0.0] [12273.7783203125, 12275.205078125]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11797.172686123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22151.361328125
inf tensor(22151.3613, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12249.8701171875
tensor(22151.3613, grad_fn=<NegBackward0>) tensor(12249.8701, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12249.486328125
tensor(12249.8701, grad_fn=<NegBackward0>) tensor(12249.4863, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12249.41796875
tensor(12249.4863, grad_fn=<NegBackward0>) tensor(12249.4180, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12249.3837890625
tensor(12249.4180, grad_fn=<NegBackward0>) tensor(12249.3838, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12249.3583984375
tensor(12249.3838, grad_fn=<NegBackward0>) tensor(12249.3584, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12249.3359375
tensor(12249.3584, grad_fn=<NegBackward0>) tensor(12249.3359, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12249.3212890625
tensor(12249.3359, grad_fn=<NegBackward0>) tensor(12249.3213, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12249.306640625
tensor(12249.3213, grad_fn=<NegBackward0>) tensor(12249.3066, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12249.2919921875
tensor(12249.3066, grad_fn=<NegBackward0>) tensor(12249.2920, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12249.279296875
tensor(12249.2920, grad_fn=<NegBackward0>) tensor(12249.2793, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12249.267578125
tensor(12249.2793, grad_fn=<NegBackward0>) tensor(12249.2676, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12249.255859375
tensor(12249.2676, grad_fn=<NegBackward0>) tensor(12249.2559, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12249.2421875
tensor(12249.2559, grad_fn=<NegBackward0>) tensor(12249.2422, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12249.2275390625
tensor(12249.2422, grad_fn=<NegBackward0>) tensor(12249.2275, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12249.2080078125
tensor(12249.2275, grad_fn=<NegBackward0>) tensor(12249.2080, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12249.1826171875
tensor(12249.2080, grad_fn=<NegBackward0>) tensor(12249.1826, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12249.1142578125
tensor(12249.1826, grad_fn=<NegBackward0>) tensor(12249.1143, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12248.3623046875
tensor(12249.1143, grad_fn=<NegBackward0>) tensor(12248.3623, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12247.439453125
tensor(12248.3623, grad_fn=<NegBackward0>) tensor(12247.4395, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12247.3515625
tensor(12247.4395, grad_fn=<NegBackward0>) tensor(12247.3516, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12247.3046875
tensor(12247.3516, grad_fn=<NegBackward0>) tensor(12247.3047, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12247.259765625
tensor(12247.3047, grad_fn=<NegBackward0>) tensor(12247.2598, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12247.2099609375
tensor(12247.2598, grad_fn=<NegBackward0>) tensor(12247.2100, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12247.15625
tensor(12247.2100, grad_fn=<NegBackward0>) tensor(12247.1562, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12247.103515625
tensor(12247.1562, grad_fn=<NegBackward0>) tensor(12247.1035, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12247.05859375
tensor(12247.1035, grad_fn=<NegBackward0>) tensor(12247.0586, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12247.025390625
tensor(12247.0586, grad_fn=<NegBackward0>) tensor(12247.0254, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12247.00390625
tensor(12247.0254, grad_fn=<NegBackward0>) tensor(12247.0039, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12246.9873046875
tensor(12247.0039, grad_fn=<NegBackward0>) tensor(12246.9873, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12246.9697265625
tensor(12246.9873, grad_fn=<NegBackward0>) tensor(12246.9697, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12246.953125
tensor(12246.9697, grad_fn=<NegBackward0>) tensor(12246.9531, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12246.939453125
tensor(12246.9531, grad_fn=<NegBackward0>) tensor(12246.9395, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12246.9228515625
tensor(12246.9395, grad_fn=<NegBackward0>) tensor(12246.9229, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12246.908203125
tensor(12246.9229, grad_fn=<NegBackward0>) tensor(12246.9082, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12246.8916015625
tensor(12246.9082, grad_fn=<NegBackward0>) tensor(12246.8916, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12246.8779296875
tensor(12246.8916, grad_fn=<NegBackward0>) tensor(12246.8779, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12246.8583984375
tensor(12246.8779, grad_fn=<NegBackward0>) tensor(12246.8584, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12246.83984375
tensor(12246.8584, grad_fn=<NegBackward0>) tensor(12246.8398, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12246.8173828125
tensor(12246.8398, grad_fn=<NegBackward0>) tensor(12246.8174, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12246.79296875
tensor(12246.8174, grad_fn=<NegBackward0>) tensor(12246.7930, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12246.76953125
tensor(12246.7930, grad_fn=<NegBackward0>) tensor(12246.7695, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12246.7294921875
tensor(12246.7695, grad_fn=<NegBackward0>) tensor(12246.7295, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12246.7060546875
tensor(12246.7295, grad_fn=<NegBackward0>) tensor(12246.7061, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12246.662109375
tensor(12246.7061, grad_fn=<NegBackward0>) tensor(12246.6621, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12246.6357421875
tensor(12246.6621, grad_fn=<NegBackward0>) tensor(12246.6357, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12246.6171875
tensor(12246.6357, grad_fn=<NegBackward0>) tensor(12246.6172, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12246.60546875
tensor(12246.6172, grad_fn=<NegBackward0>) tensor(12246.6055, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12246.5927734375
tensor(12246.6055, grad_fn=<NegBackward0>) tensor(12246.5928, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12246.583984375
tensor(12246.5928, grad_fn=<NegBackward0>) tensor(12246.5840, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12246.57421875
tensor(12246.5840, grad_fn=<NegBackward0>) tensor(12246.5742, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12246.5595703125
tensor(12246.5742, grad_fn=<NegBackward0>) tensor(12246.5596, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12246.53125
tensor(12246.5596, grad_fn=<NegBackward0>) tensor(12246.5312, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12246.447265625
tensor(12246.5312, grad_fn=<NegBackward0>) tensor(12246.4473, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12246.2861328125
tensor(12246.4473, grad_fn=<NegBackward0>) tensor(12246.2861, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12246.1962890625
tensor(12246.2861, grad_fn=<NegBackward0>) tensor(12246.1963, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12246.158203125
tensor(12246.1963, grad_fn=<NegBackward0>) tensor(12246.1582, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12246.140625
tensor(12246.1582, grad_fn=<NegBackward0>) tensor(12246.1406, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12246.1279296875
tensor(12246.1406, grad_fn=<NegBackward0>) tensor(12246.1279, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12246.1201171875
tensor(12246.1279, grad_fn=<NegBackward0>) tensor(12246.1201, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12246.1142578125
tensor(12246.1201, grad_fn=<NegBackward0>) tensor(12246.1143, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12246.1123046875
tensor(12246.1143, grad_fn=<NegBackward0>) tensor(12246.1123, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12246.1064453125
tensor(12246.1123, grad_fn=<NegBackward0>) tensor(12246.1064, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12246.1044921875
tensor(12246.1064, grad_fn=<NegBackward0>) tensor(12246.1045, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12246.1025390625
tensor(12246.1045, grad_fn=<NegBackward0>) tensor(12246.1025, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12246.099609375
tensor(12246.1025, grad_fn=<NegBackward0>) tensor(12246.0996, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12246.1005859375
tensor(12246.0996, grad_fn=<NegBackward0>) tensor(12246.1006, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12246.09765625
tensor(12246.0996, grad_fn=<NegBackward0>) tensor(12246.0977, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12246.095703125
tensor(12246.0977, grad_fn=<NegBackward0>) tensor(12246.0957, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12246.0947265625
tensor(12246.0957, grad_fn=<NegBackward0>) tensor(12246.0947, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12246.0947265625
tensor(12246.0947, grad_fn=<NegBackward0>) tensor(12246.0947, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12246.09375
tensor(12246.0947, grad_fn=<NegBackward0>) tensor(12246.0938, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12246.0927734375
tensor(12246.0938, grad_fn=<NegBackward0>) tensor(12246.0928, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12246.0927734375
tensor(12246.0928, grad_fn=<NegBackward0>) tensor(12246.0928, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12246.09375
tensor(12246.0928, grad_fn=<NegBackward0>) tensor(12246.0938, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12246.0908203125
tensor(12246.0928, grad_fn=<NegBackward0>) tensor(12246.0908, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12246.0908203125
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0908, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12246.09765625
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0977, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12246.08984375
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0898, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12246.0966796875
tensor(12246.0898, grad_fn=<NegBackward0>) tensor(12246.0967, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12246.0888671875
tensor(12246.0898, grad_fn=<NegBackward0>) tensor(12246.0889, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12246.08984375
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.0898, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12246.0888671875
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.0889, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12246.0888671875
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.0889, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12246.0888671875
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.0889, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12246.11328125
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.1133, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12246.087890625
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12246.20703125
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.2070, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12246.0869140625
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12246.0869140625
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12246.087890625
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12246.0859375
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0859, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12246.1044921875
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.1045, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12246.087890625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -12246.1357421875
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.1357, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -12246.087890625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -12246.1201171875
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.1201, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[3.5382e-04, 9.9965e-01],
        [9.7712e-01, 2.2877e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9135, 0.0865], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.1203],
         [0.5796, 0.1960]],

        [[0.6273, 0.2123],
         [0.6224, 0.7018]],

        [[0.6382, 0.1678],
         [0.6316, 0.6711]],

        [[0.6320, 0.1990],
         [0.5338, 0.5704]],

        [[0.5797, 0.1913],
         [0.6949, 0.6513]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.003997142393814948
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002918749315918129
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0023411493580129494
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0020437956204379564
Global Adjusted Rand Index: -0.0017380006438467337
Average Adjusted Rand Index: -0.003553096630566089
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19873.29296875
inf tensor(19873.2930, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12249.6416015625
tensor(19873.2930, grad_fn=<NegBackward0>) tensor(12249.6416, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12249.3134765625
tensor(12249.6416, grad_fn=<NegBackward0>) tensor(12249.3135, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12249.1982421875
tensor(12249.3135, grad_fn=<NegBackward0>) tensor(12249.1982, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12248.9384765625
tensor(12249.1982, grad_fn=<NegBackward0>) tensor(12248.9385, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12247.9560546875
tensor(12248.9385, grad_fn=<NegBackward0>) tensor(12247.9561, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12247.5048828125
tensor(12247.9561, grad_fn=<NegBackward0>) tensor(12247.5049, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12247.41015625
tensor(12247.5049, grad_fn=<NegBackward0>) tensor(12247.4102, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12247.365234375
tensor(12247.4102, grad_fn=<NegBackward0>) tensor(12247.3652, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12247.330078125
tensor(12247.3652, grad_fn=<NegBackward0>) tensor(12247.3301, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12247.3037109375
tensor(12247.3301, grad_fn=<NegBackward0>) tensor(12247.3037, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12247.2763671875
tensor(12247.3037, grad_fn=<NegBackward0>) tensor(12247.2764, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12247.24609375
tensor(12247.2764, grad_fn=<NegBackward0>) tensor(12247.2461, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12247.21484375
tensor(12247.2461, grad_fn=<NegBackward0>) tensor(12247.2148, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12247.1787109375
tensor(12247.2148, grad_fn=<NegBackward0>) tensor(12247.1787, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12247.138671875
tensor(12247.1787, grad_fn=<NegBackward0>) tensor(12247.1387, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12247.095703125
tensor(12247.1387, grad_fn=<NegBackward0>) tensor(12247.0957, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12247.0595703125
tensor(12247.0957, grad_fn=<NegBackward0>) tensor(12247.0596, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12247.029296875
tensor(12247.0596, grad_fn=<NegBackward0>) tensor(12247.0293, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12247.00390625
tensor(12247.0293, grad_fn=<NegBackward0>) tensor(12247.0039, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12246.984375
tensor(12247.0039, grad_fn=<NegBackward0>) tensor(12246.9844, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12246.96875
tensor(12246.9844, grad_fn=<NegBackward0>) tensor(12246.9688, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12246.953125
tensor(12246.9688, grad_fn=<NegBackward0>) tensor(12246.9531, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12246.9375
tensor(12246.9531, grad_fn=<NegBackward0>) tensor(12246.9375, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12246.923828125
tensor(12246.9375, grad_fn=<NegBackward0>) tensor(12246.9238, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12246.9091796875
tensor(12246.9238, grad_fn=<NegBackward0>) tensor(12246.9092, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12246.8955078125
tensor(12246.9092, grad_fn=<NegBackward0>) tensor(12246.8955, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12246.8798828125
tensor(12246.8955, grad_fn=<NegBackward0>) tensor(12246.8799, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12246.8642578125
tensor(12246.8799, grad_fn=<NegBackward0>) tensor(12246.8643, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12246.8466796875
tensor(12246.8643, grad_fn=<NegBackward0>) tensor(12246.8467, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12246.826171875
tensor(12246.8467, grad_fn=<NegBackward0>) tensor(12246.8262, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12246.802734375
tensor(12246.8262, grad_fn=<NegBackward0>) tensor(12246.8027, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12246.775390625
tensor(12246.8027, grad_fn=<NegBackward0>) tensor(12246.7754, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12246.748046875
tensor(12246.7754, grad_fn=<NegBackward0>) tensor(12246.7480, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12246.7109375
tensor(12246.7480, grad_fn=<NegBackward0>) tensor(12246.7109, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12246.6787109375
tensor(12246.7109, grad_fn=<NegBackward0>) tensor(12246.6787, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12246.650390625
tensor(12246.6787, grad_fn=<NegBackward0>) tensor(12246.6504, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12246.62890625
tensor(12246.6504, grad_fn=<NegBackward0>) tensor(12246.6289, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12246.61328125
tensor(12246.6289, grad_fn=<NegBackward0>) tensor(12246.6133, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12246.6025390625
tensor(12246.6133, grad_fn=<NegBackward0>) tensor(12246.6025, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12246.59375
tensor(12246.6025, grad_fn=<NegBackward0>) tensor(12246.5938, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12246.5859375
tensor(12246.5938, grad_fn=<NegBackward0>) tensor(12246.5859, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12246.576171875
tensor(12246.5859, grad_fn=<NegBackward0>) tensor(12246.5762, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12246.5634765625
tensor(12246.5762, grad_fn=<NegBackward0>) tensor(12246.5635, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12246.541015625
tensor(12246.5635, grad_fn=<NegBackward0>) tensor(12246.5410, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12246.46484375
tensor(12246.5410, grad_fn=<NegBackward0>) tensor(12246.4648, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12246.3134765625
tensor(12246.4648, grad_fn=<NegBackward0>) tensor(12246.3135, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12246.20703125
tensor(12246.3135, grad_fn=<NegBackward0>) tensor(12246.2070, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12246.1669921875
tensor(12246.2070, grad_fn=<NegBackward0>) tensor(12246.1670, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12246.1455078125
tensor(12246.1670, grad_fn=<NegBackward0>) tensor(12246.1455, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12246.1328125
tensor(12246.1455, grad_fn=<NegBackward0>) tensor(12246.1328, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12246.1240234375
tensor(12246.1328, grad_fn=<NegBackward0>) tensor(12246.1240, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12246.1162109375
tensor(12246.1240, grad_fn=<NegBackward0>) tensor(12246.1162, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12246.1123046875
tensor(12246.1162, grad_fn=<NegBackward0>) tensor(12246.1123, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12246.109375
tensor(12246.1123, grad_fn=<NegBackward0>) tensor(12246.1094, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12246.10546875
tensor(12246.1094, grad_fn=<NegBackward0>) tensor(12246.1055, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12246.103515625
tensor(12246.1055, grad_fn=<NegBackward0>) tensor(12246.1035, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12246.1025390625
tensor(12246.1035, grad_fn=<NegBackward0>) tensor(12246.1025, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12246.1005859375
tensor(12246.1025, grad_fn=<NegBackward0>) tensor(12246.1006, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12246.09765625
tensor(12246.1006, grad_fn=<NegBackward0>) tensor(12246.0977, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12246.09765625
tensor(12246.0977, grad_fn=<NegBackward0>) tensor(12246.0977, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12246.095703125
tensor(12246.0977, grad_fn=<NegBackward0>) tensor(12246.0957, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12246.095703125
tensor(12246.0957, grad_fn=<NegBackward0>) tensor(12246.0957, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12246.0947265625
tensor(12246.0957, grad_fn=<NegBackward0>) tensor(12246.0947, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12246.09375
tensor(12246.0947, grad_fn=<NegBackward0>) tensor(12246.0938, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12246.0927734375
tensor(12246.0938, grad_fn=<NegBackward0>) tensor(12246.0928, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12246.0927734375
tensor(12246.0928, grad_fn=<NegBackward0>) tensor(12246.0928, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12246.091796875
tensor(12246.0928, grad_fn=<NegBackward0>) tensor(12246.0918, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12246.0908203125
tensor(12246.0918, grad_fn=<NegBackward0>) tensor(12246.0908, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12246.091796875
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0918, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12246.0908203125
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0908, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12246.091796875
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0918, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12246.109375
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.1094, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12246.1015625
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.1016, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -12246.15234375
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.1523, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -12246.0888671875
tensor(12246.0908, grad_fn=<NegBackward0>) tensor(12246.0889, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12246.087890625
tensor(12246.0889, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12246.109375
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.1094, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12246.087890625
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12246.0888671875
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.0889, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12246.1650390625
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.1650, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12246.0869140625
tensor(12246.0879, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12246.0927734375
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0928, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12246.087890625
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12246.0869140625
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12246.0859375
tensor(12246.0869, grad_fn=<NegBackward0>) tensor(12246.0859, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12246.0859375
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0859, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12246.0869140625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12246.087890625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12246.0859375
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0859, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12246.0869140625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12246.0869140625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0869, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12246.087890625
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0879, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12246.0986328125
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.0986, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -12246.1064453125
tensor(12246.0859, grad_fn=<NegBackward0>) tensor(12246.1064, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[2.7910e-04, 9.9972e-01],
        [9.7662e-01, 2.3380e-02]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9120, 0.0880], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2030, 0.1203],
         [0.5966, 0.1957]],

        [[0.6919, 0.2122],
         [0.7164, 0.5218]],

        [[0.5983, 0.1677],
         [0.6944, 0.6068]],

        [[0.5604, 0.1987],
         [0.6368, 0.5378]],

        [[0.6304, 0.1910],
         [0.6996, 0.7254]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.003997142393814948
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002918749315918129
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0023411493580129494
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0020437956204379564
Global Adjusted Rand Index: -0.0018507325922533723
Average Adjusted Rand Index: -0.003957137034606493
[-0.0017380006438467337, -0.0018507325922533723] [-0.003553096630566089, -0.003957137034606493] [12246.1201171875, 12246.1064453125]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11865.130447650363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20817.875
inf tensor(20817.8750, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12361.470703125
tensor(20817.8750, grad_fn=<NegBackward0>) tensor(12361.4707, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12361.1826171875
tensor(12361.4707, grad_fn=<NegBackward0>) tensor(12361.1826, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12361.05859375
tensor(12361.1826, grad_fn=<NegBackward0>) tensor(12361.0586, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12360.9072265625
tensor(12361.0586, grad_fn=<NegBackward0>) tensor(12360.9072, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12360.736328125
tensor(12360.9072, grad_fn=<NegBackward0>) tensor(12360.7363, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12360.611328125
tensor(12360.7363, grad_fn=<NegBackward0>) tensor(12360.6113, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12360.5498046875
tensor(12360.6113, grad_fn=<NegBackward0>) tensor(12360.5498, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12360.5126953125
tensor(12360.5498, grad_fn=<NegBackward0>) tensor(12360.5127, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12360.486328125
tensor(12360.5127, grad_fn=<NegBackward0>) tensor(12360.4863, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12360.466796875
tensor(12360.4863, grad_fn=<NegBackward0>) tensor(12360.4668, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12360.4521484375
tensor(12360.4668, grad_fn=<NegBackward0>) tensor(12360.4521, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12360.443359375
tensor(12360.4521, grad_fn=<NegBackward0>) tensor(12360.4434, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12360.439453125
tensor(12360.4434, grad_fn=<NegBackward0>) tensor(12360.4395, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12360.435546875
tensor(12360.4395, grad_fn=<NegBackward0>) tensor(12360.4355, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12360.431640625
tensor(12360.4355, grad_fn=<NegBackward0>) tensor(12360.4316, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12360.4306640625
tensor(12360.4316, grad_fn=<NegBackward0>) tensor(12360.4307, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12360.4287109375
tensor(12360.4307, grad_fn=<NegBackward0>) tensor(12360.4287, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12360.4296875
tensor(12360.4287, grad_fn=<NegBackward0>) tensor(12360.4297, grad_fn=<NegBackward0>)
1
Iteration 1900: Loss = -12360.427734375
tensor(12360.4287, grad_fn=<NegBackward0>) tensor(12360.4277, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12360.4287109375
tensor(12360.4277, grad_fn=<NegBackward0>) tensor(12360.4287, grad_fn=<NegBackward0>)
1
Iteration 2100: Loss = -12360.427734375
tensor(12360.4277, grad_fn=<NegBackward0>) tensor(12360.4277, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12360.4267578125
tensor(12360.4277, grad_fn=<NegBackward0>) tensor(12360.4268, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12360.4267578125
tensor(12360.4268, grad_fn=<NegBackward0>) tensor(12360.4268, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12360.4248046875
tensor(12360.4268, grad_fn=<NegBackward0>) tensor(12360.4248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12360.4248046875
tensor(12360.4248, grad_fn=<NegBackward0>) tensor(12360.4248, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12360.4228515625
tensor(12360.4248, grad_fn=<NegBackward0>) tensor(12360.4229, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12360.421875
tensor(12360.4229, grad_fn=<NegBackward0>) tensor(12360.4219, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12360.421875
tensor(12360.4219, grad_fn=<NegBackward0>) tensor(12360.4219, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12360.419921875
tensor(12360.4219, grad_fn=<NegBackward0>) tensor(12360.4199, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12360.4169921875
tensor(12360.4199, grad_fn=<NegBackward0>) tensor(12360.4170, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12360.416015625
tensor(12360.4170, grad_fn=<NegBackward0>) tensor(12360.4160, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12360.4130859375
tensor(12360.4160, grad_fn=<NegBackward0>) tensor(12360.4131, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12360.40625
tensor(12360.4131, grad_fn=<NegBackward0>) tensor(12360.4062, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12360.40234375
tensor(12360.4062, grad_fn=<NegBackward0>) tensor(12360.4023, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12360.396484375
tensor(12360.4023, grad_fn=<NegBackward0>) tensor(12360.3965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12360.388671875
tensor(12360.3965, grad_fn=<NegBackward0>) tensor(12360.3887, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12360.3828125
tensor(12360.3887, grad_fn=<NegBackward0>) tensor(12360.3828, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12360.375
tensor(12360.3828, grad_fn=<NegBackward0>) tensor(12360.3750, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12360.3701171875
tensor(12360.3750, grad_fn=<NegBackward0>) tensor(12360.3701, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12360.3671875
tensor(12360.3701, grad_fn=<NegBackward0>) tensor(12360.3672, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12360.3623046875
tensor(12360.3672, grad_fn=<NegBackward0>) tensor(12360.3623, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12360.361328125
tensor(12360.3623, grad_fn=<NegBackward0>) tensor(12360.3613, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12360.359375
tensor(12360.3613, grad_fn=<NegBackward0>) tensor(12360.3594, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12360.3583984375
tensor(12360.3594, grad_fn=<NegBackward0>) tensor(12360.3584, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12360.357421875
tensor(12360.3584, grad_fn=<NegBackward0>) tensor(12360.3574, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12360.357421875
tensor(12360.3574, grad_fn=<NegBackward0>) tensor(12360.3574, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12360.357421875
tensor(12360.3574, grad_fn=<NegBackward0>) tensor(12360.3574, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12360.35546875
tensor(12360.3574, grad_fn=<NegBackward0>) tensor(12360.3555, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12360.3564453125
tensor(12360.3555, grad_fn=<NegBackward0>) tensor(12360.3564, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12360.353515625
tensor(12360.3555, grad_fn=<NegBackward0>) tensor(12360.3535, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12360.3505859375
tensor(12360.3535, grad_fn=<NegBackward0>) tensor(12360.3506, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12357.787109375
tensor(12360.3506, grad_fn=<NegBackward0>) tensor(12357.7871, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12355.8125
tensor(12357.7871, grad_fn=<NegBackward0>) tensor(12355.8125, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12355.771484375
tensor(12355.8125, grad_fn=<NegBackward0>) tensor(12355.7715, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12355.75390625
tensor(12355.7715, grad_fn=<NegBackward0>) tensor(12355.7539, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12355.7421875
tensor(12355.7539, grad_fn=<NegBackward0>) tensor(12355.7422, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12355.73828125
tensor(12355.7422, grad_fn=<NegBackward0>) tensor(12355.7383, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12355.736328125
tensor(12355.7383, grad_fn=<NegBackward0>) tensor(12355.7363, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12355.7314453125
tensor(12355.7363, grad_fn=<NegBackward0>) tensor(12355.7314, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12355.7294921875
tensor(12355.7314, grad_fn=<NegBackward0>) tensor(12355.7295, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12355.7265625
tensor(12355.7295, grad_fn=<NegBackward0>) tensor(12355.7266, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12355.7265625
tensor(12355.7266, grad_fn=<NegBackward0>) tensor(12355.7266, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12355.724609375
tensor(12355.7266, grad_fn=<NegBackward0>) tensor(12355.7246, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12355.7236328125
tensor(12355.7246, grad_fn=<NegBackward0>) tensor(12355.7236, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12355.72265625
tensor(12355.7236, grad_fn=<NegBackward0>) tensor(12355.7227, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12355.72265625
tensor(12355.7227, grad_fn=<NegBackward0>) tensor(12355.7227, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12355.72265625
tensor(12355.7227, grad_fn=<NegBackward0>) tensor(12355.7227, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12355.720703125
tensor(12355.7227, grad_fn=<NegBackward0>) tensor(12355.7207, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12355.720703125
tensor(12355.7207, grad_fn=<NegBackward0>) tensor(12355.7207, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12355.7216796875
tensor(12355.7207, grad_fn=<NegBackward0>) tensor(12355.7217, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12355.7197265625
tensor(12355.7207, grad_fn=<NegBackward0>) tensor(12355.7197, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12355.7216796875
tensor(12355.7197, grad_fn=<NegBackward0>) tensor(12355.7217, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12355.71875
tensor(12355.7197, grad_fn=<NegBackward0>) tensor(12355.7188, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12355.71875
tensor(12355.7188, grad_fn=<NegBackward0>) tensor(12355.7188, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12355.71875
tensor(12355.7188, grad_fn=<NegBackward0>) tensor(12355.7188, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12355.7177734375
tensor(12355.7188, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12355.7177734375
tensor(12355.7178, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12355.7197265625
tensor(12355.7178, grad_fn=<NegBackward0>) tensor(12355.7197, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12355.7177734375
tensor(12355.7178, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12355.7265625
tensor(12355.7178, grad_fn=<NegBackward0>) tensor(12355.7266, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12355.716796875
tensor(12355.7178, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12355.716796875
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12355.716796875
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12355.716796875
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12355.9853515625
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.9854, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12355.716796875
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12355.71875
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7188, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12355.7158203125
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7158, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12355.791015625
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7910, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12355.7158203125
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7158, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12355.7177734375
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12355.7158203125
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7158, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12355.716796875
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12355.7158203125
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7158, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12355.716796875
tensor(12355.7158, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9996e-01, 3.5175e-05],
        [1.0434e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0579, 0.9421], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2297, 0.1592],
         [0.5858, 0.2031]],

        [[0.5556, 0.1142],
         [0.6218, 0.5831]],

        [[0.6709, 0.1879],
         [0.7149, 0.6596]],

        [[0.6786, 0.1539],
         [0.6411, 0.6343]],

        [[0.7210, 0.2297],
         [0.7299, 0.5447]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.026390525206175977
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.020872495483242322
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.006809752538456861
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.010020462578284864
Global Adjusted Rand Index: 0.015355669249368521
Average Adjusted Rand Index: 0.013652031113124127
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22604.228515625
inf tensor(22604.2285, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12361.5869140625
tensor(22604.2285, grad_fn=<NegBackward0>) tensor(12361.5869, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12361.369140625
tensor(12361.5869, grad_fn=<NegBackward0>) tensor(12361.3691, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12361.306640625
tensor(12361.3691, grad_fn=<NegBackward0>) tensor(12361.3066, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12361.2431640625
tensor(12361.3066, grad_fn=<NegBackward0>) tensor(12361.2432, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12361.177734375
tensor(12361.2432, grad_fn=<NegBackward0>) tensor(12361.1777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12361.1083984375
tensor(12361.1777, grad_fn=<NegBackward0>) tensor(12361.1084, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12361.0498046875
tensor(12361.1084, grad_fn=<NegBackward0>) tensor(12361.0498, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12360.994140625
tensor(12361.0498, grad_fn=<NegBackward0>) tensor(12360.9941, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12360.9384765625
tensor(12360.9941, grad_fn=<NegBackward0>) tensor(12360.9385, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12360.873046875
tensor(12360.9385, grad_fn=<NegBackward0>) tensor(12360.8730, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12360.7783203125
tensor(12360.8730, grad_fn=<NegBackward0>) tensor(12360.7783, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12360.6611328125
tensor(12360.7783, grad_fn=<NegBackward0>) tensor(12360.6611, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12360.572265625
tensor(12360.6611, grad_fn=<NegBackward0>) tensor(12360.5723, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12360.5185546875
tensor(12360.5723, grad_fn=<NegBackward0>) tensor(12360.5186, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12360.4873046875
tensor(12360.5186, grad_fn=<NegBackward0>) tensor(12360.4873, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12360.4638671875
tensor(12360.4873, grad_fn=<NegBackward0>) tensor(12360.4639, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12360.44921875
tensor(12360.4639, grad_fn=<NegBackward0>) tensor(12360.4492, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12360.44140625
tensor(12360.4492, grad_fn=<NegBackward0>) tensor(12360.4414, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12360.4365234375
tensor(12360.4414, grad_fn=<NegBackward0>) tensor(12360.4365, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12360.4345703125
tensor(12360.4365, grad_fn=<NegBackward0>) tensor(12360.4346, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12360.4326171875
tensor(12360.4346, grad_fn=<NegBackward0>) tensor(12360.4326, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12360.4296875
tensor(12360.4326, grad_fn=<NegBackward0>) tensor(12360.4297, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12360.4306640625
tensor(12360.4297, grad_fn=<NegBackward0>) tensor(12360.4307, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -12360.4296875
tensor(12360.4297, grad_fn=<NegBackward0>) tensor(12360.4297, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12360.4287109375
tensor(12360.4297, grad_fn=<NegBackward0>) tensor(12360.4287, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12360.4287109375
tensor(12360.4287, grad_fn=<NegBackward0>) tensor(12360.4287, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12360.427734375
tensor(12360.4287, grad_fn=<NegBackward0>) tensor(12360.4277, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12360.427734375
tensor(12360.4277, grad_fn=<NegBackward0>) tensor(12360.4277, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12360.427734375
tensor(12360.4277, grad_fn=<NegBackward0>) tensor(12360.4277, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12360.42578125
tensor(12360.4277, grad_fn=<NegBackward0>) tensor(12360.4258, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12360.4267578125
tensor(12360.4258, grad_fn=<NegBackward0>) tensor(12360.4268, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -12360.42578125
tensor(12360.4258, grad_fn=<NegBackward0>) tensor(12360.4258, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12360.42578125
tensor(12360.4258, grad_fn=<NegBackward0>) tensor(12360.4258, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12360.423828125
tensor(12360.4258, grad_fn=<NegBackward0>) tensor(12360.4238, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12360.423828125
tensor(12360.4238, grad_fn=<NegBackward0>) tensor(12360.4238, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12360.4208984375
tensor(12360.4238, grad_fn=<NegBackward0>) tensor(12360.4209, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12360.4208984375
tensor(12360.4209, grad_fn=<NegBackward0>) tensor(12360.4209, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12360.41796875
tensor(12360.4209, grad_fn=<NegBackward0>) tensor(12360.4180, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12360.41796875
tensor(12360.4180, grad_fn=<NegBackward0>) tensor(12360.4180, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12360.412109375
tensor(12360.4180, grad_fn=<NegBackward0>) tensor(12360.4121, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12360.408203125
tensor(12360.4121, grad_fn=<NegBackward0>) tensor(12360.4082, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12360.404296875
tensor(12360.4082, grad_fn=<NegBackward0>) tensor(12360.4043, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12360.3994140625
tensor(12360.4043, grad_fn=<NegBackward0>) tensor(12360.3994, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12360.392578125
tensor(12360.3994, grad_fn=<NegBackward0>) tensor(12360.3926, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12360.384765625
tensor(12360.3926, grad_fn=<NegBackward0>) tensor(12360.3848, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12360.3779296875
tensor(12360.3848, grad_fn=<NegBackward0>) tensor(12360.3779, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12360.373046875
tensor(12360.3779, grad_fn=<NegBackward0>) tensor(12360.3730, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12360.3681640625
tensor(12360.3730, grad_fn=<NegBackward0>) tensor(12360.3682, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12360.3642578125
tensor(12360.3682, grad_fn=<NegBackward0>) tensor(12360.3643, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12360.3642578125
tensor(12360.3643, grad_fn=<NegBackward0>) tensor(12360.3643, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12360.361328125
tensor(12360.3643, grad_fn=<NegBackward0>) tensor(12360.3613, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12360.3603515625
tensor(12360.3613, grad_fn=<NegBackward0>) tensor(12360.3604, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12360.359375
tensor(12360.3604, grad_fn=<NegBackward0>) tensor(12360.3594, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12360.3583984375
tensor(12360.3594, grad_fn=<NegBackward0>) tensor(12360.3584, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12360.3583984375
tensor(12360.3584, grad_fn=<NegBackward0>) tensor(12360.3584, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12360.3564453125
tensor(12360.3584, grad_fn=<NegBackward0>) tensor(12360.3564, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12360.357421875
tensor(12360.3564, grad_fn=<NegBackward0>) tensor(12360.3574, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12360.3564453125
tensor(12360.3564, grad_fn=<NegBackward0>) tensor(12360.3564, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12360.35546875
tensor(12360.3564, grad_fn=<NegBackward0>) tensor(12360.3555, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12360.3515625
tensor(12360.3555, grad_fn=<NegBackward0>) tensor(12360.3516, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12360.3388671875
tensor(12360.3516, grad_fn=<NegBackward0>) tensor(12360.3389, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12355.91796875
tensor(12360.3389, grad_fn=<NegBackward0>) tensor(12355.9180, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12355.7880859375
tensor(12355.9180, grad_fn=<NegBackward0>) tensor(12355.7881, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12355.76171875
tensor(12355.7881, grad_fn=<NegBackward0>) tensor(12355.7617, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12355.748046875
tensor(12355.7617, grad_fn=<NegBackward0>) tensor(12355.7480, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12355.7421875
tensor(12355.7480, grad_fn=<NegBackward0>) tensor(12355.7422, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12355.73828125
tensor(12355.7422, grad_fn=<NegBackward0>) tensor(12355.7383, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12355.7333984375
tensor(12355.7383, grad_fn=<NegBackward0>) tensor(12355.7334, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12355.7314453125
tensor(12355.7334, grad_fn=<NegBackward0>) tensor(12355.7314, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12355.7275390625
tensor(12355.7314, grad_fn=<NegBackward0>) tensor(12355.7275, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12355.728515625
tensor(12355.7275, grad_fn=<NegBackward0>) tensor(12355.7285, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12355.7265625
tensor(12355.7275, grad_fn=<NegBackward0>) tensor(12355.7266, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12355.7255859375
tensor(12355.7266, grad_fn=<NegBackward0>) tensor(12355.7256, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12355.7490234375
tensor(12355.7256, grad_fn=<NegBackward0>) tensor(12355.7490, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12355.724609375
tensor(12355.7256, grad_fn=<NegBackward0>) tensor(12355.7246, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12355.7216796875
tensor(12355.7246, grad_fn=<NegBackward0>) tensor(12355.7217, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12355.7236328125
tensor(12355.7217, grad_fn=<NegBackward0>) tensor(12355.7236, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12355.7216796875
tensor(12355.7217, grad_fn=<NegBackward0>) tensor(12355.7217, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12355.7216796875
tensor(12355.7217, grad_fn=<NegBackward0>) tensor(12355.7217, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12355.7197265625
tensor(12355.7217, grad_fn=<NegBackward0>) tensor(12355.7197, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12355.72265625
tensor(12355.7197, grad_fn=<NegBackward0>) tensor(12355.7227, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12355.71875
tensor(12355.7197, grad_fn=<NegBackward0>) tensor(12355.7188, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12355.744140625
tensor(12355.7188, grad_fn=<NegBackward0>) tensor(12355.7441, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12355.7197265625
tensor(12355.7188, grad_fn=<NegBackward0>) tensor(12355.7197, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -12355.716796875
tensor(12355.7188, grad_fn=<NegBackward0>) tensor(12355.7168, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12355.7216796875
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7217, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -12355.7177734375
tensor(12355.7168, grad_fn=<NegBackward0>) tensor(12355.7178, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[9.9991e-01, 9.0652e-05],
        [1.4274e-05, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0578, 0.9422], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2297, 0.1592],
         [0.5175, 0.2030]],

        [[0.6241, 0.1142],
         [0.6969, 0.6542]],

        [[0.6779, 0.1879],
         [0.6098, 0.6244]],

        [[0.5345, 0.1539],
         [0.5587, 0.6893]],

        [[0.6519, 0.2297],
         [0.5960, 0.5860]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.026390525206175977
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.020872495483242322
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.006809752538456861
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.010020462578284864
Global Adjusted Rand Index: 0.015355669249368521
Average Adjusted Rand Index: 0.013652031113124127
[0.015355669249368521, 0.015355669249368521] [0.013652031113124127, 0.013652031113124127] [12355.748046875, 12355.7177734375]
-------------------------------------
This iteration is 32
True Objective function: Loss = -11868.57507092151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22040.701171875
inf tensor(22040.7012, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12363.4267578125
tensor(22040.7012, grad_fn=<NegBackward0>) tensor(12363.4268, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12363.0302734375
tensor(12363.4268, grad_fn=<NegBackward0>) tensor(12363.0303, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12362.9345703125
tensor(12363.0303, grad_fn=<NegBackward0>) tensor(12362.9346, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.8779296875
tensor(12362.9346, grad_fn=<NegBackward0>) tensor(12362.8779, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.83984375
tensor(12362.8779, grad_fn=<NegBackward0>) tensor(12362.8398, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.8076171875
tensor(12362.8398, grad_fn=<NegBackward0>) tensor(12362.8076, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.7822265625
tensor(12362.8076, grad_fn=<NegBackward0>) tensor(12362.7822, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.76171875
tensor(12362.7822, grad_fn=<NegBackward0>) tensor(12362.7617, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.7451171875
tensor(12362.7617, grad_fn=<NegBackward0>) tensor(12362.7451, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.7275390625
tensor(12362.7451, grad_fn=<NegBackward0>) tensor(12362.7275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.712890625
tensor(12362.7275, grad_fn=<NegBackward0>) tensor(12362.7129, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.6982421875
tensor(12362.7129, grad_fn=<NegBackward0>) tensor(12362.6982, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12362.68359375
tensor(12362.6982, grad_fn=<NegBackward0>) tensor(12362.6836, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12362.6708984375
tensor(12362.6836, grad_fn=<NegBackward0>) tensor(12362.6709, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12362.6552734375
tensor(12362.6709, grad_fn=<NegBackward0>) tensor(12362.6553, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12362.638671875
tensor(12362.6553, grad_fn=<NegBackward0>) tensor(12362.6387, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12362.623046875
tensor(12362.6387, grad_fn=<NegBackward0>) tensor(12362.6230, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12362.607421875
tensor(12362.6230, grad_fn=<NegBackward0>) tensor(12362.6074, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12362.587890625
tensor(12362.6074, grad_fn=<NegBackward0>) tensor(12362.5879, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12362.5693359375
tensor(12362.5879, grad_fn=<NegBackward0>) tensor(12362.5693, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12362.5478515625
tensor(12362.5693, grad_fn=<NegBackward0>) tensor(12362.5479, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12362.5244140625
tensor(12362.5479, grad_fn=<NegBackward0>) tensor(12362.5244, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12362.501953125
tensor(12362.5244, grad_fn=<NegBackward0>) tensor(12362.5020, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12362.4755859375
tensor(12362.5020, grad_fn=<NegBackward0>) tensor(12362.4756, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12362.4521484375
tensor(12362.4756, grad_fn=<NegBackward0>) tensor(12362.4521, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12362.4248046875
tensor(12362.4521, grad_fn=<NegBackward0>) tensor(12362.4248, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12362.4013671875
tensor(12362.4248, grad_fn=<NegBackward0>) tensor(12362.4014, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12362.376953125
tensor(12362.4014, grad_fn=<NegBackward0>) tensor(12362.3770, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12362.353515625
tensor(12362.3770, grad_fn=<NegBackward0>) tensor(12362.3535, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12362.3369140625
tensor(12362.3535, grad_fn=<NegBackward0>) tensor(12362.3369, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12362.322265625
tensor(12362.3369, grad_fn=<NegBackward0>) tensor(12362.3223, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12362.3056640625
tensor(12362.3223, grad_fn=<NegBackward0>) tensor(12362.3057, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12362.29296875
tensor(12362.3057, grad_fn=<NegBackward0>) tensor(12362.2930, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12362.28125
tensor(12362.2930, grad_fn=<NegBackward0>) tensor(12362.2812, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12362.267578125
tensor(12362.2812, grad_fn=<NegBackward0>) tensor(12362.2676, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12362.2548828125
tensor(12362.2676, grad_fn=<NegBackward0>) tensor(12362.2549, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12362.2431640625
tensor(12362.2549, grad_fn=<NegBackward0>) tensor(12362.2432, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12362.232421875
tensor(12362.2432, grad_fn=<NegBackward0>) tensor(12362.2324, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12362.220703125
tensor(12362.2324, grad_fn=<NegBackward0>) tensor(12362.2207, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12362.2109375
tensor(12362.2207, grad_fn=<NegBackward0>) tensor(12362.2109, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12362.2041015625
tensor(12362.2109, grad_fn=<NegBackward0>) tensor(12362.2041, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12362.1962890625
tensor(12362.2041, grad_fn=<NegBackward0>) tensor(12362.1963, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12362.19140625
tensor(12362.1963, grad_fn=<NegBackward0>) tensor(12362.1914, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12362.1875
tensor(12362.1914, grad_fn=<NegBackward0>) tensor(12362.1875, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12362.18359375
tensor(12362.1875, grad_fn=<NegBackward0>) tensor(12362.1836, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12362.1806640625
tensor(12362.1836, grad_fn=<NegBackward0>) tensor(12362.1807, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12362.1787109375
tensor(12362.1807, grad_fn=<NegBackward0>) tensor(12362.1787, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12362.1767578125
tensor(12362.1787, grad_fn=<NegBackward0>) tensor(12362.1768, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12362.1748046875
tensor(12362.1768, grad_fn=<NegBackward0>) tensor(12362.1748, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12362.173828125
tensor(12362.1748, grad_fn=<NegBackward0>) tensor(12362.1738, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12362.1728515625
tensor(12362.1738, grad_fn=<NegBackward0>) tensor(12362.1729, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12362.1767578125
tensor(12362.1729, grad_fn=<NegBackward0>) tensor(12362.1768, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12362.1708984375
tensor(12362.1729, grad_fn=<NegBackward0>) tensor(12362.1709, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12362.171875
tensor(12362.1709, grad_fn=<NegBackward0>) tensor(12362.1719, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12362.16796875
tensor(12362.1709, grad_fn=<NegBackward0>) tensor(12362.1680, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12362.1669921875
tensor(12362.1680, grad_fn=<NegBackward0>) tensor(12362.1670, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12362.1630859375
tensor(12362.1670, grad_fn=<NegBackward0>) tensor(12362.1631, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12362.1650390625
tensor(12362.1631, grad_fn=<NegBackward0>) tensor(12362.1650, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12362.1591796875
tensor(12362.1631, grad_fn=<NegBackward0>) tensor(12362.1592, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12362.158203125
tensor(12362.1592, grad_fn=<NegBackward0>) tensor(12362.1582, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12362.154296875
tensor(12362.1582, grad_fn=<NegBackward0>) tensor(12362.1543, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12362.150390625
tensor(12362.1543, grad_fn=<NegBackward0>) tensor(12362.1504, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12362.1474609375
tensor(12362.1504, grad_fn=<NegBackward0>) tensor(12362.1475, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12362.1416015625
tensor(12362.1475, grad_fn=<NegBackward0>) tensor(12362.1416, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12362.1357421875
tensor(12362.1416, grad_fn=<NegBackward0>) tensor(12362.1357, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12362.1279296875
tensor(12362.1357, grad_fn=<NegBackward0>) tensor(12362.1279, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12362.11328125
tensor(12362.1279, grad_fn=<NegBackward0>) tensor(12362.1133, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12362.0732421875
tensor(12362.1133, grad_fn=<NegBackward0>) tensor(12362.0732, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12361.9228515625
tensor(12362.0732, grad_fn=<NegBackward0>) tensor(12361.9229, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12361.7314453125
tensor(12361.9229, grad_fn=<NegBackward0>) tensor(12361.7314, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12361.5693359375
tensor(12361.7314, grad_fn=<NegBackward0>) tensor(12361.5693, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12360.87890625
tensor(12361.5693, grad_fn=<NegBackward0>) tensor(12360.8789, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12360.66796875
tensor(12360.8789, grad_fn=<NegBackward0>) tensor(12360.6680, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12360.6318359375
tensor(12360.6680, grad_fn=<NegBackward0>) tensor(12360.6318, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12360.6162109375
tensor(12360.6318, grad_fn=<NegBackward0>) tensor(12360.6162, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12360.607421875
tensor(12360.6162, grad_fn=<NegBackward0>) tensor(12360.6074, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12360.8232421875
tensor(12360.6074, grad_fn=<NegBackward0>) tensor(12360.8232, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12360.59765625
tensor(12360.6074, grad_fn=<NegBackward0>) tensor(12360.5977, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12360.595703125
tensor(12360.5977, grad_fn=<NegBackward0>) tensor(12360.5957, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12360.59375
tensor(12360.5957, grad_fn=<NegBackward0>) tensor(12360.5938, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12360.5908203125
tensor(12360.5938, grad_fn=<NegBackward0>) tensor(12360.5908, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12360.5888671875
tensor(12360.5908, grad_fn=<NegBackward0>) tensor(12360.5889, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12360.587890625
tensor(12360.5889, grad_fn=<NegBackward0>) tensor(12360.5879, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12360.5888671875
tensor(12360.5879, grad_fn=<NegBackward0>) tensor(12360.5889, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12360.650390625
tensor(12360.5879, grad_fn=<NegBackward0>) tensor(12360.6504, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12360.5859375
tensor(12360.5879, grad_fn=<NegBackward0>) tensor(12360.5859, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12360.5849609375
tensor(12360.5859, grad_fn=<NegBackward0>) tensor(12360.5850, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12360.58203125
tensor(12360.5850, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12360.58203125
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12360.5869140625
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5869, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12360.58203125
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12360.58203125
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12360.5810546875
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5811, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12360.580078125
tensor(12360.5811, grad_fn=<NegBackward0>) tensor(12360.5801, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12360.5830078125
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.5830, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12360.5810546875
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.5811, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12360.6005859375
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.6006, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -12360.5810546875
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.5811, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -12360.5791015625
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.5791, grad_fn=<NegBackward0>)
pi: tensor([[9.9999e-01, 1.4133e-05],
        [1.6203e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9641, 0.0359], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.1782],
         [0.5075, 0.3440]],

        [[0.5773, 0.1369],
         [0.5927, 0.6374]],

        [[0.6629, 0.1345],
         [0.7227, 0.6295]],

        [[0.5336, 0.1502],
         [0.5674, 0.5844]],

        [[0.6558, 0.2046],
         [0.5948, 0.7160]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0006921612735767433
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.018772080923839488
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.012285862605987194
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
Global Adjusted Rand Index: 0.003573458745730763
Average Adjusted Rand Index: 0.001479977806508063
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20192.650390625
inf tensor(20192.6504, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12363.2529296875
tensor(20192.6504, grad_fn=<NegBackward0>) tensor(12363.2529, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12362.8759765625
tensor(12363.2529, grad_fn=<NegBackward0>) tensor(12362.8760, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12362.7900390625
tensor(12362.8760, grad_fn=<NegBackward0>) tensor(12362.7900, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.7451171875
tensor(12362.7900, grad_fn=<NegBackward0>) tensor(12362.7451, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.7138671875
tensor(12362.7451, grad_fn=<NegBackward0>) tensor(12362.7139, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.6875
tensor(12362.7139, grad_fn=<NegBackward0>) tensor(12362.6875, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.6669921875
tensor(12362.6875, grad_fn=<NegBackward0>) tensor(12362.6670, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.6474609375
tensor(12362.6670, grad_fn=<NegBackward0>) tensor(12362.6475, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.62890625
tensor(12362.6475, grad_fn=<NegBackward0>) tensor(12362.6289, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.6103515625
tensor(12362.6289, grad_fn=<NegBackward0>) tensor(12362.6104, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.5927734375
tensor(12362.6104, grad_fn=<NegBackward0>) tensor(12362.5928, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.5751953125
tensor(12362.5928, grad_fn=<NegBackward0>) tensor(12362.5752, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12362.5546875
tensor(12362.5752, grad_fn=<NegBackward0>) tensor(12362.5547, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12362.5341796875
tensor(12362.5547, grad_fn=<NegBackward0>) tensor(12362.5342, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12362.5107421875
tensor(12362.5342, grad_fn=<NegBackward0>) tensor(12362.5107, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12362.4873046875
tensor(12362.5107, grad_fn=<NegBackward0>) tensor(12362.4873, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12362.4609375
tensor(12362.4873, grad_fn=<NegBackward0>) tensor(12362.4609, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12362.4365234375
tensor(12362.4609, grad_fn=<NegBackward0>) tensor(12362.4365, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12362.41015625
tensor(12362.4365, grad_fn=<NegBackward0>) tensor(12362.4102, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12362.384765625
tensor(12362.4102, grad_fn=<NegBackward0>) tensor(12362.3848, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12362.361328125
tensor(12362.3848, grad_fn=<NegBackward0>) tensor(12362.3613, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12362.33984375
tensor(12362.3613, grad_fn=<NegBackward0>) tensor(12362.3398, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12362.3193359375
tensor(12362.3398, grad_fn=<NegBackward0>) tensor(12362.3193, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12362.3017578125
tensor(12362.3193, grad_fn=<NegBackward0>) tensor(12362.3018, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12362.2880859375
tensor(12362.3018, grad_fn=<NegBackward0>) tensor(12362.2881, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12362.2744140625
tensor(12362.2881, grad_fn=<NegBackward0>) tensor(12362.2744, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12362.2626953125
tensor(12362.2744, grad_fn=<NegBackward0>) tensor(12362.2627, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12362.2490234375
tensor(12362.2627, grad_fn=<NegBackward0>) tensor(12362.2490, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12362.2373046875
tensor(12362.2490, grad_fn=<NegBackward0>) tensor(12362.2373, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12362.2275390625
tensor(12362.2373, grad_fn=<NegBackward0>) tensor(12362.2275, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12362.21875
tensor(12362.2275, grad_fn=<NegBackward0>) tensor(12362.2188, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12362.2099609375
tensor(12362.2188, grad_fn=<NegBackward0>) tensor(12362.2100, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12362.201171875
tensor(12362.2100, grad_fn=<NegBackward0>) tensor(12362.2012, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12362.1953125
tensor(12362.2012, grad_fn=<NegBackward0>) tensor(12362.1953, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12362.1904296875
tensor(12362.1953, grad_fn=<NegBackward0>) tensor(12362.1904, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12362.1875
tensor(12362.1904, grad_fn=<NegBackward0>) tensor(12362.1875, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12362.18359375
tensor(12362.1875, grad_fn=<NegBackward0>) tensor(12362.1836, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12362.1806640625
tensor(12362.1836, grad_fn=<NegBackward0>) tensor(12362.1807, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12362.177734375
tensor(12362.1807, grad_fn=<NegBackward0>) tensor(12362.1777, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12362.19140625
tensor(12362.1777, grad_fn=<NegBackward0>) tensor(12362.1914, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12362.173828125
tensor(12362.1777, grad_fn=<NegBackward0>) tensor(12362.1738, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12362.1728515625
tensor(12362.1738, grad_fn=<NegBackward0>) tensor(12362.1729, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12362.1728515625
tensor(12362.1729, grad_fn=<NegBackward0>) tensor(12362.1729, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12362.169921875
tensor(12362.1729, grad_fn=<NegBackward0>) tensor(12362.1699, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12362.1708984375
tensor(12362.1699, grad_fn=<NegBackward0>) tensor(12362.1709, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12362.1689453125
tensor(12362.1699, grad_fn=<NegBackward0>) tensor(12362.1689, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12362.1669921875
tensor(12362.1689, grad_fn=<NegBackward0>) tensor(12362.1670, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12362.1640625
tensor(12362.1670, grad_fn=<NegBackward0>) tensor(12362.1641, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12362.1640625
tensor(12362.1641, grad_fn=<NegBackward0>) tensor(12362.1641, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12362.1630859375
tensor(12362.1641, grad_fn=<NegBackward0>) tensor(12362.1631, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12362.158203125
tensor(12362.1631, grad_fn=<NegBackward0>) tensor(12362.1582, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12362.158203125
tensor(12362.1582, grad_fn=<NegBackward0>) tensor(12362.1582, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12362.1533203125
tensor(12362.1582, grad_fn=<NegBackward0>) tensor(12362.1533, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12362.150390625
tensor(12362.1533, grad_fn=<NegBackward0>) tensor(12362.1504, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12362.1484375
tensor(12362.1504, grad_fn=<NegBackward0>) tensor(12362.1484, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12362.1416015625
tensor(12362.1484, grad_fn=<NegBackward0>) tensor(12362.1416, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12362.13671875
tensor(12362.1416, grad_fn=<NegBackward0>) tensor(12362.1367, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12362.12890625
tensor(12362.1367, grad_fn=<NegBackward0>) tensor(12362.1289, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12362.1123046875
tensor(12362.1289, grad_fn=<NegBackward0>) tensor(12362.1123, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12362.0751953125
tensor(12362.1123, grad_fn=<NegBackward0>) tensor(12362.0752, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12361.9248046875
tensor(12362.0752, grad_fn=<NegBackward0>) tensor(12361.9248, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12361.7529296875
tensor(12361.9248, grad_fn=<NegBackward0>) tensor(12361.7529, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12361.6142578125
tensor(12361.7529, grad_fn=<NegBackward0>) tensor(12361.6143, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12361.15625
tensor(12361.6143, grad_fn=<NegBackward0>) tensor(12361.1562, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12360.6923828125
tensor(12361.1562, grad_fn=<NegBackward0>) tensor(12360.6924, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12360.99609375
tensor(12360.6924, grad_fn=<NegBackward0>) tensor(12360.9961, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12360.619140625
tensor(12360.6924, grad_fn=<NegBackward0>) tensor(12360.6191, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12360.609375
tensor(12360.6191, grad_fn=<NegBackward0>) tensor(12360.6094, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12360.6025390625
tensor(12360.6094, grad_fn=<NegBackward0>) tensor(12360.6025, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12360.5986328125
tensor(12360.6025, grad_fn=<NegBackward0>) tensor(12360.5986, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12360.595703125
tensor(12360.5986, grad_fn=<NegBackward0>) tensor(12360.5957, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12360.615234375
tensor(12360.5957, grad_fn=<NegBackward0>) tensor(12360.6152, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12360.5908203125
tensor(12360.5957, grad_fn=<NegBackward0>) tensor(12360.5908, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12360.5908203125
tensor(12360.5908, grad_fn=<NegBackward0>) tensor(12360.5908, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12360.5888671875
tensor(12360.5908, grad_fn=<NegBackward0>) tensor(12360.5889, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12360.5869140625
tensor(12360.5889, grad_fn=<NegBackward0>) tensor(12360.5869, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12360.595703125
tensor(12360.5869, grad_fn=<NegBackward0>) tensor(12360.5957, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12360.5849609375
tensor(12360.5869, grad_fn=<NegBackward0>) tensor(12360.5850, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12360.5859375
tensor(12360.5850, grad_fn=<NegBackward0>) tensor(12360.5859, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12360.583984375
tensor(12360.5850, grad_fn=<NegBackward0>) tensor(12360.5840, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12360.583984375
tensor(12360.5840, grad_fn=<NegBackward0>) tensor(12360.5840, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12360.5908203125
tensor(12360.5840, grad_fn=<NegBackward0>) tensor(12360.5908, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12360.58203125
tensor(12360.5840, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12360.7890625
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.7891, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12360.58203125
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5820, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12360.580078125
tensor(12360.5820, grad_fn=<NegBackward0>) tensor(12360.5801, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12360.580078125
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.5801, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12360.5791015625
tensor(12360.5801, grad_fn=<NegBackward0>) tensor(12360.5791, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12360.5810546875
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5811, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12360.5791015625
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5791, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12360.658203125
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.6582, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12360.580078125
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5801, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -12360.5791015625
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5791, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12360.5791015625
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5791, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12360.5791015625
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5791, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12360.5810546875
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5811, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12360.580078125
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5801, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12360.578125
tensor(12360.5791, grad_fn=<NegBackward0>) tensor(12360.5781, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12360.5908203125
tensor(12360.5781, grad_fn=<NegBackward0>) tensor(12360.5908, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9999e-01, 1.0659e-05],
        [1.1227e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9641, 0.0359], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2021, 0.1782],
         [0.7089, 0.3440]],

        [[0.6545, 0.1369],
         [0.6510, 0.5003]],

        [[0.5009, 0.1345],
         [0.7293, 0.6481]],

        [[0.5829, 0.1502],
         [0.7142, 0.5852]],

        [[0.6766, 0.2046],
         [0.7104, 0.6569]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0006921612735767433
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.018772080923839488
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.012285862605987194
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
Global Adjusted Rand Index: 0.003573458745730763
Average Adjusted Rand Index: 0.001479977806508063
[0.003573458745730763, 0.003573458745730763] [0.001479977806508063, 0.001479977806508063] [12360.5791015625, 12360.5771484375]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11946.807164415199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25099.599609375
inf tensor(25099.5996, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12434.9296875
tensor(25099.5996, grad_fn=<NegBackward0>) tensor(12434.9297, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12434.3232421875
tensor(12434.9297, grad_fn=<NegBackward0>) tensor(12434.3232, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12433.9951171875
tensor(12434.3232, grad_fn=<NegBackward0>) tensor(12433.9951, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12433.85546875
tensor(12433.9951, grad_fn=<NegBackward0>) tensor(12433.8555, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12433.7685546875
tensor(12433.8555, grad_fn=<NegBackward0>) tensor(12433.7686, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12433.697265625
tensor(12433.7686, grad_fn=<NegBackward0>) tensor(12433.6973, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12433.619140625
tensor(12433.6973, grad_fn=<NegBackward0>) tensor(12433.6191, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12433.5048828125
tensor(12433.6191, grad_fn=<NegBackward0>) tensor(12433.5049, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12433.33203125
tensor(12433.5049, grad_fn=<NegBackward0>) tensor(12433.3320, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12433.1806640625
tensor(12433.3320, grad_fn=<NegBackward0>) tensor(12433.1807, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12433.08984375
tensor(12433.1807, grad_fn=<NegBackward0>) tensor(12433.0898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12432.958984375
tensor(12433.0898, grad_fn=<NegBackward0>) tensor(12432.9590, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12432.69921875
tensor(12432.9590, grad_fn=<NegBackward0>) tensor(12432.6992, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12432.3681640625
tensor(12432.6992, grad_fn=<NegBackward0>) tensor(12432.3682, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12432.1748046875
tensor(12432.3682, grad_fn=<NegBackward0>) tensor(12432.1748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12432.0791015625
tensor(12432.1748, grad_fn=<NegBackward0>) tensor(12432.0791, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12432.0205078125
tensor(12432.0791, grad_fn=<NegBackward0>) tensor(12432.0205, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12431.9794921875
tensor(12432.0205, grad_fn=<NegBackward0>) tensor(12431.9795, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12431.9453125
tensor(12431.9795, grad_fn=<NegBackward0>) tensor(12431.9453, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12310.822265625
tensor(12431.9453, grad_fn=<NegBackward0>) tensor(12310.8223, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12119.7900390625
tensor(12310.8223, grad_fn=<NegBackward0>) tensor(12119.7900, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12077.2880859375
tensor(12119.7900, grad_fn=<NegBackward0>) tensor(12077.2881, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12028.111328125
tensor(12077.2881, grad_fn=<NegBackward0>) tensor(12028.1113, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11974.3623046875
tensor(12028.1113, grad_fn=<NegBackward0>) tensor(11974.3623, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11974.326171875
tensor(11974.3623, grad_fn=<NegBackward0>) tensor(11974.3262, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11970.1552734375
tensor(11974.3262, grad_fn=<NegBackward0>) tensor(11970.1553, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11970.13671875
tensor(11970.1553, grad_fn=<NegBackward0>) tensor(11970.1367, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11970.12890625
tensor(11970.1367, grad_fn=<NegBackward0>) tensor(11970.1289, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11949.908203125
tensor(11970.1289, grad_fn=<NegBackward0>) tensor(11949.9082, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11949.9013671875
tensor(11949.9082, grad_fn=<NegBackward0>) tensor(11949.9014, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11949.8994140625
tensor(11949.9014, grad_fn=<NegBackward0>) tensor(11949.8994, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11949.8935546875
tensor(11949.8994, grad_fn=<NegBackward0>) tensor(11949.8936, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11937.884765625
tensor(11949.8936, grad_fn=<NegBackward0>) tensor(11937.8848, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11937.8837890625
tensor(11937.8848, grad_fn=<NegBackward0>) tensor(11937.8838, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11937.8818359375
tensor(11937.8838, grad_fn=<NegBackward0>) tensor(11937.8818, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11937.880859375
tensor(11937.8818, grad_fn=<NegBackward0>) tensor(11937.8809, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11937.8818359375
tensor(11937.8809, grad_fn=<NegBackward0>) tensor(11937.8818, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11937.87890625
tensor(11937.8809, grad_fn=<NegBackward0>) tensor(11937.8789, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11937.8798828125
tensor(11937.8789, grad_fn=<NegBackward0>) tensor(11937.8799, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11937.87890625
tensor(11937.8789, grad_fn=<NegBackward0>) tensor(11937.8789, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11937.8779296875
tensor(11937.8789, grad_fn=<NegBackward0>) tensor(11937.8779, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11937.8720703125
tensor(11937.8779, grad_fn=<NegBackward0>) tensor(11937.8721, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11937.83984375
tensor(11937.8721, grad_fn=<NegBackward0>) tensor(11937.8398, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11937.83984375
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8398, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11937.83984375
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8398, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11937.837890625
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8379, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11937.837890625
tensor(11937.8379, grad_fn=<NegBackward0>) tensor(11937.8379, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11937.837890625
tensor(11937.8379, grad_fn=<NegBackward0>) tensor(11937.8379, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11937.8427734375
tensor(11937.8379, grad_fn=<NegBackward0>) tensor(11937.8428, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11937.837890625
tensor(11937.8379, grad_fn=<NegBackward0>) tensor(11937.8379, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11937.84375
tensor(11937.8379, grad_fn=<NegBackward0>) tensor(11937.8438, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11937.8359375
tensor(11937.8379, grad_fn=<NegBackward0>) tensor(11937.8359, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11937.8427734375
tensor(11937.8359, grad_fn=<NegBackward0>) tensor(11937.8428, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11937.833984375
tensor(11937.8359, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11937.8330078125
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8330, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11937.8349609375
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8350, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11937.833984375
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11937.833984375
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11937.833984375
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11937.83203125
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8320, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11937.677734375
tensor(11937.8320, grad_fn=<NegBackward0>) tensor(11937.6777, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11937.6708984375
tensor(11937.6777, grad_fn=<NegBackward0>) tensor(11937.6709, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11937.6708984375
tensor(11937.6709, grad_fn=<NegBackward0>) tensor(11937.6709, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11937.6708984375
tensor(11937.6709, grad_fn=<NegBackward0>) tensor(11937.6709, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11937.6689453125
tensor(11937.6709, grad_fn=<NegBackward0>) tensor(11937.6689, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11937.6689453125
tensor(11937.6689, grad_fn=<NegBackward0>) tensor(11937.6689, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11937.677734375
tensor(11937.6689, grad_fn=<NegBackward0>) tensor(11937.6777, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11937.6669921875
tensor(11937.6689, grad_fn=<NegBackward0>) tensor(11937.6670, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11937.6689453125
tensor(11937.6670, grad_fn=<NegBackward0>) tensor(11937.6689, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11937.66796875
tensor(11937.6670, grad_fn=<NegBackward0>) tensor(11937.6680, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11937.66796875
tensor(11937.6670, grad_fn=<NegBackward0>) tensor(11937.6680, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -11937.6689453125
tensor(11937.6670, grad_fn=<NegBackward0>) tensor(11937.6689, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -11937.66796875
tensor(11937.6670, grad_fn=<NegBackward0>) tensor(11937.6680, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[0.6807, 0.3193],
        [0.2304, 0.7696]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5200, 0.4800], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3067, 0.1032],
         [0.5644, 0.2955]],

        [[0.6205, 0.0916],
         [0.6545, 0.6424]],

        [[0.5498, 0.1003],
         [0.5371, 0.5268]],

        [[0.5806, 0.1086],
         [0.5076, 0.5620]],

        [[0.5036, 0.1032],
         [0.6765, 0.6680]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999034306759
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21433.408203125
inf tensor(21433.4082, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12434.80859375
tensor(21433.4082, grad_fn=<NegBackward0>) tensor(12434.8086, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12433.884765625
tensor(12434.8086, grad_fn=<NegBackward0>) tensor(12433.8848, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12433.3037109375
tensor(12433.8848, grad_fn=<NegBackward0>) tensor(12433.3037, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12433.1669921875
tensor(12433.3037, grad_fn=<NegBackward0>) tensor(12433.1670, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12433.107421875
tensor(12433.1670, grad_fn=<NegBackward0>) tensor(12433.1074, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12433.0595703125
tensor(12433.1074, grad_fn=<NegBackward0>) tensor(12433.0596, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12432.9970703125
tensor(12433.0596, grad_fn=<NegBackward0>) tensor(12432.9971, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12432.890625
tensor(12432.9971, grad_fn=<NegBackward0>) tensor(12432.8906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12432.7158203125
tensor(12432.8906, grad_fn=<NegBackward0>) tensor(12432.7158, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12432.4921875
tensor(12432.7158, grad_fn=<NegBackward0>) tensor(12432.4922, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12432.302734375
tensor(12432.4922, grad_fn=<NegBackward0>) tensor(12432.3027, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12432.1865234375
tensor(12432.3027, grad_fn=<NegBackward0>) tensor(12432.1865, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12432.11328125
tensor(12432.1865, grad_fn=<NegBackward0>) tensor(12432.1133, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12432.060546875
tensor(12432.1133, grad_fn=<NegBackward0>) tensor(12432.0605, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12432.0234375
tensor(12432.0605, grad_fn=<NegBackward0>) tensor(12432.0234, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12431.9921875
tensor(12432.0234, grad_fn=<NegBackward0>) tensor(12431.9922, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12431.966796875
tensor(12431.9922, grad_fn=<NegBackward0>) tensor(12431.9668, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12431.8916015625
tensor(12431.9668, grad_fn=<NegBackward0>) tensor(12431.8916, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12174.7421875
tensor(12431.8916, grad_fn=<NegBackward0>) tensor(12174.7422, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12116.748046875
tensor(12174.7422, grad_fn=<NegBackward0>) tensor(12116.7480, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12059.0166015625
tensor(12116.7480, grad_fn=<NegBackward0>) tensor(12059.0166, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12051.8515625
tensor(12059.0166, grad_fn=<NegBackward0>) tensor(12051.8516, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12004.7265625
tensor(12051.8516, grad_fn=<NegBackward0>) tensor(12004.7266, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11974.326171875
tensor(12004.7266, grad_fn=<NegBackward0>) tensor(11974.3262, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11974.3134765625
tensor(11974.3262, grad_fn=<NegBackward0>) tensor(11974.3135, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11974.306640625
tensor(11974.3135, grad_fn=<NegBackward0>) tensor(11974.3066, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11974.302734375
tensor(11974.3066, grad_fn=<NegBackward0>) tensor(11974.3027, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11974.2958984375
tensor(11974.3027, grad_fn=<NegBackward0>) tensor(11974.2959, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11974.2861328125
tensor(11974.2959, grad_fn=<NegBackward0>) tensor(11974.2861, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11974.2802734375
tensor(11974.2861, grad_fn=<NegBackward0>) tensor(11974.2803, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11970.1142578125
tensor(11974.2803, grad_fn=<NegBackward0>) tensor(11970.1143, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11970.09375
tensor(11970.1143, grad_fn=<NegBackward0>) tensor(11970.0938, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11970.091796875
tensor(11970.0938, grad_fn=<NegBackward0>) tensor(11970.0918, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11970.0908203125
tensor(11970.0918, grad_fn=<NegBackward0>) tensor(11970.0908, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11965.9169921875
tensor(11970.0908, grad_fn=<NegBackward0>) tensor(11965.9170, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11954.4609375
tensor(11965.9170, grad_fn=<NegBackward0>) tensor(11954.4609, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11954.4599609375
tensor(11954.4609, grad_fn=<NegBackward0>) tensor(11954.4600, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11954.4580078125
tensor(11954.4600, grad_fn=<NegBackward0>) tensor(11954.4580, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11954.458984375
tensor(11954.4580, grad_fn=<NegBackward0>) tensor(11954.4590, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11954.4541015625
tensor(11954.4580, grad_fn=<NegBackward0>) tensor(11954.4541, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11937.8984375
tensor(11954.4541, grad_fn=<NegBackward0>) tensor(11937.8984, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11937.849609375
tensor(11937.8984, grad_fn=<NegBackward0>) tensor(11937.8496, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11937.84375
tensor(11937.8496, grad_fn=<NegBackward0>) tensor(11937.8438, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11937.8486328125
tensor(11937.8438, grad_fn=<NegBackward0>) tensor(11937.8486, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11937.8427734375
tensor(11937.8438, grad_fn=<NegBackward0>) tensor(11937.8428, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11937.841796875
tensor(11937.8428, grad_fn=<NegBackward0>) tensor(11937.8418, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11937.841796875
tensor(11937.8418, grad_fn=<NegBackward0>) tensor(11937.8418, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11937.841796875
tensor(11937.8418, grad_fn=<NegBackward0>) tensor(11937.8418, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11937.8408203125
tensor(11937.8418, grad_fn=<NegBackward0>) tensor(11937.8408, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11937.8408203125
tensor(11937.8408, grad_fn=<NegBackward0>) tensor(11937.8408, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11937.8447265625
tensor(11937.8408, grad_fn=<NegBackward0>) tensor(11937.8447, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11937.83984375
tensor(11937.8408, grad_fn=<NegBackward0>) tensor(11937.8398, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11937.8447265625
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8447, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11937.83984375
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8398, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11937.8505859375
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8506, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11937.8408203125
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8408, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11937.841796875
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8418, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11937.8466796875
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8467, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11937.8359375
tensor(11937.8398, grad_fn=<NegBackward0>) tensor(11937.8359, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11937.833984375
tensor(11937.8359, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11937.833984375
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11937.8359375
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8359, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11937.8349609375
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8350, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11937.8349609375
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8350, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11937.833984375
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11937.8330078125
tensor(11937.8340, grad_fn=<NegBackward0>) tensor(11937.8330, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11937.8330078125
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8330, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11937.84765625
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8477, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11937.833984375
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11937.837890625
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8379, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11937.83203125
tensor(11937.8330, grad_fn=<NegBackward0>) tensor(11937.8320, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11937.83203125
tensor(11937.8320, grad_fn=<NegBackward0>) tensor(11937.8320, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11937.8369140625
tensor(11937.8320, grad_fn=<NegBackward0>) tensor(11937.8369, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11937.83203125
tensor(11937.8320, grad_fn=<NegBackward0>) tensor(11937.8320, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11937.8291015625
tensor(11937.8320, grad_fn=<NegBackward0>) tensor(11937.8291, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11937.830078125
tensor(11937.8291, grad_fn=<NegBackward0>) tensor(11937.8301, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11937.83203125
tensor(11937.8291, grad_fn=<NegBackward0>) tensor(11937.8320, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11937.833984375
tensor(11937.8291, grad_fn=<NegBackward0>) tensor(11937.8340, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11937.875
tensor(11937.8291, grad_fn=<NegBackward0>) tensor(11937.8750, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11937.830078125
tensor(11937.8291, grad_fn=<NegBackward0>) tensor(11937.8301, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.7704, 0.2296],
        [0.3195, 0.6805]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4776, 0.5224], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2957, 0.1031],
         [0.5046, 0.3063]],

        [[0.7132, 0.0916],
         [0.5812, 0.6035]],

        [[0.7200, 0.1003],
         [0.5663, 0.5120]],

        [[0.6462, 0.1086],
         [0.5716, 0.6096]],

        [[0.6727, 0.1032],
         [0.5492, 0.5991]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999034306759
Average Adjusted Rand Index: 0.9919993417272899
[0.9919999034306759, 0.9919999034306759] [0.9919993417272899, 0.9919993417272899] [11937.66796875, 11937.830078125]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11898.07399173985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22812.724609375
inf tensor(22812.7246, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12363.6201171875
tensor(22812.7246, grad_fn=<NegBackward0>) tensor(12363.6201, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12363.0625
tensor(12363.6201, grad_fn=<NegBackward0>) tensor(12363.0625, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12362.9326171875
tensor(12363.0625, grad_fn=<NegBackward0>) tensor(12362.9326, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.86328125
tensor(12362.9326, grad_fn=<NegBackward0>) tensor(12362.8633, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.8212890625
tensor(12362.8633, grad_fn=<NegBackward0>) tensor(12362.8213, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.7919921875
tensor(12362.8213, grad_fn=<NegBackward0>) tensor(12362.7920, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.7744140625
tensor(12362.7920, grad_fn=<NegBackward0>) tensor(12362.7744, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.7578125
tensor(12362.7744, grad_fn=<NegBackward0>) tensor(12362.7578, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.7470703125
tensor(12362.7578, grad_fn=<NegBackward0>) tensor(12362.7471, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.736328125
tensor(12362.7471, grad_fn=<NegBackward0>) tensor(12362.7363, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.7275390625
tensor(12362.7363, grad_fn=<NegBackward0>) tensor(12362.7275, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.7197265625
tensor(12362.7275, grad_fn=<NegBackward0>) tensor(12362.7197, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12362.7138671875
tensor(12362.7197, grad_fn=<NegBackward0>) tensor(12362.7139, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12362.70703125
tensor(12362.7139, grad_fn=<NegBackward0>) tensor(12362.7070, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12362.6982421875
tensor(12362.7070, grad_fn=<NegBackward0>) tensor(12362.6982, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12362.69140625
tensor(12362.6982, grad_fn=<NegBackward0>) tensor(12362.6914, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12362.6845703125
tensor(12362.6914, grad_fn=<NegBackward0>) tensor(12362.6846, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12362.677734375
tensor(12362.6846, grad_fn=<NegBackward0>) tensor(12362.6777, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12362.6689453125
tensor(12362.6777, grad_fn=<NegBackward0>) tensor(12362.6689, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12362.662109375
tensor(12362.6689, grad_fn=<NegBackward0>) tensor(12362.6621, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12362.6552734375
tensor(12362.6621, grad_fn=<NegBackward0>) tensor(12362.6553, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12362.650390625
tensor(12362.6553, grad_fn=<NegBackward0>) tensor(12362.6504, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12362.642578125
tensor(12362.6504, grad_fn=<NegBackward0>) tensor(12362.6426, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12362.6376953125
tensor(12362.6426, grad_fn=<NegBackward0>) tensor(12362.6377, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12362.6318359375
tensor(12362.6377, grad_fn=<NegBackward0>) tensor(12362.6318, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12362.6240234375
tensor(12362.6318, grad_fn=<NegBackward0>) tensor(12362.6240, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12362.615234375
tensor(12362.6240, grad_fn=<NegBackward0>) tensor(12362.6152, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12362.60546875
tensor(12362.6152, grad_fn=<NegBackward0>) tensor(12362.6055, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12362.5947265625
tensor(12362.6055, grad_fn=<NegBackward0>) tensor(12362.5947, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12362.583984375
tensor(12362.5947, grad_fn=<NegBackward0>) tensor(12362.5840, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12362.5703125
tensor(12362.5840, grad_fn=<NegBackward0>) tensor(12362.5703, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12362.55859375
tensor(12362.5703, grad_fn=<NegBackward0>) tensor(12362.5586, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12362.5478515625
tensor(12362.5586, grad_fn=<NegBackward0>) tensor(12362.5479, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12362.537109375
tensor(12362.5479, grad_fn=<NegBackward0>) tensor(12362.5371, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12362.5283203125
tensor(12362.5371, grad_fn=<NegBackward0>) tensor(12362.5283, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12362.5224609375
tensor(12362.5283, grad_fn=<NegBackward0>) tensor(12362.5225, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12362.517578125
tensor(12362.5225, grad_fn=<NegBackward0>) tensor(12362.5176, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12362.51171875
tensor(12362.5176, grad_fn=<NegBackward0>) tensor(12362.5117, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12362.505859375
tensor(12362.5117, grad_fn=<NegBackward0>) tensor(12362.5059, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12362.5029296875
tensor(12362.5059, grad_fn=<NegBackward0>) tensor(12362.5029, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12362.501953125
tensor(12362.5029, grad_fn=<NegBackward0>) tensor(12362.5020, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12362.4970703125
tensor(12362.5020, grad_fn=<NegBackward0>) tensor(12362.4971, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12362.5087890625
tensor(12362.4971, grad_fn=<NegBackward0>) tensor(12362.5088, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12362.490234375
tensor(12362.4971, grad_fn=<NegBackward0>) tensor(12362.4902, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12362.486328125
tensor(12362.4902, grad_fn=<NegBackward0>) tensor(12362.4863, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12362.4833984375
tensor(12362.4863, grad_fn=<NegBackward0>) tensor(12362.4834, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12362.4814453125
tensor(12362.4834, grad_fn=<NegBackward0>) tensor(12362.4814, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12362.4912109375
tensor(12362.4814, grad_fn=<NegBackward0>) tensor(12362.4912, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12362.4765625
tensor(12362.4814, grad_fn=<NegBackward0>) tensor(12362.4766, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12362.474609375
tensor(12362.4766, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12362.4736328125
tensor(12362.4746, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12362.474609375
tensor(12362.4736, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12362.4736328125
tensor(12362.4736, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12362.4814453125
tensor(12362.4736, grad_fn=<NegBackward0>) tensor(12362.4814, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12362.4716796875
tensor(12362.4736, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12362.4736328125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12362.4736328125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12362.5556640625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.5557, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12362.494140625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4941, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12362.470703125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4707, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12362.5068359375
tensor(12362.4707, grad_fn=<NegBackward0>) tensor(12362.5068, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12362.47265625
tensor(12362.4707, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12362.482421875
tensor(12362.4707, grad_fn=<NegBackward0>) tensor(12362.4824, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12362.47265625
tensor(12362.4707, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12362.4697265625
tensor(12362.4707, grad_fn=<NegBackward0>) tensor(12362.4697, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12362.47265625
tensor(12362.4697, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12362.4833984375
tensor(12362.4697, grad_fn=<NegBackward0>) tensor(12362.4834, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12362.478515625
tensor(12362.4697, grad_fn=<NegBackward0>) tensor(12362.4785, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12362.4716796875
tensor(12362.4697, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -12362.474609375
tensor(12362.4697, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.0352, 0.9648],
        [0.7126, 0.2874]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0461, 0.9539], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.1950],
         [0.7028, 0.2004]],

        [[0.6619, 0.1953],
         [0.5262, 0.6001]],

        [[0.6629, 0.2018],
         [0.6841, 0.5613]],

        [[0.6736, 0.2025],
         [0.5291, 0.7242]],

        [[0.7004, 0.1979],
         [0.5592, 0.7291]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015322869495162917
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19866.251953125
inf tensor(19866.2520, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12363.2919921875
tensor(19866.2520, grad_fn=<NegBackward0>) tensor(12363.2920, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12362.9033203125
tensor(12363.2920, grad_fn=<NegBackward0>) tensor(12362.9033, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12362.8095703125
tensor(12362.9033, grad_fn=<NegBackward0>) tensor(12362.8096, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12362.765625
tensor(12362.8096, grad_fn=<NegBackward0>) tensor(12362.7656, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12362.7392578125
tensor(12362.7656, grad_fn=<NegBackward0>) tensor(12362.7393, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12362.7216796875
tensor(12362.7393, grad_fn=<NegBackward0>) tensor(12362.7217, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12362.7119140625
tensor(12362.7217, grad_fn=<NegBackward0>) tensor(12362.7119, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12362.701171875
tensor(12362.7119, grad_fn=<NegBackward0>) tensor(12362.7012, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12362.693359375
tensor(12362.7012, grad_fn=<NegBackward0>) tensor(12362.6934, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12362.6875
tensor(12362.6934, grad_fn=<NegBackward0>) tensor(12362.6875, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12362.6796875
tensor(12362.6875, grad_fn=<NegBackward0>) tensor(12362.6797, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12362.67578125
tensor(12362.6797, grad_fn=<NegBackward0>) tensor(12362.6758, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12362.671875
tensor(12362.6758, grad_fn=<NegBackward0>) tensor(12362.6719, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12362.6650390625
tensor(12362.6719, grad_fn=<NegBackward0>) tensor(12362.6650, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12362.662109375
tensor(12362.6650, grad_fn=<NegBackward0>) tensor(12362.6621, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12362.6572265625
tensor(12362.6621, grad_fn=<NegBackward0>) tensor(12362.6572, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12362.654296875
tensor(12362.6572, grad_fn=<NegBackward0>) tensor(12362.6543, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12362.6494140625
tensor(12362.6543, grad_fn=<NegBackward0>) tensor(12362.6494, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12362.6474609375
tensor(12362.6494, grad_fn=<NegBackward0>) tensor(12362.6475, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12362.6435546875
tensor(12362.6475, grad_fn=<NegBackward0>) tensor(12362.6436, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12362.6416015625
tensor(12362.6436, grad_fn=<NegBackward0>) tensor(12362.6416, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12362.6396484375
tensor(12362.6416, grad_fn=<NegBackward0>) tensor(12362.6396, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12362.6357421875
tensor(12362.6396, grad_fn=<NegBackward0>) tensor(12362.6357, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12362.6337890625
tensor(12362.6357, grad_fn=<NegBackward0>) tensor(12362.6338, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12362.6298828125
tensor(12362.6338, grad_fn=<NegBackward0>) tensor(12362.6299, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12362.6279296875
tensor(12362.6299, grad_fn=<NegBackward0>) tensor(12362.6279, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12362.6240234375
tensor(12362.6279, grad_fn=<NegBackward0>) tensor(12362.6240, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12362.623046875
tensor(12362.6240, grad_fn=<NegBackward0>) tensor(12362.6230, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12362.619140625
tensor(12362.6230, grad_fn=<NegBackward0>) tensor(12362.6191, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12362.6142578125
tensor(12362.6191, grad_fn=<NegBackward0>) tensor(12362.6143, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12362.611328125
tensor(12362.6143, grad_fn=<NegBackward0>) tensor(12362.6113, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12362.6044921875
tensor(12362.6113, grad_fn=<NegBackward0>) tensor(12362.6045, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12362.5986328125
tensor(12362.6045, grad_fn=<NegBackward0>) tensor(12362.5986, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12362.5908203125
tensor(12362.5986, grad_fn=<NegBackward0>) tensor(12362.5908, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12362.583984375
tensor(12362.5908, grad_fn=<NegBackward0>) tensor(12362.5840, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12362.576171875
tensor(12362.5840, grad_fn=<NegBackward0>) tensor(12362.5762, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12362.5693359375
tensor(12362.5762, grad_fn=<NegBackward0>) tensor(12362.5693, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12362.5615234375
tensor(12362.5693, grad_fn=<NegBackward0>) tensor(12362.5615, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12362.5546875
tensor(12362.5615, grad_fn=<NegBackward0>) tensor(12362.5547, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12362.5458984375
tensor(12362.5547, grad_fn=<NegBackward0>) tensor(12362.5459, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12362.5419921875
tensor(12362.5459, grad_fn=<NegBackward0>) tensor(12362.5420, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12362.5361328125
tensor(12362.5420, grad_fn=<NegBackward0>) tensor(12362.5361, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12362.5302734375
tensor(12362.5361, grad_fn=<NegBackward0>) tensor(12362.5303, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12362.525390625
tensor(12362.5303, grad_fn=<NegBackward0>) tensor(12362.5254, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12362.5205078125
tensor(12362.5254, grad_fn=<NegBackward0>) tensor(12362.5205, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12362.517578125
tensor(12362.5205, grad_fn=<NegBackward0>) tensor(12362.5176, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12362.513671875
tensor(12362.5176, grad_fn=<NegBackward0>) tensor(12362.5137, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12362.509765625
tensor(12362.5137, grad_fn=<NegBackward0>) tensor(12362.5098, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12362.5068359375
tensor(12362.5098, grad_fn=<NegBackward0>) tensor(12362.5068, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12362.541015625
tensor(12362.5068, grad_fn=<NegBackward0>) tensor(12362.5410, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12362.5009765625
tensor(12362.5068, grad_fn=<NegBackward0>) tensor(12362.5010, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12362.5439453125
tensor(12362.5010, grad_fn=<NegBackward0>) tensor(12362.5439, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12362.498046875
tensor(12362.5010, grad_fn=<NegBackward0>) tensor(12362.4980, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12362.494140625
tensor(12362.4980, grad_fn=<NegBackward0>) tensor(12362.4941, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12362.4931640625
tensor(12362.4941, grad_fn=<NegBackward0>) tensor(12362.4932, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12362.4912109375
tensor(12362.4932, grad_fn=<NegBackward0>) tensor(12362.4912, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12362.4873046875
tensor(12362.4912, grad_fn=<NegBackward0>) tensor(12362.4873, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12362.486328125
tensor(12362.4873, grad_fn=<NegBackward0>) tensor(12362.4863, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12362.5361328125
tensor(12362.4863, grad_fn=<NegBackward0>) tensor(12362.5361, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12362.4794921875
tensor(12362.4863, grad_fn=<NegBackward0>) tensor(12362.4795, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12362.478515625
tensor(12362.4795, grad_fn=<NegBackward0>) tensor(12362.4785, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12362.4775390625
tensor(12362.4785, grad_fn=<NegBackward0>) tensor(12362.4775, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12362.474609375
tensor(12362.4775, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12362.474609375
tensor(12362.4746, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12362.47265625
tensor(12362.4746, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12362.4736328125
tensor(12362.4727, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12362.47265625
tensor(12362.4727, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12362.474609375
tensor(12362.4727, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12362.47265625
tensor(12362.4727, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12362.4716796875
tensor(12362.4727, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12362.4736328125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12362.4736328125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12362.6689453125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.6689, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12362.4853515625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4854, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12362.4765625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4766, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12362.4736328125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12362.474609375
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4746, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12362.4716796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4717, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12362.5341796875
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.5342, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12362.4736328125
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4736, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12362.7197265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.7197, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -12362.47265625
tensor(12362.4717, grad_fn=<NegBackward0>) tensor(12362.4727, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.0373, 0.9627],
        [0.7145, 0.2855]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0461, 0.9539], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1975, 0.1950],
         [0.7219, 0.2004]],

        [[0.6986, 0.1952],
         [0.6296, 0.6922]],

        [[0.5490, 0.2017],
         [0.6498, 0.6006]],

        [[0.6004, 0.2024],
         [0.5074, 0.5280]],

        [[0.6979, 0.1979],
         [0.6819, 0.5756]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0015322869495162917
Average Adjusted Rand Index: 0.0
[-0.0015322869495162917, -0.0015322869495162917] [0.0, 0.0] [12362.474609375, 12362.47265625]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11645.988579278048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21720.287109375
inf tensor(21720.2871, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12187.30078125
tensor(21720.2871, grad_fn=<NegBackward0>) tensor(12187.3008, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12186.990234375
tensor(12187.3008, grad_fn=<NegBackward0>) tensor(12186.9902, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12186.9248046875
tensor(12186.9902, grad_fn=<NegBackward0>) tensor(12186.9248, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12186.8525390625
tensor(12186.9248, grad_fn=<NegBackward0>) tensor(12186.8525, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12186.744140625
tensor(12186.8525, grad_fn=<NegBackward0>) tensor(12186.7441, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12186.580078125
tensor(12186.7441, grad_fn=<NegBackward0>) tensor(12186.5801, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12186.4052734375
tensor(12186.5801, grad_fn=<NegBackward0>) tensor(12186.4053, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12186.287109375
tensor(12186.4053, grad_fn=<NegBackward0>) tensor(12186.2871, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12186.220703125
tensor(12186.2871, grad_fn=<NegBackward0>) tensor(12186.2207, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12186.1796875
tensor(12186.2207, grad_fn=<NegBackward0>) tensor(12186.1797, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12186.150390625
tensor(12186.1797, grad_fn=<NegBackward0>) tensor(12186.1504, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12186.1279296875
tensor(12186.1504, grad_fn=<NegBackward0>) tensor(12186.1279, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12186.111328125
tensor(12186.1279, grad_fn=<NegBackward0>) tensor(12186.1113, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12186.09765625
tensor(12186.1113, grad_fn=<NegBackward0>) tensor(12186.0977, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12186.0888671875
tensor(12186.0977, grad_fn=<NegBackward0>) tensor(12186.0889, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12186.080078125
tensor(12186.0889, grad_fn=<NegBackward0>) tensor(12186.0801, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12186.07421875
tensor(12186.0801, grad_fn=<NegBackward0>) tensor(12186.0742, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12186.0693359375
tensor(12186.0742, grad_fn=<NegBackward0>) tensor(12186.0693, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12186.06640625
tensor(12186.0693, grad_fn=<NegBackward0>) tensor(12186.0664, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12186.0654296875
tensor(12186.0664, grad_fn=<NegBackward0>) tensor(12186.0654, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12186.0654296875
tensor(12186.0654, grad_fn=<NegBackward0>) tensor(12186.0654, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12186.064453125
tensor(12186.0654, grad_fn=<NegBackward0>) tensor(12186.0645, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12186.0625
tensor(12186.0645, grad_fn=<NegBackward0>) tensor(12186.0625, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12186.060546875
tensor(12186.0625, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12186.0615234375
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0615, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -12186.060546875
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12186.060546875
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12186.060546875
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12186.0595703125
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12186.0595703125
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12186.0595703125
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12186.060546875
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12186.05859375
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0586, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12186.056640625
tensor(12186.0586, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12186.0576171875
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0576, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12186.05859375
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0586, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -12186.0576171875
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0576, grad_fn=<NegBackward0>)
3
Iteration 3800: Loss = -12186.056640625
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12186.0556640625
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0557, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12186.056640625
tensor(12186.0557, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12186.056640625
tensor(12186.0557, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12186.0546875
tensor(12186.0557, grad_fn=<NegBackward0>) tensor(12186.0547, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12186.0537109375
tensor(12186.0547, grad_fn=<NegBackward0>) tensor(12186.0537, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12186.0546875
tensor(12186.0537, grad_fn=<NegBackward0>) tensor(12186.0547, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12186.0537109375
tensor(12186.0537, grad_fn=<NegBackward0>) tensor(12186.0537, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12186.0537109375
tensor(12186.0537, grad_fn=<NegBackward0>) tensor(12186.0537, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12186.05078125
tensor(12186.0537, grad_fn=<NegBackward0>) tensor(12186.0508, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12186.05078125
tensor(12186.0508, grad_fn=<NegBackward0>) tensor(12186.0508, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12186.048828125
tensor(12186.0508, grad_fn=<NegBackward0>) tensor(12186.0488, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12186.0478515625
tensor(12186.0488, grad_fn=<NegBackward0>) tensor(12186.0479, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12186.0439453125
tensor(12186.0479, grad_fn=<NegBackward0>) tensor(12186.0439, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12186.0341796875
tensor(12186.0439, grad_fn=<NegBackward0>) tensor(12186.0342, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12185.98828125
tensor(12186.0342, grad_fn=<NegBackward0>) tensor(12185.9883, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12184.8935546875
tensor(12185.9883, grad_fn=<NegBackward0>) tensor(12184.8936, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12183.7265625
tensor(12184.8936, grad_fn=<NegBackward0>) tensor(12183.7266, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12182.8759765625
tensor(12183.7266, grad_fn=<NegBackward0>) tensor(12182.8760, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12182.55078125
tensor(12182.8760, grad_fn=<NegBackward0>) tensor(12182.5508, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12182.513671875
tensor(12182.5508, grad_fn=<NegBackward0>) tensor(12182.5137, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12182.4970703125
tensor(12182.5137, grad_fn=<NegBackward0>) tensor(12182.4971, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12182.486328125
tensor(12182.4971, grad_fn=<NegBackward0>) tensor(12182.4863, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12182.48046875
tensor(12182.4863, grad_fn=<NegBackward0>) tensor(12182.4805, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12182.4736328125
tensor(12182.4805, grad_fn=<NegBackward0>) tensor(12182.4736, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12182.46875
tensor(12182.4736, grad_fn=<NegBackward0>) tensor(12182.4688, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12182.4677734375
tensor(12182.4688, grad_fn=<NegBackward0>) tensor(12182.4678, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12182.4638671875
tensor(12182.4678, grad_fn=<NegBackward0>) tensor(12182.4639, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12182.462890625
tensor(12182.4639, grad_fn=<NegBackward0>) tensor(12182.4629, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12182.4619140625
tensor(12182.4629, grad_fn=<NegBackward0>) tensor(12182.4619, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12182.4609375
tensor(12182.4619, grad_fn=<NegBackward0>) tensor(12182.4609, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12182.458984375
tensor(12182.4609, grad_fn=<NegBackward0>) tensor(12182.4590, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12182.45703125
tensor(12182.4590, grad_fn=<NegBackward0>) tensor(12182.4570, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12182.458984375
tensor(12182.4570, grad_fn=<NegBackward0>) tensor(12182.4590, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12182.572265625
tensor(12182.4570, grad_fn=<NegBackward0>) tensor(12182.5723, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12182.4560546875
tensor(12182.4570, grad_fn=<NegBackward0>) tensor(12182.4561, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12182.4560546875
tensor(12182.4561, grad_fn=<NegBackward0>) tensor(12182.4561, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12182.4541015625
tensor(12182.4561, grad_fn=<NegBackward0>) tensor(12182.4541, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12182.4580078125
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4580, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12182.4541015625
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4541, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12182.4521484375
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4521, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12182.453125
tensor(12182.4521, grad_fn=<NegBackward0>) tensor(12182.4531, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12182.431640625
tensor(12182.4521, grad_fn=<NegBackward0>) tensor(12182.4316, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12182.4306640625
tensor(12182.4316, grad_fn=<NegBackward0>) tensor(12182.4307, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12182.4306640625
tensor(12182.4307, grad_fn=<NegBackward0>) tensor(12182.4307, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12182.4306640625
tensor(12182.4307, grad_fn=<NegBackward0>) tensor(12182.4307, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12182.4326171875
tensor(12182.4307, grad_fn=<NegBackward0>) tensor(12182.4326, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12182.4287109375
tensor(12182.4307, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12182.427734375
tensor(12182.4287, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12182.4306640625
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4307, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12182.427734375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12182.4423828125
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4424, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12182.4287109375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12182.4287109375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12182.427734375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12182.4296875
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4297, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12182.4287109375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -12182.427734375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12182.4287109375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12182.427734375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12182.466796875
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4668, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12182.427734375
tensor(12182.4277, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
pi: tensor([[9.9997e-01, 2.8705e-05],
        [6.4554e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0705, 0.9295], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4102, 0.1390],
         [0.6245, 0.1937]],

        [[0.6654, 0.2067],
         [0.5835, 0.6943]],

        [[0.5427, 0.2177],
         [0.6356, 0.7237]],

        [[0.5733, 0.1581],
         [0.5936, 0.5317]],

        [[0.6623, 0.2282],
         [0.6546, 0.7257]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.020424516829035635
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013289782914094903
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 36%|███▌      | 36/100 [8:47:18<17:49:40, 1002.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 37%|███▋      | 37/100 [9:05:03<17:52:31, 1021.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 38%|███▊      | 38/100 [9:17:09<16:03:52, 932.78s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 39%|███▉      | 39/100 [9:34:25<16:19:45, 963.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 40%|████      | 40/100 [9:48:43<15:32:01, 932.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 41%|████      | 41/100 [10:01:42<14:31:35, 886.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 42%|████▏     | 42/100 [10:15:13<13:54:48, 863.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 43%|████▎     | 43/100 [10:27:42<13:07:39, 829.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 44%|████▍     | 44/100 [10:44:40<13:46:48, 885.87s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 45%|████▌     | 45/100 [10:59:33<13:34:02, 888.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 46%|████▌     | 46/100 [11:17:34<14:11:14, 945.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 47%|████▋     | 47/100 [11:35:22<14:27:53, 982.52s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 48%|████▊     | 48/100 [11:48:56<13:27:52, 932.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 49%|████▉     | 49/100 [12:02:04<12:35:27, 888.77s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 50%|█████     | 50/100 [12:18:12<12:40:21, 912.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 51%|█████     | 51/100 [12:29:42<11:30:44, 845.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 52%|█████▏    | 52/100 [12:45:01<11:34:11, 867.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 53%|█████▎    | 53/100 [13:03:20<12:14:09, 937.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 54%|█████▍    | 54/100 [13:18:34<11:53:12, 930.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 55%|█████▌    | 55/100 [13:33:58<11:36:09, 928.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 56%|█████▌    | 56/100 [13:46:30<10:42:03, 875.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 57%|█████▋    | 57/100 [13:58:18<9:51:28, 825.31s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 58%|█████▊    | 58/100 [14:08:59<8:59:00, 770.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 59%|█████▉    | 59/100 [14:21:24<8:40:53, 762.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 60%|██████    | 60/100 [14:33:01<8:15:15, 742.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 61%|██████    | 61/100 [14:50:13<8:59:06, 829.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 62%|██████▏   | 62/100 [15:01:19<8:14:25, 780.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 63%|██████▎   | 63/100 [15:13:33<7:52:39, 766.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 64%|██████▍   | 64/100 [15:31:18<8:33:39, 856.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 65%|██████▌   | 65/100 [15:48:38<8:51:28, 911.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 66%|██████▌   | 66/100 [16:04:02<8:38:34, 915.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 67%|██████▋   | 67/100 [16:21:57<8:49:42, 963.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 68%|██████▊   | 68/100 [16:32:49<7:43:52, 869.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 69%|██████▉   | 69/100 [16:48:41<7:42:09, 894.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 70%|███████   | 70/100 [17:00:23<6:58:16, 836.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 71%|███████   | 71/100 [17:16:29<7:03:07, 875.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
43
Adjusted Rand Index: 0.01701445012943135
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.00021025550424088273
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.00021025550424088273
Global Adjusted Rand Index: -0.0008043148361097795
Average Adjusted Rand Index: 0.007137895531715145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20812.388671875
inf tensor(20812.3887, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12187.2158203125
tensor(20812.3887, grad_fn=<NegBackward0>) tensor(12187.2158, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12186.9755859375
tensor(12187.2158, grad_fn=<NegBackward0>) tensor(12186.9756, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12186.927734375
tensor(12186.9756, grad_fn=<NegBackward0>) tensor(12186.9277, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12186.8779296875
tensor(12186.9277, grad_fn=<NegBackward0>) tensor(12186.8779, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12186.8134765625
tensor(12186.8779, grad_fn=<NegBackward0>) tensor(12186.8135, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12186.712890625
tensor(12186.8135, grad_fn=<NegBackward0>) tensor(12186.7129, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12186.568359375
tensor(12186.7129, grad_fn=<NegBackward0>) tensor(12186.5684, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12186.4072265625
tensor(12186.5684, grad_fn=<NegBackward0>) tensor(12186.4072, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12186.2900390625
tensor(12186.4072, grad_fn=<NegBackward0>) tensor(12186.2900, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12186.2255859375
tensor(12186.2900, grad_fn=<NegBackward0>) tensor(12186.2256, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12186.185546875
tensor(12186.2256, grad_fn=<NegBackward0>) tensor(12186.1855, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12186.154296875
tensor(12186.1855, grad_fn=<NegBackward0>) tensor(12186.1543, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12186.1318359375
tensor(12186.1543, grad_fn=<NegBackward0>) tensor(12186.1318, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12186.1123046875
tensor(12186.1318, grad_fn=<NegBackward0>) tensor(12186.1123, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12186.0986328125
tensor(12186.1123, grad_fn=<NegBackward0>) tensor(12186.0986, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12186.0859375
tensor(12186.0986, grad_fn=<NegBackward0>) tensor(12186.0859, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12186.078125
tensor(12186.0859, grad_fn=<NegBackward0>) tensor(12186.0781, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12186.072265625
tensor(12186.0781, grad_fn=<NegBackward0>) tensor(12186.0723, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12186.06640625
tensor(12186.0723, grad_fn=<NegBackward0>) tensor(12186.0664, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12186.064453125
tensor(12186.0664, grad_fn=<NegBackward0>) tensor(12186.0645, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12186.0625
tensor(12186.0645, grad_fn=<NegBackward0>) tensor(12186.0625, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12186.0625
tensor(12186.0625, grad_fn=<NegBackward0>) tensor(12186.0625, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12186.0625
tensor(12186.0625, grad_fn=<NegBackward0>) tensor(12186.0625, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12186.0615234375
tensor(12186.0625, grad_fn=<NegBackward0>) tensor(12186.0615, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12186.0615234375
tensor(12186.0615, grad_fn=<NegBackward0>) tensor(12186.0615, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12186.0615234375
tensor(12186.0615, grad_fn=<NegBackward0>) tensor(12186.0615, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12186.060546875
tensor(12186.0615, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12186.060546875
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0605, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12186.0595703125
tensor(12186.0605, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12186.0595703125
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12186.0595703125
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12186.0595703125
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0596, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12186.05859375
tensor(12186.0596, grad_fn=<NegBackward0>) tensor(12186.0586, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12186.0576171875
tensor(12186.0586, grad_fn=<NegBackward0>) tensor(12186.0576, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12186.05859375
tensor(12186.0576, grad_fn=<NegBackward0>) tensor(12186.0586, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12186.0576171875
tensor(12186.0576, grad_fn=<NegBackward0>) tensor(12186.0576, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12186.056640625
tensor(12186.0576, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12186.056640625
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12186.0576171875
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0576, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12186.0576171875
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0576, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -12186.056640625
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0566, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12186.0546875
tensor(12186.0566, grad_fn=<NegBackward0>) tensor(12186.0547, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12186.0546875
tensor(12186.0547, grad_fn=<NegBackward0>) tensor(12186.0547, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12186.0546875
tensor(12186.0547, grad_fn=<NegBackward0>) tensor(12186.0547, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12186.0546875
tensor(12186.0547, grad_fn=<NegBackward0>) tensor(12186.0547, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12186.0537109375
tensor(12186.0547, grad_fn=<NegBackward0>) tensor(12186.0537, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12186.05078125
tensor(12186.0537, grad_fn=<NegBackward0>) tensor(12186.0508, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12186.0478515625
tensor(12186.0508, grad_fn=<NegBackward0>) tensor(12186.0479, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12186.04296875
tensor(12186.0479, grad_fn=<NegBackward0>) tensor(12186.0430, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12186.0029296875
tensor(12186.0430, grad_fn=<NegBackward0>) tensor(12186.0029, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12184.6494140625
tensor(12186.0029, grad_fn=<NegBackward0>) tensor(12184.6494, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12183.49609375
tensor(12184.6494, grad_fn=<NegBackward0>) tensor(12183.4961, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12182.576171875
tensor(12183.4961, grad_fn=<NegBackward0>) tensor(12182.5762, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12182.513671875
tensor(12182.5762, grad_fn=<NegBackward0>) tensor(12182.5137, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12182.4931640625
tensor(12182.5137, grad_fn=<NegBackward0>) tensor(12182.4932, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12182.4833984375
tensor(12182.4932, grad_fn=<NegBackward0>) tensor(12182.4834, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12182.4765625
tensor(12182.4834, grad_fn=<NegBackward0>) tensor(12182.4766, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12182.4716796875
tensor(12182.4766, grad_fn=<NegBackward0>) tensor(12182.4717, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12182.466796875
tensor(12182.4717, grad_fn=<NegBackward0>) tensor(12182.4668, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12182.466796875
tensor(12182.4668, grad_fn=<NegBackward0>) tensor(12182.4668, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12182.462890625
tensor(12182.4668, grad_fn=<NegBackward0>) tensor(12182.4629, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12182.462890625
tensor(12182.4629, grad_fn=<NegBackward0>) tensor(12182.4629, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12182.4599609375
tensor(12182.4629, grad_fn=<NegBackward0>) tensor(12182.4600, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12182.458984375
tensor(12182.4600, grad_fn=<NegBackward0>) tensor(12182.4590, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12182.4580078125
tensor(12182.4590, grad_fn=<NegBackward0>) tensor(12182.4580, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12182.4580078125
tensor(12182.4580, grad_fn=<NegBackward0>) tensor(12182.4580, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12182.4560546875
tensor(12182.4580, grad_fn=<NegBackward0>) tensor(12182.4561, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12182.45703125
tensor(12182.4561, grad_fn=<NegBackward0>) tensor(12182.4570, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12182.4541015625
tensor(12182.4561, grad_fn=<NegBackward0>) tensor(12182.4541, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12182.4541015625
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4541, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12182.4541015625
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4541, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12182.4541015625
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4541, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12182.453125
tensor(12182.4541, grad_fn=<NegBackward0>) tensor(12182.4531, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12182.4521484375
tensor(12182.4531, grad_fn=<NegBackward0>) tensor(12182.4521, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12182.453125
tensor(12182.4521, grad_fn=<NegBackward0>) tensor(12182.4531, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12182.4521484375
tensor(12182.4521, grad_fn=<NegBackward0>) tensor(12182.4521, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12182.4609375
tensor(12182.4521, grad_fn=<NegBackward0>) tensor(12182.4609, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12182.451171875
tensor(12182.4521, grad_fn=<NegBackward0>) tensor(12182.4512, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12182.46875
tensor(12182.4512, grad_fn=<NegBackward0>) tensor(12182.4688, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12182.451171875
tensor(12182.4512, grad_fn=<NegBackward0>) tensor(12182.4512, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12182.453125
tensor(12182.4512, grad_fn=<NegBackward0>) tensor(12182.4531, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12182.451171875
tensor(12182.4512, grad_fn=<NegBackward0>) tensor(12182.4512, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12182.4501953125
tensor(12182.4512, grad_fn=<NegBackward0>) tensor(12182.4502, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12182.451171875
tensor(12182.4502, grad_fn=<NegBackward0>) tensor(12182.4512, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12182.5009765625
tensor(12182.4502, grad_fn=<NegBackward0>) tensor(12182.5010, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12182.4501953125
tensor(12182.4502, grad_fn=<NegBackward0>) tensor(12182.4502, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12182.4501953125
tensor(12182.4502, grad_fn=<NegBackward0>) tensor(12182.4502, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12182.44921875
tensor(12182.4502, grad_fn=<NegBackward0>) tensor(12182.4492, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12182.4501953125
tensor(12182.4492, grad_fn=<NegBackward0>) tensor(12182.4502, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12182.46484375
tensor(12182.4492, grad_fn=<NegBackward0>) tensor(12182.4648, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12182.4482421875
tensor(12182.4492, grad_fn=<NegBackward0>) tensor(12182.4482, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12182.44921875
tensor(12182.4482, grad_fn=<NegBackward0>) tensor(12182.4492, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12182.451171875
tensor(12182.4482, grad_fn=<NegBackward0>) tensor(12182.4512, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -12182.4287109375
tensor(12182.4482, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12182.4267578125
tensor(12182.4287, grad_fn=<NegBackward0>) tensor(12182.4268, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12182.4287109375
tensor(12182.4268, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12182.427734375
tensor(12182.4268, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12182.4287109375
tensor(12182.4268, grad_fn=<NegBackward0>) tensor(12182.4287, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -12182.427734375
tensor(12182.4268, grad_fn=<NegBackward0>) tensor(12182.4277, grad_fn=<NegBackward0>)
4
pi: tensor([[9.9998e-01, 1.7294e-05],
        [5.9012e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0703, 0.9297], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4103, 0.1390],
         [0.6037, 0.1934]],

        [[0.5605, 0.2068],
         [0.5399, 0.6200]],

        [[0.7212, 0.2177],
         [0.5787, 0.5588]],

        [[0.5069, 0.1581],
         [0.5270, 0.6417]],

        [[0.6914, 0.2283],
         [0.6419, 0.7258]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.020424516829035635
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013289782914094903
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.01701445012943135
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.00021025550424088273
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.00021025550424088273
Global Adjusted Rand Index: -0.0008043148361097795
Average Adjusted Rand Index: 0.007137895531715145
[-0.0008043148361097795, -0.0008043148361097795] [0.007137895531715145, 0.007137895531715145] [12182.4296875, 12182.4521484375]
-------------------------------------
This iteration is 36
True Objective function: Loss = -12050.106197778518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22362.95703125
inf tensor(22362.9570, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12563.591796875
tensor(22362.9570, grad_fn=<NegBackward0>) tensor(12563.5918, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12563.13671875
tensor(12563.5918, grad_fn=<NegBackward0>) tensor(12563.1367, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12562.9072265625
tensor(12563.1367, grad_fn=<NegBackward0>) tensor(12562.9072, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12562.5859375
tensor(12562.9072, grad_fn=<NegBackward0>) tensor(12562.5859, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12562.125
tensor(12562.5859, grad_fn=<NegBackward0>) tensor(12562.1250, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12561.9599609375
tensor(12562.1250, grad_fn=<NegBackward0>) tensor(12561.9600, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12561.8271484375
tensor(12561.9600, grad_fn=<NegBackward0>) tensor(12561.8271, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12561.7001953125
tensor(12561.8271, grad_fn=<NegBackward0>) tensor(12561.7002, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12561.580078125
tensor(12561.7002, grad_fn=<NegBackward0>) tensor(12561.5801, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12561.4619140625
tensor(12561.5801, grad_fn=<NegBackward0>) tensor(12561.4619, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12561.3515625
tensor(12561.4619, grad_fn=<NegBackward0>) tensor(12561.3516, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12561.25
tensor(12561.3516, grad_fn=<NegBackward0>) tensor(12561.2500, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12561.15625
tensor(12561.2500, grad_fn=<NegBackward0>) tensor(12561.1562, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12561.0693359375
tensor(12561.1562, grad_fn=<NegBackward0>) tensor(12561.0693, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12560.9951171875
tensor(12561.0693, grad_fn=<NegBackward0>) tensor(12560.9951, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12560.9296875
tensor(12560.9951, grad_fn=<NegBackward0>) tensor(12560.9297, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12560.87890625
tensor(12560.9297, grad_fn=<NegBackward0>) tensor(12560.8789, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12560.8369140625
tensor(12560.8789, grad_fn=<NegBackward0>) tensor(12560.8369, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12560.798828125
tensor(12560.8369, grad_fn=<NegBackward0>) tensor(12560.7988, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12560.7666015625
tensor(12560.7988, grad_fn=<NegBackward0>) tensor(12560.7666, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12560.736328125
tensor(12560.7666, grad_fn=<NegBackward0>) tensor(12560.7363, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12560.708984375
tensor(12560.7363, grad_fn=<NegBackward0>) tensor(12560.7090, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12560.681640625
tensor(12560.7090, grad_fn=<NegBackward0>) tensor(12560.6816, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12560.658203125
tensor(12560.6816, grad_fn=<NegBackward0>) tensor(12560.6582, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12560.6337890625
tensor(12560.6582, grad_fn=<NegBackward0>) tensor(12560.6338, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12560.611328125
tensor(12560.6338, grad_fn=<NegBackward0>) tensor(12560.6113, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12560.58984375
tensor(12560.6113, grad_fn=<NegBackward0>) tensor(12560.5898, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12560.568359375
tensor(12560.5898, grad_fn=<NegBackward0>) tensor(12560.5684, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12560.5478515625
tensor(12560.5684, grad_fn=<NegBackward0>) tensor(12560.5479, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12560.537109375
tensor(12560.5479, grad_fn=<NegBackward0>) tensor(12560.5371, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12560.51171875
tensor(12560.5371, grad_fn=<NegBackward0>) tensor(12560.5117, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12560.4951171875
tensor(12560.5117, grad_fn=<NegBackward0>) tensor(12560.4951, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12560.4794921875
tensor(12560.4951, grad_fn=<NegBackward0>) tensor(12560.4795, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12560.4609375
tensor(12560.4795, grad_fn=<NegBackward0>) tensor(12560.4609, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12560.439453125
tensor(12560.4609, grad_fn=<NegBackward0>) tensor(12560.4395, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12560.41796875
tensor(12560.4395, grad_fn=<NegBackward0>) tensor(12560.4180, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12560.392578125
tensor(12560.4180, grad_fn=<NegBackward0>) tensor(12560.3926, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12560.3623046875
tensor(12560.3926, grad_fn=<NegBackward0>) tensor(12560.3623, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12560.3203125
tensor(12560.3623, grad_fn=<NegBackward0>) tensor(12560.3203, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12560.2568359375
tensor(12560.3203, grad_fn=<NegBackward0>) tensor(12560.2568, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12560.1337890625
tensor(12560.2568, grad_fn=<NegBackward0>) tensor(12560.1338, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12559.263671875
tensor(12560.1338, grad_fn=<NegBackward0>) tensor(12559.2637, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12558.2822265625
tensor(12559.2637, grad_fn=<NegBackward0>) tensor(12558.2822, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12558.19921875
tensor(12558.2822, grad_fn=<NegBackward0>) tensor(12558.1992, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12558.1689453125
tensor(12558.1992, grad_fn=<NegBackward0>) tensor(12558.1689, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12558.150390625
tensor(12558.1689, grad_fn=<NegBackward0>) tensor(12558.1504, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12558.138671875
tensor(12558.1504, grad_fn=<NegBackward0>) tensor(12558.1387, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12558.1318359375
tensor(12558.1387, grad_fn=<NegBackward0>) tensor(12558.1318, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12558.123046875
tensor(12558.1318, grad_fn=<NegBackward0>) tensor(12558.1230, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12558.1171875
tensor(12558.1230, grad_fn=<NegBackward0>) tensor(12558.1172, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12558.11328125
tensor(12558.1172, grad_fn=<NegBackward0>) tensor(12558.1133, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12558.1083984375
tensor(12558.1133, grad_fn=<NegBackward0>) tensor(12558.1084, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12558.1064453125
tensor(12558.1084, grad_fn=<NegBackward0>) tensor(12558.1064, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12558.103515625
tensor(12558.1064, grad_fn=<NegBackward0>) tensor(12558.1035, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12558.1015625
tensor(12558.1035, grad_fn=<NegBackward0>) tensor(12558.1016, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12558.0986328125
tensor(12558.1016, grad_fn=<NegBackward0>) tensor(12558.0986, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12558.0947265625
tensor(12558.0986, grad_fn=<NegBackward0>) tensor(12558.0947, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12558.0947265625
tensor(12558.0947, grad_fn=<NegBackward0>) tensor(12558.0947, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12558.09375
tensor(12558.0947, grad_fn=<NegBackward0>) tensor(12558.0938, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12558.091796875
tensor(12558.0938, grad_fn=<NegBackward0>) tensor(12558.0918, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12558.0908203125
tensor(12558.0918, grad_fn=<NegBackward0>) tensor(12558.0908, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12558.0908203125
tensor(12558.0908, grad_fn=<NegBackward0>) tensor(12558.0908, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12558.0888671875
tensor(12558.0908, grad_fn=<NegBackward0>) tensor(12558.0889, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12558.087890625
tensor(12558.0889, grad_fn=<NegBackward0>) tensor(12558.0879, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12558.0869140625
tensor(12558.0879, grad_fn=<NegBackward0>) tensor(12558.0869, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12558.0859375
tensor(12558.0869, grad_fn=<NegBackward0>) tensor(12558.0859, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12558.0859375
tensor(12558.0859, grad_fn=<NegBackward0>) tensor(12558.0859, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12558.083984375
tensor(12558.0859, grad_fn=<NegBackward0>) tensor(12558.0840, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12558.0859375
tensor(12558.0840, grad_fn=<NegBackward0>) tensor(12558.0859, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12558.0849609375
tensor(12558.0840, grad_fn=<NegBackward0>) tensor(12558.0850, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12558.08203125
tensor(12558.0840, grad_fn=<NegBackward0>) tensor(12558.0820, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12558.08203125
tensor(12558.0820, grad_fn=<NegBackward0>) tensor(12558.0820, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12558.0810546875
tensor(12558.0820, grad_fn=<NegBackward0>) tensor(12558.0811, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12558.11328125
tensor(12558.0811, grad_fn=<NegBackward0>) tensor(12558.1133, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12558.0810546875
tensor(12558.0811, grad_fn=<NegBackward0>) tensor(12558.0811, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12558.080078125
tensor(12558.0811, grad_fn=<NegBackward0>) tensor(12558.0801, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12558.0791015625
tensor(12558.0801, grad_fn=<NegBackward0>) tensor(12558.0791, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12558.0791015625
tensor(12558.0791, grad_fn=<NegBackward0>) tensor(12558.0791, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12558.080078125
tensor(12558.0791, grad_fn=<NegBackward0>) tensor(12558.0801, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12558.078125
tensor(12558.0791, grad_fn=<NegBackward0>) tensor(12558.0781, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12558.078125
tensor(12558.0781, grad_fn=<NegBackward0>) tensor(12558.0781, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12558.0771484375
tensor(12558.0781, grad_fn=<NegBackward0>) tensor(12558.0771, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12558.0771484375
tensor(12558.0771, grad_fn=<NegBackward0>) tensor(12558.0771, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12558.2041015625
tensor(12558.0771, grad_fn=<NegBackward0>) tensor(12558.2041, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12558.076171875
tensor(12558.0771, grad_fn=<NegBackward0>) tensor(12558.0762, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12558.076171875
tensor(12558.0762, grad_fn=<NegBackward0>) tensor(12558.0762, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12558.0771484375
tensor(12558.0762, grad_fn=<NegBackward0>) tensor(12558.0771, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12558.076171875
tensor(12558.0762, grad_fn=<NegBackward0>) tensor(12558.0762, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12558.2119140625
tensor(12558.0762, grad_fn=<NegBackward0>) tensor(12558.2119, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12558.0751953125
tensor(12558.0762, grad_fn=<NegBackward0>) tensor(12558.0752, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12558.0751953125
tensor(12558.0752, grad_fn=<NegBackward0>) tensor(12558.0752, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12558.1162109375
tensor(12558.0752, grad_fn=<NegBackward0>) tensor(12558.1162, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12558.0751953125
tensor(12558.0752, grad_fn=<NegBackward0>) tensor(12558.0752, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12558.197265625
tensor(12558.0752, grad_fn=<NegBackward0>) tensor(12558.1973, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12558.07421875
tensor(12558.0752, grad_fn=<NegBackward0>) tensor(12558.0742, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12558.07421875
tensor(12558.0742, grad_fn=<NegBackward0>) tensor(12558.0742, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12558.0751953125
tensor(12558.0742, grad_fn=<NegBackward0>) tensor(12558.0752, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12558.07421875
tensor(12558.0742, grad_fn=<NegBackward0>) tensor(12558.0742, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12558.1181640625
tensor(12558.0742, grad_fn=<NegBackward0>) tensor(12558.1182, grad_fn=<NegBackward0>)
1
pi: tensor([[9.9984e-01, 1.5745e-04],
        [8.6414e-05, 9.9991e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0573, 0.9427], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3544, 0.2442],
         [0.6590, 0.2030]],

        [[0.6488, 0.2684],
         [0.6733, 0.6755]],

        [[0.5711, 0.1404],
         [0.6499, 0.5980]],

        [[0.5849, 0.2372],
         [0.5544, 0.5744]],

        [[0.6353, 0.2088],
         [0.6860, 0.6225]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0035158395898187145
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0015793426848825644
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
Global Adjusted Rand Index: 7.86924440400643e-05
Average Adjusted Rand Index: -0.0017596168279687499
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21980.849609375
inf tensor(21980.8496, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12563.9501953125
tensor(21980.8496, grad_fn=<NegBackward0>) tensor(12563.9502, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12563.2021484375
tensor(12563.9502, grad_fn=<NegBackward0>) tensor(12563.2021, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12562.892578125
tensor(12563.2021, grad_fn=<NegBackward0>) tensor(12562.8926, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12562.54296875
tensor(12562.8926, grad_fn=<NegBackward0>) tensor(12562.5430, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12562.140625
tensor(12562.5430, grad_fn=<NegBackward0>) tensor(12562.1406, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12561.931640625
tensor(12562.1406, grad_fn=<NegBackward0>) tensor(12561.9316, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12561.7841796875
tensor(12561.9316, grad_fn=<NegBackward0>) tensor(12561.7842, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12561.65625
tensor(12561.7842, grad_fn=<NegBackward0>) tensor(12561.6562, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12561.5361328125
tensor(12561.6562, grad_fn=<NegBackward0>) tensor(12561.5361, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12561.42578125
tensor(12561.5361, grad_fn=<NegBackward0>) tensor(12561.4258, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12561.326171875
tensor(12561.4258, grad_fn=<NegBackward0>) tensor(12561.3262, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12561.2333984375
tensor(12561.3262, grad_fn=<NegBackward0>) tensor(12561.2334, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12561.138671875
tensor(12561.2334, grad_fn=<NegBackward0>) tensor(12561.1387, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12561.0458984375
tensor(12561.1387, grad_fn=<NegBackward0>) tensor(12561.0459, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12560.9560546875
tensor(12561.0459, grad_fn=<NegBackward0>) tensor(12560.9561, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12560.8759765625
tensor(12560.9561, grad_fn=<NegBackward0>) tensor(12560.8760, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12560.810546875
tensor(12560.8760, grad_fn=<NegBackward0>) tensor(12560.8105, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12560.7626953125
tensor(12560.8105, grad_fn=<NegBackward0>) tensor(12560.7627, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12560.72265625
tensor(12560.7627, grad_fn=<NegBackward0>) tensor(12560.7227, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12560.6875
tensor(12560.7227, grad_fn=<NegBackward0>) tensor(12560.6875, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12560.6572265625
tensor(12560.6875, grad_fn=<NegBackward0>) tensor(12560.6572, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12560.62890625
tensor(12560.6572, grad_fn=<NegBackward0>) tensor(12560.6289, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12560.6005859375
tensor(12560.6289, grad_fn=<NegBackward0>) tensor(12560.6006, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12560.572265625
tensor(12560.6006, grad_fn=<NegBackward0>) tensor(12560.5723, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12560.5478515625
tensor(12560.5723, grad_fn=<NegBackward0>) tensor(12560.5479, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12560.5244140625
tensor(12560.5479, grad_fn=<NegBackward0>) tensor(12560.5244, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12560.5
tensor(12560.5244, grad_fn=<NegBackward0>) tensor(12560.5000, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12560.478515625
tensor(12560.5000, grad_fn=<NegBackward0>) tensor(12560.4785, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12560.4560546875
tensor(12560.4785, grad_fn=<NegBackward0>) tensor(12560.4561, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12560.43359375
tensor(12560.4561, grad_fn=<NegBackward0>) tensor(12560.4336, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12560.40625
tensor(12560.4336, grad_fn=<NegBackward0>) tensor(12560.4062, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12560.3779296875
tensor(12560.4062, grad_fn=<NegBackward0>) tensor(12560.3779, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12560.3447265625
tensor(12560.3779, grad_fn=<NegBackward0>) tensor(12560.3447, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12560.3017578125
tensor(12560.3447, grad_fn=<NegBackward0>) tensor(12560.3018, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12560.2392578125
tensor(12560.3018, grad_fn=<NegBackward0>) tensor(12560.2393, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12560.1328125
tensor(12560.2393, grad_fn=<NegBackward0>) tensor(12560.1328, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12559.8564453125
tensor(12560.1328, grad_fn=<NegBackward0>) tensor(12559.8564, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12557.708984375
tensor(12559.8564, grad_fn=<NegBackward0>) tensor(12557.7090, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12351.7294921875
tensor(12557.7090, grad_fn=<NegBackward0>) tensor(12351.7295, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12220.7236328125
tensor(12351.7295, grad_fn=<NegBackward0>) tensor(12220.7236, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12220.103515625
tensor(12220.7236, grad_fn=<NegBackward0>) tensor(12220.1035, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12211.2431640625
tensor(12220.1035, grad_fn=<NegBackward0>) tensor(12211.2432, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12198.7373046875
tensor(12211.2432, grad_fn=<NegBackward0>) tensor(12198.7373, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12193.517578125
tensor(12198.7373, grad_fn=<NegBackward0>) tensor(12193.5176, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12193.505859375
tensor(12193.5176, grad_fn=<NegBackward0>) tensor(12193.5059, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12193.4931640625
tensor(12193.5059, grad_fn=<NegBackward0>) tensor(12193.4932, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12183.65625
tensor(12193.4932, grad_fn=<NegBackward0>) tensor(12183.6562, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12182.115234375
tensor(12183.6562, grad_fn=<NegBackward0>) tensor(12182.1152, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12182.1044921875
tensor(12182.1152, grad_fn=<NegBackward0>) tensor(12182.1045, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12182.0927734375
tensor(12182.1045, grad_fn=<NegBackward0>) tensor(12182.0928, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12182.0908203125
tensor(12182.0928, grad_fn=<NegBackward0>) tensor(12182.0908, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12182.08984375
tensor(12182.0908, grad_fn=<NegBackward0>) tensor(12182.0898, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12182.0888671875
tensor(12182.0898, grad_fn=<NegBackward0>) tensor(12182.0889, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12182.0859375
tensor(12182.0889, grad_fn=<NegBackward0>) tensor(12182.0859, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12182.0849609375
tensor(12182.0859, grad_fn=<NegBackward0>) tensor(12182.0850, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12182.083984375
tensor(12182.0850, grad_fn=<NegBackward0>) tensor(12182.0840, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12182.0849609375
tensor(12182.0840, grad_fn=<NegBackward0>) tensor(12182.0850, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12182.0830078125
tensor(12182.0840, grad_fn=<NegBackward0>) tensor(12182.0830, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12182.083984375
tensor(12182.0830, grad_fn=<NegBackward0>) tensor(12182.0840, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12182.08203125
tensor(12182.0830, grad_fn=<NegBackward0>) tensor(12182.0820, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12182.076171875
tensor(12182.0820, grad_fn=<NegBackward0>) tensor(12182.0762, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12182.0751953125
tensor(12182.0762, grad_fn=<NegBackward0>) tensor(12182.0752, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12182.0751953125
tensor(12182.0752, grad_fn=<NegBackward0>) tensor(12182.0752, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12182.076171875
tensor(12182.0752, grad_fn=<NegBackward0>) tensor(12182.0762, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12182.0732421875
tensor(12182.0752, grad_fn=<NegBackward0>) tensor(12182.0732, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12166.0078125
tensor(12182.0732, grad_fn=<NegBackward0>) tensor(12166.0078, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12165.46484375
tensor(12166.0078, grad_fn=<NegBackward0>) tensor(12165.4648, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12156.2841796875
tensor(12165.4648, grad_fn=<NegBackward0>) tensor(12156.2842, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12156.2822265625
tensor(12156.2842, grad_fn=<NegBackward0>) tensor(12156.2822, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12156.2861328125
tensor(12156.2822, grad_fn=<NegBackward0>) tensor(12156.2861, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12156.28125
tensor(12156.2822, grad_fn=<NegBackward0>) tensor(12156.2812, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12156.28125
tensor(12156.2812, grad_fn=<NegBackward0>) tensor(12156.2812, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12156.2783203125
tensor(12156.2812, grad_fn=<NegBackward0>) tensor(12156.2783, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12156.2861328125
tensor(12156.2783, grad_fn=<NegBackward0>) tensor(12156.2861, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12156.2734375
tensor(12156.2783, grad_fn=<NegBackward0>) tensor(12156.2734, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12149.5615234375
tensor(12156.2734, grad_fn=<NegBackward0>) tensor(12149.5615, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12149.5712890625
tensor(12149.5615, grad_fn=<NegBackward0>) tensor(12149.5713, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12149.5537109375
tensor(12149.5615, grad_fn=<NegBackward0>) tensor(12149.5537, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12149.5009765625
tensor(12149.5537, grad_fn=<NegBackward0>) tensor(12149.5010, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12149.4990234375
tensor(12149.5010, grad_fn=<NegBackward0>) tensor(12149.4990, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12149.4921875
tensor(12149.4990, grad_fn=<NegBackward0>) tensor(12149.4922, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12149.484375
tensor(12149.4922, grad_fn=<NegBackward0>) tensor(12149.4844, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12149.48046875
tensor(12149.4844, grad_fn=<NegBackward0>) tensor(12149.4805, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12149.48046875
tensor(12149.4805, grad_fn=<NegBackward0>) tensor(12149.4805, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12149.48046875
tensor(12149.4805, grad_fn=<NegBackward0>) tensor(12149.4805, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12149.4794921875
tensor(12149.4805, grad_fn=<NegBackward0>) tensor(12149.4795, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12149.4775390625
tensor(12149.4795, grad_fn=<NegBackward0>) tensor(12149.4775, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12149.48828125
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4883, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12149.484375
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4844, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -12149.4794921875
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4795, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -12149.4775390625
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4775, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12149.478515625
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4785, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12149.478515625
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4785, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -12149.5
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.5000, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -12149.482421875
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.4824, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -12149.515625
tensor(12149.4775, grad_fn=<NegBackward0>) tensor(12149.5156, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.8015, 0.1985],
        [0.2153, 0.7847]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3650, 0.6350], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3076, 0.1313],
         [0.6358, 0.2874]],

        [[0.6032, 0.1221],
         [0.6158, 0.6989]],

        [[0.6955, 0.0957],
         [0.6523, 0.7244]],

        [[0.5781, 0.1031],
         [0.5503, 0.5507]],

        [[0.6106, 0.1084],
         [0.5451, 0.7166]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6691609048003068
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8609067903349998
Average Adjusted Rand Index: 0.8653867435849886
[7.86924440400643e-05, 0.8609067903349998] [-0.0017596168279687499, 0.8653867435849886] [12558.0732421875, 12149.515625]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11958.475010022208
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23101.447265625
inf tensor(23101.4473, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12393.7294921875
tensor(23101.4473, grad_fn=<NegBackward0>) tensor(12393.7295, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12393.3134765625
tensor(12393.7295, grad_fn=<NegBackward0>) tensor(12393.3135, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12393.1923828125
tensor(12393.3135, grad_fn=<NegBackward0>) tensor(12393.1924, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12393.087890625
tensor(12393.1924, grad_fn=<NegBackward0>) tensor(12393.0879, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12392.978515625
tensor(12393.0879, grad_fn=<NegBackward0>) tensor(12392.9785, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12392.87890625
tensor(12392.9785, grad_fn=<NegBackward0>) tensor(12392.8789, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12392.7890625
tensor(12392.8789, grad_fn=<NegBackward0>) tensor(12392.7891, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12392.701171875
tensor(12392.7891, grad_fn=<NegBackward0>) tensor(12392.7012, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12392.6044921875
tensor(12392.7012, grad_fn=<NegBackward0>) tensor(12392.6045, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12392.4921875
tensor(12392.6045, grad_fn=<NegBackward0>) tensor(12392.4922, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12392.353515625
tensor(12392.4922, grad_fn=<NegBackward0>) tensor(12392.3535, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12392.1162109375
tensor(12392.3535, grad_fn=<NegBackward0>) tensor(12392.1162, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12390.9208984375
tensor(12392.1162, grad_fn=<NegBackward0>) tensor(12390.9209, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12390.1923828125
tensor(12390.9209, grad_fn=<NegBackward0>) tensor(12390.1924, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12389.8095703125
tensor(12390.1924, grad_fn=<NegBackward0>) tensor(12389.8096, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12389.5615234375
tensor(12389.8096, grad_fn=<NegBackward0>) tensor(12389.5615, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12389.400390625
tensor(12389.5615, grad_fn=<NegBackward0>) tensor(12389.4004, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12389.298828125
tensor(12389.4004, grad_fn=<NegBackward0>) tensor(12389.2988, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12389.2353515625
tensor(12389.2988, grad_fn=<NegBackward0>) tensor(12389.2354, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12389.1904296875
tensor(12389.2354, grad_fn=<NegBackward0>) tensor(12389.1904, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12389.1591796875
tensor(12389.1904, grad_fn=<NegBackward0>) tensor(12389.1592, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12389.13671875
tensor(12389.1592, grad_fn=<NegBackward0>) tensor(12389.1367, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12389.119140625
tensor(12389.1367, grad_fn=<NegBackward0>) tensor(12389.1191, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12389.103515625
tensor(12389.1191, grad_fn=<NegBackward0>) tensor(12389.1035, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12389.0927734375
tensor(12389.1035, grad_fn=<NegBackward0>) tensor(12389.0928, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12389.083984375
tensor(12389.0928, grad_fn=<NegBackward0>) tensor(12389.0840, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12389.078125
tensor(12389.0840, grad_fn=<NegBackward0>) tensor(12389.0781, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12389.0693359375
tensor(12389.0781, grad_fn=<NegBackward0>) tensor(12389.0693, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12389.0654296875
tensor(12389.0693, grad_fn=<NegBackward0>) tensor(12389.0654, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12389.0595703125
tensor(12389.0654, grad_fn=<NegBackward0>) tensor(12389.0596, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12389.0556640625
tensor(12389.0596, grad_fn=<NegBackward0>) tensor(12389.0557, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12389.052734375
tensor(12389.0557, grad_fn=<NegBackward0>) tensor(12389.0527, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12389.0498046875
tensor(12389.0527, grad_fn=<NegBackward0>) tensor(12389.0498, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12389.046875
tensor(12389.0498, grad_fn=<NegBackward0>) tensor(12389.0469, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12389.0439453125
tensor(12389.0469, grad_fn=<NegBackward0>) tensor(12389.0439, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12389.04296875
tensor(12389.0439, grad_fn=<NegBackward0>) tensor(12389.0430, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12389.041015625
tensor(12389.0430, grad_fn=<NegBackward0>) tensor(12389.0410, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12389.0400390625
tensor(12389.0410, grad_fn=<NegBackward0>) tensor(12389.0400, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12389.0380859375
tensor(12389.0400, grad_fn=<NegBackward0>) tensor(12389.0381, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12389.0361328125
tensor(12389.0381, grad_fn=<NegBackward0>) tensor(12389.0361, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12389.0361328125
tensor(12389.0361, grad_fn=<NegBackward0>) tensor(12389.0361, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12389.03515625
tensor(12389.0361, grad_fn=<NegBackward0>) tensor(12389.0352, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12389.03515625
tensor(12389.0352, grad_fn=<NegBackward0>) tensor(12389.0352, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12389.033203125
tensor(12389.0352, grad_fn=<NegBackward0>) tensor(12389.0332, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12389.0341796875
tensor(12389.0332, grad_fn=<NegBackward0>) tensor(12389.0342, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12389.03125
tensor(12389.0332, grad_fn=<NegBackward0>) tensor(12389.0312, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12389.0302734375
tensor(12389.0312, grad_fn=<NegBackward0>) tensor(12389.0303, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12389.0302734375
tensor(12389.0303, grad_fn=<NegBackward0>) tensor(12389.0303, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12389.0283203125
tensor(12389.0303, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12389.029296875
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0293, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12389.0283203125
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12389.029296875
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0293, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12389.02734375
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12389.02734375
tensor(12389.0273, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12389.02734375
tensor(12389.0273, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12389.0283203125
tensor(12389.0273, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12389.025390625
tensor(12389.0273, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12389.02734375
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12389.02734375
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.9793, 0.0207],
        [0.9857, 0.0143]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0254, 0.9746], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.2086],
         [0.5556, 0.2104]],

        [[0.6729, 0.1792],
         [0.7006, 0.5156]],

        [[0.6230, 0.2418],
         [0.6194, 0.6926]],

        [[0.6157, 0.3345],
         [0.6121, 0.5269]],

        [[0.7223, 0.2199],
         [0.6578, 0.6109]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.008768249852741087
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20558.095703125
inf tensor(20558.0957, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12394.0654296875
tensor(20558.0957, grad_fn=<NegBackward0>) tensor(12394.0654, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12393.4521484375
tensor(12394.0654, grad_fn=<NegBackward0>) tensor(12393.4521, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12393.123046875
tensor(12393.4521, grad_fn=<NegBackward0>) tensor(12393.1230, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12392.8271484375
tensor(12393.1230, grad_fn=<NegBackward0>) tensor(12392.8271, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12392.642578125
tensor(12392.8271, grad_fn=<NegBackward0>) tensor(12392.6426, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12392.484375
tensor(12392.6426, grad_fn=<NegBackward0>) tensor(12392.4844, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12392.3173828125
tensor(12392.4844, grad_fn=<NegBackward0>) tensor(12392.3174, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12392.1279296875
tensor(12392.3174, grad_fn=<NegBackward0>) tensor(12392.1279, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12391.91796875
tensor(12392.1279, grad_fn=<NegBackward0>) tensor(12391.9180, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12391.703125
tensor(12391.9180, grad_fn=<NegBackward0>) tensor(12391.7031, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12391.5146484375
tensor(12391.7031, grad_fn=<NegBackward0>) tensor(12391.5146, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12391.3759765625
tensor(12391.5146, grad_fn=<NegBackward0>) tensor(12391.3760, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12391.2861328125
tensor(12391.3760, grad_fn=<NegBackward0>) tensor(12391.2861, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12391.2255859375
tensor(12391.2861, grad_fn=<NegBackward0>) tensor(12391.2256, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12391.18359375
tensor(12391.2256, grad_fn=<NegBackward0>) tensor(12391.1836, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12391.1533203125
tensor(12391.1836, grad_fn=<NegBackward0>) tensor(12391.1533, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12391.130859375
tensor(12391.1533, grad_fn=<NegBackward0>) tensor(12391.1309, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12391.111328125
tensor(12391.1309, grad_fn=<NegBackward0>) tensor(12391.1113, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12391.095703125
tensor(12391.1113, grad_fn=<NegBackward0>) tensor(12391.0957, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12391.0810546875
tensor(12391.0957, grad_fn=<NegBackward0>) tensor(12391.0811, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12391.064453125
tensor(12391.0811, grad_fn=<NegBackward0>) tensor(12391.0645, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12391.0341796875
tensor(12391.0645, grad_fn=<NegBackward0>) tensor(12391.0342, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12390.08984375
tensor(12391.0342, grad_fn=<NegBackward0>) tensor(12390.0898, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12389.244140625
tensor(12390.0898, grad_fn=<NegBackward0>) tensor(12389.2441, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12389.1396484375
tensor(12389.2441, grad_fn=<NegBackward0>) tensor(12389.1396, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12389.1025390625
tensor(12389.1396, grad_fn=<NegBackward0>) tensor(12389.1025, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12389.0830078125
tensor(12389.1025, grad_fn=<NegBackward0>) tensor(12389.0830, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12389.0712890625
tensor(12389.0830, grad_fn=<NegBackward0>) tensor(12389.0713, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12389.0634765625
tensor(12389.0713, grad_fn=<NegBackward0>) tensor(12389.0635, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12389.056640625
tensor(12389.0635, grad_fn=<NegBackward0>) tensor(12389.0566, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12389.052734375
tensor(12389.0566, grad_fn=<NegBackward0>) tensor(12389.0527, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12389.0478515625
tensor(12389.0527, grad_fn=<NegBackward0>) tensor(12389.0479, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12389.044921875
tensor(12389.0479, grad_fn=<NegBackward0>) tensor(12389.0449, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12389.0419921875
tensor(12389.0449, grad_fn=<NegBackward0>) tensor(12389.0420, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12389.0390625
tensor(12389.0420, grad_fn=<NegBackward0>) tensor(12389.0391, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12389.037109375
tensor(12389.0391, grad_fn=<NegBackward0>) tensor(12389.0371, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12389.037109375
tensor(12389.0371, grad_fn=<NegBackward0>) tensor(12389.0371, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12389.03515625
tensor(12389.0371, grad_fn=<NegBackward0>) tensor(12389.0352, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12389.0341796875
tensor(12389.0352, grad_fn=<NegBackward0>) tensor(12389.0342, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12389.033203125
tensor(12389.0342, grad_fn=<NegBackward0>) tensor(12389.0332, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12389.03125
tensor(12389.0332, grad_fn=<NegBackward0>) tensor(12389.0312, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12389.0322265625
tensor(12389.0312, grad_fn=<NegBackward0>) tensor(12389.0322, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12389.0322265625
tensor(12389.0312, grad_fn=<NegBackward0>) tensor(12389.0322, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -12389.03125
tensor(12389.0312, grad_fn=<NegBackward0>) tensor(12389.0312, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12389.029296875
tensor(12389.0312, grad_fn=<NegBackward0>) tensor(12389.0293, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12389.0283203125
tensor(12389.0293, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12389.029296875
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0293, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12389.0283203125
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12389.0283203125
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12389.02734375
tensor(12389.0283, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12389.0263671875
tensor(12389.0273, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12389.0283203125
tensor(12389.0264, grad_fn=<NegBackward0>) tensor(12389.0283, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12389.0263671875
tensor(12389.0264, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12389.0263671875
tensor(12389.0264, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12389.02734375
tensor(12389.0264, grad_fn=<NegBackward0>) tensor(12389.0273, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12389.025390625
tensor(12389.0264, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -12389.025390625
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12389.0263671875
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0264, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12389.025390625
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12389.025390625
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12389.0244140625
tensor(12389.0254, grad_fn=<NegBackward0>) tensor(12389.0244, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12389.025390625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12389.0244140625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0244, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12389.0244140625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0244, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12389.025390625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12389.025390625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12389.0966796875
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0967, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -12389.025390625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -12389.025390625
tensor(12389.0244, grad_fn=<NegBackward0>) tensor(12389.0254, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.9793, 0.0207],
        [0.9860, 0.0140]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0130, 0.9870], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.2093],
         [0.5047, 0.2103]],

        [[0.5648, 0.1791],
         [0.6791, 0.6699]],

        [[0.6230, 0.2418],
         [0.5088, 0.5332]],

        [[0.5260, 0.3346],
         [0.6637, 0.6032]],

        [[0.6029, 0.2199],
         [0.5509, 0.6693]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.008768249852741087
Average Adjusted Rand Index: 0.0
[0.008768249852741087, 0.008768249852741087] [0.0, 0.0] [12389.0263671875, 12389.025390625]
-------------------------------------
This iteration is 38
True Objective function: Loss = -11871.025081519521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20585.525390625
inf tensor(20585.5254, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12484.228515625
tensor(20585.5254, grad_fn=<NegBackward0>) tensor(12484.2285, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12483.9462890625
tensor(12484.2285, grad_fn=<NegBackward0>) tensor(12483.9463, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12483.8251953125
tensor(12483.9463, grad_fn=<NegBackward0>) tensor(12483.8252, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12483.66796875
tensor(12483.8252, grad_fn=<NegBackward0>) tensor(12483.6680, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12483.4619140625
tensor(12483.6680, grad_fn=<NegBackward0>) tensor(12483.4619, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12483.33203125
tensor(12483.4619, grad_fn=<NegBackward0>) tensor(12483.3320, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12483.2822265625
tensor(12483.3320, grad_fn=<NegBackward0>) tensor(12483.2822, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12483.2490234375
tensor(12483.2822, grad_fn=<NegBackward0>) tensor(12483.2490, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12483.2197265625
tensor(12483.2490, grad_fn=<NegBackward0>) tensor(12483.2197, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12483.19140625
tensor(12483.2197, grad_fn=<NegBackward0>) tensor(12483.1914, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12483.1640625
tensor(12483.1914, grad_fn=<NegBackward0>) tensor(12483.1641, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12483.134765625
tensor(12483.1641, grad_fn=<NegBackward0>) tensor(12483.1348, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12483.10546875
tensor(12483.1348, grad_fn=<NegBackward0>) tensor(12483.1055, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12483.0791015625
tensor(12483.1055, grad_fn=<NegBackward0>) tensor(12483.0791, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12483.0537109375
tensor(12483.0791, grad_fn=<NegBackward0>) tensor(12483.0537, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12483.0302734375
tensor(12483.0537, grad_fn=<NegBackward0>) tensor(12483.0303, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12483.01171875
tensor(12483.0303, grad_fn=<NegBackward0>) tensor(12483.0117, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12482.9951171875
tensor(12483.0117, grad_fn=<NegBackward0>) tensor(12482.9951, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12482.982421875
tensor(12482.9951, grad_fn=<NegBackward0>) tensor(12482.9824, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12482.9716796875
tensor(12482.9824, grad_fn=<NegBackward0>) tensor(12482.9717, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12482.962890625
tensor(12482.9717, grad_fn=<NegBackward0>) tensor(12482.9629, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12482.955078125
tensor(12482.9629, grad_fn=<NegBackward0>) tensor(12482.9551, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12482.947265625
tensor(12482.9551, grad_fn=<NegBackward0>) tensor(12482.9473, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12482.939453125
tensor(12482.9473, grad_fn=<NegBackward0>) tensor(12482.9395, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12482.931640625
tensor(12482.9395, grad_fn=<NegBackward0>) tensor(12482.9316, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12482.9208984375
tensor(12482.9316, grad_fn=<NegBackward0>) tensor(12482.9209, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12482.908203125
tensor(12482.9209, grad_fn=<NegBackward0>) tensor(12482.9082, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12482.8837890625
tensor(12482.9082, grad_fn=<NegBackward0>) tensor(12482.8838, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12482.8330078125
tensor(12482.8838, grad_fn=<NegBackward0>) tensor(12482.8330, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12482.7529296875
tensor(12482.8330, grad_fn=<NegBackward0>) tensor(12482.7529, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12482.6845703125
tensor(12482.7529, grad_fn=<NegBackward0>) tensor(12482.6846, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12482.654296875
tensor(12482.6846, grad_fn=<NegBackward0>) tensor(12482.6543, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12482.6416015625
tensor(12482.6543, grad_fn=<NegBackward0>) tensor(12482.6416, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12482.634765625
tensor(12482.6416, grad_fn=<NegBackward0>) tensor(12482.6348, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12482.630859375
tensor(12482.6348, grad_fn=<NegBackward0>) tensor(12482.6309, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12482.626953125
tensor(12482.6309, grad_fn=<NegBackward0>) tensor(12482.6270, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12482.625
tensor(12482.6270, grad_fn=<NegBackward0>) tensor(12482.6250, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12482.6220703125
tensor(12482.6250, grad_fn=<NegBackward0>) tensor(12482.6221, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12482.6201171875
tensor(12482.6221, grad_fn=<NegBackward0>) tensor(12482.6201, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12482.619140625
tensor(12482.6201, grad_fn=<NegBackward0>) tensor(12482.6191, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12482.6162109375
tensor(12482.6191, grad_fn=<NegBackward0>) tensor(12482.6162, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12482.6142578125
tensor(12482.6162, grad_fn=<NegBackward0>) tensor(12482.6143, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12482.61328125
tensor(12482.6143, grad_fn=<NegBackward0>) tensor(12482.6133, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12482.609375
tensor(12482.6133, grad_fn=<NegBackward0>) tensor(12482.6094, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12482.609375
tensor(12482.6094, grad_fn=<NegBackward0>) tensor(12482.6094, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12482.607421875
tensor(12482.6094, grad_fn=<NegBackward0>) tensor(12482.6074, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12482.6025390625
tensor(12482.6074, grad_fn=<NegBackward0>) tensor(12482.6025, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12482.5859375
tensor(12482.6025, grad_fn=<NegBackward0>) tensor(12482.5859, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12482.4248046875
tensor(12482.5859, grad_fn=<NegBackward0>) tensor(12482.4248, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12481.6455078125
tensor(12482.4248, grad_fn=<NegBackward0>) tensor(12481.6455, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12481.44921875
tensor(12481.6455, grad_fn=<NegBackward0>) tensor(12481.4492, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12481.3896484375
tensor(12481.4492, grad_fn=<NegBackward0>) tensor(12481.3896, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12481.3642578125
tensor(12481.3896, grad_fn=<NegBackward0>) tensor(12481.3643, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12481.349609375
tensor(12481.3643, grad_fn=<NegBackward0>) tensor(12481.3496, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12481.3408203125
tensor(12481.3496, grad_fn=<NegBackward0>) tensor(12481.3408, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12481.3349609375
tensor(12481.3408, grad_fn=<NegBackward0>) tensor(12481.3350, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12481.3310546875
tensor(12481.3350, grad_fn=<NegBackward0>) tensor(12481.3311, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12481.3291015625
tensor(12481.3311, grad_fn=<NegBackward0>) tensor(12481.3291, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12481.3251953125
tensor(12481.3291, grad_fn=<NegBackward0>) tensor(12481.3252, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12481.3251953125
tensor(12481.3252, grad_fn=<NegBackward0>) tensor(12481.3252, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12481.322265625
tensor(12481.3252, grad_fn=<NegBackward0>) tensor(12481.3223, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12481.322265625
tensor(12481.3223, grad_fn=<NegBackward0>) tensor(12481.3223, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12481.3212890625
tensor(12481.3223, grad_fn=<NegBackward0>) tensor(12481.3213, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12481.3203125
tensor(12481.3213, grad_fn=<NegBackward0>) tensor(12481.3203, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12481.318359375
tensor(12481.3203, grad_fn=<NegBackward0>) tensor(12481.3184, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12481.392578125
tensor(12481.3184, grad_fn=<NegBackward0>) tensor(12481.3926, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12481.318359375
tensor(12481.3184, grad_fn=<NegBackward0>) tensor(12481.3184, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12481.31640625
tensor(12481.3184, grad_fn=<NegBackward0>) tensor(12481.3164, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12481.3212890625
tensor(12481.3164, grad_fn=<NegBackward0>) tensor(12481.3213, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12481.3154296875
tensor(12481.3164, grad_fn=<NegBackward0>) tensor(12481.3154, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12481.3154296875
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3154, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12481.31640625
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3164, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12481.3173828125
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3174, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12481.31640625
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3164, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12481.3154296875
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3154, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12481.322265625
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3223, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12481.314453125
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12481.3603515625
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3604, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12481.3369140625
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3369, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12481.314453125
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12481.31640625
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3164, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12481.314453125
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12481.3447265625
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3447, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12481.3134765625
tensor(12481.3145, grad_fn=<NegBackward0>) tensor(12481.3135, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12481.34375
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3438, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12481.314453125
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12481.3525390625
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3525, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12481.314453125
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -12481.3134765625
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3135, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12481.3271484375
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3271, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12481.314453125
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12481.341796875
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3418, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12481.333984375
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3340, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -12481.3134765625
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3135, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12481.33984375
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3398, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12481.3125
tensor(12481.3135, grad_fn=<NegBackward0>) tensor(12481.3125, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12481.3154296875
tensor(12481.3125, grad_fn=<NegBackward0>) tensor(12481.3154, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12481.3134765625
tensor(12481.3125, grad_fn=<NegBackward0>) tensor(12481.3135, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12481.314453125
tensor(12481.3125, grad_fn=<NegBackward0>) tensor(12481.3145, grad_fn=<NegBackward0>)
3
pi: tensor([[4.4453e-06, 1.0000e+00],
        [9.9999e-01, 9.7996e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0147, 0.9853], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2060, 0.2437],
         [0.6484, 0.2003]],

        [[0.6007, 0.2447],
         [0.5022, 0.6002]],

        [[0.6522, 0.1307],
         [0.6129, 0.5821]],

        [[0.7050, 0.1982],
         [0.6101, 0.5524]],

        [[0.7056, 0.2978],
         [0.6435, 0.5437]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0019013329579368875
Average Adjusted Rand Index: -0.0013502490046180212
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20735.234375
inf tensor(20735.2344, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12484.537109375
tensor(20735.2344, grad_fn=<NegBackward0>) tensor(12484.5371, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12484.2109375
tensor(12484.5371, grad_fn=<NegBackward0>) tensor(12484.2109, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12484.09375
tensor(12484.2109, grad_fn=<NegBackward0>) tensor(12484.0938, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12483.99609375
tensor(12484.0938, grad_fn=<NegBackward0>) tensor(12483.9961, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12483.8916015625
tensor(12483.9961, grad_fn=<NegBackward0>) tensor(12483.8916, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12483.759765625
tensor(12483.8916, grad_fn=<NegBackward0>) tensor(12483.7598, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12483.6025390625
tensor(12483.7598, grad_fn=<NegBackward0>) tensor(12483.6025, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12483.4619140625
tensor(12483.6025, grad_fn=<NegBackward0>) tensor(12483.4619, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12483.3818359375
tensor(12483.4619, grad_fn=<NegBackward0>) tensor(12483.3818, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12483.337890625
tensor(12483.3818, grad_fn=<NegBackward0>) tensor(12483.3379, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12483.306640625
tensor(12483.3379, grad_fn=<NegBackward0>) tensor(12483.3066, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12483.2783203125
tensor(12483.3066, grad_fn=<NegBackward0>) tensor(12483.2783, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12483.2529296875
tensor(12483.2783, grad_fn=<NegBackward0>) tensor(12483.2529, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12483.2265625
tensor(12483.2529, grad_fn=<NegBackward0>) tensor(12483.2266, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12483.205078125
tensor(12483.2266, grad_fn=<NegBackward0>) tensor(12483.2051, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12483.1806640625
tensor(12483.2051, grad_fn=<NegBackward0>) tensor(12483.1807, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12483.1572265625
tensor(12483.1807, grad_fn=<NegBackward0>) tensor(12483.1572, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12483.130859375
tensor(12483.1572, grad_fn=<NegBackward0>) tensor(12483.1309, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12483.107421875
tensor(12483.1309, grad_fn=<NegBackward0>) tensor(12483.1074, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12483.0849609375
tensor(12483.1074, grad_fn=<NegBackward0>) tensor(12483.0850, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12483.0634765625
tensor(12483.0850, grad_fn=<NegBackward0>) tensor(12483.0635, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12483.0439453125
tensor(12483.0635, grad_fn=<NegBackward0>) tensor(12483.0439, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12483.025390625
tensor(12483.0439, grad_fn=<NegBackward0>) tensor(12483.0254, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12483.01171875
tensor(12483.0254, grad_fn=<NegBackward0>) tensor(12483.0117, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12483.0
tensor(12483.0117, grad_fn=<NegBackward0>) tensor(12483., grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12482.9892578125
tensor(12483., grad_fn=<NegBackward0>) tensor(12482.9893, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12482.9794921875
tensor(12482.9893, grad_fn=<NegBackward0>) tensor(12482.9795, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12482.97265625
tensor(12482.9795, grad_fn=<NegBackward0>) tensor(12482.9727, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12482.9638671875
tensor(12482.9727, grad_fn=<NegBackward0>) tensor(12482.9639, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12482.9599609375
tensor(12482.9639, grad_fn=<NegBackward0>) tensor(12482.9600, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12482.953125
tensor(12482.9600, grad_fn=<NegBackward0>) tensor(12482.9531, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12482.947265625
tensor(12482.9531, grad_fn=<NegBackward0>) tensor(12482.9473, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12482.9423828125
tensor(12482.9473, grad_fn=<NegBackward0>) tensor(12482.9424, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12482.935546875
tensor(12482.9424, grad_fn=<NegBackward0>) tensor(12482.9355, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12482.9287109375
tensor(12482.9355, grad_fn=<NegBackward0>) tensor(12482.9287, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12482.919921875
tensor(12482.9287, grad_fn=<NegBackward0>) tensor(12482.9199, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12482.9052734375
tensor(12482.9199, grad_fn=<NegBackward0>) tensor(12482.9053, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12482.8857421875
tensor(12482.9053, grad_fn=<NegBackward0>) tensor(12482.8857, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12482.8486328125
tensor(12482.8857, grad_fn=<NegBackward0>) tensor(12482.8486, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12482.7802734375
tensor(12482.8486, grad_fn=<NegBackward0>) tensor(12482.7803, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12482.7021484375
tensor(12482.7803, grad_fn=<NegBackward0>) tensor(12482.7021, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12482.6552734375
tensor(12482.7021, grad_fn=<NegBackward0>) tensor(12482.6553, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12482.6337890625
tensor(12482.6553, grad_fn=<NegBackward0>) tensor(12482.6338, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12482.625
tensor(12482.6338, grad_fn=<NegBackward0>) tensor(12482.6250, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12482.62109375
tensor(12482.6250, grad_fn=<NegBackward0>) tensor(12482.6211, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12482.6171875
tensor(12482.6211, grad_fn=<NegBackward0>) tensor(12482.6172, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12482.615234375
tensor(12482.6172, grad_fn=<NegBackward0>) tensor(12482.6152, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12482.615234375
tensor(12482.6152, grad_fn=<NegBackward0>) tensor(12482.6152, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12482.6142578125
tensor(12482.6152, grad_fn=<NegBackward0>) tensor(12482.6143, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12482.61328125
tensor(12482.6143, grad_fn=<NegBackward0>) tensor(12482.6133, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12482.6103515625
tensor(12482.6133, grad_fn=<NegBackward0>) tensor(12482.6104, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12482.6123046875
tensor(12482.6104, grad_fn=<NegBackward0>) tensor(12482.6123, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12482.6103515625
tensor(12482.6104, grad_fn=<NegBackward0>) tensor(12482.6104, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12482.6103515625
tensor(12482.6104, grad_fn=<NegBackward0>) tensor(12482.6104, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12482.6103515625
tensor(12482.6104, grad_fn=<NegBackward0>) tensor(12482.6104, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12482.607421875
tensor(12482.6104, grad_fn=<NegBackward0>) tensor(12482.6074, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12482.607421875
tensor(12482.6074, grad_fn=<NegBackward0>) tensor(12482.6074, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12482.6064453125
tensor(12482.6074, grad_fn=<NegBackward0>) tensor(12482.6064, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12482.6015625
tensor(12482.6064, grad_fn=<NegBackward0>) tensor(12482.6016, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12482.591796875
tensor(12482.6016, grad_fn=<NegBackward0>) tensor(12482.5918, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12482.4833984375
tensor(12482.5918, grad_fn=<NegBackward0>) tensor(12482.4834, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12481.5927734375
tensor(12482.4834, grad_fn=<NegBackward0>) tensor(12481.5928, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12481.4208984375
tensor(12481.5928, grad_fn=<NegBackward0>) tensor(12481.4209, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12481.3740234375
tensor(12481.4209, grad_fn=<NegBackward0>) tensor(12481.3740, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12481.353515625
tensor(12481.3740, grad_fn=<NegBackward0>) tensor(12481.3535, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12481.341796875
tensor(12481.3535, grad_fn=<NegBackward0>) tensor(12481.3418, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12481.3369140625
tensor(12481.3418, grad_fn=<NegBackward0>) tensor(12481.3369, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12481.33203125
tensor(12481.3369, grad_fn=<NegBackward0>) tensor(12481.3320, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12481.330078125
tensor(12481.3320, grad_fn=<NegBackward0>) tensor(12481.3301, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12481.328125
tensor(12481.3301, grad_fn=<NegBackward0>) tensor(12481.3281, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12481.3251953125
tensor(12481.3281, grad_fn=<NegBackward0>) tensor(12481.3252, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12481.3232421875
tensor(12481.3252, grad_fn=<NegBackward0>) tensor(12481.3232, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12481.322265625
tensor(12481.3232, grad_fn=<NegBackward0>) tensor(12481.3223, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12481.3212890625
tensor(12481.3223, grad_fn=<NegBackward0>) tensor(12481.3213, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12481.3212890625
tensor(12481.3213, grad_fn=<NegBackward0>) tensor(12481.3213, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12481.3193359375
tensor(12481.3213, grad_fn=<NegBackward0>) tensor(12481.3193, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12481.564453125
tensor(12481.3193, grad_fn=<NegBackward0>) tensor(12481.5645, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12481.318359375
tensor(12481.3193, grad_fn=<NegBackward0>) tensor(12481.3184, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12481.3193359375
tensor(12481.3184, grad_fn=<NegBackward0>) tensor(12481.3193, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12481.3193359375
tensor(12481.3184, grad_fn=<NegBackward0>) tensor(12481.3193, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12481.31640625
tensor(12481.3184, grad_fn=<NegBackward0>) tensor(12481.3164, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12481.3154296875
tensor(12481.3164, grad_fn=<NegBackward0>) tensor(12481.3154, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12481.3427734375
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3428, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12481.3232421875
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3232, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -12481.37109375
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3711, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -12481.3154296875
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3154, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12481.3193359375
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3193, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12481.333984375
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3340, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12481.341796875
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3418, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -12481.322265625
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3223, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -12481.3388671875
tensor(12481.3154, grad_fn=<NegBackward0>) tensor(12481.3389, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[1.7805e-05, 9.9998e-01],
        [9.9999e-01, 7.6389e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9854, 0.0146], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1993, 0.2437],
         [0.5583, 0.2056]],

        [[0.5261, 0.2448],
         [0.5261, 0.5506]],

        [[0.6104, 0.1307],
         [0.5432, 0.5587]],

        [[0.5022, 0.1982],
         [0.5257, 0.7097]],

        [[0.5469, 0.2978],
         [0.5603, 0.6824]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0019013329579368875
Average Adjusted Rand Index: -0.0013502490046180212
[-0.0019013329579368875, -0.0019013329579368875] [-0.0013502490046180212, -0.0013502490046180212] [12481.318359375, 12481.3388671875]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11719.738743468703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23107.123046875
inf tensor(23107.1230, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12207.126953125
tensor(23107.1230, grad_fn=<NegBackward0>) tensor(12207.1270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12206.6552734375
tensor(12207.1270, grad_fn=<NegBackward0>) tensor(12206.6553, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12206.4326171875
tensor(12206.6553, grad_fn=<NegBackward0>) tensor(12206.4326, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12206.1767578125
tensor(12206.4326, grad_fn=<NegBackward0>) tensor(12206.1768, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12205.986328125
tensor(12206.1768, grad_fn=<NegBackward0>) tensor(12205.9863, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12205.873046875
tensor(12205.9863, grad_fn=<NegBackward0>) tensor(12205.8730, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12205.798828125
tensor(12205.8730, grad_fn=<NegBackward0>) tensor(12205.7988, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12205.7509765625
tensor(12205.7988, grad_fn=<NegBackward0>) tensor(12205.7510, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12205.7236328125
tensor(12205.7510, grad_fn=<NegBackward0>) tensor(12205.7236, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12205.7060546875
tensor(12205.7236, grad_fn=<NegBackward0>) tensor(12205.7061, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12205.6962890625
tensor(12205.7061, grad_fn=<NegBackward0>) tensor(12205.6963, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12205.6884765625
tensor(12205.6963, grad_fn=<NegBackward0>) tensor(12205.6885, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12205.6845703125
tensor(12205.6885, grad_fn=<NegBackward0>) tensor(12205.6846, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12205.681640625
tensor(12205.6846, grad_fn=<NegBackward0>) tensor(12205.6816, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12205.6787109375
tensor(12205.6816, grad_fn=<NegBackward0>) tensor(12205.6787, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12205.677734375
tensor(12205.6787, grad_fn=<NegBackward0>) tensor(12205.6777, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12205.6767578125
tensor(12205.6777, grad_fn=<NegBackward0>) tensor(12205.6768, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12205.67578125
tensor(12205.6768, grad_fn=<NegBackward0>) tensor(12205.6758, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12205.6748046875
tensor(12205.6758, grad_fn=<NegBackward0>) tensor(12205.6748, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12205.6728515625
tensor(12205.6748, grad_fn=<NegBackward0>) tensor(12205.6729, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12205.673828125
tensor(12205.6729, grad_fn=<NegBackward0>) tensor(12205.6738, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -12205.671875
tensor(12205.6729, grad_fn=<NegBackward0>) tensor(12205.6719, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12205.671875
tensor(12205.6719, grad_fn=<NegBackward0>) tensor(12205.6719, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12205.6728515625
tensor(12205.6719, grad_fn=<NegBackward0>) tensor(12205.6729, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -12205.671875
tensor(12205.6719, grad_fn=<NegBackward0>) tensor(12205.6719, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12205.6708984375
tensor(12205.6719, grad_fn=<NegBackward0>) tensor(12205.6709, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12205.669921875
tensor(12205.6709, grad_fn=<NegBackward0>) tensor(12205.6699, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12205.669921875
tensor(12205.6699, grad_fn=<NegBackward0>) tensor(12205.6699, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12205.6689453125
tensor(12205.6699, grad_fn=<NegBackward0>) tensor(12205.6689, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12205.66796875
tensor(12205.6689, grad_fn=<NegBackward0>) tensor(12205.6680, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12205.6669921875
tensor(12205.6680, grad_fn=<NegBackward0>) tensor(12205.6670, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12205.6669921875
tensor(12205.6670, grad_fn=<NegBackward0>) tensor(12205.6670, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12205.666015625
tensor(12205.6670, grad_fn=<NegBackward0>) tensor(12205.6660, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12205.6630859375
tensor(12205.6660, grad_fn=<NegBackward0>) tensor(12205.6631, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12205.658203125
tensor(12205.6631, grad_fn=<NegBackward0>) tensor(12205.6582, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12205.646484375
tensor(12205.6582, grad_fn=<NegBackward0>) tensor(12205.6465, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12205.564453125
tensor(12205.6465, grad_fn=<NegBackward0>) tensor(12205.5645, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12205.3388671875
tensor(12205.5645, grad_fn=<NegBackward0>) tensor(12205.3389, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12205.2861328125
tensor(12205.3389, grad_fn=<NegBackward0>) tensor(12205.2861, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12205.271484375
tensor(12205.2861, grad_fn=<NegBackward0>) tensor(12205.2715, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12205.2666015625
tensor(12205.2715, grad_fn=<NegBackward0>) tensor(12205.2666, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12205.263671875
tensor(12205.2666, grad_fn=<NegBackward0>) tensor(12205.2637, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12205.2626953125
tensor(12205.2637, grad_fn=<NegBackward0>) tensor(12205.2627, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12205.2607421875
tensor(12205.2627, grad_fn=<NegBackward0>) tensor(12205.2607, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12205.259765625
tensor(12205.2607, grad_fn=<NegBackward0>) tensor(12205.2598, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12205.259765625
tensor(12205.2598, grad_fn=<NegBackward0>) tensor(12205.2598, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12205.2578125
tensor(12205.2598, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12205.2578125
tensor(12205.2578, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12205.2587890625
tensor(12205.2578, grad_fn=<NegBackward0>) tensor(12205.2588, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12205.2578125
tensor(12205.2578, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12205.2578125
tensor(12205.2578, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12205.2568359375
tensor(12205.2578, grad_fn=<NegBackward0>) tensor(12205.2568, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12205.255859375
tensor(12205.2568, grad_fn=<NegBackward0>) tensor(12205.2559, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12205.2578125
tensor(12205.2559, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12205.2578125
tensor(12205.2559, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12205.2578125
tensor(12205.2559, grad_fn=<NegBackward0>) tensor(12205.2578, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12205.2568359375
tensor(12205.2559, grad_fn=<NegBackward0>) tensor(12205.2568, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12205.2568359375
tensor(12205.2559, grad_fn=<NegBackward0>) tensor(12205.2568, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.9797, 0.0203],
        [0.9904, 0.0096]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0339, 0.9661], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.1896],
         [0.7154, 0.1874]],

        [[0.5505, 0.1949],
         [0.5840, 0.5499]],

        [[0.7104, 0.2891],
         [0.6994, 0.7239]],

        [[0.6135, 0.1983],
         [0.6855, 0.6617]],

        [[0.6088, 0.2024],
         [0.5899, 0.5662]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002376816793798192
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18882.296875
inf tensor(18882.2969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12207.0419921875
tensor(18882.2969, grad_fn=<NegBackward0>) tensor(12207.0420, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12206.7607421875
tensor(12207.0420, grad_fn=<NegBackward0>) tensor(12206.7607, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12206.6552734375
tensor(12206.7607, grad_fn=<NegBackward0>) tensor(12206.6553, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12206.501953125
tensor(12206.6553, grad_fn=<NegBackward0>) tensor(12206.5020, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12206.380859375
tensor(12206.5020, grad_fn=<NegBackward0>) tensor(12206.3809, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12206.2578125
tensor(12206.3809, grad_fn=<NegBackward0>) tensor(12206.2578, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12205.994140625
tensor(12206.2578, grad_fn=<NegBackward0>) tensor(12205.9941, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12205.759765625
tensor(12205.9941, grad_fn=<NegBackward0>) tensor(12205.7598, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12205.701171875
tensor(12205.7598, grad_fn=<NegBackward0>) tensor(12205.7012, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12205.685546875
tensor(12205.7012, grad_fn=<NegBackward0>) tensor(12205.6855, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12205.67578125
tensor(12205.6855, grad_fn=<NegBackward0>) tensor(12205.6758, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12205.6708984375
tensor(12205.6758, grad_fn=<NegBackward0>) tensor(12205.6709, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12205.6669921875
tensor(12205.6709, grad_fn=<NegBackward0>) tensor(12205.6670, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12205.66015625
tensor(12205.6670, grad_fn=<NegBackward0>) tensor(12205.6602, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12205.6533203125
tensor(12205.6602, grad_fn=<NegBackward0>) tensor(12205.6533, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12205.6416015625
tensor(12205.6533, grad_fn=<NegBackward0>) tensor(12205.6416, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12205.625
tensor(12205.6416, grad_fn=<NegBackward0>) tensor(12205.6250, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12205.5849609375
tensor(12205.6250, grad_fn=<NegBackward0>) tensor(12205.5850, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12205.4990234375
tensor(12205.5850, grad_fn=<NegBackward0>) tensor(12205.4990, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12205.404296875
tensor(12205.4990, grad_fn=<NegBackward0>) tensor(12205.4043, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12205.3642578125
tensor(12205.4043, grad_fn=<NegBackward0>) tensor(12205.3643, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12205.330078125
tensor(12205.3643, grad_fn=<NegBackward0>) tensor(12205.3301, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12205.3076171875
tensor(12205.3301, grad_fn=<NegBackward0>) tensor(12205.3076, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12205.2919921875
tensor(12205.3076, grad_fn=<NegBackward0>) tensor(12205.2920, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12205.2841796875
tensor(12205.2920, grad_fn=<NegBackward0>) tensor(12205.2842, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12205.275390625
tensor(12205.2842, grad_fn=<NegBackward0>) tensor(12205.2754, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12205.271484375
tensor(12205.2754, grad_fn=<NegBackward0>) tensor(12205.2715, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12205.265625
tensor(12205.2715, grad_fn=<NegBackward0>) tensor(12205.2656, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12205.259765625
tensor(12205.2656, grad_fn=<NegBackward0>) tensor(12205.2598, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12205.248046875
tensor(12205.2598, grad_fn=<NegBackward0>) tensor(12205.2480, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12205.234375
tensor(12205.2480, grad_fn=<NegBackward0>) tensor(12205.2344, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12205.216796875
tensor(12205.2344, grad_fn=<NegBackward0>) tensor(12205.2168, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12205.205078125
tensor(12205.2168, grad_fn=<NegBackward0>) tensor(12205.2051, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12205.197265625
tensor(12205.2051, grad_fn=<NegBackward0>) tensor(12205.1973, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12205.189453125
tensor(12205.1973, grad_fn=<NegBackward0>) tensor(12205.1895, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12205.1845703125
tensor(12205.1895, grad_fn=<NegBackward0>) tensor(12205.1846, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12205.1796875
tensor(12205.1846, grad_fn=<NegBackward0>) tensor(12205.1797, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12205.1865234375
tensor(12205.1797, grad_fn=<NegBackward0>) tensor(12205.1865, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12205.1728515625
tensor(12205.1797, grad_fn=<NegBackward0>) tensor(12205.1729, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12205.17578125
tensor(12205.1729, grad_fn=<NegBackward0>) tensor(12205.1758, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12205.16796875
tensor(12205.1729, grad_fn=<NegBackward0>) tensor(12205.1680, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12205.1748046875
tensor(12205.1680, grad_fn=<NegBackward0>) tensor(12205.1748, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12205.1640625
tensor(12205.1680, grad_fn=<NegBackward0>) tensor(12205.1641, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12205.1611328125
tensor(12205.1641, grad_fn=<NegBackward0>) tensor(12205.1611, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12205.1611328125
tensor(12205.1611, grad_fn=<NegBackward0>) tensor(12205.1611, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12205.1591796875
tensor(12205.1611, grad_fn=<NegBackward0>) tensor(12205.1592, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12205.1572265625
tensor(12205.1592, grad_fn=<NegBackward0>) tensor(12205.1572, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12205.15625
tensor(12205.1572, grad_fn=<NegBackward0>) tensor(12205.1562, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12205.1552734375
tensor(12205.1562, grad_fn=<NegBackward0>) tensor(12205.1553, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12205.1552734375
tensor(12205.1553, grad_fn=<NegBackward0>) tensor(12205.1553, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12205.1533203125
tensor(12205.1553, grad_fn=<NegBackward0>) tensor(12205.1533, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12205.1513671875
tensor(12205.1533, grad_fn=<NegBackward0>) tensor(12205.1514, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12205.15234375
tensor(12205.1514, grad_fn=<NegBackward0>) tensor(12205.1523, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12205.1513671875
tensor(12205.1514, grad_fn=<NegBackward0>) tensor(12205.1514, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12205.150390625
tensor(12205.1514, grad_fn=<NegBackward0>) tensor(12205.1504, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12205.1494140625
tensor(12205.1504, grad_fn=<NegBackward0>) tensor(12205.1494, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12205.1484375
tensor(12205.1494, grad_fn=<NegBackward0>) tensor(12205.1484, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12205.1484375
tensor(12205.1484, grad_fn=<NegBackward0>) tensor(12205.1484, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12205.1494140625
tensor(12205.1484, grad_fn=<NegBackward0>) tensor(12205.1494, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12205.146484375
tensor(12205.1484, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12205.1484375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1484, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12205.146484375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12205.1640625
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1641, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12205.1474609375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1475, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12205.146484375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12205.1474609375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1475, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12205.1474609375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1475, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12205.146484375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12205.1513671875
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1514, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12205.1494140625
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1494, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12205.146484375
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12205.1455078125
tensor(12205.1465, grad_fn=<NegBackward0>) tensor(12205.1455, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12205.1455078125
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.1455, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12205.146484375
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12205.146484375
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.1465, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12205.21484375
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.2148, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -12205.1455078125
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.1455, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12205.1611328125
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.1611, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12205.2509765625
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.2510, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12205.14453125
tensor(12205.1455, grad_fn=<NegBackward0>) tensor(12205.1445, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12205.14453125
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1445, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12205.1455078125
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1455, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12205.1474609375
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1475, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12205.14453125
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1445, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12205.14453125
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1445, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12205.1455078125
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1455, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12205.1455078125
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1455, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12205.1474609375
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1475, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12205.1435546875
tensor(12205.1445, grad_fn=<NegBackward0>) tensor(12205.1436, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12205.14453125
tensor(12205.1436, grad_fn=<NegBackward0>) tensor(12205.1445, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12205.40625
tensor(12205.1436, grad_fn=<NegBackward0>) tensor(12205.4062, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12205.1435546875
tensor(12205.1436, grad_fn=<NegBackward0>) tensor(12205.1436, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12205.216796875
tensor(12205.1436, grad_fn=<NegBackward0>) tensor(12205.2168, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12205.1474609375
tensor(12205.1436, grad_fn=<NegBackward0>) tensor(12205.1475, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -12205.142578125
tensor(12205.1436, grad_fn=<NegBackward0>) tensor(12205.1426, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12205.142578125
tensor(12205.1426, grad_fn=<NegBackward0>) tensor(12205.1426, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12205.1435546875
tensor(12205.1426, grad_fn=<NegBackward0>) tensor(12205.1436, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12205.1640625
tensor(12205.1426, grad_fn=<NegBackward0>) tensor(12205.1641, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12205.1875
tensor(12205.1426, grad_fn=<NegBackward0>) tensor(12205.1875, grad_fn=<NegBackward0>)
3
pi: tensor([[9.9995e-01, 5.4979e-05],
        [8.1149e-01, 1.8851e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4302, 0.5698], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1970, 0.1886],
         [0.5458, 0.1810]],

        [[0.6310, 0.1856],
         [0.5612, 0.5660]],

        [[0.6083, 0.2916],
         [0.6967, 0.5133]],

        [[0.5309, 0.1858],
         [0.6787, 0.6251]],

        [[0.5277, 0.2164],
         [0.6298, 0.6905]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.010924207834367757
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.004800617908674532
Average Adjusted Rand Index: 0.0018985230888872162
[0.002376816793798192, 0.004800617908674532] [0.0, 0.0018985230888872162] [12205.2568359375, 12205.177734375]
-------------------------------------
This iteration is 40
True Objective function: Loss = -11897.642269019521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23606.70703125
inf tensor(23606.7070, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12377.845703125
tensor(23606.7070, grad_fn=<NegBackward0>) tensor(12377.8457, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12206.7060546875
tensor(12377.8457, grad_fn=<NegBackward0>) tensor(12206.7061, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11924.849609375
tensor(12206.7061, grad_fn=<NegBackward0>) tensor(11924.8496, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11924.208984375
tensor(11924.8496, grad_fn=<NegBackward0>) tensor(11924.2090, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11924.025390625
tensor(11924.2090, grad_fn=<NegBackward0>) tensor(11924.0254, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11923.8955078125
tensor(11924.0254, grad_fn=<NegBackward0>) tensor(11923.8955, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11923.8251953125
tensor(11923.8955, grad_fn=<NegBackward0>) tensor(11923.8252, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11923.787109375
tensor(11923.8252, grad_fn=<NegBackward0>) tensor(11923.7871, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11923.763671875
tensor(11923.7871, grad_fn=<NegBackward0>) tensor(11923.7637, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11923.74609375
tensor(11923.7637, grad_fn=<NegBackward0>) tensor(11923.7461, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11923.73046875
tensor(11923.7461, grad_fn=<NegBackward0>) tensor(11923.7305, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11923.72265625
tensor(11923.7305, grad_fn=<NegBackward0>) tensor(11923.7227, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11923.7138671875
tensor(11923.7227, grad_fn=<NegBackward0>) tensor(11923.7139, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11923.7080078125
tensor(11923.7139, grad_fn=<NegBackward0>) tensor(11923.7080, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11923.701171875
tensor(11923.7080, grad_fn=<NegBackward0>) tensor(11923.7012, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11923.6943359375
tensor(11923.7012, grad_fn=<NegBackward0>) tensor(11923.6943, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11923.6826171875
tensor(11923.6943, grad_fn=<NegBackward0>) tensor(11923.6826, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11923.6787109375
tensor(11923.6826, grad_fn=<NegBackward0>) tensor(11923.6787, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11923.677734375
tensor(11923.6787, grad_fn=<NegBackward0>) tensor(11923.6777, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11923.6728515625
tensor(11923.6777, grad_fn=<NegBackward0>) tensor(11923.6729, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11923.671875
tensor(11923.6729, grad_fn=<NegBackward0>) tensor(11923.6719, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11923.669921875
tensor(11923.6719, grad_fn=<NegBackward0>) tensor(11923.6699, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11923.6669921875
tensor(11923.6699, grad_fn=<NegBackward0>) tensor(11923.6670, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11923.6650390625
tensor(11923.6670, grad_fn=<NegBackward0>) tensor(11923.6650, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11923.6650390625
tensor(11923.6650, grad_fn=<NegBackward0>) tensor(11923.6650, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11923.6640625
tensor(11923.6650, grad_fn=<NegBackward0>) tensor(11923.6641, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11923.662109375
tensor(11923.6641, grad_fn=<NegBackward0>) tensor(11923.6621, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11923.6630859375
tensor(11923.6621, grad_fn=<NegBackward0>) tensor(11923.6631, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11923.662109375
tensor(11923.6621, grad_fn=<NegBackward0>) tensor(11923.6621, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11923.6611328125
tensor(11923.6621, grad_fn=<NegBackward0>) tensor(11923.6611, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11923.6591796875
tensor(11923.6611, grad_fn=<NegBackward0>) tensor(11923.6592, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11923.658203125
tensor(11923.6592, grad_fn=<NegBackward0>) tensor(11923.6582, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11923.658203125
tensor(11923.6582, grad_fn=<NegBackward0>) tensor(11923.6582, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11923.6572265625
tensor(11923.6582, grad_fn=<NegBackward0>) tensor(11923.6572, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11923.6572265625
tensor(11923.6572, grad_fn=<NegBackward0>) tensor(11923.6572, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11923.65625
tensor(11923.6572, grad_fn=<NegBackward0>) tensor(11923.6562, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11923.65625
tensor(11923.6562, grad_fn=<NegBackward0>) tensor(11923.6562, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11923.65625
tensor(11923.6562, grad_fn=<NegBackward0>) tensor(11923.6562, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11923.6552734375
tensor(11923.6562, grad_fn=<NegBackward0>) tensor(11923.6553, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11923.65625
tensor(11923.6553, grad_fn=<NegBackward0>) tensor(11923.6562, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11923.6552734375
tensor(11923.6553, grad_fn=<NegBackward0>) tensor(11923.6553, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11923.654296875
tensor(11923.6553, grad_fn=<NegBackward0>) tensor(11923.6543, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11923.6533203125
tensor(11923.6543, grad_fn=<NegBackward0>) tensor(11923.6533, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11923.654296875
tensor(11923.6533, grad_fn=<NegBackward0>) tensor(11923.6543, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11923.658203125
tensor(11923.6533, grad_fn=<NegBackward0>) tensor(11923.6582, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11923.6572265625
tensor(11923.6533, grad_fn=<NegBackward0>) tensor(11923.6572, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11923.625
tensor(11923.6533, grad_fn=<NegBackward0>) tensor(11923.6250, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11923.623046875
tensor(11923.6250, grad_fn=<NegBackward0>) tensor(11923.6230, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11923.623046875
tensor(11923.6230, grad_fn=<NegBackward0>) tensor(11923.6230, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11923.6318359375
tensor(11923.6230, grad_fn=<NegBackward0>) tensor(11923.6318, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11923.6220703125
tensor(11923.6230, grad_fn=<NegBackward0>) tensor(11923.6221, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11923.6220703125
tensor(11923.6221, grad_fn=<NegBackward0>) tensor(11923.6221, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11923.6220703125
tensor(11923.6221, grad_fn=<NegBackward0>) tensor(11923.6221, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11923.6220703125
tensor(11923.6221, grad_fn=<NegBackward0>) tensor(11923.6221, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11923.642578125
tensor(11923.6221, grad_fn=<NegBackward0>) tensor(11923.6426, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11923.62109375
tensor(11923.6221, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11923.62109375
tensor(11923.6211, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11923.62109375
tensor(11923.6211, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11923.62109375
tensor(11923.6211, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11923.6201171875
tensor(11923.6211, grad_fn=<NegBackward0>) tensor(11923.6201, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11923.62109375
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11923.6201171875
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6201, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11923.62109375
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11923.62109375
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11923.62109375
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11923.6220703125
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6221, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11923.6201171875
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6201, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11923.62109375
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11923.6494140625
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6494, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11923.619140625
tensor(11923.6201, grad_fn=<NegBackward0>) tensor(11923.6191, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11923.62109375
tensor(11923.6191, grad_fn=<NegBackward0>) tensor(11923.6211, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11923.6181640625
tensor(11923.6191, grad_fn=<NegBackward0>) tensor(11923.6182, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11923.619140625
tensor(11923.6182, grad_fn=<NegBackward0>) tensor(11923.6191, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11923.619140625
tensor(11923.6182, grad_fn=<NegBackward0>) tensor(11923.6191, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11923.619140625
tensor(11923.6182, grad_fn=<NegBackward0>) tensor(11923.6191, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11923.6201171875
tensor(11923.6182, grad_fn=<NegBackward0>) tensor(11923.6201, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11923.6201171875
tensor(11923.6182, grad_fn=<NegBackward0>) tensor(11923.6201, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.6365, 0.3635],
        [0.4387, 0.5613]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5517, 0.4483], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3076, 0.1078],
         [0.6946, 0.2930]],

        [[0.5004, 0.0925],
         [0.6698, 0.6164]],

        [[0.5496, 0.0893],
         [0.7031, 0.7305]],

        [[0.6961, 0.1033],
         [0.5839, 0.5630]],

        [[0.6870, 0.1054],
         [0.6328, 0.7284]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.34440352613250064
Average Adjusted Rand Index: 0.9761609542216967
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21763.974609375
inf tensor(21763.9746, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11936.9951171875
tensor(21763.9746, grad_fn=<NegBackward0>) tensor(11936.9951, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11935.1162109375
tensor(11936.9951, grad_fn=<NegBackward0>) tensor(11935.1162, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11934.873046875
tensor(11935.1162, grad_fn=<NegBackward0>) tensor(11934.8730, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11934.755859375
tensor(11934.8730, grad_fn=<NegBackward0>) tensor(11934.7559, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11934.52734375
tensor(11934.7559, grad_fn=<NegBackward0>) tensor(11934.5273, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11934.494140625
tensor(11934.5273, grad_fn=<NegBackward0>) tensor(11934.4941, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11934.4716796875
tensor(11934.4941, grad_fn=<NegBackward0>) tensor(11934.4717, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11934.45703125
tensor(11934.4717, grad_fn=<NegBackward0>) tensor(11934.4570, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11934.4482421875
tensor(11934.4570, grad_fn=<NegBackward0>) tensor(11934.4482, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11934.4375
tensor(11934.4482, grad_fn=<NegBackward0>) tensor(11934.4375, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11934.4111328125
tensor(11934.4375, grad_fn=<NegBackward0>) tensor(11934.4111, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11934.4052734375
tensor(11934.4111, grad_fn=<NegBackward0>) tensor(11934.4053, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11934.400390625
tensor(11934.4053, grad_fn=<NegBackward0>) tensor(11934.4004, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11934.3984375
tensor(11934.4004, grad_fn=<NegBackward0>) tensor(11934.3984, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11934.396484375
tensor(11934.3984, grad_fn=<NegBackward0>) tensor(11934.3965, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11934.3955078125
tensor(11934.3965, grad_fn=<NegBackward0>) tensor(11934.3955, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11934.3935546875
tensor(11934.3955, grad_fn=<NegBackward0>) tensor(11934.3936, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11934.3916015625
tensor(11934.3936, grad_fn=<NegBackward0>) tensor(11934.3916, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11934.3876953125
tensor(11934.3916, grad_fn=<NegBackward0>) tensor(11934.3877, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11934.3876953125
tensor(11934.3877, grad_fn=<NegBackward0>) tensor(11934.3877, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11934.384765625
tensor(11934.3877, grad_fn=<NegBackward0>) tensor(11934.3848, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11934.38671875
tensor(11934.3848, grad_fn=<NegBackward0>) tensor(11934.3867, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11934.384765625
tensor(11934.3848, grad_fn=<NegBackward0>) tensor(11934.3848, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11934.3828125
tensor(11934.3848, grad_fn=<NegBackward0>) tensor(11934.3828, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11934.3828125
tensor(11934.3828, grad_fn=<NegBackward0>) tensor(11934.3828, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11934.3828125
tensor(11934.3828, grad_fn=<NegBackward0>) tensor(11934.3828, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11934.3828125
tensor(11934.3828, grad_fn=<NegBackward0>) tensor(11934.3828, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11934.380859375
tensor(11934.3828, grad_fn=<NegBackward0>) tensor(11934.3809, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11934.37890625
tensor(11934.3809, grad_fn=<NegBackward0>) tensor(11934.3789, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11934.37890625
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3789, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11934.3828125
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3828, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11934.3798828125
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3799, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -11934.37890625
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3789, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11934.37890625
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3789, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11934.3837890625
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3838, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11934.376953125
tensor(11934.3789, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11934.3779296875
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3779, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11934.3798828125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3799, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11934.3779296875
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3779, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11934.376953125
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3770, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11934.375
tensor(11934.3770, grad_fn=<NegBackward0>) tensor(11934.3750, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11934.3759765625
tensor(11934.3750, grad_fn=<NegBackward0>) tensor(11934.3760, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11934.375
tensor(11934.3750, grad_fn=<NegBackward0>) tensor(11934.3750, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11934.3759765625
tensor(11934.3750, grad_fn=<NegBackward0>) tensor(11934.3760, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11934.375
tensor(11934.3750, grad_fn=<NegBackward0>) tensor(11934.3750, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11934.3818359375
tensor(11934.3750, grad_fn=<NegBackward0>) tensor(11934.3818, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11934.3740234375
tensor(11934.3750, grad_fn=<NegBackward0>) tensor(11934.3740, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11934.3740234375
tensor(11934.3740, grad_fn=<NegBackward0>) tensor(11934.3740, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11934.373046875
tensor(11934.3740, grad_fn=<NegBackward0>) tensor(11934.3730, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11934.3779296875
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3779, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11934.3740234375
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3740, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11934.3759765625
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3760, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11934.3740234375
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3740, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11934.373046875
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3730, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11934.4150390625
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.4150, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11934.373046875
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3730, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11934.39453125
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3945, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11934.3740234375
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3740, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11934.3779296875
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3779, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11934.4072265625
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.4072, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11934.3740234375
tensor(11934.3730, grad_fn=<NegBackward0>) tensor(11934.3740, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.5192, 0.4808],
        [0.4912, 0.5088]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4426, 0.5574], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3043, 0.1077],
         [0.7256, 0.2992]],

        [[0.5793, 0.0926],
         [0.5604, 0.5767]],

        [[0.5028, 0.0893],
         [0.6756, 0.5540]],

        [[0.5264, 0.1032],
         [0.7189, 0.6879]],

        [[0.7243, 0.1053],
         [0.5356, 0.6474]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.344438669442637
Average Adjusted Rand Index: 0.9761609542216967
[0.34440352613250064, 0.344438669442637] [0.9761609542216967, 0.9761609542216967] [11923.6201171875, 11934.3740234375]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11859.773251537514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21843.48828125
inf tensor(21843.4883, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12372.0712890625
tensor(21843.4883, grad_fn=<NegBackward0>) tensor(12372.0713, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12371.4501953125
tensor(12372.0713, grad_fn=<NegBackward0>) tensor(12371.4502, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12371.2490234375
tensor(12371.4502, grad_fn=<NegBackward0>) tensor(12371.2490, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12371.0654296875
tensor(12371.2490, grad_fn=<NegBackward0>) tensor(12371.0654, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12370.818359375
tensor(12371.0654, grad_fn=<NegBackward0>) tensor(12370.8184, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12370.458984375
tensor(12370.8184, grad_fn=<NegBackward0>) tensor(12370.4590, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12369.6728515625
tensor(12370.4590, grad_fn=<NegBackward0>) tensor(12369.6729, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12367.923828125
tensor(12369.6729, grad_fn=<NegBackward0>) tensor(12367.9238, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12367.33203125
tensor(12367.9238, grad_fn=<NegBackward0>) tensor(12367.3320, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12367.099609375
tensor(12367.3320, grad_fn=<NegBackward0>) tensor(12367.0996, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12366.970703125
tensor(12367.0996, grad_fn=<NegBackward0>) tensor(12366.9707, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12366.884765625
tensor(12366.9707, grad_fn=<NegBackward0>) tensor(12366.8848, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12366.8359375
tensor(12366.8848, grad_fn=<NegBackward0>) tensor(12366.8359, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12366.806640625
tensor(12366.8359, grad_fn=<NegBackward0>) tensor(12366.8066, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12366.787109375
tensor(12366.8066, grad_fn=<NegBackward0>) tensor(12366.7871, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12366.779296875
tensor(12366.7871, grad_fn=<NegBackward0>) tensor(12366.7793, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12366.765625
tensor(12366.7793, grad_fn=<NegBackward0>) tensor(12366.7656, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12366.763671875
tensor(12366.7656, grad_fn=<NegBackward0>) tensor(12366.7637, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12366.7548828125
tensor(12366.7637, grad_fn=<NegBackward0>) tensor(12366.7549, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12366.7509765625
tensor(12366.7549, grad_fn=<NegBackward0>) tensor(12366.7510, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12366.7470703125
tensor(12366.7510, grad_fn=<NegBackward0>) tensor(12366.7471, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12366.74609375
tensor(12366.7471, grad_fn=<NegBackward0>) tensor(12366.7461, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12366.7431640625
tensor(12366.7461, grad_fn=<NegBackward0>) tensor(12366.7432, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12366.7412109375
tensor(12366.7432, grad_fn=<NegBackward0>) tensor(12366.7412, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12366.740234375
tensor(12366.7412, grad_fn=<NegBackward0>) tensor(12366.7402, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12366.7373046875
tensor(12366.7402, grad_fn=<NegBackward0>) tensor(12366.7373, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12366.7373046875
tensor(12366.7373, grad_fn=<NegBackward0>) tensor(12366.7373, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12366.736328125
tensor(12366.7373, grad_fn=<NegBackward0>) tensor(12366.7363, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12366.7353515625
tensor(12366.7363, grad_fn=<NegBackward0>) tensor(12366.7354, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12366.734375
tensor(12366.7354, grad_fn=<NegBackward0>) tensor(12366.7344, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12366.734375
tensor(12366.7344, grad_fn=<NegBackward0>) tensor(12366.7344, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12366.732421875
tensor(12366.7344, grad_fn=<NegBackward0>) tensor(12366.7324, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12366.732421875
tensor(12366.7324, grad_fn=<NegBackward0>) tensor(12366.7324, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12366.7314453125
tensor(12366.7324, grad_fn=<NegBackward0>) tensor(12366.7314, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12366.7294921875
tensor(12366.7314, grad_fn=<NegBackward0>) tensor(12366.7295, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12366.728515625
tensor(12366.7295, grad_fn=<NegBackward0>) tensor(12366.7285, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12366.7294921875
tensor(12366.7285, grad_fn=<NegBackward0>) tensor(12366.7295, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12366.7275390625
tensor(12366.7285, grad_fn=<NegBackward0>) tensor(12366.7275, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12366.728515625
tensor(12366.7275, grad_fn=<NegBackward0>) tensor(12366.7285, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12366.7265625
tensor(12366.7275, grad_fn=<NegBackward0>) tensor(12366.7266, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12366.7294921875
tensor(12366.7266, grad_fn=<NegBackward0>) tensor(12366.7295, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12366.7255859375
tensor(12366.7266, grad_fn=<NegBackward0>) tensor(12366.7256, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12366.7255859375
tensor(12366.7256, grad_fn=<NegBackward0>) tensor(12366.7256, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12366.7255859375
tensor(12366.7256, grad_fn=<NegBackward0>) tensor(12366.7256, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12366.7236328125
tensor(12366.7256, grad_fn=<NegBackward0>) tensor(12366.7236, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12366.7255859375
tensor(12366.7236, grad_fn=<NegBackward0>) tensor(12366.7256, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12366.7236328125
tensor(12366.7236, grad_fn=<NegBackward0>) tensor(12366.7236, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12366.72265625
tensor(12366.7236, grad_fn=<NegBackward0>) tensor(12366.7227, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12366.72265625
tensor(12366.7227, grad_fn=<NegBackward0>) tensor(12366.7227, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12366.7216796875
tensor(12366.7227, grad_fn=<NegBackward0>) tensor(12366.7217, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12366.7236328125
tensor(12366.7217, grad_fn=<NegBackward0>) tensor(12366.7236, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12366.7255859375
tensor(12366.7217, grad_fn=<NegBackward0>) tensor(12366.7256, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -12366.720703125
tensor(12366.7217, grad_fn=<NegBackward0>) tensor(12366.7207, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12366.720703125
tensor(12366.7207, grad_fn=<NegBackward0>) tensor(12366.7207, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12366.7197265625
tensor(12366.7207, grad_fn=<NegBackward0>) tensor(12366.7197, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12366.7236328125
tensor(12366.7197, grad_fn=<NegBackward0>) tensor(12366.7236, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12366.7177734375
tensor(12366.7197, grad_fn=<NegBackward0>) tensor(12366.7178, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12366.7177734375
tensor(12366.7178, grad_fn=<NegBackward0>) tensor(12366.7178, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12366.71875
tensor(12366.7178, grad_fn=<NegBackward0>) tensor(12366.7188, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12366.720703125
tensor(12366.7178, grad_fn=<NegBackward0>) tensor(12366.7207, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -12366.716796875
tensor(12366.7178, grad_fn=<NegBackward0>) tensor(12366.7168, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12366.716796875
tensor(12366.7168, grad_fn=<NegBackward0>) tensor(12366.7168, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12366.7177734375
tensor(12366.7168, grad_fn=<NegBackward0>) tensor(12366.7178, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12366.7158203125
tensor(12366.7168, grad_fn=<NegBackward0>) tensor(12366.7158, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12366.716796875
tensor(12366.7158, grad_fn=<NegBackward0>) tensor(12366.7168, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12366.716796875
tensor(12366.7158, grad_fn=<NegBackward0>) tensor(12366.7168, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12366.7158203125
tensor(12366.7158, grad_fn=<NegBackward0>) tensor(12366.7158, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12366.716796875
tensor(12366.7158, grad_fn=<NegBackward0>) tensor(12366.7168, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12366.7216796875
tensor(12366.7158, grad_fn=<NegBackward0>) tensor(12366.7217, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12366.71484375
tensor(12366.7158, grad_fn=<NegBackward0>) tensor(12366.7148, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12366.7158203125
tensor(12366.7148, grad_fn=<NegBackward0>) tensor(12366.7158, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12366.7294921875
tensor(12366.7148, grad_fn=<NegBackward0>) tensor(12366.7295, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12366.7158203125
tensor(12366.7148, grad_fn=<NegBackward0>) tensor(12366.7158, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -12366.71484375
tensor(12366.7148, grad_fn=<NegBackward0>) tensor(12366.7148, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12366.7138671875
tensor(12366.7148, grad_fn=<NegBackward0>) tensor(12366.7139, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12366.712890625
tensor(12366.7139, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12366.7138671875
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7139, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12366.7890625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7891, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12366.712890625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12366.7138671875
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7139, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12366.744140625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7441, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -12366.7138671875
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7139, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -12366.712890625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12366.7158203125
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7158, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12366.7138671875
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7139, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12366.712890625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12366.7265625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7266, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12366.712890625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12366.7119140625
tensor(12366.7129, grad_fn=<NegBackward0>) tensor(12366.7119, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12366.7119140625
tensor(12366.7119, grad_fn=<NegBackward0>) tensor(12366.7119, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12366.712890625
tensor(12366.7119, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12366.7119140625
tensor(12366.7119, grad_fn=<NegBackward0>) tensor(12366.7119, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12366.7109375
tensor(12366.7119, grad_fn=<NegBackward0>) tensor(12366.7109, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12366.7119140625
tensor(12366.7109, grad_fn=<NegBackward0>) tensor(12366.7119, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12366.7763671875
tensor(12366.7109, grad_fn=<NegBackward0>) tensor(12366.7764, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12366.712890625
tensor(12366.7109, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -12366.712890625
tensor(12366.7109, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -12366.712890625
tensor(12366.7109, grad_fn=<NegBackward0>) tensor(12366.7129, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.0011, 0.9989],
        [0.0331, 0.9669]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0213, 0.9787], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2337, 0.2385],
         [0.6203, 0.1994]],

        [[0.5012, 0.2657],
         [0.6840, 0.6743]],

        [[0.6164, 0.1879],
         [0.6272, 0.5035]],

        [[0.6118, 0.2286],
         [0.5338, 0.7073]],

        [[0.6379, 0.0925],
         [0.6218, 0.5307]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0003586039060303023
Average Adjusted Rand Index: 0.0001617442499919128
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22389.12890625
inf tensor(22389.1289, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12371.2294921875
tensor(22389.1289, grad_fn=<NegBackward0>) tensor(12371.2295, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12370.8388671875
tensor(12371.2295, grad_fn=<NegBackward0>) tensor(12370.8389, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12370.6962890625
tensor(12370.8389, grad_fn=<NegBackward0>) tensor(12370.6963, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12370.5595703125
tensor(12370.6963, grad_fn=<NegBackward0>) tensor(12370.5596, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12370.419921875
tensor(12370.5596, grad_fn=<NegBackward0>) tensor(12370.4199, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12370.275390625
tensor(12370.4199, grad_fn=<NegBackward0>) tensor(12370.2754, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12370.1064453125
tensor(12370.2754, grad_fn=<NegBackward0>) tensor(12370.1064, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12369.9609375
tensor(12370.1064, grad_fn=<NegBackward0>) tensor(12369.9609, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12369.875
tensor(12369.9609, grad_fn=<NegBackward0>) tensor(12369.8750, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12369.8173828125
tensor(12369.8750, grad_fn=<NegBackward0>) tensor(12369.8174, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12369.7548828125
tensor(12369.8174, grad_fn=<NegBackward0>) tensor(12369.7549, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12369.6826171875
tensor(12369.7549, grad_fn=<NegBackward0>) tensor(12369.6826, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12369.578125
tensor(12369.6826, grad_fn=<NegBackward0>) tensor(12369.5781, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12369.419921875
tensor(12369.5781, grad_fn=<NegBackward0>) tensor(12369.4199, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12369.2265625
tensor(12369.4199, grad_fn=<NegBackward0>) tensor(12369.2266, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12369.05859375
tensor(12369.2266, grad_fn=<NegBackward0>) tensor(12369.0586, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12368.953125
tensor(12369.0586, grad_fn=<NegBackward0>) tensor(12368.9531, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12368.8935546875
tensor(12368.9531, grad_fn=<NegBackward0>) tensor(12368.8936, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12368.8583984375
tensor(12368.8936, grad_fn=<NegBackward0>) tensor(12368.8584, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12368.8271484375
tensor(12368.8584, grad_fn=<NegBackward0>) tensor(12368.8271, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12368.775390625
tensor(12368.8271, grad_fn=<NegBackward0>) tensor(12368.7754, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12368.5634765625
tensor(12368.7754, grad_fn=<NegBackward0>) tensor(12368.5635, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12367.1240234375
tensor(12368.5635, grad_fn=<NegBackward0>) tensor(12367.1240, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12364.716796875
tensor(12367.1240, grad_fn=<NegBackward0>) tensor(12364.7168, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12364.3486328125
tensor(12364.7168, grad_fn=<NegBackward0>) tensor(12364.3486, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12364.0546875
tensor(12364.3486, grad_fn=<NegBackward0>) tensor(12364.0547, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12364.0166015625
tensor(12364.0547, grad_fn=<NegBackward0>) tensor(12364.0166, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12364.0068359375
tensor(12364.0166, grad_fn=<NegBackward0>) tensor(12364.0068, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12364.0
tensor(12364.0068, grad_fn=<NegBackward0>) tensor(12364., grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12363.998046875
tensor(12364., grad_fn=<NegBackward0>) tensor(12363.9980, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12363.9951171875
tensor(12363.9980, grad_fn=<NegBackward0>) tensor(12363.9951, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12363.9931640625
tensor(12363.9951, grad_fn=<NegBackward0>) tensor(12363.9932, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12363.990234375
tensor(12363.9932, grad_fn=<NegBackward0>) tensor(12363.9902, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12363.98828125
tensor(12363.9902, grad_fn=<NegBackward0>) tensor(12363.9883, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12363.98828125
tensor(12363.9883, grad_fn=<NegBackward0>) tensor(12363.9883, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12363.9873046875
tensor(12363.9883, grad_fn=<NegBackward0>) tensor(12363.9873, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12363.986328125
tensor(12363.9873, grad_fn=<NegBackward0>) tensor(12363.9863, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12363.9853515625
tensor(12363.9863, grad_fn=<NegBackward0>) tensor(12363.9854, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12363.986328125
tensor(12363.9854, grad_fn=<NegBackward0>) tensor(12363.9863, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12363.9853515625
tensor(12363.9854, grad_fn=<NegBackward0>) tensor(12363.9854, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12363.984375
tensor(12363.9854, grad_fn=<NegBackward0>) tensor(12363.9844, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12363.986328125
tensor(12363.9844, grad_fn=<NegBackward0>) tensor(12363.9863, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12363.986328125
tensor(12363.9844, grad_fn=<NegBackward0>) tensor(12363.9863, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -12363.9853515625
tensor(12363.9844, grad_fn=<NegBackward0>) tensor(12363.9854, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -12363.9833984375
tensor(12363.9844, grad_fn=<NegBackward0>) tensor(12363.9834, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12363.984375
tensor(12363.9834, grad_fn=<NegBackward0>) tensor(12363.9844, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12363.984375
tensor(12363.9834, grad_fn=<NegBackward0>) tensor(12363.9844, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -12363.9853515625
tensor(12363.9834, grad_fn=<NegBackward0>) tensor(12363.9854, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -12363.984375
tensor(12363.9834, grad_fn=<NegBackward0>) tensor(12363.9844, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -12363.984375
tensor(12363.9834, grad_fn=<NegBackward0>) tensor(12363.9844, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5000 due to no improvement.
pi: tensor([[0.9927, 0.0073],
        [0.1577, 0.8423]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9438, 0.0562], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.2419],
         [0.5944, 0.3399]],

        [[0.5101, 0.2548],
         [0.7025, 0.5653]],

        [[0.5404, 0.1916],
         [0.5413, 0.5910]],

        [[0.6880, 0.1936],
         [0.6098, 0.5185]],

        [[0.6391, 0.0976],
         [0.6349, 0.5983]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015539260913902783
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.00485601903559462
Global Adjusted Rand Index: 0.00038982005796010615
Average Adjusted Rand Index: -0.0006637637902685656
[0.0003586039060303023, 0.00038982005796010615] [0.0001617442499919128, -0.0006637637902685656] [12366.712890625, 12363.984375]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11901.563146933191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22093.62109375
inf tensor(22093.6211, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12141.7666015625
tensor(22093.6211, grad_fn=<NegBackward0>) tensor(12141.7666, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11914.6123046875
tensor(12141.7666, grad_fn=<NegBackward0>) tensor(11914.6123, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11911.23046875
tensor(11914.6123, grad_fn=<NegBackward0>) tensor(11911.2305, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11911.10546875
tensor(11911.2305, grad_fn=<NegBackward0>) tensor(11911.1055, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11911.04296875
tensor(11911.1055, grad_fn=<NegBackward0>) tensor(11911.0430, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11911.0087890625
tensor(11911.0430, grad_fn=<NegBackward0>) tensor(11911.0088, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11910.9833984375
tensor(11911.0088, grad_fn=<NegBackward0>) tensor(11910.9834, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11893.6298828125
tensor(11910.9834, grad_fn=<NegBackward0>) tensor(11893.6299, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11893.607421875
tensor(11893.6299, grad_fn=<NegBackward0>) tensor(11893.6074, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11893.5986328125
tensor(11893.6074, grad_fn=<NegBackward0>) tensor(11893.5986, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11893.58984375
tensor(11893.5986, grad_fn=<NegBackward0>) tensor(11893.5898, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11893.583984375
tensor(11893.5898, grad_fn=<NegBackward0>) tensor(11893.5840, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11893.578125
tensor(11893.5840, grad_fn=<NegBackward0>) tensor(11893.5781, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11893.576171875
tensor(11893.5781, grad_fn=<NegBackward0>) tensor(11893.5762, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11893.572265625
tensor(11893.5762, grad_fn=<NegBackward0>) tensor(11893.5723, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11893.5693359375
tensor(11893.5723, grad_fn=<NegBackward0>) tensor(11893.5693, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11893.5712890625
tensor(11893.5693, grad_fn=<NegBackward0>) tensor(11893.5713, grad_fn=<NegBackward0>)
1
Iteration 1800: Loss = -11893.5654296875
tensor(11893.5693, grad_fn=<NegBackward0>) tensor(11893.5654, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11893.5634765625
tensor(11893.5654, grad_fn=<NegBackward0>) tensor(11893.5635, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11893.5615234375
tensor(11893.5635, grad_fn=<NegBackward0>) tensor(11893.5615, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11893.5615234375
tensor(11893.5615, grad_fn=<NegBackward0>) tensor(11893.5615, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11893.568359375
tensor(11893.5615, grad_fn=<NegBackward0>) tensor(11893.5684, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -11893.55859375
tensor(11893.5615, grad_fn=<NegBackward0>) tensor(11893.5586, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11893.556640625
tensor(11893.5586, grad_fn=<NegBackward0>) tensor(11893.5566, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11893.556640625
tensor(11893.5566, grad_fn=<NegBackward0>) tensor(11893.5566, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11893.5556640625
tensor(11893.5566, grad_fn=<NegBackward0>) tensor(11893.5557, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11893.5556640625
tensor(11893.5557, grad_fn=<NegBackward0>) tensor(11893.5557, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11893.5546875
tensor(11893.5557, grad_fn=<NegBackward0>) tensor(11893.5547, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11893.5546875
tensor(11893.5547, grad_fn=<NegBackward0>) tensor(11893.5547, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11893.5537109375
tensor(11893.5547, grad_fn=<NegBackward0>) tensor(11893.5537, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11893.5537109375
tensor(11893.5537, grad_fn=<NegBackward0>) tensor(11893.5537, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11893.5556640625
tensor(11893.5537, grad_fn=<NegBackward0>) tensor(11893.5557, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11893.5556640625
tensor(11893.5537, grad_fn=<NegBackward0>) tensor(11893.5557, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -11893.552734375
tensor(11893.5537, grad_fn=<NegBackward0>) tensor(11893.5527, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11893.55078125
tensor(11893.5527, grad_fn=<NegBackward0>) tensor(11893.5508, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11893.5517578125
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5518, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11893.552734375
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5527, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11893.5517578125
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5518, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -11893.5517578125
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5518, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -11893.55078125
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5508, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11893.55078125
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5508, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11893.5498046875
tensor(11893.5508, grad_fn=<NegBackward0>) tensor(11893.5498, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11893.5537109375
tensor(11893.5498, grad_fn=<NegBackward0>) tensor(11893.5537, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11893.548828125
tensor(11893.5498, grad_fn=<NegBackward0>) tensor(11893.5488, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11893.55078125
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5508, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11893.548828125
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5488, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11893.5498046875
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5498, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11893.5546875
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5547, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11893.5546875
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5547, grad_fn=<NegBackward0>)
3
Iteration 5000: Loss = -11893.55078125
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5508, grad_fn=<NegBackward0>)
4
Iteration 5100: Loss = -11893.548828125
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5488, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11893.548828125
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5488, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11893.5478515625
tensor(11893.5488, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11893.5478515625
tensor(11893.5479, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11893.5478515625
tensor(11893.5479, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11893.552734375
tensor(11893.5479, grad_fn=<NegBackward0>) tensor(11893.5527, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11893.548828125
tensor(11893.5479, grad_fn=<NegBackward0>) tensor(11893.5488, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11893.5478515625
tensor(11893.5479, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11893.546875
tensor(11893.5479, grad_fn=<NegBackward0>) tensor(11893.5469, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11893.548828125
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5488, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11893.546875
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5469, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -11893.546875
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5469, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11893.546875
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5469, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11893.546875
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5469, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11893.5498046875
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5498, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11893.5498046875
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5498, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11893.5478515625
tensor(11893.5469, grad_fn=<NegBackward0>) tensor(11893.5479, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.2638, 0.7362],
        [0.7470, 0.2530]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4069, 0.5931], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2951, 0.0947],
         [0.5612, 0.2982]],

        [[0.5298, 0.1024],
         [0.7026, 0.6829]],

        [[0.5747, 0.0906],
         [0.6355, 0.5729]],

        [[0.6432, 0.0946],
         [0.5380, 0.5839]],

        [[0.6670, 0.0971],
         [0.6171, 0.5760]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207675179163246
Global Adjusted Rand Index: 0.03498287434827501
Average Adjusted Rand Index: 0.9841535035832649
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20151.521484375
inf tensor(20151.5215, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12418.8037109375
tensor(20151.5215, grad_fn=<NegBackward0>) tensor(12418.8037, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12417.5986328125
tensor(12418.8037, grad_fn=<NegBackward0>) tensor(12417.5986, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12416.23828125
tensor(12417.5986, grad_fn=<NegBackward0>) tensor(12416.2383, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12415.654296875
tensor(12416.2383, grad_fn=<NegBackward0>) tensor(12415.6543, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12414.8935546875
tensor(12415.6543, grad_fn=<NegBackward0>) tensor(12414.8936, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12413.9326171875
tensor(12414.8936, grad_fn=<NegBackward0>) tensor(12413.9326, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12412.9912109375
tensor(12413.9326, grad_fn=<NegBackward0>) tensor(12412.9912, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12343.2822265625
tensor(12412.9912, grad_fn=<NegBackward0>) tensor(12343.2822, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12140.39453125
tensor(12343.2822, grad_fn=<NegBackward0>) tensor(12140.3945, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12103.25
tensor(12140.3945, grad_fn=<NegBackward0>) tensor(12103.2500, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12103.04296875
tensor(12103.2500, grad_fn=<NegBackward0>) tensor(12103.0430, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12102.88671875
tensor(12103.0430, grad_fn=<NegBackward0>) tensor(12102.8867, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12099.0263671875
tensor(12102.8867, grad_fn=<NegBackward0>) tensor(12099.0264, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12098.9912109375
tensor(12099.0264, grad_fn=<NegBackward0>) tensor(12098.9912, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12096.3642578125
tensor(12098.9912, grad_fn=<NegBackward0>) tensor(12096.3643, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12096.34765625
tensor(12096.3643, grad_fn=<NegBackward0>) tensor(12096.3477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12091.89453125
tensor(12096.3477, grad_fn=<NegBackward0>) tensor(12091.8945, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12091.880859375
tensor(12091.8945, grad_fn=<NegBackward0>) tensor(12091.8809, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12091.8525390625
tensor(12091.8809, grad_fn=<NegBackward0>) tensor(12091.8525, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12091.14453125
tensor(12091.8525, grad_fn=<NegBackward0>) tensor(12091.1445, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12091.115234375
tensor(12091.1445, grad_fn=<NegBackward0>) tensor(12091.1152, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12091.0966796875
tensor(12091.1152, grad_fn=<NegBackward0>) tensor(12091.0967, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12091.087890625
tensor(12091.0967, grad_fn=<NegBackward0>) tensor(12091.0879, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12091.076171875
tensor(12091.0879, grad_fn=<NegBackward0>) tensor(12091.0762, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12091.0673828125
tensor(12091.0762, grad_fn=<NegBackward0>) tensor(12091.0674, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12090.9287109375
tensor(12091.0674, grad_fn=<NegBackward0>) tensor(12090.9287, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12070.046875
tensor(12090.9287, grad_fn=<NegBackward0>) tensor(12070.0469, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12049.748046875
tensor(12070.0469, grad_fn=<NegBackward0>) tensor(12049.7480, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12049.3046875
tensor(12049.7480, grad_fn=<NegBackward0>) tensor(12049.3047, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12049.294921875
tensor(12049.3047, grad_fn=<NegBackward0>) tensor(12049.2949, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12039.921875
tensor(12049.2949, grad_fn=<NegBackward0>) tensor(12039.9219, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12028.091796875
tensor(12039.9219, grad_fn=<NegBackward0>) tensor(12028.0918, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12028.0849609375
tensor(12028.0918, grad_fn=<NegBackward0>) tensor(12028.0850, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12028.0654296875
tensor(12028.0850, grad_fn=<NegBackward0>) tensor(12028.0654, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12028.0400390625
tensor(12028.0654, grad_fn=<NegBackward0>) tensor(12028.0400, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12028.0322265625
tensor(12028.0400, grad_fn=<NegBackward0>) tensor(12028.0322, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12022.8310546875
tensor(12028.0322, grad_fn=<NegBackward0>) tensor(12022.8311, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12017.4921875
tensor(12022.8311, grad_fn=<NegBackward0>) tensor(12017.4922, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12000.42578125
tensor(12017.4922, grad_fn=<NegBackward0>) tensor(12000.4258, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12000.3916015625
tensor(12000.4258, grad_fn=<NegBackward0>) tensor(12000.3916, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12000.3896484375
tensor(12000.3916, grad_fn=<NegBackward0>) tensor(12000.3896, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12000.3876953125
tensor(12000.3896, grad_fn=<NegBackward0>) tensor(12000.3877, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11994.314453125
tensor(12000.3877, grad_fn=<NegBackward0>) tensor(11994.3145, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11976.5947265625
tensor(11994.3145, grad_fn=<NegBackward0>) tensor(11976.5947, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11976.5947265625
tensor(11976.5947, grad_fn=<NegBackward0>) tensor(11976.5947, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11976.5927734375
tensor(11976.5947, grad_fn=<NegBackward0>) tensor(11976.5928, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11976.5927734375
tensor(11976.5928, grad_fn=<NegBackward0>) tensor(11976.5928, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11976.611328125
tensor(11976.5928, grad_fn=<NegBackward0>) tensor(11976.6113, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11976.591796875
tensor(11976.5928, grad_fn=<NegBackward0>) tensor(11976.5918, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11976.6162109375
tensor(11976.5918, grad_fn=<NegBackward0>) tensor(11976.6162, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11976.58984375
tensor(11976.5918, grad_fn=<NegBackward0>) tensor(11976.5898, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11976.60546875
tensor(11976.5898, grad_fn=<NegBackward0>) tensor(11976.6055, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11976.5908203125
tensor(11976.5898, grad_fn=<NegBackward0>) tensor(11976.5908, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -11976.58984375
tensor(11976.5898, grad_fn=<NegBackward0>) tensor(11976.5898, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11976.58984375
tensor(11976.5898, grad_fn=<NegBackward0>) tensor(11976.5898, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11976.5888671875
tensor(11976.5898, grad_fn=<NegBackward0>) tensor(11976.5889, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11976.5869140625
tensor(11976.5889, grad_fn=<NegBackward0>) tensor(11976.5869, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11976.5830078125
tensor(11976.5869, grad_fn=<NegBackward0>) tensor(11976.5830, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11976.583984375
tensor(11976.5830, grad_fn=<NegBackward0>) tensor(11976.5840, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11976.5810546875
tensor(11976.5830, grad_fn=<NegBackward0>) tensor(11976.5811, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11976.58203125
tensor(11976.5811, grad_fn=<NegBackward0>) tensor(11976.5820, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11976.58203125
tensor(11976.5811, grad_fn=<NegBackward0>) tensor(11976.5820, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11976.5888671875
tensor(11976.5811, grad_fn=<NegBackward0>) tensor(11976.5889, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11976.58203125
tensor(11976.5811, grad_fn=<NegBackward0>) tensor(11976.5820, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -11976.58203125
tensor(11976.5811, grad_fn=<NegBackward0>) tensor(11976.5820, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.3939, 0.6061],
        [0.4466, 0.5534]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6387, 0.3613], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2848, 0.1053],
         [0.5398, 0.2998]],

        [[0.6144, 0.1027],
         [0.5665, 0.5408]],

        [[0.6279, 0.0906],
         [0.6856, 0.6260]],

        [[0.7240, 0.0948],
         [0.6812, 0.7073]],

        [[0.7035, 0.0964],
         [0.6288, 0.7086]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8077803169987683
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
Global Adjusted Rand Index: 0.04816873941670034
Average Adjusted Rand Index: 0.9535505744817321
[0.03498287434827501, 0.04816873941670034] [0.9841535035832649, 0.9535505744817321] [11893.5478515625, 11976.58203125]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11750.429604156672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22897.884765625
inf tensor(22897.8848, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12300.5576171875
tensor(22897.8848, grad_fn=<NegBackward0>) tensor(12300.5576, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12300.201171875
tensor(12300.5576, grad_fn=<NegBackward0>) tensor(12300.2012, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12300.09375
tensor(12300.2012, grad_fn=<NegBackward0>) tensor(12300.0938, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12300.0263671875
tensor(12300.0938, grad_fn=<NegBackward0>) tensor(12300.0264, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12299.970703125
tensor(12300.0264, grad_fn=<NegBackward0>) tensor(12299.9707, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12299.919921875
tensor(12299.9707, grad_fn=<NegBackward0>) tensor(12299.9199, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12299.87109375
tensor(12299.9199, grad_fn=<NegBackward0>) tensor(12299.8711, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12299.8212890625
tensor(12299.8711, grad_fn=<NegBackward0>) tensor(12299.8213, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12299.7705078125
tensor(12299.8213, grad_fn=<NegBackward0>) tensor(12299.7705, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12299.7099609375
tensor(12299.7705, grad_fn=<NegBackward0>) tensor(12299.7100, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12299.6318359375
tensor(12299.7100, grad_fn=<NegBackward0>) tensor(12299.6318, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12299.501953125
tensor(12299.6318, grad_fn=<NegBackward0>) tensor(12299.5020, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12299.2294921875
tensor(12299.5020, grad_fn=<NegBackward0>) tensor(12299.2295, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12298.638671875
tensor(12299.2295, grad_fn=<NegBackward0>) tensor(12298.6387, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12298.1337890625
tensor(12298.6387, grad_fn=<NegBackward0>) tensor(12298.1338, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12297.8447265625
tensor(12298.1338, grad_fn=<NegBackward0>) tensor(12297.8447, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12297.6455078125
tensor(12297.8447, grad_fn=<NegBackward0>) tensor(12297.6455, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12297.486328125
tensor(12297.6455, grad_fn=<NegBackward0>) tensor(12297.4863, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12297.361328125
tensor(12297.4863, grad_fn=<NegBackward0>) tensor(12297.3613, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12297.2666015625
tensor(12297.3613, grad_fn=<NegBackward0>) tensor(12297.2666, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12297.1728515625
tensor(12297.2666, grad_fn=<NegBackward0>) tensor(12297.1729, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12297.0986328125
tensor(12297.1729, grad_fn=<NegBackward0>) tensor(12297.0986, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12297.0361328125
tensor(12297.0986, grad_fn=<NegBackward0>) tensor(12297.0361, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12296.9755859375
tensor(12297.0361, grad_fn=<NegBackward0>) tensor(12296.9756, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12296.9111328125
tensor(12296.9756, grad_fn=<NegBackward0>) tensor(12296.9111, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12296.849609375
tensor(12296.9111, grad_fn=<NegBackward0>) tensor(12296.8496, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12296.791015625
tensor(12296.8496, grad_fn=<NegBackward0>) tensor(12296.7910, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12296.732421875
tensor(12296.7910, grad_fn=<NegBackward0>) tensor(12296.7324, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12296.6728515625
tensor(12296.7324, grad_fn=<NegBackward0>) tensor(12296.6729, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12296.146484375
tensor(12296.6729, grad_fn=<NegBackward0>) tensor(12296.1465, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12036.4375
tensor(12296.1465, grad_fn=<NegBackward0>) tensor(12036.4375, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11916.3974609375
tensor(12036.4375, grad_fn=<NegBackward0>) tensor(11916.3975, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11841.05078125
tensor(11916.3975, grad_fn=<NegBackward0>) tensor(11841.0508, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11831.2734375
tensor(11841.0508, grad_fn=<NegBackward0>) tensor(11831.2734, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11802.185546875
tensor(11831.2734, grad_fn=<NegBackward0>) tensor(11802.1855, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11802.1376953125
tensor(11802.1855, grad_fn=<NegBackward0>) tensor(11802.1377, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11802.0908203125
tensor(11802.1377, grad_fn=<NegBackward0>) tensor(11802.0908, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11789.4765625
tensor(11802.0908, grad_fn=<NegBackward0>) tensor(11789.4766, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11789.462890625
tensor(11789.4766, grad_fn=<NegBackward0>) tensor(11789.4629, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11789.46875
tensor(11789.4629, grad_fn=<NegBackward0>) tensor(11789.4688, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11789.4462890625
tensor(11789.4629, grad_fn=<NegBackward0>) tensor(11789.4463, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11789.439453125
tensor(11789.4463, grad_fn=<NegBackward0>) tensor(11789.4395, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11789.435546875
tensor(11789.4395, grad_fn=<NegBackward0>) tensor(11789.4355, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11789.435546875
tensor(11789.4355, grad_fn=<NegBackward0>) tensor(11789.4355, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11780.9111328125
tensor(11789.4355, grad_fn=<NegBackward0>) tensor(11780.9111, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11780.904296875
tensor(11780.9111, grad_fn=<NegBackward0>) tensor(11780.9043, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11780.90625
tensor(11780.9043, grad_fn=<NegBackward0>) tensor(11780.9062, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11780.900390625
tensor(11780.9043, grad_fn=<NegBackward0>) tensor(11780.9004, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11780.8984375
tensor(11780.9004, grad_fn=<NegBackward0>) tensor(11780.8984, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11780.8955078125
tensor(11780.8984, grad_fn=<NegBackward0>) tensor(11780.8955, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11780.89453125
tensor(11780.8955, grad_fn=<NegBackward0>) tensor(11780.8945, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11780.8896484375
tensor(11780.8945, grad_fn=<NegBackward0>) tensor(11780.8896, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11780.8818359375
tensor(11780.8896, grad_fn=<NegBackward0>) tensor(11780.8818, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11780.8818359375
tensor(11780.8818, grad_fn=<NegBackward0>) tensor(11780.8818, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11780.880859375
tensor(11780.8818, grad_fn=<NegBackward0>) tensor(11780.8809, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11780.8779296875
tensor(11780.8809, grad_fn=<NegBackward0>) tensor(11780.8779, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11780.8779296875
tensor(11780.8779, grad_fn=<NegBackward0>) tensor(11780.8779, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11780.875
tensor(11780.8779, grad_fn=<NegBackward0>) tensor(11780.8750, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11780.8759765625
tensor(11780.8750, grad_fn=<NegBackward0>) tensor(11780.8760, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11780.875
tensor(11780.8750, grad_fn=<NegBackward0>) tensor(11780.8750, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11780.8740234375
tensor(11780.8750, grad_fn=<NegBackward0>) tensor(11780.8740, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11780.875
tensor(11780.8740, grad_fn=<NegBackward0>) tensor(11780.8750, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11780.869140625
tensor(11780.8740, grad_fn=<NegBackward0>) tensor(11780.8691, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11780.8701171875
tensor(11780.8691, grad_fn=<NegBackward0>) tensor(11780.8701, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11780.87109375
tensor(11780.8691, grad_fn=<NegBackward0>) tensor(11780.8711, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -11780.8681640625
tensor(11780.8691, grad_fn=<NegBackward0>) tensor(11780.8682, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11780.8681640625
tensor(11780.8682, grad_fn=<NegBackward0>) tensor(11780.8682, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11780.8671875
tensor(11780.8682, grad_fn=<NegBackward0>) tensor(11780.8672, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11780.8662109375
tensor(11780.8672, grad_fn=<NegBackward0>) tensor(11780.8662, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11780.8662109375
tensor(11780.8662, grad_fn=<NegBackward0>) tensor(11780.8662, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11780.865234375
tensor(11780.8662, grad_fn=<NegBackward0>) tensor(11780.8652, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11780.865234375
tensor(11780.8652, grad_fn=<NegBackward0>) tensor(11780.8652, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11780.865234375
tensor(11780.8652, grad_fn=<NegBackward0>) tensor(11780.8652, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11780.869140625
tensor(11780.8652, grad_fn=<NegBackward0>) tensor(11780.8691, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11780.86328125
tensor(11780.8652, grad_fn=<NegBackward0>) tensor(11780.8633, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11780.861328125
tensor(11780.8633, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11780.8623046875
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8623, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11780.86328125
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8633, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11780.8603515625
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8604, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11780.8662109375
tensor(11780.8604, grad_fn=<NegBackward0>) tensor(11780.8662, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11780.8642578125
tensor(11780.8604, grad_fn=<NegBackward0>) tensor(11780.8643, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11780.8583984375
tensor(11780.8604, grad_fn=<NegBackward0>) tensor(11780.8584, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11780.8583984375
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8584, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11780.916015625
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.9160, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11780.8603515625
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8604, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11780.861328125
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11780.859375
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -11780.8623046875
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8623, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.3812, 0.6188],
        [0.7167, 0.2833]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5098, 0.4902], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2996, 0.0913],
         [0.6442, 0.2992]],

        [[0.5220, 0.0956],
         [0.5068, 0.5106]],

        [[0.6411, 0.0943],
         [0.5316, 0.5477]],

        [[0.7207, 0.1012],
         [0.6547, 0.5777]],

        [[0.6235, 0.0990],
         [0.6721, 0.7016]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8447690146932073
Global Adjusted Rand Index: 0.031922860130110545
Average Adjusted Rand Index: 0.9689538029386414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22937.33203125
inf tensor(22937.3320, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12300.130859375
tensor(22937.3320, grad_fn=<NegBackward0>) tensor(12300.1309, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12299.2119140625
tensor(12300.1309, grad_fn=<NegBackward0>) tensor(12299.2119, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12298.6611328125
tensor(12299.2119, grad_fn=<NegBackward0>) tensor(12298.6611, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12298.2666015625
tensor(12298.6611, grad_fn=<NegBackward0>) tensor(12298.2666, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12298.037109375
tensor(12298.2666, grad_fn=<NegBackward0>) tensor(12298.0371, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12297.8896484375
tensor(12298.0371, grad_fn=<NegBackward0>) tensor(12297.8896, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12297.7734375
tensor(12297.8896, grad_fn=<NegBackward0>) tensor(12297.7734, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12297.669921875
tensor(12297.7734, grad_fn=<NegBackward0>) tensor(12297.6699, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12297.5693359375
tensor(12297.6699, grad_fn=<NegBackward0>) tensor(12297.5693, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12297.4697265625
tensor(12297.5693, grad_fn=<NegBackward0>) tensor(12297.4697, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12297.37109375
tensor(12297.4697, grad_fn=<NegBackward0>) tensor(12297.3711, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12297.2705078125
tensor(12297.3711, grad_fn=<NegBackward0>) tensor(12297.2705, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12297.158203125
tensor(12297.2705, grad_fn=<NegBackward0>) tensor(12297.1582, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12297.03515625
tensor(12297.1582, grad_fn=<NegBackward0>) tensor(12297.0352, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12296.9111328125
tensor(12297.0352, grad_fn=<NegBackward0>) tensor(12296.9111, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12296.8134765625
tensor(12296.9111, grad_fn=<NegBackward0>) tensor(12296.8135, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12296.7265625
tensor(12296.8135, grad_fn=<NegBackward0>) tensor(12296.7266, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12296.5380859375
tensor(12296.7266, grad_fn=<NegBackward0>) tensor(12296.5381, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11993.46484375
tensor(12296.5381, grad_fn=<NegBackward0>) tensor(11993.4648, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11893.2333984375
tensor(11993.4648, grad_fn=<NegBackward0>) tensor(11893.2334, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11831.3818359375
tensor(11893.2334, grad_fn=<NegBackward0>) tensor(11831.3818, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11814.7060546875
tensor(11831.3818, grad_fn=<NegBackward0>) tensor(11814.7061, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11814.677734375
tensor(11814.7061, grad_fn=<NegBackward0>) tensor(11814.6777, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11814.6533203125
tensor(11814.6777, grad_fn=<NegBackward0>) tensor(11814.6533, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11802.0966796875
tensor(11814.6533, grad_fn=<NegBackward0>) tensor(11802.0967, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11789.4658203125
tensor(11802.0967, grad_fn=<NegBackward0>) tensor(11789.4658, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11789.45703125
tensor(11789.4658, grad_fn=<NegBackward0>) tensor(11789.4570, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11789.4521484375
tensor(11789.4570, grad_fn=<NegBackward0>) tensor(11789.4521, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11789.4482421875
tensor(11789.4521, grad_fn=<NegBackward0>) tensor(11789.4482, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11789.4443359375
tensor(11789.4482, grad_fn=<NegBackward0>) tensor(11789.4443, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11789.44140625
tensor(11789.4443, grad_fn=<NegBackward0>) tensor(11789.4414, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11789.4384765625
tensor(11789.4414, grad_fn=<NegBackward0>) tensor(11789.4385, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11789.43359375
tensor(11789.4385, grad_fn=<NegBackward0>) tensor(11789.4336, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11789.404296875
tensor(11789.4336, grad_fn=<NegBackward0>) tensor(11789.4043, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11789.40234375
tensor(11789.4043, grad_fn=<NegBackward0>) tensor(11789.4023, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11789.40234375
tensor(11789.4023, grad_fn=<NegBackward0>) tensor(11789.4023, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11789.3984375
tensor(11789.4023, grad_fn=<NegBackward0>) tensor(11789.3984, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11789.3984375
tensor(11789.3984, grad_fn=<NegBackward0>) tensor(11789.3984, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11789.3974609375
tensor(11789.3984, grad_fn=<NegBackward0>) tensor(11789.3975, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11789.396484375
tensor(11789.3975, grad_fn=<NegBackward0>) tensor(11789.3965, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11789.38671875
tensor(11789.3965, grad_fn=<NegBackward0>) tensor(11789.3867, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11780.8740234375
tensor(11789.3867, grad_fn=<NegBackward0>) tensor(11780.8740, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11780.873046875
tensor(11780.8740, grad_fn=<NegBackward0>) tensor(11780.8730, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11780.8720703125
tensor(11780.8730, grad_fn=<NegBackward0>) tensor(11780.8721, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11780.8720703125
tensor(11780.8721, grad_fn=<NegBackward0>) tensor(11780.8721, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11780.87109375
tensor(11780.8721, grad_fn=<NegBackward0>) tensor(11780.8711, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11780.8701171875
tensor(11780.8711, grad_fn=<NegBackward0>) tensor(11780.8701, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11780.8857421875
tensor(11780.8701, grad_fn=<NegBackward0>) tensor(11780.8857, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11780.869140625
tensor(11780.8701, grad_fn=<NegBackward0>) tensor(11780.8691, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11780.869140625
tensor(11780.8691, grad_fn=<NegBackward0>) tensor(11780.8691, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11780.87109375
tensor(11780.8691, grad_fn=<NegBackward0>) tensor(11780.8711, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11780.8681640625
tensor(11780.8691, grad_fn=<NegBackward0>) tensor(11780.8682, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11780.8662109375
tensor(11780.8682, grad_fn=<NegBackward0>) tensor(11780.8662, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11780.8662109375
tensor(11780.8662, grad_fn=<NegBackward0>) tensor(11780.8662, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11780.8642578125
tensor(11780.8662, grad_fn=<NegBackward0>) tensor(11780.8643, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11780.8642578125
tensor(11780.8643, grad_fn=<NegBackward0>) tensor(11780.8643, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11780.8642578125
tensor(11780.8643, grad_fn=<NegBackward0>) tensor(11780.8643, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11780.86328125
tensor(11780.8643, grad_fn=<NegBackward0>) tensor(11780.8633, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11780.8720703125
tensor(11780.8633, grad_fn=<NegBackward0>) tensor(11780.8721, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11780.8623046875
tensor(11780.8633, grad_fn=<NegBackward0>) tensor(11780.8623, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11780.86328125
tensor(11780.8623, grad_fn=<NegBackward0>) tensor(11780.8633, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11780.86328125
tensor(11780.8623, grad_fn=<NegBackward0>) tensor(11780.8633, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11780.861328125
tensor(11780.8623, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11780.86328125
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8633, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11780.861328125
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11780.8642578125
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8643, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11780.8603515625
tensor(11780.8613, grad_fn=<NegBackward0>) tensor(11780.8604, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11780.859375
tensor(11780.8604, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11780.859375
tensor(11780.8594, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11780.8583984375
tensor(11780.8594, grad_fn=<NegBackward0>) tensor(11780.8584, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11780.8720703125
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8721, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11780.859375
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11780.8583984375
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8584, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11780.8623046875
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8623, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11780.861328125
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11780.8583984375
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8584, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11780.859375
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11780.857421875
tensor(11780.8584, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11780.8603515625
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8604, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11780.869140625
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8691, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11780.87109375
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8711, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11780.859375
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11780.8583984375
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8584, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11780.8984375
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8984, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11780.861328125
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11780.8603515625
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8604, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11780.857421875
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8574, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11780.861328125
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8613, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11780.859375
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8594, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11781.0830078125
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11781.0830, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11780.8564453125
tensor(11780.8574, grad_fn=<NegBackward0>) tensor(11780.8564, grad_fn=<NegBackward0>)
pi: tensor([[0.3823, 0.6177],
        [0.7170, 0.2830]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5097, 0.4903], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2992, 0.0913],
         [0.6821, 0.2989]],

        [[0.5585, 0.0956],
         [0.6123, 0.5068]],

        [[0.5115, 0.0943],
         [0.7036, 0.7240]],

        [[0.6818, 0.1012],
         [0.7182, 0.7252]],

        [[0.6421, 0.0991],
         [0.6971, 0.5575]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8447690146932073
Global Adjusted Rand Index: 0.031922860130110545
Average Adjusted Rand Index: 0.9689538029386414
[0.031922860130110545, 0.031922860130110545] [0.9689538029386414, 0.9689538029386414] [11780.8623046875, 11780.857421875]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11892.544653817185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20680.798828125
inf tensor(20680.7988, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12423.9912109375
tensor(20680.7988, grad_fn=<NegBackward0>) tensor(12423.9912, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12423.701171875
tensor(12423.9912, grad_fn=<NegBackward0>) tensor(12423.7012, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12423.625
tensor(12423.7012, grad_fn=<NegBackward0>) tensor(12423.6250, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12423.5595703125
tensor(12423.6250, grad_fn=<NegBackward0>) tensor(12423.5596, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12423.494140625
tensor(12423.5596, grad_fn=<NegBackward0>) tensor(12423.4941, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12423.419921875
tensor(12423.4941, grad_fn=<NegBackward0>) tensor(12423.4199, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12423.3369140625
tensor(12423.4199, grad_fn=<NegBackward0>) tensor(12423.3369, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12423.2294921875
tensor(12423.3369, grad_fn=<NegBackward0>) tensor(12423.2295, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12423.08984375
tensor(12423.2295, grad_fn=<NegBackward0>) tensor(12423.0898, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12422.923828125
tensor(12423.0898, grad_fn=<NegBackward0>) tensor(12422.9238, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12422.77734375
tensor(12422.9238, grad_fn=<NegBackward0>) tensor(12422.7773, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12422.6796875
tensor(12422.7773, grad_fn=<NegBackward0>) tensor(12422.6797, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12422.6220703125
tensor(12422.6797, grad_fn=<NegBackward0>) tensor(12422.6221, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12422.58984375
tensor(12422.6221, grad_fn=<NegBackward0>) tensor(12422.5898, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12422.5693359375
tensor(12422.5898, grad_fn=<NegBackward0>) tensor(12422.5693, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12422.5576171875
tensor(12422.5693, grad_fn=<NegBackward0>) tensor(12422.5576, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12422.5517578125
tensor(12422.5576, grad_fn=<NegBackward0>) tensor(12422.5518, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12422.5458984375
tensor(12422.5518, grad_fn=<NegBackward0>) tensor(12422.5459, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12422.5419921875
tensor(12422.5459, grad_fn=<NegBackward0>) tensor(12422.5420, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12422.5380859375
tensor(12422.5420, grad_fn=<NegBackward0>) tensor(12422.5381, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12422.53515625
tensor(12422.5381, grad_fn=<NegBackward0>) tensor(12422.5352, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12422.5322265625
tensor(12422.5352, grad_fn=<NegBackward0>) tensor(12422.5322, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12422.5302734375
tensor(12422.5322, grad_fn=<NegBackward0>) tensor(12422.5303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12422.52734375
tensor(12422.5303, grad_fn=<NegBackward0>) tensor(12422.5273, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12422.5263671875
tensor(12422.5273, grad_fn=<NegBackward0>) tensor(12422.5264, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12422.5263671875
tensor(12422.5264, grad_fn=<NegBackward0>) tensor(12422.5264, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12422.521484375
tensor(12422.5264, grad_fn=<NegBackward0>) tensor(12422.5215, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12422.521484375
tensor(12422.5215, grad_fn=<NegBackward0>) tensor(12422.5215, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12422.5205078125
tensor(12422.5215, grad_fn=<NegBackward0>) tensor(12422.5205, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12422.5185546875
tensor(12422.5205, grad_fn=<NegBackward0>) tensor(12422.5186, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12422.517578125
tensor(12422.5186, grad_fn=<NegBackward0>) tensor(12422.5176, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12422.517578125
tensor(12422.5176, grad_fn=<NegBackward0>) tensor(12422.5176, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12422.515625
tensor(12422.5176, grad_fn=<NegBackward0>) tensor(12422.5156, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12422.515625
tensor(12422.5156, grad_fn=<NegBackward0>) tensor(12422.5156, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12422.5146484375
tensor(12422.5156, grad_fn=<NegBackward0>) tensor(12422.5146, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12422.515625
tensor(12422.5146, grad_fn=<NegBackward0>) tensor(12422.5156, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12422.5146484375
tensor(12422.5146, grad_fn=<NegBackward0>) tensor(12422.5146, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12422.513671875
tensor(12422.5146, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12422.5126953125
tensor(12422.5137, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12422.5146484375
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5146, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12422.513671875
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12422.5126953125
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12422.51171875
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12422.5126953125
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12422.51171875
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12422.51171875
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12422.51171875
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12422.5107421875
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12422.51171875
tensor(12422.5107, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12422.5107421875
tensor(12422.5107, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12422.509765625
tensor(12422.5107, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12422.51171875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -12422.509765625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12422.51171875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12422.509765625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12422.509765625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12422.509765625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12422.5087890625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5088, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12422.5107421875
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12422.5400390625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5400, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -12422.5087890625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5088, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12422.5322265625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5322, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12422.5087890625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5088, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12422.5107421875
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12422.5205078125
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5205, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -12422.5107421875
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[7.4751e-04, 9.9925e-01],
        [2.6075e-02, 9.7392e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0124, 0.9876], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2087, 0.1016],
         [0.6059, 0.2008]],

        [[0.5926, 0.2085],
         [0.5854, 0.5168]],

        [[0.6505, 0.2835],
         [0.7089, 0.6390]],

        [[0.6792, 0.1524],
         [0.6341, 0.6769]],

        [[0.6363, 0.2302],
         [0.5177, 0.7045]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.2092736862769894e-05
Average Adjusted Rand Index: 0.0005810929960403342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21941.75
inf tensor(21941.7500, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12424.640625
tensor(21941.7500, grad_fn=<NegBackward0>) tensor(12424.6406, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12424.048828125
tensor(12424.6406, grad_fn=<NegBackward0>) tensor(12424.0488, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12423.8935546875
tensor(12424.0488, grad_fn=<NegBackward0>) tensor(12423.8936, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12423.791015625
tensor(12423.8936, grad_fn=<NegBackward0>) tensor(12423.7910, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12423.7109375
tensor(12423.7910, grad_fn=<NegBackward0>) tensor(12423.7109, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12423.6416015625
tensor(12423.7109, grad_fn=<NegBackward0>) tensor(12423.6416, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12423.5654296875
tensor(12423.6416, grad_fn=<NegBackward0>) tensor(12423.5654, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12423.4658203125
tensor(12423.5654, grad_fn=<NegBackward0>) tensor(12423.4658, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12423.3408203125
tensor(12423.4658, grad_fn=<NegBackward0>) tensor(12423.3408, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12423.236328125
tensor(12423.3408, grad_fn=<NegBackward0>) tensor(12423.2363, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12423.1650390625
tensor(12423.2363, grad_fn=<NegBackward0>) tensor(12423.1650, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12423.1142578125
tensor(12423.1650, grad_fn=<NegBackward0>) tensor(12423.1143, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12423.0751953125
tensor(12423.1143, grad_fn=<NegBackward0>) tensor(12423.0752, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12423.0439453125
tensor(12423.0752, grad_fn=<NegBackward0>) tensor(12423.0439, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12423.0185546875
tensor(12423.0439, grad_fn=<NegBackward0>) tensor(12423.0186, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12423.0009765625
tensor(12423.0186, grad_fn=<NegBackward0>) tensor(12423.0010, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12422.990234375
tensor(12423.0010, grad_fn=<NegBackward0>) tensor(12422.9902, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12422.978515625
tensor(12422.9902, grad_fn=<NegBackward0>) tensor(12422.9785, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12422.9716796875
tensor(12422.9785, grad_fn=<NegBackward0>) tensor(12422.9717, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12422.962890625
tensor(12422.9717, grad_fn=<NegBackward0>) tensor(12422.9629, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12422.94921875
tensor(12422.9629, grad_fn=<NegBackward0>) tensor(12422.9492, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12422.927734375
tensor(12422.9492, grad_fn=<NegBackward0>) tensor(12422.9277, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12422.888671875
tensor(12422.9277, grad_fn=<NegBackward0>) tensor(12422.8887, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12422.830078125
tensor(12422.8887, grad_fn=<NegBackward0>) tensor(12422.8301, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12422.78515625
tensor(12422.8301, grad_fn=<NegBackward0>) tensor(12422.7852, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12422.744140625
tensor(12422.7852, grad_fn=<NegBackward0>) tensor(12422.7441, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12422.7060546875
tensor(12422.7441, grad_fn=<NegBackward0>) tensor(12422.7061, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12422.671875
tensor(12422.7061, grad_fn=<NegBackward0>) tensor(12422.6719, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12422.6455078125
tensor(12422.6719, grad_fn=<NegBackward0>) tensor(12422.6455, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12422.623046875
tensor(12422.6455, grad_fn=<NegBackward0>) tensor(12422.6230, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12422.6044921875
tensor(12422.6230, grad_fn=<NegBackward0>) tensor(12422.6045, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12422.5908203125
tensor(12422.6045, grad_fn=<NegBackward0>) tensor(12422.5908, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12422.578125
tensor(12422.5908, grad_fn=<NegBackward0>) tensor(12422.5781, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12422.5693359375
tensor(12422.5781, grad_fn=<NegBackward0>) tensor(12422.5693, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12422.560546875
tensor(12422.5693, grad_fn=<NegBackward0>) tensor(12422.5605, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12422.5546875
tensor(12422.5605, grad_fn=<NegBackward0>) tensor(12422.5547, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12422.546875
tensor(12422.5547, grad_fn=<NegBackward0>) tensor(12422.5469, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12422.5419921875
tensor(12422.5469, grad_fn=<NegBackward0>) tensor(12422.5420, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12422.537109375
tensor(12422.5420, grad_fn=<NegBackward0>) tensor(12422.5371, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12422.53515625
tensor(12422.5371, grad_fn=<NegBackward0>) tensor(12422.5352, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12422.5322265625
tensor(12422.5352, grad_fn=<NegBackward0>) tensor(12422.5322, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12422.529296875
tensor(12422.5322, grad_fn=<NegBackward0>) tensor(12422.5293, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12422.52734375
tensor(12422.5293, grad_fn=<NegBackward0>) tensor(12422.5273, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12422.5244140625
tensor(12422.5273, grad_fn=<NegBackward0>) tensor(12422.5244, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12422.5244140625
tensor(12422.5244, grad_fn=<NegBackward0>) tensor(12422.5244, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12422.5224609375
tensor(12422.5244, grad_fn=<NegBackward0>) tensor(12422.5225, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12422.5205078125
tensor(12422.5225, grad_fn=<NegBackward0>) tensor(12422.5205, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12422.5205078125
tensor(12422.5205, grad_fn=<NegBackward0>) tensor(12422.5205, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12422.5185546875
tensor(12422.5205, grad_fn=<NegBackward0>) tensor(12422.5186, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12422.51953125
tensor(12422.5186, grad_fn=<NegBackward0>) tensor(12422.5195, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12422.517578125
tensor(12422.5186, grad_fn=<NegBackward0>) tensor(12422.5176, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12422.517578125
tensor(12422.5176, grad_fn=<NegBackward0>) tensor(12422.5176, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12422.515625
tensor(12422.5176, grad_fn=<NegBackward0>) tensor(12422.5156, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12422.515625
tensor(12422.5156, grad_fn=<NegBackward0>) tensor(12422.5156, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12422.5166015625
tensor(12422.5156, grad_fn=<NegBackward0>) tensor(12422.5166, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12422.513671875
tensor(12422.5156, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12422.5146484375
tensor(12422.5137, grad_fn=<NegBackward0>) tensor(12422.5146, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12422.5146484375
tensor(12422.5137, grad_fn=<NegBackward0>) tensor(12422.5146, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12422.513671875
tensor(12422.5137, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12422.5126953125
tensor(12422.5137, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12422.513671875
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12422.5126953125
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12422.513671875
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12422.513671875
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5137, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12422.51171875
tensor(12422.5127, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12422.5126953125
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12422.509765625
tensor(12422.5117, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12422.51171875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12422.5126953125
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5127, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12422.5107421875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -12422.5205078125
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5205, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -12422.509765625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12422.51171875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12422.51171875
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5117, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12422.5087890625
tensor(12422.5098, grad_fn=<NegBackward0>) tensor(12422.5088, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12422.5146484375
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5146, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12422.509765625
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -12422.5078125
tensor(12422.5088, grad_fn=<NegBackward0>) tensor(12422.5078, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12422.509765625
tensor(12422.5078, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12422.525390625
tensor(12422.5078, grad_fn=<NegBackward0>) tensor(12422.5254, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -12422.509765625
tensor(12422.5078, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -12422.5107421875
tensor(12422.5078, grad_fn=<NegBackward0>) tensor(12422.5107, grad_fn=<NegBackward0>)
4
Iteration 8400: Loss = -12422.509765625
tensor(12422.5078, grad_fn=<NegBackward0>) tensor(12422.5098, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[0.9740, 0.0260],
        [0.9987, 0.0013]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9876, 0.0124], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2009, 0.1016],
         [0.6063, 0.2088]],

        [[0.5644, 0.2085],
         [0.6196, 0.5819]],

        [[0.5120, 0.2835],
         [0.6111, 0.7153]],

        [[0.6208, 0.1525],
         [0.6022, 0.6487]],

        [[0.6078, 0.2302],
         [0.6949, 0.7288]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 4.2092736862769894e-05
Average Adjusted Rand Index: 0.0005810929960403342
[4.2092736862769894e-05, 4.2092736862769894e-05] [0.0005810929960403342, 0.0005810929960403342] [12422.5107421875, 12422.509765625]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11788.456927382842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21824.3671875
inf tensor(21824.3672, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12380.3330078125
tensor(21824.3672, grad_fn=<NegBackward0>) tensor(12380.3330, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12379.7958984375
tensor(12380.3330, grad_fn=<NegBackward0>) tensor(12379.7959, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12379.6396484375
tensor(12379.7959, grad_fn=<NegBackward0>) tensor(12379.6396, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12379.546875
tensor(12379.6396, grad_fn=<NegBackward0>) tensor(12379.5469, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12379.4873046875
tensor(12379.5469, grad_fn=<NegBackward0>) tensor(12379.4873, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12379.4462890625
tensor(12379.4873, grad_fn=<NegBackward0>) tensor(12379.4463, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12379.41796875
tensor(12379.4463, grad_fn=<NegBackward0>) tensor(12379.4180, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12379.3994140625
tensor(12379.4180, grad_fn=<NegBackward0>) tensor(12379.3994, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12379.3837890625
tensor(12379.3994, grad_fn=<NegBackward0>) tensor(12379.3838, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12379.369140625
tensor(12379.3838, grad_fn=<NegBackward0>) tensor(12379.3691, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12379.361328125
tensor(12379.3691, grad_fn=<NegBackward0>) tensor(12379.3613, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12379.3525390625
tensor(12379.3613, grad_fn=<NegBackward0>) tensor(12379.3525, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12379.3427734375
tensor(12379.3525, grad_fn=<NegBackward0>) tensor(12379.3428, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12379.3359375
tensor(12379.3428, grad_fn=<NegBackward0>) tensor(12379.3359, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12379.3291015625
tensor(12379.3359, grad_fn=<NegBackward0>) tensor(12379.3291, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12379.322265625
tensor(12379.3291, grad_fn=<NegBackward0>) tensor(12379.3223, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12379.3154296875
tensor(12379.3223, grad_fn=<NegBackward0>) tensor(12379.3154, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12379.3076171875
tensor(12379.3154, grad_fn=<NegBackward0>) tensor(12379.3076, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12379.2978515625
tensor(12379.3076, grad_fn=<NegBackward0>) tensor(12379.2979, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12379.291015625
tensor(12379.2979, grad_fn=<NegBackward0>) tensor(12379.2910, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12379.2802734375
tensor(12379.2910, grad_fn=<NegBackward0>) tensor(12379.2803, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12379.267578125
tensor(12379.2803, grad_fn=<NegBackward0>) tensor(12379.2676, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12379.2529296875
tensor(12379.2676, grad_fn=<NegBackward0>) tensor(12379.2529, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12379.2353515625
tensor(12379.2529, grad_fn=<NegBackward0>) tensor(12379.2354, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12379.21484375
tensor(12379.2354, grad_fn=<NegBackward0>) tensor(12379.2148, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12379.1904296875
tensor(12379.2148, grad_fn=<NegBackward0>) tensor(12379.1904, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12379.166015625
tensor(12379.1904, grad_fn=<NegBackward0>) tensor(12379.1660, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12379.1318359375
tensor(12379.1660, grad_fn=<NegBackward0>) tensor(12379.1318, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12379.091796875
tensor(12379.1318, grad_fn=<NegBackward0>) tensor(12379.0918, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12379.0537109375
tensor(12379.0918, grad_fn=<NegBackward0>) tensor(12379.0537, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12379.0185546875
tensor(12379.0537, grad_fn=<NegBackward0>) tensor(12379.0186, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12378.984375
tensor(12379.0186, grad_fn=<NegBackward0>) tensor(12378.9844, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12378.955078125
tensor(12378.9844, grad_fn=<NegBackward0>) tensor(12378.9551, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12378.919921875
tensor(12378.9551, grad_fn=<NegBackward0>) tensor(12378.9199, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12378.8701171875
tensor(12378.9199, grad_fn=<NegBackward0>) tensor(12378.8701, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12378.814453125
tensor(12378.8701, grad_fn=<NegBackward0>) tensor(12378.8145, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12378.7412109375
tensor(12378.8145, grad_fn=<NegBackward0>) tensor(12378.7412, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12378.6572265625
tensor(12378.7412, grad_fn=<NegBackward0>) tensor(12378.6572, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12378.6044921875
tensor(12378.6572, grad_fn=<NegBackward0>) tensor(12378.6045, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12378.568359375
tensor(12378.6045, grad_fn=<NegBackward0>) tensor(12378.5684, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12378.53515625
tensor(12378.5684, grad_fn=<NegBackward0>) tensor(12378.5352, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12378.5029296875
tensor(12378.5352, grad_fn=<NegBackward0>) tensor(12378.5029, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12378.47265625
tensor(12378.5029, grad_fn=<NegBackward0>) tensor(12378.4727, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12378.42578125
tensor(12378.4727, grad_fn=<NegBackward0>) tensor(12378.4258, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11985.734375
tensor(12378.4258, grad_fn=<NegBackward0>) tensor(11985.7344, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11941.939453125
tensor(11985.7344, grad_fn=<NegBackward0>) tensor(11941.9395, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11941.8193359375
tensor(11941.9395, grad_fn=<NegBackward0>) tensor(11941.8193, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11941.7294921875
tensor(11941.8193, grad_fn=<NegBackward0>) tensor(11941.7295, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11935.8671875
tensor(11941.7295, grad_fn=<NegBackward0>) tensor(11935.8672, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11932.0966796875
tensor(11935.8672, grad_fn=<NegBackward0>) tensor(11932.0967, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11932.0908203125
tensor(11932.0967, grad_fn=<NegBackward0>) tensor(11932.0908, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11932.0869140625
tensor(11932.0908, grad_fn=<NegBackward0>) tensor(11932.0869, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11932.08203125
tensor(11932.0869, grad_fn=<NegBackward0>) tensor(11932.0820, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11932.0810546875
tensor(11932.0820, grad_fn=<NegBackward0>) tensor(11932.0811, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11932.0791015625
tensor(11932.0811, grad_fn=<NegBackward0>) tensor(11932.0791, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11932.078125
tensor(11932.0791, grad_fn=<NegBackward0>) tensor(11932.0781, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11932.0849609375
tensor(11932.0781, grad_fn=<NegBackward0>) tensor(11932.0850, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11932.076171875
tensor(11932.0781, grad_fn=<NegBackward0>) tensor(11932.0762, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11932.07421875
tensor(11932.0762, grad_fn=<NegBackward0>) tensor(11932.0742, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11932.07421875
tensor(11932.0742, grad_fn=<NegBackward0>) tensor(11932.0742, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11932.07421875
tensor(11932.0742, grad_fn=<NegBackward0>) tensor(11932.0742, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11932.0732421875
tensor(11932.0742, grad_fn=<NegBackward0>) tensor(11932.0732, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11932.0732421875
tensor(11932.0732, grad_fn=<NegBackward0>) tensor(11932.0732, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11932.072265625
tensor(11932.0732, grad_fn=<NegBackward0>) tensor(11932.0723, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11932.072265625
tensor(11932.0723, grad_fn=<NegBackward0>) tensor(11932.0723, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11932.0712890625
tensor(11932.0723, grad_fn=<NegBackward0>) tensor(11932.0713, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11932.0712890625
tensor(11932.0713, grad_fn=<NegBackward0>) tensor(11932.0713, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11932.0703125
tensor(11932.0713, grad_fn=<NegBackward0>) tensor(11932.0703, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11932.0693359375
tensor(11932.0703, grad_fn=<NegBackward0>) tensor(11932.0693, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11932.068359375
tensor(11932.0693, grad_fn=<NegBackward0>) tensor(11932.0684, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11932.0126953125
tensor(11932.0684, grad_fn=<NegBackward0>) tensor(11932.0127, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11932.0107421875
tensor(11932.0127, grad_fn=<NegBackward0>) tensor(11932.0107, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11932.01171875
tensor(11932.0107, grad_fn=<NegBackward0>) tensor(11932.0117, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11932.0107421875
tensor(11932.0107, grad_fn=<NegBackward0>) tensor(11932.0107, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11932.08203125
tensor(11932.0107, grad_fn=<NegBackward0>) tensor(11932.0820, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11932.009765625
tensor(11932.0107, grad_fn=<NegBackward0>) tensor(11932.0098, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11932.009765625
tensor(11932.0098, grad_fn=<NegBackward0>) tensor(11932.0098, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11932.0087890625
tensor(11932.0098, grad_fn=<NegBackward0>) tensor(11932.0088, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11932.0126953125
tensor(11932.0088, grad_fn=<NegBackward0>) tensor(11932.0127, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11932.0078125
tensor(11932.0088, grad_fn=<NegBackward0>) tensor(11932.0078, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11926.9755859375
tensor(11932.0078, grad_fn=<NegBackward0>) tensor(11926.9756, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11926.9658203125
tensor(11926.9756, grad_fn=<NegBackward0>) tensor(11926.9658, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11924.251953125
tensor(11926.9658, grad_fn=<NegBackward0>) tensor(11924.2520, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11924.2529296875
tensor(11924.2520, grad_fn=<NegBackward0>) tensor(11924.2529, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11924.248046875
tensor(11924.2520, grad_fn=<NegBackward0>) tensor(11924.2480, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11924.2490234375
tensor(11924.2480, grad_fn=<NegBackward0>) tensor(11924.2490, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -11924.25
tensor(11924.2480, grad_fn=<NegBackward0>) tensor(11924.2500, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -11924.2470703125
tensor(11924.2480, grad_fn=<NegBackward0>) tensor(11924.2471, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11924.248046875
tensor(11924.2471, grad_fn=<NegBackward0>) tensor(11924.2480, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11924.2734375
tensor(11924.2471, grad_fn=<NegBackward0>) tensor(11924.2734, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11924.26171875
tensor(11924.2471, grad_fn=<NegBackward0>) tensor(11924.2617, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11924.2451171875
tensor(11924.2471, grad_fn=<NegBackward0>) tensor(11924.2451, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11924.2275390625
tensor(11924.2451, grad_fn=<NegBackward0>) tensor(11924.2275, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11924.2275390625
tensor(11924.2275, grad_fn=<NegBackward0>) tensor(11924.2275, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11924.2275390625
tensor(11924.2275, grad_fn=<NegBackward0>) tensor(11924.2275, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11924.228515625
tensor(11924.2275, grad_fn=<NegBackward0>) tensor(11924.2285, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11924.232421875
tensor(11924.2275, grad_fn=<NegBackward0>) tensor(11924.2324, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -11924.234375
tensor(11924.2275, grad_fn=<NegBackward0>) tensor(11924.2344, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -11919.6748046875
tensor(11924.2275, grad_fn=<NegBackward0>) tensor(11919.6748, grad_fn=<NegBackward0>)
pi: tensor([[0.7945, 0.2055],
        [0.1967, 0.8033]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3599, 0.6401], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3066, 0.1309],
         [0.6536, 0.2822]],

        [[0.5839, 0.1138],
         [0.6826, 0.7217]],

        [[0.5074, 0.0914],
         [0.5637, 0.6111]],

        [[0.5340, 0.1023],
         [0.6550, 0.5307]],

        [[0.6448, 0.0877],
         [0.6028, 0.5959]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 90
Adjusted Rand Index: 0.6364549732959653
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8079912862954653
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8833623134766012
Average Adjusted Rand Index: 0.888889251918286
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20320.068359375
inf tensor(20320.0684, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12379.83984375
tensor(20320.0684, grad_fn=<NegBackward0>) tensor(12379.8398, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12379.5634765625
tensor(12379.8398, grad_fn=<NegBackward0>) tensor(12379.5635, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12379.484375
tensor(12379.5635, grad_fn=<NegBackward0>) tensor(12379.4844, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12379.43359375
tensor(12379.4844, grad_fn=<NegBackward0>) tensor(12379.4336, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12379.392578125
tensor(12379.4336, grad_fn=<NegBackward0>) tensor(12379.3926, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12379.359375
tensor(12379.3926, grad_fn=<NegBackward0>) tensor(12379.3594, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12379.3310546875
tensor(12379.3594, grad_fn=<NegBackward0>) tensor(12379.3311, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12379.3095703125
tensor(12379.3311, grad_fn=<NegBackward0>) tensor(12379.3096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12379.291015625
tensor(12379.3096, grad_fn=<NegBackward0>) tensor(12379.2910, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12379.2744140625
tensor(12379.2910, grad_fn=<NegBackward0>) tensor(12379.2744, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12379.2578125
tensor(12379.2744, grad_fn=<NegBackward0>) tensor(12379.2578, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12379.2392578125
tensor(12379.2578, grad_fn=<NegBackward0>) tensor(12379.2393, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12379.21484375
tensor(12379.2393, grad_fn=<NegBackward0>) tensor(12379.2148, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12379.1806640625
tensor(12379.2148, grad_fn=<NegBackward0>) tensor(12379.1807, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12379.142578125
tensor(12379.1807, grad_fn=<NegBackward0>) tensor(12379.1426, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12379.09375
tensor(12379.1426, grad_fn=<NegBackward0>) tensor(12379.0938, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12379.04296875
tensor(12379.0938, grad_fn=<NegBackward0>) tensor(12379.0430, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12378.98828125
tensor(12379.0430, grad_fn=<NegBackward0>) tensor(12378.9883, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12378.9384765625
tensor(12378.9883, grad_fn=<NegBackward0>) tensor(12378.9385, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12378.8759765625
tensor(12378.9385, grad_fn=<NegBackward0>) tensor(12378.8760, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12378.796875
tensor(12378.8760, grad_fn=<NegBackward0>) tensor(12378.7969, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12378.681640625
tensor(12378.7969, grad_fn=<NegBackward0>) tensor(12378.6816, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12378.6064453125
tensor(12378.6816, grad_fn=<NegBackward0>) tensor(12378.6064, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12378.5595703125
tensor(12378.6064, grad_fn=<NegBackward0>) tensor(12378.5596, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12378.5166015625
tensor(12378.5596, grad_fn=<NegBackward0>) tensor(12378.5166, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12378.4736328125
tensor(12378.5166, grad_fn=<NegBackward0>) tensor(12378.4736, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12378.40234375
tensor(12378.4736, grad_fn=<NegBackward0>) tensor(12378.4023, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11948.109375
tensor(12378.4023, grad_fn=<NegBackward0>) tensor(11948.1094, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11922.7529296875
tensor(11948.1094, grad_fn=<NegBackward0>) tensor(11922.7529, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11922.572265625
tensor(11922.7529, grad_fn=<NegBackward0>) tensor(11922.5723, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11922.5546875
tensor(11922.5723, grad_fn=<NegBackward0>) tensor(11922.5547, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11922.5458984375
tensor(11922.5547, grad_fn=<NegBackward0>) tensor(11922.5459, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11922.541015625
tensor(11922.5459, grad_fn=<NegBackward0>) tensor(11922.5410, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11922.5390625
tensor(11922.5410, grad_fn=<NegBackward0>) tensor(11922.5391, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11922.5361328125
tensor(11922.5391, grad_fn=<NegBackward0>) tensor(11922.5361, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11922.53515625
tensor(11922.5361, grad_fn=<NegBackward0>) tensor(11922.5352, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11922.5322265625
tensor(11922.5352, grad_fn=<NegBackward0>) tensor(11922.5322, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11922.525390625
tensor(11922.5322, grad_fn=<NegBackward0>) tensor(11922.5254, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11922.5234375
tensor(11922.5254, grad_fn=<NegBackward0>) tensor(11922.5234, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11922.5234375
tensor(11922.5234, grad_fn=<NegBackward0>) tensor(11922.5234, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11922.5224609375
tensor(11922.5234, grad_fn=<NegBackward0>) tensor(11922.5225, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11922.521484375
tensor(11922.5225, grad_fn=<NegBackward0>) tensor(11922.5215, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11922.521484375
tensor(11922.5215, grad_fn=<NegBackward0>) tensor(11922.5215, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11922.521484375
tensor(11922.5215, grad_fn=<NegBackward0>) tensor(11922.5215, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11922.5205078125
tensor(11922.5215, grad_fn=<NegBackward0>) tensor(11922.5205, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11922.515625
tensor(11922.5205, grad_fn=<NegBackward0>) tensor(11922.5156, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11922.44140625
tensor(11922.5156, grad_fn=<NegBackward0>) tensor(11922.4414, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11922.4375
tensor(11922.4414, grad_fn=<NegBackward0>) tensor(11922.4375, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11922.4345703125
tensor(11922.4375, grad_fn=<NegBackward0>) tensor(11922.4346, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11922.390625
tensor(11922.4346, grad_fn=<NegBackward0>) tensor(11922.3906, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11912.9775390625
tensor(11922.3906, grad_fn=<NegBackward0>) tensor(11912.9775, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11912.9755859375
tensor(11912.9775, grad_fn=<NegBackward0>) tensor(11912.9756, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11912.9775390625
tensor(11912.9756, grad_fn=<NegBackward0>) tensor(11912.9775, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11912.9755859375
tensor(11912.9756, grad_fn=<NegBackward0>) tensor(11912.9756, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11912.984375
tensor(11912.9756, grad_fn=<NegBackward0>) tensor(11912.9844, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11912.9736328125
tensor(11912.9756, grad_fn=<NegBackward0>) tensor(11912.9736, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11912.9619140625
tensor(11912.9736, grad_fn=<NegBackward0>) tensor(11912.9619, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11912.9658203125
tensor(11912.9619, grad_fn=<NegBackward0>) tensor(11912.9658, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11912.9658203125
tensor(11912.9619, grad_fn=<NegBackward0>) tensor(11912.9658, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11912.9609375
tensor(11912.9619, grad_fn=<NegBackward0>) tensor(11912.9609, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11912.9599609375
tensor(11912.9609, grad_fn=<NegBackward0>) tensor(11912.9600, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11912.9599609375
tensor(11912.9600, grad_fn=<NegBackward0>) tensor(11912.9600, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11912.95703125
tensor(11912.9600, grad_fn=<NegBackward0>) tensor(11912.9570, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11912.95703125
tensor(11912.9570, grad_fn=<NegBackward0>) tensor(11912.9570, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11912.9580078125
tensor(11912.9570, grad_fn=<NegBackward0>) tensor(11912.9580, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11912.95703125
tensor(11912.9570, grad_fn=<NegBackward0>) tensor(11912.9570, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11912.955078125
tensor(11912.9570, grad_fn=<NegBackward0>) tensor(11912.9551, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11901.1396484375
tensor(11912.9551, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11901.1396484375
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11901.16015625
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1602, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11901.1396484375
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11901.173828125
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1738, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11901.1396484375
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11901.298828125
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.2988, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11901.1396484375
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11901.16015625
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1602, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11901.138671875
tensor(11901.1396, grad_fn=<NegBackward0>) tensor(11901.1387, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11901.138671875
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1387, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11901.1396484375
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11901.1396484375
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1396, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11901.1494140625
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1494, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -11901.1552734375
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1553, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -11901.138671875
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1387, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11901.1376953125
tensor(11901.1387, grad_fn=<NegBackward0>) tensor(11901.1377, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11901.138671875
tensor(11901.1377, grad_fn=<NegBackward0>) tensor(11901.1387, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11901.1376953125
tensor(11901.1377, grad_fn=<NegBackward0>) tensor(11901.1377, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11901.13671875
tensor(11901.1377, grad_fn=<NegBackward0>) tensor(11901.1367, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11881.724609375
tensor(11901.1367, grad_fn=<NegBackward0>) tensor(11881.7246, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11881.7001953125
tensor(11881.7246, grad_fn=<NegBackward0>) tensor(11881.7002, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11881.708984375
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.7090, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11881.7001953125
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.7002, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11881.70703125
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.7070, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11881.775390625
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.7754, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11881.7001953125
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.7002, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11881.7001953125
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.7002, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11881.69921875
tensor(11881.7002, grad_fn=<NegBackward0>) tensor(11881.6992, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11863.2626953125
tensor(11881.6992, grad_fn=<NegBackward0>) tensor(11863.2627, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11863.2138671875
tensor(11863.2627, grad_fn=<NegBackward0>) tensor(11863.2139, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11848.4130859375
tensor(11863.2139, grad_fn=<NegBackward0>) tensor(11848.4131, grad_fn=<NegBackward0>)
pi: tensor([[0.8000, 0.2000],
        [0.2185, 0.7815]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5897, 0.4103], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2921, 0.1108],
         [0.6014, 0.3079]],

        [[0.7113, 0.1080],
         [0.7182, 0.6137]],

        [[0.6392, 0.0914],
         [0.6040, 0.5863]],

        [[0.6608, 0.1016],
         [0.5407, 0.6421]],

        [[0.6818, 0.0879],
         [0.6703, 0.6464]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080965973782139
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.882389682918764
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291529617618343
Average Adjusted Rand Index: 0.9300965977866855
[0.8833623134766012, 0.9291529617618343] [0.888889251918286, 0.9300965977866855] [11919.5087890625, 11848.42578125]
-------------------------------------
This iteration is 46
True Objective function: Loss = -11855.090983255155
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22046.810546875
inf tensor(22046.8105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12390.9189453125
tensor(22046.8105, grad_fn=<NegBackward0>) tensor(12390.9189, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12390.2724609375
tensor(12390.9189, grad_fn=<NegBackward0>) tensor(12390.2725, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12390.1962890625
tensor(12390.2725, grad_fn=<NegBackward0>) tensor(12390.1963, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12390.169921875
tensor(12390.1963, grad_fn=<NegBackward0>) tensor(12390.1699, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12390.1533203125
tensor(12390.1699, grad_fn=<NegBackward0>) tensor(12390.1533, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12390.13671875
tensor(12390.1533, grad_fn=<NegBackward0>) tensor(12390.1367, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12390.12109375
tensor(12390.1367, grad_fn=<NegBackward0>) tensor(12390.1211, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12390.10546875
tensor(12390.1211, grad_fn=<NegBackward0>) tensor(12390.1055, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12390.0888671875
tensor(12390.1055, grad_fn=<NegBackward0>) tensor(12390.0889, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12390.076171875
tensor(12390.0889, grad_fn=<NegBackward0>) tensor(12390.0762, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12390.0625
tensor(12390.0762, grad_fn=<NegBackward0>) tensor(12390.0625, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12390.0478515625
tensor(12390.0625, grad_fn=<NegBackward0>) tensor(12390.0479, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12390.037109375
tensor(12390.0479, grad_fn=<NegBackward0>) tensor(12390.0371, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12390.0224609375
tensor(12390.0371, grad_fn=<NegBackward0>) tensor(12390.0225, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12390.0087890625
tensor(12390.0225, grad_fn=<NegBackward0>) tensor(12390.0088, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12389.9921875
tensor(12390.0088, grad_fn=<NegBackward0>) tensor(12389.9922, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12389.9736328125
tensor(12389.9922, grad_fn=<NegBackward0>) tensor(12389.9736, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12389.9560546875
tensor(12389.9736, grad_fn=<NegBackward0>) tensor(12389.9561, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12389.9365234375
tensor(12389.9561, grad_fn=<NegBackward0>) tensor(12389.9365, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12389.921875
tensor(12389.9365, grad_fn=<NegBackward0>) tensor(12389.9219, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12389.9091796875
tensor(12389.9219, grad_fn=<NegBackward0>) tensor(12389.9092, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12389.8974609375
tensor(12389.9092, grad_fn=<NegBackward0>) tensor(12389.8975, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12389.892578125
tensor(12389.8975, grad_fn=<NegBackward0>) tensor(12389.8926, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12389.8896484375
tensor(12389.8926, grad_fn=<NegBackward0>) tensor(12389.8896, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12389.8857421875
tensor(12389.8896, grad_fn=<NegBackward0>) tensor(12389.8857, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12389.8837890625
tensor(12389.8857, grad_fn=<NegBackward0>) tensor(12389.8838, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12389.884765625
tensor(12389.8838, grad_fn=<NegBackward0>) tensor(12389.8848, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12389.8828125
tensor(12389.8838, grad_fn=<NegBackward0>) tensor(12389.8828, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12389.8818359375
tensor(12389.8828, grad_fn=<NegBackward0>) tensor(12389.8818, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12389.880859375
tensor(12389.8818, grad_fn=<NegBackward0>) tensor(12389.8809, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12389.8798828125
tensor(12389.8809, grad_fn=<NegBackward0>) tensor(12389.8799, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12389.8798828125
tensor(12389.8799, grad_fn=<NegBackward0>) tensor(12389.8799, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12389.8779296875
tensor(12389.8799, grad_fn=<NegBackward0>) tensor(12389.8779, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12389.876953125
tensor(12389.8779, grad_fn=<NegBackward0>) tensor(12389.8770, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12389.87890625
tensor(12389.8770, grad_fn=<NegBackward0>) tensor(12389.8789, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12389.875
tensor(12389.8770, grad_fn=<NegBackward0>) tensor(12389.8750, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12389.8759765625
tensor(12389.8750, grad_fn=<NegBackward0>) tensor(12389.8760, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12389.875
tensor(12389.8750, grad_fn=<NegBackward0>) tensor(12389.8750, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12389.8740234375
tensor(12389.8750, grad_fn=<NegBackward0>) tensor(12389.8740, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12389.8720703125
tensor(12389.8740, grad_fn=<NegBackward0>) tensor(12389.8721, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12389.869140625
tensor(12389.8721, grad_fn=<NegBackward0>) tensor(12389.8691, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12389.8642578125
tensor(12389.8691, grad_fn=<NegBackward0>) tensor(12389.8643, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12389.8544921875
tensor(12389.8643, grad_fn=<NegBackward0>) tensor(12389.8545, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12389.841796875
tensor(12389.8545, grad_fn=<NegBackward0>) tensor(12389.8418, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12389.8212890625
tensor(12389.8418, grad_fn=<NegBackward0>) tensor(12389.8213, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12389.798828125
tensor(12389.8213, grad_fn=<NegBackward0>) tensor(12389.7988, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12389.7919921875
tensor(12389.7988, grad_fn=<NegBackward0>) tensor(12389.7920, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12389.7890625
tensor(12389.7920, grad_fn=<NegBackward0>) tensor(12389.7891, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12389.7890625
tensor(12389.7891, grad_fn=<NegBackward0>) tensor(12389.7891, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12389.7880859375
tensor(12389.7891, grad_fn=<NegBackward0>) tensor(12389.7881, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12389.7880859375
tensor(12389.7881, grad_fn=<NegBackward0>) tensor(12389.7881, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12389.78515625
tensor(12389.7881, grad_fn=<NegBackward0>) tensor(12389.7852, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12389.79296875
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7930, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12389.7861328125
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -12389.7861328125
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -12389.7861328125
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
4
Iteration 5700: Loss = -12389.7841796875
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12389.7861328125
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12389.78515625
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7852, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12389.7841796875
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12389.7841796875
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12389.7822265625
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12389.783203125
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7832, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12389.7841796875
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12389.783203125
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7832, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -12389.783203125
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7832, grad_fn=<NegBackward0>)
4
Iteration 6700: Loss = -12389.7822265625
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12389.7822265625
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12389.78125
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7812, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12389.78125
tensor(12389.7812, grad_fn=<NegBackward0>) tensor(12389.7812, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12389.7822265625
tensor(12389.7812, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12389.78125
tensor(12389.7812, grad_fn=<NegBackward0>) tensor(12389.7812, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12389.78125
tensor(12389.7812, grad_fn=<NegBackward0>) tensor(12389.7812, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12389.9443359375
tensor(12389.7812, grad_fn=<NegBackward0>) tensor(12389.9443, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12389.669921875
tensor(12389.7812, grad_fn=<NegBackward0>) tensor(12389.6699, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12389.66796875
tensor(12389.6699, grad_fn=<NegBackward0>) tensor(12389.6680, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12389.66796875
tensor(12389.6680, grad_fn=<NegBackward0>) tensor(12389.6680, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12389.7080078125
tensor(12389.6680, grad_fn=<NegBackward0>) tensor(12389.7080, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12389.666015625
tensor(12389.6680, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12389.673828125
tensor(12389.6660, grad_fn=<NegBackward0>) tensor(12389.6738, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12389.6650390625
tensor(12389.6660, grad_fn=<NegBackward0>) tensor(12389.6650, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12389.666015625
tensor(12389.6650, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12389.66796875
tensor(12389.6650, grad_fn=<NegBackward0>) tensor(12389.6680, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12389.6650390625
tensor(12389.6650, grad_fn=<NegBackward0>) tensor(12389.6650, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12389.6875
tensor(12389.6650, grad_fn=<NegBackward0>) tensor(12389.6875, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12389.6640625
tensor(12389.6650, grad_fn=<NegBackward0>) tensor(12389.6641, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12389.6650390625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6650, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12389.6640625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6641, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12389.6650390625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6650, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12389.666015625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12389.666015625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12389.6650390625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6650, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -12389.666015625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[4.3482e-01, 5.6518e-01],
        [6.8398e-05, 9.9993e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1152, 0.8848], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2896, 0.2428],
         [0.6890, 0.1978]],

        [[0.6355, 0.2418],
         [0.6505, 0.5513]],

        [[0.5835, 0.1462],
         [0.7308, 0.5452]],

        [[0.6379, 0.1807],
         [0.5590, 0.6650]],

        [[0.6229, 0.1111],
         [0.5455, 0.6791]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.014553017632794063
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001513466251040077
Average Adjusted Rand Index: 0.0027552693761031255
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20739.91015625
inf tensor(20739.9102, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12392.236328125
tensor(20739.9102, grad_fn=<NegBackward0>) tensor(12392.2363, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12391.8056640625
tensor(12392.2363, grad_fn=<NegBackward0>) tensor(12391.8057, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12391.1279296875
tensor(12391.8057, grad_fn=<NegBackward0>) tensor(12391.1279, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12390.5126953125
tensor(12391.1279, grad_fn=<NegBackward0>) tensor(12390.5127, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12390.3056640625
tensor(12390.5127, grad_fn=<NegBackward0>) tensor(12390.3057, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12390.2001953125
tensor(12390.3057, grad_fn=<NegBackward0>) tensor(12390.2002, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12390.154296875
tensor(12390.2002, grad_fn=<NegBackward0>) tensor(12390.1543, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12390.1123046875
tensor(12390.1543, grad_fn=<NegBackward0>) tensor(12390.1123, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12390.0791015625
tensor(12390.1123, grad_fn=<NegBackward0>) tensor(12390.0791, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12390.0546875
tensor(12390.0791, grad_fn=<NegBackward0>) tensor(12390.0547, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12390.041015625
tensor(12390.0547, grad_fn=<NegBackward0>) tensor(12390.0410, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12390.0341796875
tensor(12390.0410, grad_fn=<NegBackward0>) tensor(12390.0342, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12390.0263671875
tensor(12390.0342, grad_fn=<NegBackward0>) tensor(12390.0264, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12390.0205078125
tensor(12390.0264, grad_fn=<NegBackward0>) tensor(12390.0205, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12390.013671875
tensor(12390.0205, grad_fn=<NegBackward0>) tensor(12390.0137, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12390.0068359375
tensor(12390.0137, grad_fn=<NegBackward0>) tensor(12390.0068, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12389.9990234375
tensor(12390.0068, grad_fn=<NegBackward0>) tensor(12389.9990, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12389.990234375
tensor(12389.9990, grad_fn=<NegBackward0>) tensor(12389.9902, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12389.98046875
tensor(12389.9902, grad_fn=<NegBackward0>) tensor(12389.9805, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12389.9716796875
tensor(12389.9805, grad_fn=<NegBackward0>) tensor(12389.9717, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12389.9599609375
tensor(12389.9717, grad_fn=<NegBackward0>) tensor(12389.9600, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12389.9462890625
tensor(12389.9600, grad_fn=<NegBackward0>) tensor(12389.9463, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12389.9365234375
tensor(12389.9463, grad_fn=<NegBackward0>) tensor(12389.9365, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12389.9248046875
tensor(12389.9365, grad_fn=<NegBackward0>) tensor(12389.9248, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12389.916015625
tensor(12389.9248, grad_fn=<NegBackward0>) tensor(12389.9160, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12389.9072265625
tensor(12389.9160, grad_fn=<NegBackward0>) tensor(12389.9072, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12389.9013671875
tensor(12389.9072, grad_fn=<NegBackward0>) tensor(12389.9014, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12389.896484375
tensor(12389.9014, grad_fn=<NegBackward0>) tensor(12389.8965, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12389.8935546875
tensor(12389.8965, grad_fn=<NegBackward0>) tensor(12389.8936, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12389.890625
tensor(12389.8936, grad_fn=<NegBackward0>) tensor(12389.8906, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12389.8896484375
tensor(12389.8906, grad_fn=<NegBackward0>) tensor(12389.8896, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12389.88671875
tensor(12389.8896, grad_fn=<NegBackward0>) tensor(12389.8867, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12389.88671875
tensor(12389.8867, grad_fn=<NegBackward0>) tensor(12389.8867, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12389.884765625
tensor(12389.8867, grad_fn=<NegBackward0>) tensor(12389.8848, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12389.8837890625
tensor(12389.8848, grad_fn=<NegBackward0>) tensor(12389.8838, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12389.8828125
tensor(12389.8838, grad_fn=<NegBackward0>) tensor(12389.8828, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12389.8818359375
tensor(12389.8828, grad_fn=<NegBackward0>) tensor(12389.8818, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12389.8818359375
tensor(12389.8818, grad_fn=<NegBackward0>) tensor(12389.8818, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12389.8828125
tensor(12389.8818, grad_fn=<NegBackward0>) tensor(12389.8828, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12389.8798828125
tensor(12389.8818, grad_fn=<NegBackward0>) tensor(12389.8799, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12389.87890625
tensor(12389.8799, grad_fn=<NegBackward0>) tensor(12389.8789, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12389.8779296875
tensor(12389.8789, grad_fn=<NegBackward0>) tensor(12389.8779, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12389.8798828125
tensor(12389.8779, grad_fn=<NegBackward0>) tensor(12389.8799, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12389.876953125
tensor(12389.8779, grad_fn=<NegBackward0>) tensor(12389.8770, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12389.8779296875
tensor(12389.8770, grad_fn=<NegBackward0>) tensor(12389.8779, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12389.8779296875
tensor(12389.8770, grad_fn=<NegBackward0>) tensor(12389.8779, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12389.8740234375
tensor(12389.8770, grad_fn=<NegBackward0>) tensor(12389.8740, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12389.8740234375
tensor(12389.8740, grad_fn=<NegBackward0>) tensor(12389.8740, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12389.8740234375
tensor(12389.8740, grad_fn=<NegBackward0>) tensor(12389.8740, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12389.873046875
tensor(12389.8740, grad_fn=<NegBackward0>) tensor(12389.8730, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12389.8701171875
tensor(12389.8730, grad_fn=<NegBackward0>) tensor(12389.8701, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12389.8681640625
tensor(12389.8701, grad_fn=<NegBackward0>) tensor(12389.8682, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12389.8642578125
tensor(12389.8682, grad_fn=<NegBackward0>) tensor(12389.8643, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12389.8583984375
tensor(12389.8643, grad_fn=<NegBackward0>) tensor(12389.8584, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12389.849609375
tensor(12389.8584, grad_fn=<NegBackward0>) tensor(12389.8496, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12389.8388671875
tensor(12389.8496, grad_fn=<NegBackward0>) tensor(12389.8389, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12389.82421875
tensor(12389.8389, grad_fn=<NegBackward0>) tensor(12389.8242, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12389.810546875
tensor(12389.8242, grad_fn=<NegBackward0>) tensor(12389.8105, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12389.7998046875
tensor(12389.8105, grad_fn=<NegBackward0>) tensor(12389.7998, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12389.794921875
tensor(12389.7998, grad_fn=<NegBackward0>) tensor(12389.7949, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12389.791015625
tensor(12389.7949, grad_fn=<NegBackward0>) tensor(12389.7910, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12389.7890625
tensor(12389.7910, grad_fn=<NegBackward0>) tensor(12389.7891, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12389.7890625
tensor(12389.7891, grad_fn=<NegBackward0>) tensor(12389.7891, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12389.7880859375
tensor(12389.7891, grad_fn=<NegBackward0>) tensor(12389.7881, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12389.7919921875
tensor(12389.7881, grad_fn=<NegBackward0>) tensor(12389.7920, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12389.78515625
tensor(12389.7881, grad_fn=<NegBackward0>) tensor(12389.7852, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12389.7861328125
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12389.7861328125
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12389.7861328125
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7861, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12389.78515625
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7852, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12389.7841796875
tensor(12389.7852, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12389.791015625
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7910, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12389.78515625
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7852, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12389.8583984375
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.8584, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12389.783203125
tensor(12389.7842, grad_fn=<NegBackward0>) tensor(12389.7832, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12389.7841796875
tensor(12389.7832, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12389.78515625
tensor(12389.7832, grad_fn=<NegBackward0>) tensor(12389.7852, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12389.7822265625
tensor(12389.7832, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12389.7841796875
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7842, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12389.783203125
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7832, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12390.1953125
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12390.1953, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -12389.7822265625
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12389.7822265625
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12389.7802734375
tensor(12389.7822, grad_fn=<NegBackward0>) tensor(12389.7803, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12389.78125
tensor(12389.7803, grad_fn=<NegBackward0>) tensor(12389.7812, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12389.814453125
tensor(12389.7803, grad_fn=<NegBackward0>) tensor(12389.8145, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12389.7822265625
tensor(12389.7803, grad_fn=<NegBackward0>) tensor(12389.7822, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12389.71875
tensor(12389.7803, grad_fn=<NegBackward0>) tensor(12389.7188, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12389.6689453125
tensor(12389.7188, grad_fn=<NegBackward0>) tensor(12389.6689, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12389.6669921875
tensor(12389.6689, grad_fn=<NegBackward0>) tensor(12389.6670, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12389.666015625
tensor(12389.6670, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12389.666015625
tensor(12389.6660, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12389.884765625
tensor(12389.6660, grad_fn=<NegBackward0>) tensor(12389.8848, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12389.6650390625
tensor(12389.6660, grad_fn=<NegBackward0>) tensor(12389.6650, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12389.6640625
tensor(12389.6650, grad_fn=<NegBackward0>) tensor(12389.6641, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12390.0859375
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12390.0859, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12389.666015625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
2
Iteration 9800: Loss = -12389.6689453125
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6689, grad_fn=<NegBackward0>)
3
Iteration 9900: Loss = -12389.666015625
tensor(12389.6641, grad_fn=<NegBackward0>) tensor(12389.6660, grad_fn=<NegBackward0>)
4
pi: tensor([[9.9992e-01, 7.8869e-05],
        [5.6555e-01, 4.3445e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8848, 0.1152], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1979, 0.2427],
         [0.6014, 0.2895]],

        [[0.7200, 0.2418],
         [0.6744, 0.6911]],

        [[0.6855, 0.1462],
         [0.5185, 0.6540]],

        [[0.5705, 0.1807],
         [0.5266, 0.7138]],

        [[0.5474, 0.1112],
         [0.6361, 0.6163]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.014553017632794063
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001513466251040077
Average Adjusted Rand Index: 0.0027552693761031255
[0.001513466251040077, 0.001513466251040077] [0.0027552693761031255, 0.0027552693761031255] [12389.666015625, 12389.666015625]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11993.178422355853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22967.169921875
inf tensor(22967.1699, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12483.0654296875
tensor(22967.1699, grad_fn=<NegBackward0>) tensor(12483.0654, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12482.767578125
tensor(12483.0654, grad_fn=<NegBackward0>) tensor(12482.7676, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12482.6767578125
tensor(12482.7676, grad_fn=<NegBackward0>) tensor(12482.6768, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12482.59375
tensor(12482.6768, grad_fn=<NegBackward0>) tensor(12482.5938, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12482.498046875
tensor(12482.5938, grad_fn=<NegBackward0>) tensor(12482.4980, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12482.3896484375
tensor(12482.4980, grad_fn=<NegBackward0>) tensor(12482.3896, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12482.2919921875
tensor(12482.3896, grad_fn=<NegBackward0>) tensor(12482.2920, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12482.2109375
tensor(12482.2920, grad_fn=<NegBackward0>) tensor(12482.2109, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12482.1552734375
tensor(12482.2109, grad_fn=<NegBackward0>) tensor(12482.1553, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12482.1103515625
tensor(12482.1553, grad_fn=<NegBackward0>) tensor(12482.1104, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12482.0751953125
tensor(12482.1104, grad_fn=<NegBackward0>) tensor(12482.0752, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12482.0458984375
tensor(12482.0752, grad_fn=<NegBackward0>) tensor(12482.0459, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12482.0205078125
tensor(12482.0459, grad_fn=<NegBackward0>) tensor(12482.0205, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12481.9990234375
tensor(12482.0205, grad_fn=<NegBackward0>) tensor(12481.9990, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12481.9697265625
tensor(12481.9990, grad_fn=<NegBackward0>) tensor(12481.9697, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12481.9326171875
tensor(12481.9697, grad_fn=<NegBackward0>) tensor(12481.9326, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12481.8759765625
tensor(12481.9326, grad_fn=<NegBackward0>) tensor(12481.8760, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12481.7626953125
tensor(12481.8760, grad_fn=<NegBackward0>) tensor(12481.7627, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12481.521484375
tensor(12481.7627, grad_fn=<NegBackward0>) tensor(12481.5215, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12481.2060546875
tensor(12481.5215, grad_fn=<NegBackward0>) tensor(12481.2061, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12480.9501953125
tensor(12481.2061, grad_fn=<NegBackward0>) tensor(12480.9502, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12480.7646484375
tensor(12480.9502, grad_fn=<NegBackward0>) tensor(12480.7646, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12480.6201171875
tensor(12480.7646, grad_fn=<NegBackward0>) tensor(12480.6201, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12480.4931640625
tensor(12480.6201, grad_fn=<NegBackward0>) tensor(12480.4932, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12480.384765625
tensor(12480.4932, grad_fn=<NegBackward0>) tensor(12480.3848, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12480.287109375
tensor(12480.3848, grad_fn=<NegBackward0>) tensor(12480.2871, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12480.193359375
tensor(12480.2871, grad_fn=<NegBackward0>) tensor(12480.1934, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12480.1064453125
tensor(12480.1934, grad_fn=<NegBackward0>) tensor(12480.1064, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12480.029296875
tensor(12480.1064, grad_fn=<NegBackward0>) tensor(12480.0293, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12479.9619140625
tensor(12480.0293, grad_fn=<NegBackward0>) tensor(12479.9619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12479.9072265625
tensor(12479.9619, grad_fn=<NegBackward0>) tensor(12479.9072, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12479.861328125
tensor(12479.9072, grad_fn=<NegBackward0>) tensor(12479.8613, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12479.8251953125
tensor(12479.8613, grad_fn=<NegBackward0>) tensor(12479.8252, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12479.7958984375
tensor(12479.8252, grad_fn=<NegBackward0>) tensor(12479.7959, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12479.76953125
tensor(12479.7959, grad_fn=<NegBackward0>) tensor(12479.7695, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12479.748046875
tensor(12479.7695, grad_fn=<NegBackward0>) tensor(12479.7480, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12479.73046875
tensor(12479.7480, grad_fn=<NegBackward0>) tensor(12479.7305, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12479.7158203125
tensor(12479.7305, grad_fn=<NegBackward0>) tensor(12479.7158, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12479.7021484375
tensor(12479.7158, grad_fn=<NegBackward0>) tensor(12479.7021, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12479.689453125
tensor(12479.7021, grad_fn=<NegBackward0>) tensor(12479.6895, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12479.6796875
tensor(12479.6895, grad_fn=<NegBackward0>) tensor(12479.6797, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12479.669921875
tensor(12479.6797, grad_fn=<NegBackward0>) tensor(12479.6699, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12479.6640625
tensor(12479.6699, grad_fn=<NegBackward0>) tensor(12479.6641, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12479.6572265625
tensor(12479.6641, grad_fn=<NegBackward0>) tensor(12479.6572, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12479.6513671875
tensor(12479.6572, grad_fn=<NegBackward0>) tensor(12479.6514, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12479.64453125
tensor(12479.6514, grad_fn=<NegBackward0>) tensor(12479.6445, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12479.6396484375
tensor(12479.6445, grad_fn=<NegBackward0>) tensor(12479.6396, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12479.63671875
tensor(12479.6396, grad_fn=<NegBackward0>) tensor(12479.6367, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12479.6318359375
tensor(12479.6367, grad_fn=<NegBackward0>) tensor(12479.6318, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12479.62890625
tensor(12479.6318, grad_fn=<NegBackward0>) tensor(12479.6289, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12479.6240234375
tensor(12479.6289, grad_fn=<NegBackward0>) tensor(12479.6240, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12479.62109375
tensor(12479.6240, grad_fn=<NegBackward0>) tensor(12479.6211, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12479.619140625
tensor(12479.6211, grad_fn=<NegBackward0>) tensor(12479.6191, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12479.6162109375
tensor(12479.6191, grad_fn=<NegBackward0>) tensor(12479.6162, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12479.61328125
tensor(12479.6162, grad_fn=<NegBackward0>) tensor(12479.6133, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12479.6123046875
tensor(12479.6133, grad_fn=<NegBackward0>) tensor(12479.6123, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12479.6103515625
tensor(12479.6123, grad_fn=<NegBackward0>) tensor(12479.6104, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12479.607421875
tensor(12479.6104, grad_fn=<NegBackward0>) tensor(12479.6074, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12479.60546875
tensor(12479.6074, grad_fn=<NegBackward0>) tensor(12479.6055, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12479.6044921875
tensor(12479.6055, grad_fn=<NegBackward0>) tensor(12479.6045, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12479.603515625
tensor(12479.6045, grad_fn=<NegBackward0>) tensor(12479.6035, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12479.6015625
tensor(12479.6035, grad_fn=<NegBackward0>) tensor(12479.6016, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12479.5986328125
tensor(12479.6016, grad_fn=<NegBackward0>) tensor(12479.5986, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12479.5986328125
tensor(12479.5986, grad_fn=<NegBackward0>) tensor(12479.5986, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12479.5986328125
tensor(12479.5986, grad_fn=<NegBackward0>) tensor(12479.5986, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12479.59765625
tensor(12479.5986, grad_fn=<NegBackward0>) tensor(12479.5977, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12479.5986328125
tensor(12479.5977, grad_fn=<NegBackward0>) tensor(12479.5986, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12479.5986328125
tensor(12479.5977, grad_fn=<NegBackward0>) tensor(12479.5986, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12479.5927734375
tensor(12479.5977, grad_fn=<NegBackward0>) tensor(12479.5928, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12479.59375
tensor(12479.5928, grad_fn=<NegBackward0>) tensor(12479.5938, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12479.5927734375
tensor(12479.5928, grad_fn=<NegBackward0>) tensor(12479.5928, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12479.591796875
tensor(12479.5928, grad_fn=<NegBackward0>) tensor(12479.5918, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12479.697265625
tensor(12479.5918, grad_fn=<NegBackward0>) tensor(12479.6973, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12479.58984375
tensor(12479.5918, grad_fn=<NegBackward0>) tensor(12479.5898, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12479.58984375
tensor(12479.5898, grad_fn=<NegBackward0>) tensor(12479.5898, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12479.5927734375
tensor(12479.5898, grad_fn=<NegBackward0>) tensor(12479.5928, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12479.5888671875
tensor(12479.5898, grad_fn=<NegBackward0>) tensor(12479.5889, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12479.5888671875
tensor(12479.5889, grad_fn=<NegBackward0>) tensor(12479.5889, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12479.5888671875
tensor(12479.5889, grad_fn=<NegBackward0>) tensor(12479.5889, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12479.5927734375
tensor(12479.5889, grad_fn=<NegBackward0>) tensor(12479.5928, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12479.587890625
tensor(12479.5889, grad_fn=<NegBackward0>) tensor(12479.5879, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12479.5869140625
tensor(12479.5879, grad_fn=<NegBackward0>) tensor(12479.5869, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12479.5849609375
tensor(12479.5869, grad_fn=<NegBackward0>) tensor(12479.5850, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12479.5869140625
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.5869, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12479.5859375
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.5859, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12479.5869140625
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.5869, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -12479.5849609375
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.5850, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12479.6396484375
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.6396, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12479.5859375
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.5859, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -12479.5830078125
tensor(12479.5850, grad_fn=<NegBackward0>) tensor(12479.5830, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12479.5830078125
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.5830, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12479.583984375
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.5840, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12479.5849609375
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.5850, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -12479.583984375
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.5840, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -12479.6533203125
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.6533, grad_fn=<NegBackward0>)
4
Iteration 9600: Loss = -12479.5830078125
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.5830, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12479.5810546875
tensor(12479.5830, grad_fn=<NegBackward0>) tensor(12479.5811, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12479.5908203125
tensor(12479.5811, grad_fn=<NegBackward0>) tensor(12479.5908, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12479.5830078125
tensor(12479.5811, grad_fn=<NegBackward0>) tensor(12479.5830, grad_fn=<NegBackward0>)
2
pi: tensor([[9.9953e-01, 4.6754e-04],
        [1.6924e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0256, 0.9744], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2269, 0.2018],
         [0.6871, 0.2005]],

        [[0.6459, 0.2061],
         [0.5965, 0.6589]],

        [[0.5501, 0.2955],
         [0.6828, 0.5204]],

        [[0.6171, 0.2702],
         [0.6665, 0.5633]],

        [[0.5698, 0.2623],
         [0.5633, 0.5869]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
Global Adjusted Rand Index: -0.0016020906861989137
Average Adjusted Rand Index: -0.0023476429318125755
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23767.95703125
inf tensor(23767.9570, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12018.5673828125
tensor(23767.9570, grad_fn=<NegBackward0>) tensor(12018.5674, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11985.6953125
tensor(12018.5674, grad_fn=<NegBackward0>) tensor(11985.6953, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11985.5302734375
tensor(11985.6953, grad_fn=<NegBackward0>) tensor(11985.5303, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11985.462890625
tensor(11985.5303, grad_fn=<NegBackward0>) tensor(11985.4629, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11985.427734375
tensor(11985.4629, grad_fn=<NegBackward0>) tensor(11985.4277, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11985.4052734375
tensor(11985.4277, grad_fn=<NegBackward0>) tensor(11985.4053, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11985.392578125
tensor(11985.4053, grad_fn=<NegBackward0>) tensor(11985.3926, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11985.3828125
tensor(11985.3926, grad_fn=<NegBackward0>) tensor(11985.3828, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11985.3759765625
tensor(11985.3828, grad_fn=<NegBackward0>) tensor(11985.3760, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11985.369140625
tensor(11985.3760, grad_fn=<NegBackward0>) tensor(11985.3691, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11985.3642578125
tensor(11985.3691, grad_fn=<NegBackward0>) tensor(11985.3643, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11985.36328125
tensor(11985.3643, grad_fn=<NegBackward0>) tensor(11985.3633, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11985.3583984375
tensor(11985.3633, grad_fn=<NegBackward0>) tensor(11985.3584, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11985.357421875
tensor(11985.3584, grad_fn=<NegBackward0>) tensor(11985.3574, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11985.3544921875
tensor(11985.3574, grad_fn=<NegBackward0>) tensor(11985.3545, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11985.353515625
tensor(11985.3545, grad_fn=<NegBackward0>) tensor(11985.3535, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11985.3525390625
tensor(11985.3535, grad_fn=<NegBackward0>) tensor(11985.3525, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11985.3505859375
tensor(11985.3525, grad_fn=<NegBackward0>) tensor(11985.3506, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11985.349609375
tensor(11985.3506, grad_fn=<NegBackward0>) tensor(11985.3496, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11985.349609375
tensor(11985.3496, grad_fn=<NegBackward0>) tensor(11985.3496, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11985.34765625
tensor(11985.3496, grad_fn=<NegBackward0>) tensor(11985.3477, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11985.3466796875
tensor(11985.3477, grad_fn=<NegBackward0>) tensor(11985.3467, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11985.3447265625
tensor(11985.3467, grad_fn=<NegBackward0>) tensor(11985.3447, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11985.3466796875
tensor(11985.3447, grad_fn=<NegBackward0>) tensor(11985.3467, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -11985.345703125
tensor(11985.3447, grad_fn=<NegBackward0>) tensor(11985.3457, grad_fn=<NegBackward0>)
2
Iteration 2600: Loss = -11985.3447265625
tensor(11985.3447, grad_fn=<NegBackward0>) tensor(11985.3447, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11985.3447265625
tensor(11985.3447, grad_fn=<NegBackward0>) tensor(11985.3447, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11985.3447265625
tensor(11985.3447, grad_fn=<NegBackward0>) tensor(11985.3447, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11985.34375
tensor(11985.3447, grad_fn=<NegBackward0>) tensor(11985.3438, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11985.3427734375
tensor(11985.3438, grad_fn=<NegBackward0>) tensor(11985.3428, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11985.3427734375
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3428, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11985.3447265625
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3447, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11985.3427734375
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3428, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11985.3447265625
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3447, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11985.3427734375
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3428, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11985.3486328125
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3486, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11985.341796875
tensor(11985.3428, grad_fn=<NegBackward0>) tensor(11985.3418, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11985.35546875
tensor(11985.3418, grad_fn=<NegBackward0>) tensor(11985.3555, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11985.341796875
tensor(11985.3418, grad_fn=<NegBackward0>) tensor(11985.3418, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11985.341796875
tensor(11985.3418, grad_fn=<NegBackward0>) tensor(11985.3418, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11985.3427734375
tensor(11985.3418, grad_fn=<NegBackward0>) tensor(11985.3428, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11985.3408203125
tensor(11985.3418, grad_fn=<NegBackward0>) tensor(11985.3408, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11985.3427734375
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3428, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11985.349609375
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3496, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11985.3408203125
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3408, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11985.34375
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3438, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11985.341796875
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3418, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -11985.34375
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3438, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -11985.341796875
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3418, grad_fn=<NegBackward0>)
4
Iteration 5000: Loss = -11985.3671875
tensor(11985.3408, grad_fn=<NegBackward0>) tensor(11985.3672, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5000 due to no improvement.
pi: tensor([[0.2286, 0.7714],
        [0.7111, 0.2889]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5184, 0.4816], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3024, 0.1031],
         [0.6356, 0.3032]],

        [[0.5891, 0.1090],
         [0.6432, 0.5999]],

        [[0.5016, 0.0970],
         [0.7146, 0.7216]],

        [[0.6174, 0.1021],
         [0.6621, 0.6500]],

        [[0.5509, 0.1069],
         [0.6517, 0.5637]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.03648964264382531
Average Adjusted Rand Index: 0.992
[-0.0016020906861989137, 0.03648964264382531] [-0.0023476429318125755, 0.992] [12479.5830078125, 11985.3671875]
-------------------------------------
This iteration is 48
True Objective function: Loss = -11928.368019147678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21537.064453125
inf tensor(21537.0645, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12418.6689453125
tensor(21537.0645, grad_fn=<NegBackward0>) tensor(12418.6689, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12418.1953125
tensor(12418.6689, grad_fn=<NegBackward0>) tensor(12418.1953, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12418.04296875
tensor(12418.1953, grad_fn=<NegBackward0>) tensor(12418.0430, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12417.8876953125
tensor(12418.0430, grad_fn=<NegBackward0>) tensor(12417.8877, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12417.6982421875
tensor(12417.8877, grad_fn=<NegBackward0>) tensor(12417.6982, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12417.4580078125
tensor(12417.6982, grad_fn=<NegBackward0>) tensor(12417.4580, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12417.2060546875
tensor(12417.4580, grad_fn=<NegBackward0>) tensor(12417.2061, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12417.1025390625
tensor(12417.2061, grad_fn=<NegBackward0>) tensor(12417.1025, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12417.0595703125
tensor(12417.1025, grad_fn=<NegBackward0>) tensor(12417.0596, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12417.0380859375
tensor(12417.0596, grad_fn=<NegBackward0>) tensor(12417.0381, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12417.0244140625
tensor(12417.0381, grad_fn=<NegBackward0>) tensor(12417.0244, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12417.0146484375
tensor(12417.0244, grad_fn=<NegBackward0>) tensor(12417.0146, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12417.0087890625
tensor(12417.0146, grad_fn=<NegBackward0>) tensor(12417.0088, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12417.00390625
tensor(12417.0088, grad_fn=<NegBackward0>) tensor(12417.0039, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12417.001953125
tensor(12417.0039, grad_fn=<NegBackward0>) tensor(12417.0020, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12416.998046875
tensor(12417.0020, grad_fn=<NegBackward0>) tensor(12416.9980, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12416.99609375
tensor(12416.9980, grad_fn=<NegBackward0>) tensor(12416.9961, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12416.9921875
tensor(12416.9961, grad_fn=<NegBackward0>) tensor(12416.9922, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12416.990234375
tensor(12416.9922, grad_fn=<NegBackward0>) tensor(12416.9902, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12416.990234375
tensor(12416.9902, grad_fn=<NegBackward0>) tensor(12416.9902, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12416.9873046875
tensor(12416.9902, grad_fn=<NegBackward0>) tensor(12416.9873, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12416.9853515625
tensor(12416.9873, grad_fn=<NegBackward0>) tensor(12416.9854, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12416.9853515625
tensor(12416.9854, grad_fn=<NegBackward0>) tensor(12416.9854, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12416.982421875
tensor(12416.9854, grad_fn=<NegBackward0>) tensor(12416.9824, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12416.9814453125
tensor(12416.9824, grad_fn=<NegBackward0>) tensor(12416.9814, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12416.978515625
tensor(12416.9814, grad_fn=<NegBackward0>) tensor(12416.9785, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12416.9765625
tensor(12416.9785, grad_fn=<NegBackward0>) tensor(12416.9766, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12416.9755859375
tensor(12416.9766, grad_fn=<NegBackward0>) tensor(12416.9756, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12416.970703125
tensor(12416.9756, grad_fn=<NegBackward0>) tensor(12416.9707, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12416.9658203125
tensor(12416.9707, grad_fn=<NegBackward0>) tensor(12416.9658, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12416.9599609375
tensor(12416.9658, grad_fn=<NegBackward0>) tensor(12416.9600, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12416.9501953125
tensor(12416.9600, grad_fn=<NegBackward0>) tensor(12416.9502, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12416.939453125
tensor(12416.9502, grad_fn=<NegBackward0>) tensor(12416.9395, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12416.9248046875
tensor(12416.9395, grad_fn=<NegBackward0>) tensor(12416.9248, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12416.9072265625
tensor(12416.9248, grad_fn=<NegBackward0>) tensor(12416.9072, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12416.8857421875
tensor(12416.9072, grad_fn=<NegBackward0>) tensor(12416.8857, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12416.861328125
tensor(12416.8857, grad_fn=<NegBackward0>) tensor(12416.8613, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12416.8388671875
tensor(12416.8613, grad_fn=<NegBackward0>) tensor(12416.8389, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12416.8193359375
tensor(12416.8389, grad_fn=<NegBackward0>) tensor(12416.8193, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12416.80078125
tensor(12416.8193, grad_fn=<NegBackward0>) tensor(12416.8008, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12416.7880859375
tensor(12416.8008, grad_fn=<NegBackward0>) tensor(12416.7881, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12416.77734375
tensor(12416.7881, grad_fn=<NegBackward0>) tensor(12416.7773, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12416.76953125
tensor(12416.7773, grad_fn=<NegBackward0>) tensor(12416.7695, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12416.763671875
tensor(12416.7695, grad_fn=<NegBackward0>) tensor(12416.7637, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12416.7587890625
tensor(12416.7637, grad_fn=<NegBackward0>) tensor(12416.7588, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12416.755859375
tensor(12416.7588, grad_fn=<NegBackward0>) tensor(12416.7559, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12416.751953125
tensor(12416.7559, grad_fn=<NegBackward0>) tensor(12416.7520, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12416.751953125
tensor(12416.7520, grad_fn=<NegBackward0>) tensor(12416.7520, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12416.7490234375
tensor(12416.7520, grad_fn=<NegBackward0>) tensor(12416.7490, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12416.7470703125
tensor(12416.7490, grad_fn=<NegBackward0>) tensor(12416.7471, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12416.74609375
tensor(12416.7471, grad_fn=<NegBackward0>) tensor(12416.7461, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12416.7451171875
tensor(12416.7461, grad_fn=<NegBackward0>) tensor(12416.7451, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12416.7451171875
tensor(12416.7451, grad_fn=<NegBackward0>) tensor(12416.7451, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12416.744140625
tensor(12416.7451, grad_fn=<NegBackward0>) tensor(12416.7441, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12416.7421875
tensor(12416.7441, grad_fn=<NegBackward0>) tensor(12416.7422, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12416.7421875
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7422, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12416.7421875
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7422, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12416.7421875
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7422, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12416.7392578125
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12416.7392578125
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12416.740234375
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7402, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12416.740234375
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7402, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12416.7392578125
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12416.7392578125
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12416.7373046875
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7373, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12416.7392578125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12416.73828125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12416.73828125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12416.740234375
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7402, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -12416.73828125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[9.8324e-01, 1.6755e-02],
        [9.9931e-01, 6.8645e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9815, 0.0185], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2009, 0.1094],
         [0.6997, 0.2917]],

        [[0.5594, 0.3068],
         [0.6401, 0.7205]],

        [[0.6818, 0.2707],
         [0.5439, 0.6218]],

        [[0.6850, 0.1235],
         [0.5803, 0.7302]],

        [[0.5879, 0.2142],
         [0.6576, 0.6340]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00032696474682979756
Average Adjusted Rand Index: -0.00021432494651782533
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21593.68359375
inf tensor(21593.6836, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12418.388671875
tensor(21593.6836, grad_fn=<NegBackward0>) tensor(12418.3887, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12418.0458984375
tensor(12418.3887, grad_fn=<NegBackward0>) tensor(12418.0459, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12417.8984375
tensor(12418.0459, grad_fn=<NegBackward0>) tensor(12417.8984, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12417.7138671875
tensor(12417.8984, grad_fn=<NegBackward0>) tensor(12417.7139, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12417.4521484375
tensor(12417.7139, grad_fn=<NegBackward0>) tensor(12417.4521, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12417.2080078125
tensor(12417.4521, grad_fn=<NegBackward0>) tensor(12417.2080, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12417.099609375
tensor(12417.2080, grad_fn=<NegBackward0>) tensor(12417.0996, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12417.05859375
tensor(12417.0996, grad_fn=<NegBackward0>) tensor(12417.0586, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12417.0322265625
tensor(12417.0586, grad_fn=<NegBackward0>) tensor(12417.0322, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12417.0166015625
tensor(12417.0322, grad_fn=<NegBackward0>) tensor(12417.0166, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12417.0048828125
tensor(12417.0166, grad_fn=<NegBackward0>) tensor(12417.0049, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12416.9970703125
tensor(12417.0049, grad_fn=<NegBackward0>) tensor(12416.9971, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12416.990234375
tensor(12416.9971, grad_fn=<NegBackward0>) tensor(12416.9902, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12416.9833984375
tensor(12416.9902, grad_fn=<NegBackward0>) tensor(12416.9834, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12416.9794921875
tensor(12416.9834, grad_fn=<NegBackward0>) tensor(12416.9795, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12416.9736328125
tensor(12416.9795, grad_fn=<NegBackward0>) tensor(12416.9736, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12416.9697265625
tensor(12416.9736, grad_fn=<NegBackward0>) tensor(12416.9697, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12416.96484375
tensor(12416.9697, grad_fn=<NegBackward0>) tensor(12416.9648, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12416.9580078125
tensor(12416.9648, grad_fn=<NegBackward0>) tensor(12416.9580, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12416.9541015625
tensor(12416.9580, grad_fn=<NegBackward0>) tensor(12416.9541, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12416.9482421875
tensor(12416.9541, grad_fn=<NegBackward0>) tensor(12416.9482, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12416.9404296875
tensor(12416.9482, grad_fn=<NegBackward0>) tensor(12416.9404, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12416.93359375
tensor(12416.9404, grad_fn=<NegBackward0>) tensor(12416.9336, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12416.9228515625
tensor(12416.9336, grad_fn=<NegBackward0>) tensor(12416.9229, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12416.9130859375
tensor(12416.9229, grad_fn=<NegBackward0>) tensor(12416.9131, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12416.8994140625
tensor(12416.9131, grad_fn=<NegBackward0>) tensor(12416.8994, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12416.884765625
tensor(12416.8994, grad_fn=<NegBackward0>) tensor(12416.8848, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12416.865234375
tensor(12416.8848, grad_fn=<NegBackward0>) tensor(12416.8652, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12416.845703125
tensor(12416.8652, grad_fn=<NegBackward0>) tensor(12416.8457, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12416.828125
tensor(12416.8457, grad_fn=<NegBackward0>) tensor(12416.8281, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12416.8125
tensor(12416.8281, grad_fn=<NegBackward0>) tensor(12416.8125, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12416.7998046875
tensor(12416.8125, grad_fn=<NegBackward0>) tensor(12416.7998, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12416.7900390625
tensor(12416.7998, grad_fn=<NegBackward0>) tensor(12416.7900, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12416.7841796875
tensor(12416.7900, grad_fn=<NegBackward0>) tensor(12416.7842, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12416.7763671875
tensor(12416.7842, grad_fn=<NegBackward0>) tensor(12416.7764, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12416.7705078125
tensor(12416.7764, grad_fn=<NegBackward0>) tensor(12416.7705, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12416.765625
tensor(12416.7705, grad_fn=<NegBackward0>) tensor(12416.7656, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12416.7607421875
tensor(12416.7656, grad_fn=<NegBackward0>) tensor(12416.7607, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12416.759765625
tensor(12416.7607, grad_fn=<NegBackward0>) tensor(12416.7598, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12416.75390625
tensor(12416.7598, grad_fn=<NegBackward0>) tensor(12416.7539, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12416.7548828125
tensor(12416.7539, grad_fn=<NegBackward0>) tensor(12416.7549, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12416.751953125
tensor(12416.7539, grad_fn=<NegBackward0>) tensor(12416.7520, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12416.75
tensor(12416.7520, grad_fn=<NegBackward0>) tensor(12416.7500, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12416.75
tensor(12416.7500, grad_fn=<NegBackward0>) tensor(12416.7500, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12416.7470703125
tensor(12416.7500, grad_fn=<NegBackward0>) tensor(12416.7471, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12416.7470703125
tensor(12416.7471, grad_fn=<NegBackward0>) tensor(12416.7471, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12416.7470703125
tensor(12416.7471, grad_fn=<NegBackward0>) tensor(12416.7471, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12416.744140625
tensor(12416.7471, grad_fn=<NegBackward0>) tensor(12416.7441, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12416.7451171875
tensor(12416.7441, grad_fn=<NegBackward0>) tensor(12416.7451, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12416.744140625
tensor(12416.7441, grad_fn=<NegBackward0>) tensor(12416.7441, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12416.7421875
tensor(12416.7441, grad_fn=<NegBackward0>) tensor(12416.7422, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12416.7431640625
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7432, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12416.7421875
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7422, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12416.7412109375
tensor(12416.7422, grad_fn=<NegBackward0>) tensor(12416.7412, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12416.7412109375
tensor(12416.7412, grad_fn=<NegBackward0>) tensor(12416.7412, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12416.7412109375
tensor(12416.7412, grad_fn=<NegBackward0>) tensor(12416.7412, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12416.7412109375
tensor(12416.7412, grad_fn=<NegBackward0>) tensor(12416.7412, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12416.7412109375
tensor(12416.7412, grad_fn=<NegBackward0>) tensor(12416.7412, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12416.7392578125
tensor(12416.7412, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12416.7392578125
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12416.740234375
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7402, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12416.73828125
tensor(12416.7393, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12416.7392578125
tensor(12416.7383, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12416.7392578125
tensor(12416.7383, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12416.73828125
tensor(12416.7383, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12416.7392578125
tensor(12416.7383, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12416.740234375
tensor(12416.7383, grad_fn=<NegBackward0>) tensor(12416.7402, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12416.7373046875
tensor(12416.7383, grad_fn=<NegBackward0>) tensor(12416.7373, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12416.7392578125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12416.73828125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12416.73828125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7383, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12416.7392578125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7393, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -12416.751953125
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.7520, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7300 due to no improvement.
pi: tensor([[5.7470e-04, 9.9943e-01],
        [1.6757e-02, 9.8324e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0184, 0.9816], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2920, 0.1093],
         [0.6271, 0.2003]],

        [[0.5923, 0.3068],
         [0.6051, 0.6112]],

        [[0.6568, 0.2707],
         [0.5649, 0.5115]],

        [[0.5664, 0.1235],
         [0.5758, 0.6435]],

        [[0.6034, 0.2142],
         [0.6115, 0.6064]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00032696474682979756
Average Adjusted Rand Index: -0.00021432494651782533
[0.00032696474682979756, 0.00032696474682979756] [-0.00021432494651782533, -0.00021432494651782533] [12416.73828125, 12416.751953125]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11876.015418513682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21685.6640625
inf tensor(21685.6641, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.3193359375
tensor(21685.6641, grad_fn=<NegBackward0>) tensor(12356.3193, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12355.9580078125
tensor(12356.3193, grad_fn=<NegBackward0>) tensor(12355.9580, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12355.8994140625
tensor(12355.9580, grad_fn=<NegBackward0>) tensor(12355.8994, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12355.8623046875
tensor(12355.8994, grad_fn=<NegBackward0>) tensor(12355.8623, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12355.833984375
tensor(12355.8623, grad_fn=<NegBackward0>) tensor(12355.8340, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12355.8115234375
tensor(12355.8340, grad_fn=<NegBackward0>) tensor(12355.8115, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12355.794921875
tensor(12355.8115, grad_fn=<NegBackward0>) tensor(12355.7949, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12355.779296875
tensor(12355.7949, grad_fn=<NegBackward0>) tensor(12355.7793, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12355.7685546875
tensor(12355.7793, grad_fn=<NegBackward0>) tensor(12355.7686, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12355.7607421875
tensor(12355.7686, grad_fn=<NegBackward0>) tensor(12355.7607, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12355.751953125
tensor(12355.7607, grad_fn=<NegBackward0>) tensor(12355.7520, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12355.7431640625
tensor(12355.7520, grad_fn=<NegBackward0>) tensor(12355.7432, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12355.734375
tensor(12355.7432, grad_fn=<NegBackward0>) tensor(12355.7344, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12355.7255859375
tensor(12355.7344, grad_fn=<NegBackward0>) tensor(12355.7256, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12355.71484375
tensor(12355.7256, grad_fn=<NegBackward0>) tensor(12355.7148, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12355.701171875
tensor(12355.7148, grad_fn=<NegBackward0>) tensor(12355.7012, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12355.6845703125
tensor(12355.7012, grad_fn=<NegBackward0>) tensor(12355.6846, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12355.6591796875
tensor(12355.6846, grad_fn=<NegBackward0>) tensor(12355.6592, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12355.6201171875
tensor(12355.6592, grad_fn=<NegBackward0>) tensor(12355.6201, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12355.5654296875
tensor(12355.6201, grad_fn=<NegBackward0>) tensor(12355.5654, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12355.5078125
tensor(12355.5654, grad_fn=<NegBackward0>) tensor(12355.5078, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12355.455078125
tensor(12355.5078, grad_fn=<NegBackward0>) tensor(12355.4551, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12355.3974609375
tensor(12355.4551, grad_fn=<NegBackward0>) tensor(12355.3975, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12355.3486328125
tensor(12355.3975, grad_fn=<NegBackward0>) tensor(12355.3486, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12355.30859375
tensor(12355.3486, grad_fn=<NegBackward0>) tensor(12355.3086, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12355.279296875
tensor(12355.3086, grad_fn=<NegBackward0>) tensor(12355.2793, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12355.263671875
tensor(12355.2793, grad_fn=<NegBackward0>) tensor(12355.2637, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12355.25390625
tensor(12355.2637, grad_fn=<NegBackward0>) tensor(12355.2539, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12355.2490234375
tensor(12355.2539, grad_fn=<NegBackward0>) tensor(12355.2490, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12355.24609375
tensor(12355.2490, grad_fn=<NegBackward0>) tensor(12355.2461, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12355.2431640625
tensor(12355.2461, grad_fn=<NegBackward0>) tensor(12355.2432, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12355.2392578125
tensor(12355.2432, grad_fn=<NegBackward0>) tensor(12355.2393, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12355.240234375
tensor(12355.2393, grad_fn=<NegBackward0>) tensor(12355.2402, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12355.2353515625
tensor(12355.2393, grad_fn=<NegBackward0>) tensor(12355.2354, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12355.2353515625
tensor(12355.2354, grad_fn=<NegBackward0>) tensor(12355.2354, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12355.234375
tensor(12355.2354, grad_fn=<NegBackward0>) tensor(12355.2344, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12355.2314453125
tensor(12355.2344, grad_fn=<NegBackward0>) tensor(12355.2314, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12355.2294921875
tensor(12355.2314, grad_fn=<NegBackward0>) tensor(12355.2295, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12355.228515625
tensor(12355.2295, grad_fn=<NegBackward0>) tensor(12355.2285, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12355.2265625
tensor(12355.2285, grad_fn=<NegBackward0>) tensor(12355.2266, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12355.224609375
tensor(12355.2266, grad_fn=<NegBackward0>) tensor(12355.2246, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12355.2236328125
tensor(12355.2246, grad_fn=<NegBackward0>) tensor(12355.2236, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12355.2216796875
tensor(12355.2236, grad_fn=<NegBackward0>) tensor(12355.2217, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12355.220703125
tensor(12355.2217, grad_fn=<NegBackward0>) tensor(12355.2207, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12355.2177734375
tensor(12355.2207, grad_fn=<NegBackward0>) tensor(12355.2178, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12355.2158203125
tensor(12355.2178, grad_fn=<NegBackward0>) tensor(12355.2158, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12355.212890625
tensor(12355.2158, grad_fn=<NegBackward0>) tensor(12355.2129, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12355.212890625
tensor(12355.2129, grad_fn=<NegBackward0>) tensor(12355.2129, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12355.2109375
tensor(12355.2129, grad_fn=<NegBackward0>) tensor(12355.2109, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12355.20703125
tensor(12355.2109, grad_fn=<NegBackward0>) tensor(12355.2070, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12355.20703125
tensor(12355.2070, grad_fn=<NegBackward0>) tensor(12355.2070, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12355.205078125
tensor(12355.2070, grad_fn=<NegBackward0>) tensor(12355.2051, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12355.203125
tensor(12355.2051, grad_fn=<NegBackward0>) tensor(12355.2031, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12355.201171875
tensor(12355.2031, grad_fn=<NegBackward0>) tensor(12355.2012, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12355.2001953125
tensor(12355.2012, grad_fn=<NegBackward0>) tensor(12355.2002, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12355.1982421875
tensor(12355.2002, grad_fn=<NegBackward0>) tensor(12355.1982, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12355.197265625
tensor(12355.1982, grad_fn=<NegBackward0>) tensor(12355.1973, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12355.1962890625
tensor(12355.1973, grad_fn=<NegBackward0>) tensor(12355.1963, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12355.1953125
tensor(12355.1963, grad_fn=<NegBackward0>) tensor(12355.1953, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12355.1943359375
tensor(12355.1953, grad_fn=<NegBackward0>) tensor(12355.1943, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12355.193359375
tensor(12355.1943, grad_fn=<NegBackward0>) tensor(12355.1934, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12355.19140625
tensor(12355.1934, grad_fn=<NegBackward0>) tensor(12355.1914, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12355.193359375
tensor(12355.1914, grad_fn=<NegBackward0>) tensor(12355.1934, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12355.1923828125
tensor(12355.1914, grad_fn=<NegBackward0>) tensor(12355.1924, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12355.193359375
tensor(12355.1914, grad_fn=<NegBackward0>) tensor(12355.1934, grad_fn=<NegBackward0>)
3
Iteration 6600: Loss = -12355.1904296875
tensor(12355.1914, grad_fn=<NegBackward0>) tensor(12355.1904, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12355.1923828125
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1924, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12355.1904296875
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1904, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12355.19140625
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1914, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12355.19140625
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1914, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12355.1904296875
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1904, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12355.189453125
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1895, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12355.22265625
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.2227, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12355.21484375
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.2148, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12355.3466796875
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.3467, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12355.189453125
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.1895, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12355.19140625
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.1914, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12355.189453125
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.1895, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12355.2353515625
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.2354, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12355.1884765625
tensor(12355.1895, grad_fn=<NegBackward0>) tensor(12355.1885, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12355.189453125
tensor(12355.1885, grad_fn=<NegBackward0>) tensor(12355.1895, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12355.564453125
tensor(12355.1885, grad_fn=<NegBackward0>) tensor(12355.5645, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12355.1875
tensor(12355.1885, grad_fn=<NegBackward0>) tensor(12355.1875, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12355.1884765625
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.1885, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12355.318359375
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.3184, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12355.208984375
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.2090, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -12355.189453125
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.1895, grad_fn=<NegBackward0>)
4
Iteration 8800: Loss = -12355.1884765625
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.1885, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.9756, 0.0244],
        [0.9786, 0.0214]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0358, 0.9642], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1968, 0.2020],
         [0.5053, 0.2036]],

        [[0.5393, 0.2699],
         [0.6376, 0.5707]],

        [[0.6512, 0.1930],
         [0.6145, 0.5179]],

        [[0.5578, 0.2360],
         [0.6635, 0.5853]],

        [[0.6852, 0.2091],
         [0.6231, 0.6663]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016034899259868422
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25431.080078125
inf tensor(25431.0801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.416015625
tensor(25431.0801, grad_fn=<NegBackward0>) tensor(12356.4160, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12355.9794921875
tensor(12356.4160, grad_fn=<NegBackward0>) tensor(12355.9795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12355.8779296875
tensor(12355.9795, grad_fn=<NegBackward0>) tensor(12355.8779, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12355.83203125
tensor(12355.8779, grad_fn=<NegBackward0>) tensor(12355.8320, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12355.8046875
tensor(12355.8320, grad_fn=<NegBackward0>) tensor(12355.8047, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12355.787109375
tensor(12355.8047, grad_fn=<NegBackward0>) tensor(12355.7871, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12355.7724609375
tensor(12355.7871, grad_fn=<NegBackward0>) tensor(12355.7725, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12355.7607421875
tensor(12355.7725, grad_fn=<NegBackward0>) tensor(12355.7607, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12355.7490234375
tensor(12355.7607, grad_fn=<NegBackward0>) tensor(12355.7490, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12355.7353515625
tensor(12355.7490, grad_fn=<NegBackward0>) tensor(12355.7354, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12355.7197265625
tensor(12355.7354, grad_fn=<NegBackward0>) tensor(12355.7197, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12355.689453125
tensor(12355.7197, grad_fn=<NegBackward0>) tensor(12355.6895, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12355.623046875
tensor(12355.6895, grad_fn=<NegBackward0>) tensor(12355.6230, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12355.5146484375
tensor(12355.6230, grad_fn=<NegBackward0>) tensor(12355.5146, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12355.4443359375
tensor(12355.5146, grad_fn=<NegBackward0>) tensor(12355.4443, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12355.39453125
tensor(12355.4443, grad_fn=<NegBackward0>) tensor(12355.3945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12355.310546875
tensor(12355.3945, grad_fn=<NegBackward0>) tensor(12355.3105, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12355.1259765625
tensor(12355.3105, grad_fn=<NegBackward0>) tensor(12355.1260, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12354.92578125
tensor(12355.1260, grad_fn=<NegBackward0>) tensor(12354.9258, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12354.7900390625
tensor(12354.9258, grad_fn=<NegBackward0>) tensor(12354.7900, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12354.7109375
tensor(12354.7900, grad_fn=<NegBackward0>) tensor(12354.7109, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12354.66015625
tensor(12354.7109, grad_fn=<NegBackward0>) tensor(12354.6602, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12354.626953125
tensor(12354.6602, grad_fn=<NegBackward0>) tensor(12354.6270, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12354.6064453125
tensor(12354.6270, grad_fn=<NegBackward0>) tensor(12354.6064, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12354.5888671875
tensor(12354.6064, grad_fn=<NegBackward0>) tensor(12354.5889, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12354.5791015625
tensor(12354.5889, grad_fn=<NegBackward0>) tensor(12354.5791, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12354.5673828125
tensor(12354.5791, grad_fn=<NegBackward0>) tensor(12354.5674, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12354.5595703125
tensor(12354.5674, grad_fn=<NegBackward0>) tensor(12354.5596, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12354.5537109375
tensor(12354.5596, grad_fn=<NegBackward0>) tensor(12354.5537, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12354.5458984375
tensor(12354.5537, grad_fn=<NegBackward0>) tensor(12354.5459, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12354.5419921875
tensor(12354.5459, grad_fn=<NegBackward0>) tensor(12354.5420, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12354.537109375
tensor(12354.5420, grad_fn=<NegBackward0>) tensor(12354.5371, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12354.5322265625
tensor(12354.5371, grad_fn=<NegBackward0>) tensor(12354.5322, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12354.5302734375
tensor(12354.5322, grad_fn=<NegBackward0>) tensor(12354.5303, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12354.5283203125
tensor(12354.5303, grad_fn=<NegBackward0>) tensor(12354.5283, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12354.5244140625
tensor(12354.5283, grad_fn=<NegBackward0>) tensor(12354.5244, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12354.5244140625
tensor(12354.5244, grad_fn=<NegBackward0>) tensor(12354.5244, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12354.5234375
tensor(12354.5244, grad_fn=<NegBackward0>) tensor(12354.5234, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12354.5224609375
tensor(12354.5234, grad_fn=<NegBackward0>) tensor(12354.5225, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12354.51953125
tensor(12354.5225, grad_fn=<NegBackward0>) tensor(12354.5195, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12354.51953125
tensor(12354.5195, grad_fn=<NegBackward0>) tensor(12354.5195, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12354.5185546875
tensor(12354.5195, grad_fn=<NegBackward0>) tensor(12354.5186, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12354.5166015625
tensor(12354.5186, grad_fn=<NegBackward0>) tensor(12354.5166, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12354.5166015625
tensor(12354.5166, grad_fn=<NegBackward0>) tensor(12354.5166, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12354.517578125
tensor(12354.5166, grad_fn=<NegBackward0>) tensor(12354.5176, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12354.5166015625
tensor(12354.5166, grad_fn=<NegBackward0>) tensor(12354.5166, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12354.515625
tensor(12354.5166, grad_fn=<NegBackward0>) tensor(12354.5156, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12354.515625
tensor(12354.5156, grad_fn=<NegBackward0>) tensor(12354.5156, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12354.5146484375
tensor(12354.5156, grad_fn=<NegBackward0>) tensor(12354.5146, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12354.513671875
tensor(12354.5146, grad_fn=<NegBackward0>) tensor(12354.5137, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12354.513671875
tensor(12354.5137, grad_fn=<NegBackward0>) tensor(12354.5137, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12354.513671875
tensor(12354.5137, grad_fn=<NegBackward0>) tensor(12354.5137, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12354.51171875
tensor(12354.5137, grad_fn=<NegBackward0>) tensor(12354.5117, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12354.51171875
tensor(12354.5117, grad_fn=<NegBackward0>) tensor(12354.5117, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12354.5107421875
tensor(12354.5117, grad_fn=<NegBackward0>) tensor(12354.5107, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12354.51171875
tensor(12354.5107, grad_fn=<NegBackward0>) tensor(12354.5117, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12354.5107421875
tensor(12354.5107, grad_fn=<NegBackward0>) tensor(12354.5107, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12354.5107421875
tensor(12354.5107, grad_fn=<NegBackward0>) tensor(12354.5107, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12354.5107421875
tensor(12354.5107, grad_fn=<NegBackward0>) tensor(12354.5107, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12354.509765625
tensor(12354.5107, grad_fn=<NegBackward0>) tensor(12354.5098, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12354.5107421875
tensor(12354.5098, grad_fn=<NegBackward0>) tensor(12354.5107, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12354.509765625
tensor(12354.5098, grad_fn=<NegBackward0>) tensor(12354.5098, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12354.509765625
tensor(12354.5098, grad_fn=<NegBackward0>) tensor(12354.5098, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12354.5087890625
tensor(12354.5098, grad_fn=<NegBackward0>) tensor(12354.5088, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12354.5068359375
tensor(12354.5088, grad_fn=<NegBackward0>) tensor(12354.5068, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12354.5087890625
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5088, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -12354.5068359375
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5068, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -12354.5078125
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5078, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -12354.5068359375
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5068, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12354.505859375
tensor(12354.5068, grad_fn=<NegBackward0>) tensor(12354.5059, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12354.5068359375
tensor(12354.5059, grad_fn=<NegBackward0>) tensor(12354.5068, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12354.5048828125
tensor(12354.5059, grad_fn=<NegBackward0>) tensor(12354.5049, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12354.5087890625
tensor(12354.5049, grad_fn=<NegBackward0>) tensor(12354.5088, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12354.50390625
tensor(12354.5049, grad_fn=<NegBackward0>) tensor(12354.5039, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12354.5048828125
tensor(12354.5039, grad_fn=<NegBackward0>) tensor(12354.5049, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12354.5048828125
tensor(12354.5039, grad_fn=<NegBackward0>) tensor(12354.5049, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12354.505859375
tensor(12354.5039, grad_fn=<NegBackward0>) tensor(12354.5059, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12354.5048828125
tensor(12354.5039, grad_fn=<NegBackward0>) tensor(12354.5049, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12354.8447265625
tensor(12354.5039, grad_fn=<NegBackward0>) tensor(12354.8447, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.9970e-01, 3.0022e-04],
        [1.8009e-05, 9.9998e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0158, 0.9842], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2009, 0.2631],
         [0.5586, 0.1971]],

        [[0.6099, 0.1202],
         [0.7108, 0.5806]],

        [[0.6449, 0.2179],
         [0.5538, 0.5239]],

        [[0.6439, 0.1643],
         [0.5773, 0.6516]],

        [[0.6016, 0.1823],
         [0.6407, 0.7069]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: 0.0003206463799380193
Average Adjusted Rand Index: 0.002834124272196674
[-0.0016034899259868422, 0.0003206463799380193] [0.0, 0.002834124272196674] [12355.1884765625, 12354.8447265625]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11876.389298284359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23433.095703125
inf tensor(23433.0957, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12373.1611328125
tensor(23433.0957, grad_fn=<NegBackward0>) tensor(12373.1611, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12372.66015625
tensor(12373.1611, grad_fn=<NegBackward0>) tensor(12372.6602, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12372.5517578125
tensor(12372.6602, grad_fn=<NegBackward0>) tensor(12372.5518, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12372.4736328125
tensor(12372.5518, grad_fn=<NegBackward0>) tensor(12372.4736, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12372.392578125
tensor(12372.4736, grad_fn=<NegBackward0>) tensor(12372.3926, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12372.296875
tensor(12372.3926, grad_fn=<NegBackward0>) tensor(12372.2969, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12372.1806640625
tensor(12372.2969, grad_fn=<NegBackward0>) tensor(12372.1807, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12372.0771484375
tensor(12372.1807, grad_fn=<NegBackward0>) tensor(12372.0771, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12372.0185546875
tensor(12372.0771, grad_fn=<NegBackward0>) tensor(12372.0186, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12371.986328125
tensor(12372.0186, grad_fn=<NegBackward0>) tensor(12371.9863, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12371.9599609375
tensor(12371.9863, grad_fn=<NegBackward0>) tensor(12371.9600, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12371.9423828125
tensor(12371.9600, grad_fn=<NegBackward0>) tensor(12371.9424, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12371.9287109375
tensor(12371.9424, grad_fn=<NegBackward0>) tensor(12371.9287, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12371.9140625
tensor(12371.9287, grad_fn=<NegBackward0>) tensor(12371.9141, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12371.904296875
tensor(12371.9141, grad_fn=<NegBackward0>) tensor(12371.9043, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12371.89453125
tensor(12371.9043, grad_fn=<NegBackward0>) tensor(12371.8945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12371.8896484375
tensor(12371.8945, grad_fn=<NegBackward0>) tensor(12371.8896, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12371.8828125
tensor(12371.8896, grad_fn=<NegBackward0>) tensor(12371.8828, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12371.8779296875
tensor(12371.8828, grad_fn=<NegBackward0>) tensor(12371.8779, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12371.875
tensor(12371.8779, grad_fn=<NegBackward0>) tensor(12371.8750, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12371.873046875
tensor(12371.8750, grad_fn=<NegBackward0>) tensor(12371.8730, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12371.8701171875
tensor(12371.8730, grad_fn=<NegBackward0>) tensor(12371.8701, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12371.8681640625
tensor(12371.8701, grad_fn=<NegBackward0>) tensor(12371.8682, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12371.8671875
tensor(12371.8682, grad_fn=<NegBackward0>) tensor(12371.8672, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12371.8662109375
tensor(12371.8672, grad_fn=<NegBackward0>) tensor(12371.8662, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12371.865234375
tensor(12371.8662, grad_fn=<NegBackward0>) tensor(12371.8652, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12371.861328125
tensor(12371.8652, grad_fn=<NegBackward0>) tensor(12371.8613, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12371.8583984375
tensor(12371.8613, grad_fn=<NegBackward0>) tensor(12371.8584, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12371.85546875
tensor(12371.8584, grad_fn=<NegBackward0>) tensor(12371.8555, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12371.8515625
tensor(12371.8555, grad_fn=<NegBackward0>) tensor(12371.8516, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12371.8466796875
tensor(12371.8516, grad_fn=<NegBackward0>) tensor(12371.8467, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12371.8349609375
tensor(12371.8467, grad_fn=<NegBackward0>) tensor(12371.8350, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12371.833984375
tensor(12371.8350, grad_fn=<NegBackward0>) tensor(12371.8340, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12371.7666015625
tensor(12371.8340, grad_fn=<NegBackward0>) tensor(12371.7666, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12371.638671875
tensor(12371.7666, grad_fn=<NegBackward0>) tensor(12371.6387, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12371.3701171875
tensor(12371.6387, grad_fn=<NegBackward0>) tensor(12371.3701, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12371.2783203125
tensor(12371.3701, grad_fn=<NegBackward0>) tensor(12371.2783, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12371.25390625
tensor(12371.2783, grad_fn=<NegBackward0>) tensor(12371.2539, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12371.2509765625
tensor(12371.2539, grad_fn=<NegBackward0>) tensor(12371.2510, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12371.234375
tensor(12371.2510, grad_fn=<NegBackward0>) tensor(12371.2344, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12371.228515625
tensor(12371.2344, grad_fn=<NegBackward0>) tensor(12371.2285, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12371.2255859375
tensor(12371.2285, grad_fn=<NegBackward0>) tensor(12371.2256, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12371.2216796875
tensor(12371.2256, grad_fn=<NegBackward0>) tensor(12371.2217, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12371.2197265625
tensor(12371.2217, grad_fn=<NegBackward0>) tensor(12371.2197, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12371.216796875
tensor(12371.2197, grad_fn=<NegBackward0>) tensor(12371.2168, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12371.2109375
tensor(12371.2168, grad_fn=<NegBackward0>) tensor(12371.2109, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12371.2041015625
tensor(12371.2109, grad_fn=<NegBackward0>) tensor(12371.2041, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12371.1806640625
tensor(12371.2041, grad_fn=<NegBackward0>) tensor(12371.1807, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12370.958984375
tensor(12371.1807, grad_fn=<NegBackward0>) tensor(12370.9590, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12370.5654296875
tensor(12370.9590, grad_fn=<NegBackward0>) tensor(12370.5654, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12370.53515625
tensor(12370.5654, grad_fn=<NegBackward0>) tensor(12370.5352, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12370.52734375
tensor(12370.5352, grad_fn=<NegBackward0>) tensor(12370.5273, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12370.5224609375
tensor(12370.5273, grad_fn=<NegBackward0>) tensor(12370.5225, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12370.51953125
tensor(12370.5225, grad_fn=<NegBackward0>) tensor(12370.5195, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12370.5185546875
tensor(12370.5195, grad_fn=<NegBackward0>) tensor(12370.5186, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12370.5166015625
tensor(12370.5186, grad_fn=<NegBackward0>) tensor(12370.5166, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12370.5166015625
tensor(12370.5166, grad_fn=<NegBackward0>) tensor(12370.5166, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12370.5166015625
tensor(12370.5166, grad_fn=<NegBackward0>) tensor(12370.5166, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12370.5146484375
tensor(12370.5166, grad_fn=<NegBackward0>) tensor(12370.5146, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12370.513671875
tensor(12370.5146, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12370.513671875
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12370.513671875
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12370.513671875
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12370.513671875
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12370.5146484375
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5146, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12370.53515625
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5352, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12370.513671875
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12370.515625
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5156, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12370.513671875
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12370.5126953125
tensor(12370.5137, grad_fn=<NegBackward0>) tensor(12370.5127, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12370.6142578125
tensor(12370.5127, grad_fn=<NegBackward0>) tensor(12370.6143, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12370.51171875
tensor(12370.5127, grad_fn=<NegBackward0>) tensor(12370.5117, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12370.5146484375
tensor(12370.5117, grad_fn=<NegBackward0>) tensor(12370.5146, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12370.5126953125
tensor(12370.5117, grad_fn=<NegBackward0>) tensor(12370.5127, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12370.5244140625
tensor(12370.5117, grad_fn=<NegBackward0>) tensor(12370.5244, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12370.5126953125
tensor(12370.5117, grad_fn=<NegBackward0>) tensor(12370.5127, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -12370.513671875
tensor(12370.5117, grad_fn=<NegBackward0>) tensor(12370.5137, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[9.1555e-01, 8.4454e-02],
        [1.1195e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3638, 0.6362], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1744, 0.1893],
         [0.5659, 0.2120]],

        [[0.6842, 0.1861],
         [0.5240, 0.6856]],

        [[0.6887, 0.1890],
         [0.7101, 0.6362]],

        [[0.6030, 0.2000],
         [0.5319, 0.6962]],

        [[0.5300, 0.1937],
         [0.5991, 0.6794]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.007230534351145038
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005710847386602341
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.004800028106294493
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.020755256732954187
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.016557637579923866
Global Adjusted Rand Index: 0.003510721557713619
Average Adjusted Rand Index: 0.003914296893767237
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21842.55078125
inf tensor(21842.5508, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12034.4599609375
tensor(21842.5508, grad_fn=<NegBackward0>) tensor(12034.4600, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11887.494140625
tensor(12034.4600, grad_fn=<NegBackward0>) tensor(11887.4941, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11886.8056640625
tensor(11887.4941, grad_fn=<NegBackward0>) tensor(11886.8057, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11886.689453125
tensor(11886.8057, grad_fn=<NegBackward0>) tensor(11886.6895, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11886.6318359375
tensor(11886.6895, grad_fn=<NegBackward0>) tensor(11886.6318, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11886.5966796875
tensor(11886.6318, grad_fn=<NegBackward0>) tensor(11886.5967, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11886.57421875
tensor(11886.5967, grad_fn=<NegBackward0>) tensor(11886.5742, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11886.560546875
tensor(11886.5742, grad_fn=<NegBackward0>) tensor(11886.5605, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11886.5478515625
tensor(11886.5605, grad_fn=<NegBackward0>) tensor(11886.5479, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11886.5400390625
tensor(11886.5479, grad_fn=<NegBackward0>) tensor(11886.5400, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11886.533203125
tensor(11886.5400, grad_fn=<NegBackward0>) tensor(11886.5332, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11886.5283203125
tensor(11886.5332, grad_fn=<NegBackward0>) tensor(11886.5283, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11886.5234375
tensor(11886.5283, grad_fn=<NegBackward0>) tensor(11886.5234, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11886.521484375
tensor(11886.5234, grad_fn=<NegBackward0>) tensor(11886.5215, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11886.5185546875
tensor(11886.5215, grad_fn=<NegBackward0>) tensor(11886.5186, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11886.5166015625
tensor(11886.5186, grad_fn=<NegBackward0>) tensor(11886.5166, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11886.5126953125
tensor(11886.5166, grad_fn=<NegBackward0>) tensor(11886.5127, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11886.458984375
tensor(11886.5127, grad_fn=<NegBackward0>) tensor(11886.4590, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11886.3984375
tensor(11886.4590, grad_fn=<NegBackward0>) tensor(11886.3984, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11886.3984375
tensor(11886.3984, grad_fn=<NegBackward0>) tensor(11886.3984, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11886.396484375
tensor(11886.3984, grad_fn=<NegBackward0>) tensor(11886.3965, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11886.39453125
tensor(11886.3965, grad_fn=<NegBackward0>) tensor(11886.3945, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11886.3955078125
tensor(11886.3945, grad_fn=<NegBackward0>) tensor(11886.3955, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11886.3935546875
tensor(11886.3945, grad_fn=<NegBackward0>) tensor(11886.3936, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11886.392578125
tensor(11886.3936, grad_fn=<NegBackward0>) tensor(11886.3926, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11886.392578125
tensor(11886.3926, grad_fn=<NegBackward0>) tensor(11886.3926, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11886.392578125
tensor(11886.3926, grad_fn=<NegBackward0>) tensor(11886.3926, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11886.390625
tensor(11886.3926, grad_fn=<NegBackward0>) tensor(11886.3906, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11886.390625
tensor(11886.3906, grad_fn=<NegBackward0>) tensor(11886.3906, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11886.390625
tensor(11886.3906, grad_fn=<NegBackward0>) tensor(11886.3906, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11886.390625
tensor(11886.3906, grad_fn=<NegBackward0>) tensor(11886.3906, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11886.3896484375
tensor(11886.3906, grad_fn=<NegBackward0>) tensor(11886.3896, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11886.390625
tensor(11886.3896, grad_fn=<NegBackward0>) tensor(11886.3906, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -11886.3974609375
tensor(11886.3896, grad_fn=<NegBackward0>) tensor(11886.3975, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -11886.3896484375
tensor(11886.3896, grad_fn=<NegBackward0>) tensor(11886.3896, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11886.3994140625
tensor(11886.3896, grad_fn=<NegBackward0>) tensor(11886.3994, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11886.3916015625
tensor(11886.3896, grad_fn=<NegBackward0>) tensor(11886.3916, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -11886.38671875
tensor(11886.3896, grad_fn=<NegBackward0>) tensor(11886.3867, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11886.384765625
tensor(11886.3867, grad_fn=<NegBackward0>) tensor(11886.3848, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11886.38671875
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3867, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11886.38671875
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3867, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -11886.384765625
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3848, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11886.38671875
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3867, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11886.3896484375
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3896, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -11886.3857421875
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3857, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -11886.3857421875
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3857, grad_fn=<NegBackward0>)
4
Iteration 4700: Loss = -11886.3974609375
tensor(11886.3848, grad_fn=<NegBackward0>) tensor(11886.3975, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4700 due to no improvement.
pi: tensor([[0.6701, 0.3299],
        [0.4057, 0.5943]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4920, 0.5080], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2967, 0.0988],
         [0.5532, 0.3032]],

        [[0.7244, 0.0937],
         [0.5543, 0.5598]],

        [[0.7256, 0.0925],
         [0.6975, 0.5580]],

        [[0.5132, 0.1124],
         [0.6063, 0.5240]],

        [[0.6412, 0.1007],
         [0.6715, 0.6614]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 1
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
Global Adjusted Rand Index: 0.03969396808424886
Average Adjusted Rand Index: 0.9288004080126807
[0.003510721557713619, 0.03969396808424886] [0.003914296893767237, 0.9288004080126807] [12370.513671875, 11886.3974609375]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11889.313311123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19964.986328125
inf tensor(19964.9863, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12414.36328125
tensor(19964.9863, grad_fn=<NegBackward0>) tensor(12414.3633, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12413.9560546875
tensor(12414.3633, grad_fn=<NegBackward0>) tensor(12413.9561, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12413.7880859375
tensor(12413.9561, grad_fn=<NegBackward0>) tensor(12413.7881, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12413.4521484375
tensor(12413.7881, grad_fn=<NegBackward0>) tensor(12413.4521, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12412.0966796875
tensor(12413.4521, grad_fn=<NegBackward0>) tensor(12412.0967, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12411.8505859375
tensor(12412.0967, grad_fn=<NegBackward0>) tensor(12411.8506, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12411.796875
tensor(12411.8506, grad_fn=<NegBackward0>) tensor(12411.7969, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12411.767578125
tensor(12411.7969, grad_fn=<NegBackward0>) tensor(12411.7676, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12411.7470703125
tensor(12411.7676, grad_fn=<NegBackward0>) tensor(12411.7471, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12411.7294921875
tensor(12411.7471, grad_fn=<NegBackward0>) tensor(12411.7295, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12411.7158203125
tensor(12411.7295, grad_fn=<NegBackward0>) tensor(12411.7158, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12411.705078125
tensor(12411.7158, grad_fn=<NegBackward0>) tensor(12411.7051, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12411.6923828125
tensor(12411.7051, grad_fn=<NegBackward0>) tensor(12411.6924, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12411.6796875
tensor(12411.6924, grad_fn=<NegBackward0>) tensor(12411.6797, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12411.6650390625
tensor(12411.6797, grad_fn=<NegBackward0>) tensor(12411.6650, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12411.64453125
tensor(12411.6650, grad_fn=<NegBackward0>) tensor(12411.6445, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12411.626953125
tensor(12411.6445, grad_fn=<NegBackward0>) tensor(12411.6270, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12411.6181640625
tensor(12411.6270, grad_fn=<NegBackward0>) tensor(12411.6182, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12411.61328125
tensor(12411.6182, grad_fn=<NegBackward0>) tensor(12411.6133, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12411.6083984375
tensor(12411.6133, grad_fn=<NegBackward0>) tensor(12411.6084, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12411.6064453125
tensor(12411.6084, grad_fn=<NegBackward0>) tensor(12411.6064, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12411.6044921875
tensor(12411.6064, grad_fn=<NegBackward0>) tensor(12411.6045, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12411.6015625
tensor(12411.6045, grad_fn=<NegBackward0>) tensor(12411.6016, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12411.599609375
tensor(12411.6016, grad_fn=<NegBackward0>) tensor(12411.5996, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12411.5966796875
tensor(12411.5996, grad_fn=<NegBackward0>) tensor(12411.5967, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12411.5947265625
tensor(12411.5967, grad_fn=<NegBackward0>) tensor(12411.5947, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12411.59375
tensor(12411.5947, grad_fn=<NegBackward0>) tensor(12411.5938, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12411.5947265625
tensor(12411.5938, grad_fn=<NegBackward0>) tensor(12411.5947, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12411.5908203125
tensor(12411.5938, grad_fn=<NegBackward0>) tensor(12411.5908, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12411.5908203125
tensor(12411.5908, grad_fn=<NegBackward0>) tensor(12411.5908, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12411.5888671875
tensor(12411.5908, grad_fn=<NegBackward0>) tensor(12411.5889, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12411.587890625
tensor(12411.5889, grad_fn=<NegBackward0>) tensor(12411.5879, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12411.5859375
tensor(12411.5879, grad_fn=<NegBackward0>) tensor(12411.5859, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12411.5869140625
tensor(12411.5859, grad_fn=<NegBackward0>) tensor(12411.5869, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12411.5869140625
tensor(12411.5859, grad_fn=<NegBackward0>) tensor(12411.5869, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -12411.5849609375
tensor(12411.5859, grad_fn=<NegBackward0>) tensor(12411.5850, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12411.5849609375
tensor(12411.5850, grad_fn=<NegBackward0>) tensor(12411.5850, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12411.583984375
tensor(12411.5850, grad_fn=<NegBackward0>) tensor(12411.5840, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12411.5830078125
tensor(12411.5840, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12411.5810546875
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5811, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12411.58203125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12411.5830078125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -12411.5830078125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -12411.5810546875
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5811, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12411.58203125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12411.5791015625
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12411.58203125
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12411.580078125
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5801, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12411.5810546875
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5811, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -12411.580078125
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5801, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -12411.5791015625
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12411.578125
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12411.580078125
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5801, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12411.5791015625
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12411.578125
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12411.576171875
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5762, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12411.578125
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12411.5791015625
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -12411.5791015625
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -12411.578125
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -12411.578125
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[6.0726e-04, 9.9939e-01],
        [7.3383e-02, 9.2662e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0297, 0.9703], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2557, 0.2340],
         [0.5485, 0.1972]],

        [[0.5269, 0.2441],
         [0.5510, 0.6924]],

        [[0.6037, 0.2626],
         [0.6269, 0.6796]],

        [[0.7275, 0.1888],
         [0.7075, 0.5792]],

        [[0.6688, 0.2068],
         [0.6191, 0.5839]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009303106223477708
Average Adjusted Rand Index: -0.0015521208230856337
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21179.138671875
inf tensor(21179.1387, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12414.7353515625
tensor(21179.1387, grad_fn=<NegBackward0>) tensor(12414.7354, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12414.1611328125
tensor(12414.7354, grad_fn=<NegBackward0>) tensor(12414.1611, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12413.9580078125
tensor(12414.1611, grad_fn=<NegBackward0>) tensor(12413.9580, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12413.7236328125
tensor(12413.9580, grad_fn=<NegBackward0>) tensor(12413.7236, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12412.9267578125
tensor(12413.7236, grad_fn=<NegBackward0>) tensor(12412.9268, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12412.287109375
tensor(12412.9268, grad_fn=<NegBackward0>) tensor(12412.2871, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12412.0693359375
tensor(12412.2871, grad_fn=<NegBackward0>) tensor(12412.0693, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12411.96484375
tensor(12412.0693, grad_fn=<NegBackward0>) tensor(12411.9648, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12411.8994140625
tensor(12411.9648, grad_fn=<NegBackward0>) tensor(12411.8994, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12411.853515625
tensor(12411.8994, grad_fn=<NegBackward0>) tensor(12411.8535, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12411.8173828125
tensor(12411.8535, grad_fn=<NegBackward0>) tensor(12411.8174, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12411.79296875
tensor(12411.8174, grad_fn=<NegBackward0>) tensor(12411.7930, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12411.7744140625
tensor(12411.7930, grad_fn=<NegBackward0>) tensor(12411.7744, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12411.7607421875
tensor(12411.7744, grad_fn=<NegBackward0>) tensor(12411.7607, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12411.7490234375
tensor(12411.7607, grad_fn=<NegBackward0>) tensor(12411.7490, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12411.7373046875
tensor(12411.7490, grad_fn=<NegBackward0>) tensor(12411.7373, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12411.73046875
tensor(12411.7373, grad_fn=<NegBackward0>) tensor(12411.7305, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12411.72265625
tensor(12411.7305, grad_fn=<NegBackward0>) tensor(12411.7227, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12411.7158203125
tensor(12411.7227, grad_fn=<NegBackward0>) tensor(12411.7158, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12411.7109375
tensor(12411.7158, grad_fn=<NegBackward0>) tensor(12411.7109, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12411.7060546875
tensor(12411.7109, grad_fn=<NegBackward0>) tensor(12411.7061, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12411.7001953125
tensor(12411.7061, grad_fn=<NegBackward0>) tensor(12411.7002, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12411.6953125
tensor(12411.7002, grad_fn=<NegBackward0>) tensor(12411.6953, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12411.6923828125
tensor(12411.6953, grad_fn=<NegBackward0>) tensor(12411.6924, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12411.6884765625
tensor(12411.6924, grad_fn=<NegBackward0>) tensor(12411.6885, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12411.685546875
tensor(12411.6885, grad_fn=<NegBackward0>) tensor(12411.6855, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12411.6806640625
tensor(12411.6855, grad_fn=<NegBackward0>) tensor(12411.6807, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12411.6787109375
tensor(12411.6807, grad_fn=<NegBackward0>) tensor(12411.6787, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12411.67578125
tensor(12411.6787, grad_fn=<NegBackward0>) tensor(12411.6758, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12411.6728515625
tensor(12411.6758, grad_fn=<NegBackward0>) tensor(12411.6729, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12411.669921875
tensor(12411.6729, grad_fn=<NegBackward0>) tensor(12411.6699, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12411.6669921875
tensor(12411.6699, grad_fn=<NegBackward0>) tensor(12411.6670, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12411.6591796875
tensor(12411.6670, grad_fn=<NegBackward0>) tensor(12411.6592, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12411.6513671875
tensor(12411.6592, grad_fn=<NegBackward0>) tensor(12411.6514, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12411.6337890625
tensor(12411.6514, grad_fn=<NegBackward0>) tensor(12411.6338, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12411.6142578125
tensor(12411.6338, grad_fn=<NegBackward0>) tensor(12411.6143, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12411.6083984375
tensor(12411.6143, grad_fn=<NegBackward0>) tensor(12411.6084, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12411.60546875
tensor(12411.6084, grad_fn=<NegBackward0>) tensor(12411.6055, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12411.6005859375
tensor(12411.6055, grad_fn=<NegBackward0>) tensor(12411.6006, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12411.6005859375
tensor(12411.6006, grad_fn=<NegBackward0>) tensor(12411.6006, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12411.599609375
tensor(12411.6006, grad_fn=<NegBackward0>) tensor(12411.5996, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12411.595703125
tensor(12411.5996, grad_fn=<NegBackward0>) tensor(12411.5957, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12411.5947265625
tensor(12411.5957, grad_fn=<NegBackward0>) tensor(12411.5947, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12411.59375
tensor(12411.5947, grad_fn=<NegBackward0>) tensor(12411.5938, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12411.591796875
tensor(12411.5938, grad_fn=<NegBackward0>) tensor(12411.5918, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12411.5927734375
tensor(12411.5918, grad_fn=<NegBackward0>) tensor(12411.5928, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12411.58984375
tensor(12411.5918, grad_fn=<NegBackward0>) tensor(12411.5898, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12411.58984375
tensor(12411.5898, grad_fn=<NegBackward0>) tensor(12411.5898, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12411.587890625
tensor(12411.5898, grad_fn=<NegBackward0>) tensor(12411.5879, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12411.5869140625
tensor(12411.5879, grad_fn=<NegBackward0>) tensor(12411.5869, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12411.5859375
tensor(12411.5869, grad_fn=<NegBackward0>) tensor(12411.5859, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12411.5859375
tensor(12411.5859, grad_fn=<NegBackward0>) tensor(12411.5859, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12411.5859375
tensor(12411.5859, grad_fn=<NegBackward0>) tensor(12411.5859, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12411.583984375
tensor(12411.5859, grad_fn=<NegBackward0>) tensor(12411.5840, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12411.5830078125
tensor(12411.5840, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12411.5830078125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12411.58203125
tensor(12411.5830, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12411.5830078125
tensor(12411.5820, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12411.58203125
tensor(12411.5820, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12411.5810546875
tensor(12411.5820, grad_fn=<NegBackward0>) tensor(12411.5811, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12411.58203125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12411.58203125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5820, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12411.5830078125
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5830, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12411.603515625
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.6035, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -12411.5810546875
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5811, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12411.5791015625
tensor(12411.5811, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12411.5947265625
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5947, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12411.5791015625
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12411.580078125
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5801, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12411.578125
tensor(12411.5791, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12411.5859375
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5859, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12411.578125
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12411.5791015625
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12411.5849609375
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5850, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12411.578125
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12411.5771484375
tensor(12411.5781, grad_fn=<NegBackward0>) tensor(12411.5771, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12411.578125
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12411.578125
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12411.5771484375
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5771, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12411.578125
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12411.578125
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12411.72265625
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.7227, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -12411.5771484375
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5771, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12411.5771484375
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5771, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12411.5791015625
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5791, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12411.578125
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12411.58984375
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5898, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12411.5771484375
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5771, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12411.576171875
tensor(12411.5771, grad_fn=<NegBackward0>) tensor(12411.5762, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12411.576171875
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5762, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12411.578125
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5781, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12411.6767578125
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.6768, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12411.576171875
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5762, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12411.5771484375
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5771, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12411.5888671875
tensor(12411.5762, grad_fn=<NegBackward0>) tensor(12411.5889, grad_fn=<NegBackward0>)
2
pi: tensor([[9.2657e-01, 7.3434e-02],
        [9.9974e-01, 2.5541e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9702, 0.0298], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1972, 0.2340],
         [0.5780, 0.2557]],

        [[0.6310, 0.2441],
         [0.5720, 0.5503]],

        [[0.6930, 0.2626],
         [0.7204, 0.5493]],

        [[0.6026, 0.1888],
         [0.5485, 0.6916]],

        [[0.5030, 0.2068],
         [0.6131, 0.6461]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009303106223477708
Average Adjusted Rand Index: -0.0015521208230856337
[-0.0009303106223477708, -0.0009303106223477708] [-0.0015521208230856337, -0.0015521208230856337] [12411.578125, 12411.576171875]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11879.598282659359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21791.470703125
inf tensor(21791.4707, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12343.9228515625
tensor(21791.4707, grad_fn=<NegBackward0>) tensor(12343.9229, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12343.45703125
tensor(12343.9229, grad_fn=<NegBackward0>) tensor(12343.4570, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12343.296875
tensor(12343.4570, grad_fn=<NegBackward0>) tensor(12343.2969, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12343.154296875
tensor(12343.2969, grad_fn=<NegBackward0>) tensor(12343.1543, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12343.0205078125
tensor(12343.1543, grad_fn=<NegBackward0>) tensor(12343.0205, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12342.9541015625
tensor(12343.0205, grad_fn=<NegBackward0>) tensor(12342.9541, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12342.90625
tensor(12342.9541, grad_fn=<NegBackward0>) tensor(12342.9062, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12342.865234375
tensor(12342.9062, grad_fn=<NegBackward0>) tensor(12342.8652, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12342.822265625
tensor(12342.8652, grad_fn=<NegBackward0>) tensor(12342.8223, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12342.779296875
tensor(12342.8223, grad_fn=<NegBackward0>) tensor(12342.7793, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12342.734375
tensor(12342.7793, grad_fn=<NegBackward0>) tensor(12342.7344, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12342.689453125
tensor(12342.7344, grad_fn=<NegBackward0>) tensor(12342.6895, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12342.6416015625
tensor(12342.6895, grad_fn=<NegBackward0>) tensor(12342.6416, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12342.591796875
tensor(12342.6416, grad_fn=<NegBackward0>) tensor(12342.5918, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12342.537109375
tensor(12342.5918, grad_fn=<NegBackward0>) tensor(12342.5371, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12342.4814453125
tensor(12342.5371, grad_fn=<NegBackward0>) tensor(12342.4814, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12342.42578125
tensor(12342.4814, grad_fn=<NegBackward0>) tensor(12342.4258, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12342.369140625
tensor(12342.4258, grad_fn=<NegBackward0>) tensor(12342.3691, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12342.3154296875
tensor(12342.3691, grad_fn=<NegBackward0>) tensor(12342.3154, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12342.267578125
tensor(12342.3154, grad_fn=<NegBackward0>) tensor(12342.2676, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12342.2265625
tensor(12342.2676, grad_fn=<NegBackward0>) tensor(12342.2266, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12342.1826171875
tensor(12342.2266, grad_fn=<NegBackward0>) tensor(12342.1826, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12342.142578125
tensor(12342.1826, grad_fn=<NegBackward0>) tensor(12342.1426, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12342.107421875
tensor(12342.1426, grad_fn=<NegBackward0>) tensor(12342.1074, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12342.07421875
tensor(12342.1074, grad_fn=<NegBackward0>) tensor(12342.0742, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12342.0439453125
tensor(12342.0742, grad_fn=<NegBackward0>) tensor(12342.0439, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12342.0185546875
tensor(12342.0439, grad_fn=<NegBackward0>) tensor(12342.0186, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12341.994140625
tensor(12342.0186, grad_fn=<NegBackward0>) tensor(12341.9941, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12341.974609375
tensor(12341.9941, grad_fn=<NegBackward0>) tensor(12341.9746, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12341.9619140625
tensor(12341.9746, grad_fn=<NegBackward0>) tensor(12341.9619, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12341.9462890625
tensor(12341.9619, grad_fn=<NegBackward0>) tensor(12341.9463, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12341.9345703125
tensor(12341.9463, grad_fn=<NegBackward0>) tensor(12341.9346, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12341.927734375
tensor(12341.9346, grad_fn=<NegBackward0>) tensor(12341.9277, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12341.91796875
tensor(12341.9277, grad_fn=<NegBackward0>) tensor(12341.9180, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12341.9111328125
tensor(12341.9180, grad_fn=<NegBackward0>) tensor(12341.9111, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12341.9052734375
tensor(12341.9111, grad_fn=<NegBackward0>) tensor(12341.9053, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12341.8974609375
tensor(12341.9053, grad_fn=<NegBackward0>) tensor(12341.8975, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12341.890625
tensor(12341.8975, grad_fn=<NegBackward0>) tensor(12341.8906, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12341.8818359375
tensor(12341.8906, grad_fn=<NegBackward0>) tensor(12341.8818, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12341.875
tensor(12341.8818, grad_fn=<NegBackward0>) tensor(12341.8750, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12341.86328125
tensor(12341.8750, grad_fn=<NegBackward0>) tensor(12341.8633, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12341.841796875
tensor(12341.8633, grad_fn=<NegBackward0>) tensor(12341.8418, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12341.8056640625
tensor(12341.8418, grad_fn=<NegBackward0>) tensor(12341.8057, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12341.7373046875
tensor(12341.8057, grad_fn=<NegBackward0>) tensor(12341.7373, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12341.6474609375
tensor(12341.7373, grad_fn=<NegBackward0>) tensor(12341.6475, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12341.5888671875
tensor(12341.6475, grad_fn=<NegBackward0>) tensor(12341.5889, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12341.55859375
tensor(12341.5889, grad_fn=<NegBackward0>) tensor(12341.5586, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12341.5439453125
tensor(12341.5586, grad_fn=<NegBackward0>) tensor(12341.5439, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12341.5322265625
tensor(12341.5439, grad_fn=<NegBackward0>) tensor(12341.5322, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12341.525390625
tensor(12341.5322, grad_fn=<NegBackward0>) tensor(12341.5254, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12341.51953125
tensor(12341.5254, grad_fn=<NegBackward0>) tensor(12341.5195, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12341.513671875
tensor(12341.5195, grad_fn=<NegBackward0>) tensor(12341.5137, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12341.5087890625
tensor(12341.5137, grad_fn=<NegBackward0>) tensor(12341.5088, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12341.50390625
tensor(12341.5088, grad_fn=<NegBackward0>) tensor(12341.5039, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12341.4990234375
tensor(12341.5039, grad_fn=<NegBackward0>) tensor(12341.4990, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12341.494140625
tensor(12341.4990, grad_fn=<NegBackward0>) tensor(12341.4941, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12341.490234375
tensor(12341.4941, grad_fn=<NegBackward0>) tensor(12341.4902, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12341.48828125
tensor(12341.4902, grad_fn=<NegBackward0>) tensor(12341.4883, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12341.4833984375
tensor(12341.4883, grad_fn=<NegBackward0>) tensor(12341.4834, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12341.4814453125
tensor(12341.4834, grad_fn=<NegBackward0>) tensor(12341.4814, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12341.478515625
tensor(12341.4814, grad_fn=<NegBackward0>) tensor(12341.4785, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12341.4814453125
tensor(12341.4785, grad_fn=<NegBackward0>) tensor(12341.4814, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12341.4765625
tensor(12341.4785, grad_fn=<NegBackward0>) tensor(12341.4766, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12341.4736328125
tensor(12341.4766, grad_fn=<NegBackward0>) tensor(12341.4736, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12341.47265625
tensor(12341.4736, grad_fn=<NegBackward0>) tensor(12341.4727, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12341.47265625
tensor(12341.4727, grad_fn=<NegBackward0>) tensor(12341.4727, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12341.4716796875
tensor(12341.4727, grad_fn=<NegBackward0>) tensor(12341.4717, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12341.4697265625
tensor(12341.4717, grad_fn=<NegBackward0>) tensor(12341.4697, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12341.4697265625
tensor(12341.4697, grad_fn=<NegBackward0>) tensor(12341.4697, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12341.4697265625
tensor(12341.4697, grad_fn=<NegBackward0>) tensor(12341.4697, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12341.474609375
tensor(12341.4697, grad_fn=<NegBackward0>) tensor(12341.4746, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12341.46875
tensor(12341.4697, grad_fn=<NegBackward0>) tensor(12341.4688, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12341.466796875
tensor(12341.4688, grad_fn=<NegBackward0>) tensor(12341.4668, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12341.466796875
tensor(12341.4668, grad_fn=<NegBackward0>) tensor(12341.4668, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12341.46484375
tensor(12341.4668, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12341.46484375
tensor(12341.4648, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12341.46484375
tensor(12341.4648, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12341.4638671875
tensor(12341.4648, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12341.46484375
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12341.4638671875
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12341.4658203125
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4658, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12341.46484375
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12341.46875
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4688, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12341.4638671875
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12341.470703125
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4707, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12341.4609375
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4609, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12341.462890625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12341.4609375
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4609, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12341.4619140625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12341.46484375
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12341.462890625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12341.4619140625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -12341.4638671875
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[2.1105e-03, 9.9789e-01],
        [9.9920e-01, 7.9994e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2471, 0.7529], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1907, 0.1988],
         [0.6197, 0.2069]],

        [[0.7215, 0.2005],
         [0.5507, 0.7268]],

        [[0.6346, 0.1959],
         [0.7181, 0.5808]],

        [[0.6313, 0.1908],
         [0.7268, 0.6883]],

        [[0.5203, 0.1970],
         [0.5530, 0.6139]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.0002250377085654862
Average Adjusted Rand Index: 0.0035195712949490712
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20807.51171875
inf tensor(20807.5117, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12344.046875
tensor(20807.5117, grad_fn=<NegBackward0>) tensor(12344.0469, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12343.47265625
tensor(12344.0469, grad_fn=<NegBackward0>) tensor(12343.4727, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12343.291015625
tensor(12343.4727, grad_fn=<NegBackward0>) tensor(12343.2910, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12343.1494140625
tensor(12343.2910, grad_fn=<NegBackward0>) tensor(12343.1494, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12343.0205078125
tensor(12343.1494, grad_fn=<NegBackward0>) tensor(12343.0205, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12342.939453125
tensor(12343.0205, grad_fn=<NegBackward0>) tensor(12342.9395, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12342.8779296875
tensor(12342.9395, grad_fn=<NegBackward0>) tensor(12342.8779, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12342.81640625
tensor(12342.8779, grad_fn=<NegBackward0>) tensor(12342.8164, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12342.755859375
tensor(12342.8164, grad_fn=<NegBackward0>) tensor(12342.7559, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12342.69921875
tensor(12342.7559, grad_fn=<NegBackward0>) tensor(12342.6992, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12342.64453125
tensor(12342.6992, grad_fn=<NegBackward0>) tensor(12342.6445, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12342.587890625
tensor(12342.6445, grad_fn=<NegBackward0>) tensor(12342.5879, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12342.5302734375
tensor(12342.5879, grad_fn=<NegBackward0>) tensor(12342.5303, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12342.4716796875
tensor(12342.5303, grad_fn=<NegBackward0>) tensor(12342.4717, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12342.4130859375
tensor(12342.4717, grad_fn=<NegBackward0>) tensor(12342.4131, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12342.35546875
tensor(12342.4131, grad_fn=<NegBackward0>) tensor(12342.3555, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12342.2998046875
tensor(12342.3555, grad_fn=<NegBackward0>) tensor(12342.2998, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12342.2509765625
tensor(12342.2998, grad_fn=<NegBackward0>) tensor(12342.2510, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12342.2041015625
tensor(12342.2510, grad_fn=<NegBackward0>) tensor(12342.2041, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12342.1611328125
tensor(12342.2041, grad_fn=<NegBackward0>) tensor(12342.1611, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12342.1201171875
tensor(12342.1611, grad_fn=<NegBackward0>) tensor(12342.1201, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12342.0830078125
tensor(12342.1201, grad_fn=<NegBackward0>) tensor(12342.0830, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12342.05078125
tensor(12342.0830, grad_fn=<NegBackward0>) tensor(12342.0508, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12342.0224609375
tensor(12342.0508, grad_fn=<NegBackward0>) tensor(12342.0225, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12342.0
tensor(12342.0225, grad_fn=<NegBackward0>) tensor(12342., grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12341.978515625
tensor(12342., grad_fn=<NegBackward0>) tensor(12341.9785, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12341.9619140625
tensor(12341.9785, grad_fn=<NegBackward0>) tensor(12341.9619, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12341.947265625
tensor(12341.9619, grad_fn=<NegBackward0>) tensor(12341.9473, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12341.9365234375
tensor(12341.9473, grad_fn=<NegBackward0>) tensor(12341.9365, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12341.9248046875
tensor(12341.9365, grad_fn=<NegBackward0>) tensor(12341.9248, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12341.916015625
tensor(12341.9248, grad_fn=<NegBackward0>) tensor(12341.9160, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12341.90625
tensor(12341.9160, grad_fn=<NegBackward0>) tensor(12341.9062, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12341.8994140625
tensor(12341.9062, grad_fn=<NegBackward0>) tensor(12341.8994, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12341.8916015625
tensor(12341.8994, grad_fn=<NegBackward0>) tensor(12341.8916, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12341.8818359375
tensor(12341.8916, grad_fn=<NegBackward0>) tensor(12341.8818, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12341.873046875
tensor(12341.8818, grad_fn=<NegBackward0>) tensor(12341.8730, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12341.859375
tensor(12341.8730, grad_fn=<NegBackward0>) tensor(12341.8594, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12341.83203125
tensor(12341.8594, grad_fn=<NegBackward0>) tensor(12341.8320, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12341.78515625
tensor(12341.8320, grad_fn=<NegBackward0>) tensor(12341.7852, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12341.7021484375
tensor(12341.7852, grad_fn=<NegBackward0>) tensor(12341.7021, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12341.6162109375
tensor(12341.7021, grad_fn=<NegBackward0>) tensor(12341.6162, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12341.5693359375
tensor(12341.6162, grad_fn=<NegBackward0>) tensor(12341.5693, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12341.5478515625
tensor(12341.5693, grad_fn=<NegBackward0>) tensor(12341.5479, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12341.5361328125
tensor(12341.5479, grad_fn=<NegBackward0>) tensor(12341.5361, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12341.5263671875
tensor(12341.5361, grad_fn=<NegBackward0>) tensor(12341.5264, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12341.51953125
tensor(12341.5264, grad_fn=<NegBackward0>) tensor(12341.5195, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12341.513671875
tensor(12341.5195, grad_fn=<NegBackward0>) tensor(12341.5137, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12341.5078125
tensor(12341.5137, grad_fn=<NegBackward0>) tensor(12341.5078, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12341.5048828125
tensor(12341.5078, grad_fn=<NegBackward0>) tensor(12341.5049, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12341.498046875
tensor(12341.5049, grad_fn=<NegBackward0>) tensor(12341.4980, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12341.494140625
tensor(12341.4980, grad_fn=<NegBackward0>) tensor(12341.4941, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12341.4892578125
tensor(12341.4941, grad_fn=<NegBackward0>) tensor(12341.4893, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12341.4873046875
tensor(12341.4893, grad_fn=<NegBackward0>) tensor(12341.4873, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12341.482421875
tensor(12341.4873, grad_fn=<NegBackward0>) tensor(12341.4824, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12341.4814453125
tensor(12341.4824, grad_fn=<NegBackward0>) tensor(12341.4814, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12341.478515625
tensor(12341.4814, grad_fn=<NegBackward0>) tensor(12341.4785, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12341.478515625
tensor(12341.4785, grad_fn=<NegBackward0>) tensor(12341.4785, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12341.4755859375
tensor(12341.4785, grad_fn=<NegBackward0>) tensor(12341.4756, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12341.484375
tensor(12341.4756, grad_fn=<NegBackward0>) tensor(12341.4844, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12341.47265625
tensor(12341.4756, grad_fn=<NegBackward0>) tensor(12341.4727, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12341.4736328125
tensor(12341.4727, grad_fn=<NegBackward0>) tensor(12341.4736, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12341.4716796875
tensor(12341.4727, grad_fn=<NegBackward0>) tensor(12341.4717, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12341.470703125
tensor(12341.4717, grad_fn=<NegBackward0>) tensor(12341.4707, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12341.46875
tensor(12341.4707, grad_fn=<NegBackward0>) tensor(12341.4688, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12341.46875
tensor(12341.4688, grad_fn=<NegBackward0>) tensor(12341.4688, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12341.46875
tensor(12341.4688, grad_fn=<NegBackward0>) tensor(12341.4688, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12341.4677734375
tensor(12341.4688, grad_fn=<NegBackward0>) tensor(12341.4678, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12341.466796875
tensor(12341.4678, grad_fn=<NegBackward0>) tensor(12341.4668, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12341.4677734375
tensor(12341.4668, grad_fn=<NegBackward0>) tensor(12341.4678, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12341.4658203125
tensor(12341.4668, grad_fn=<NegBackward0>) tensor(12341.4658, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12341.46484375
tensor(12341.4658, grad_fn=<NegBackward0>) tensor(12341.4648, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12341.4794921875
tensor(12341.4648, grad_fn=<NegBackward0>) tensor(12341.4795, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12341.4638671875
tensor(12341.4648, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12341.4658203125
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4658, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12341.462890625
tensor(12341.4639, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12341.4638671875
tensor(12341.4629, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12341.4638671875
tensor(12341.4629, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12341.462890625
tensor(12341.4629, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12341.46875
tensor(12341.4629, grad_fn=<NegBackward0>) tensor(12341.4688, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12341.4619140625
tensor(12341.4629, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12341.5185546875
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.5186, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12341.462890625
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12341.4697265625
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4697, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12341.4619140625
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12341.462890625
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12341.4619140625
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12341.462890625
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4629, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12341.4609375
tensor(12341.4619, grad_fn=<NegBackward0>) tensor(12341.4609, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12341.47265625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4727, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12341.494140625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4941, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12341.4638671875
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4639, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12341.4619140625
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -12341.4609375
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4609, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12341.4599609375
tensor(12341.4609, grad_fn=<NegBackward0>) tensor(12341.4600, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12341.4619140625
tensor(12341.4600, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12341.49609375
tensor(12341.4600, grad_fn=<NegBackward0>) tensor(12341.4961, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12341.4609375
tensor(12341.4600, grad_fn=<NegBackward0>) tensor(12341.4609, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -12341.4619140625
tensor(12341.4600, grad_fn=<NegBackward0>) tensor(12341.4619, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -12341.5322265625
tensor(12341.4600, grad_fn=<NegBackward0>) tensor(12341.5322, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[3.9814e-04, 9.9960e-01],
        [9.9883e-01, 1.1720e-03]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7548, 0.2452], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2065, 0.1991],
         [0.5924, 0.1903]],

        [[0.5756, 0.2016],
         [0.7084, 0.6442]],

        [[0.5573, 0.1969],
         [0.5866, 0.7122]],

        [[0.6344, 0.1916],
         [0.7111, 0.5669]],

        [[0.6873, 0.1971],
         [0.6874, 0.6342]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.0002250377085654862
Average Adjusted Rand Index: 0.0035195712949490712
[-0.0002250377085654862, -0.0002250377085654862] [0.0035195712949490712, 0.0035195712949490712] [12341.4638671875, 12341.5322265625]
-------------------------------------
This iteration is 53
True Objective function: Loss = -11878.463660701182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23195.65625
inf tensor(23195.6562, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12403.6015625
tensor(23195.6562, grad_fn=<NegBackward0>) tensor(12403.6016, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12403.1396484375
tensor(12403.6016, grad_fn=<NegBackward0>) tensor(12403.1396, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12403.021484375
tensor(12403.1396, grad_fn=<NegBackward0>) tensor(12403.0215, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12402.9296875
tensor(12403.0215, grad_fn=<NegBackward0>) tensor(12402.9297, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12402.8505859375
tensor(12402.9297, grad_fn=<NegBackward0>) tensor(12402.8506, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12402.7822265625
tensor(12402.8506, grad_fn=<NegBackward0>) tensor(12402.7822, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12402.70703125
tensor(12402.7822, grad_fn=<NegBackward0>) tensor(12402.7070, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12402.6201171875
tensor(12402.7070, grad_fn=<NegBackward0>) tensor(12402.6201, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12402.51953125
tensor(12402.6201, grad_fn=<NegBackward0>) tensor(12402.5195, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12402.4228515625
tensor(12402.5195, grad_fn=<NegBackward0>) tensor(12402.4229, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12402.3408203125
tensor(12402.4229, grad_fn=<NegBackward0>) tensor(12402.3408, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12402.259765625
tensor(12402.3408, grad_fn=<NegBackward0>) tensor(12402.2598, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12402.1767578125
tensor(12402.2598, grad_fn=<NegBackward0>) tensor(12402.1768, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12402.0849609375
tensor(12402.1768, grad_fn=<NegBackward0>) tensor(12402.0850, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12401.990234375
tensor(12402.0850, grad_fn=<NegBackward0>) tensor(12401.9902, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12401.89453125
tensor(12401.9902, grad_fn=<NegBackward0>) tensor(12401.8945, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12401.8056640625
tensor(12401.8945, grad_fn=<NegBackward0>) tensor(12401.8057, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12401.7314453125
tensor(12401.8057, grad_fn=<NegBackward0>) tensor(12401.7314, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12401.671875
tensor(12401.7314, grad_fn=<NegBackward0>) tensor(12401.6719, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12401.6171875
tensor(12401.6719, grad_fn=<NegBackward0>) tensor(12401.6172, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12401.5576171875
tensor(12401.6172, grad_fn=<NegBackward0>) tensor(12401.5576, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12401.4775390625
tensor(12401.5576, grad_fn=<NegBackward0>) tensor(12401.4775, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12401.3525390625
tensor(12401.4775, grad_fn=<NegBackward0>) tensor(12401.3525, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12401.1201171875
tensor(12401.3525, grad_fn=<NegBackward0>) tensor(12401.1201, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12400.7880859375
tensor(12401.1201, grad_fn=<NegBackward0>) tensor(12400.7881, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12400.640625
tensor(12400.7881, grad_fn=<NegBackward0>) tensor(12400.6406, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12400.5966796875
tensor(12400.6406, grad_fn=<NegBackward0>) tensor(12400.5967, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12400.572265625
tensor(12400.5967, grad_fn=<NegBackward0>) tensor(12400.5723, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12400.5576171875
tensor(12400.5723, grad_fn=<NegBackward0>) tensor(12400.5576, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12400.5498046875
tensor(12400.5576, grad_fn=<NegBackward0>) tensor(12400.5498, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12400.5439453125
tensor(12400.5498, grad_fn=<NegBackward0>) tensor(12400.5439, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12400.5263671875
tensor(12400.5439, grad_fn=<NegBackward0>) tensor(12400.5264, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12068.7412109375
tensor(12400.5264, grad_fn=<NegBackward0>) tensor(12068.7412, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12050.6943359375
tensor(12068.7412, grad_fn=<NegBackward0>) tensor(12050.6943, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12025.568359375
tensor(12050.6943, grad_fn=<NegBackward0>) tensor(12025.5684, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12008.373046875
tensor(12025.5684, grad_fn=<NegBackward0>) tensor(12008.3730, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12008.3212890625
tensor(12008.3730, grad_fn=<NegBackward0>) tensor(12008.3213, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12008.2998046875
tensor(12008.3213, grad_fn=<NegBackward0>) tensor(12008.2998, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12006.3134765625
tensor(12008.2998, grad_fn=<NegBackward0>) tensor(12006.3135, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12004.931640625
tensor(12006.3135, grad_fn=<NegBackward0>) tensor(12004.9316, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12004.912109375
tensor(12004.9316, grad_fn=<NegBackward0>) tensor(12004.9121, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11993.974609375
tensor(12004.9121, grad_fn=<NegBackward0>) tensor(11993.9746, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11989.4697265625
tensor(11993.9746, grad_fn=<NegBackward0>) tensor(11989.4697, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11988.470703125
tensor(11989.4697, grad_fn=<NegBackward0>) tensor(11988.4707, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11988.42578125
tensor(11988.4707, grad_fn=<NegBackward0>) tensor(11988.4258, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11988.4111328125
tensor(11988.4258, grad_fn=<NegBackward0>) tensor(11988.4111, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11982.3583984375
tensor(11988.4111, grad_fn=<NegBackward0>) tensor(11982.3584, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11975.083984375
tensor(11982.3584, grad_fn=<NegBackward0>) tensor(11975.0840, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11968.810546875
tensor(11975.0840, grad_fn=<NegBackward0>) tensor(11968.8105, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11968.80078125
tensor(11968.8105, grad_fn=<NegBackward0>) tensor(11968.8008, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11961.501953125
tensor(11968.8008, grad_fn=<NegBackward0>) tensor(11961.5020, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11961.5009765625
tensor(11961.5020, grad_fn=<NegBackward0>) tensor(11961.5010, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11961.443359375
tensor(11961.5010, grad_fn=<NegBackward0>) tensor(11961.4434, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11952.6484375
tensor(11961.4434, grad_fn=<NegBackward0>) tensor(11952.6484, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11952.64453125
tensor(11952.6484, grad_fn=<NegBackward0>) tensor(11952.6445, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11952.642578125
tensor(11952.6445, grad_fn=<NegBackward0>) tensor(11952.6426, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11952.6240234375
tensor(11952.6426, grad_fn=<NegBackward0>) tensor(11952.6240, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11952.52734375
tensor(11952.6240, grad_fn=<NegBackward0>) tensor(11952.5273, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11939.2197265625
tensor(11952.5273, grad_fn=<NegBackward0>) tensor(11939.2197, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11939.0810546875
tensor(11939.2197, grad_fn=<NegBackward0>) tensor(11939.0811, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11928.9306640625
tensor(11939.0811, grad_fn=<NegBackward0>) tensor(11928.9307, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11928.92578125
tensor(11928.9307, grad_fn=<NegBackward0>) tensor(11928.9258, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11923.87109375
tensor(11928.9258, grad_fn=<NegBackward0>) tensor(11923.8711, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11923.8359375
tensor(11923.8711, grad_fn=<NegBackward0>) tensor(11923.8359, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11913.2880859375
tensor(11923.8359, grad_fn=<NegBackward0>) tensor(11913.2881, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11913.2861328125
tensor(11913.2881, grad_fn=<NegBackward0>) tensor(11913.2861, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11913.2890625
tensor(11913.2861, grad_fn=<NegBackward0>) tensor(11913.2891, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11913.28515625
tensor(11913.2861, grad_fn=<NegBackward0>) tensor(11913.2852, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11913.294921875
tensor(11913.2852, grad_fn=<NegBackward0>) tensor(11913.2949, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11913.28515625
tensor(11913.2852, grad_fn=<NegBackward0>) tensor(11913.2852, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11913.2841796875
tensor(11913.2852, grad_fn=<NegBackward0>) tensor(11913.2842, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11913.1806640625
tensor(11913.2842, grad_fn=<NegBackward0>) tensor(11913.1807, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11909.498046875
tensor(11913.1807, grad_fn=<NegBackward0>) tensor(11909.4980, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11909.501953125
tensor(11909.4980, grad_fn=<NegBackward0>) tensor(11909.5020, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11909.5009765625
tensor(11909.4980, grad_fn=<NegBackward0>) tensor(11909.5010, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11907.5712890625
tensor(11909.4980, grad_fn=<NegBackward0>) tensor(11907.5713, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11907.572265625
tensor(11907.5713, grad_fn=<NegBackward0>) tensor(11907.5723, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11900.0517578125
tensor(11907.5713, grad_fn=<NegBackward0>) tensor(11900.0518, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11899.912109375
tensor(11900.0518, grad_fn=<NegBackward0>) tensor(11899.9121, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11899.912109375
tensor(11899.9121, grad_fn=<NegBackward0>) tensor(11899.9121, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11899.912109375
tensor(11899.9121, grad_fn=<NegBackward0>) tensor(11899.9121, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11899.9111328125
tensor(11899.9121, grad_fn=<NegBackward0>) tensor(11899.9111, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11891.08984375
tensor(11899.9111, grad_fn=<NegBackward0>) tensor(11891.0898, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11891.08984375
tensor(11891.0898, grad_fn=<NegBackward0>) tensor(11891.0898, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11891.10546875
tensor(11891.0898, grad_fn=<NegBackward0>) tensor(11891.1055, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11890.8818359375
tensor(11891.0898, grad_fn=<NegBackward0>) tensor(11890.8818, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11890.8818359375
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8818, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11890.8837890625
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8838, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11890.8818359375
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8818, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11890.8837890625
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8838, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11890.8818359375
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8818, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11890.8935546875
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8936, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11891.0458984375
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11891.0459, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11890.880859375
tensor(11890.8818, grad_fn=<NegBackward0>) tensor(11890.8809, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11882.3544921875
tensor(11890.8809, grad_fn=<NegBackward0>) tensor(11882.3545, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11882.3359375
tensor(11882.3545, grad_fn=<NegBackward0>) tensor(11882.3359, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11868.421875
tensor(11882.3359, grad_fn=<NegBackward0>) tensor(11868.4219, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11868.423828125
tensor(11868.4219, grad_fn=<NegBackward0>) tensor(11868.4238, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11868.421875
tensor(11868.4219, grad_fn=<NegBackward0>) tensor(11868.4219, grad_fn=<NegBackward0>)
pi: tensor([[0.2869, 0.7131],
        [0.7993, 0.2007]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4802, 0.5198], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3058, 0.1057],
         [0.6008, 0.2963]],

        [[0.5908, 0.1069],
         [0.6063, 0.6609]],

        [[0.7183, 0.0889],
         [0.6427, 0.6630]],

        [[0.7295, 0.0985],
         [0.6078, 0.6812]],

        [[0.5122, 0.0976],
         [0.6922, 0.5924]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.0364909343744948
Average Adjusted Rand Index: 0.9759959241956121
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23381.974609375
inf tensor(23381.9746, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12403.564453125
tensor(23381.9746, grad_fn=<NegBackward0>) tensor(12403.5645, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12403.146484375
tensor(12403.5645, grad_fn=<NegBackward0>) tensor(12403.1465, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12403.021484375
tensor(12403.1465, grad_fn=<NegBackward0>) tensor(12403.0215, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12402.9326171875
tensor(12403.0215, grad_fn=<NegBackward0>) tensor(12402.9326, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12402.859375
tensor(12402.9326, grad_fn=<NegBackward0>) tensor(12402.8594, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12402.7978515625
tensor(12402.8594, grad_fn=<NegBackward0>) tensor(12402.7979, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12402.736328125
tensor(12402.7979, grad_fn=<NegBackward0>) tensor(12402.7363, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12402.6767578125
tensor(12402.7363, grad_fn=<NegBackward0>) tensor(12402.6768, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12402.6171875
tensor(12402.6768, grad_fn=<NegBackward0>) tensor(12402.6172, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12402.5546875
tensor(12402.6172, grad_fn=<NegBackward0>) tensor(12402.5547, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12402.4970703125
tensor(12402.5547, grad_fn=<NegBackward0>) tensor(12402.4971, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12402.4404296875
tensor(12402.4971, grad_fn=<NegBackward0>) tensor(12402.4404, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12402.3828125
tensor(12402.4404, grad_fn=<NegBackward0>) tensor(12402.3828, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12402.3271484375
tensor(12402.3828, grad_fn=<NegBackward0>) tensor(12402.3271, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12402.2685546875
tensor(12402.3271, grad_fn=<NegBackward0>) tensor(12402.2686, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12402.2109375
tensor(12402.2686, grad_fn=<NegBackward0>) tensor(12402.2109, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12402.15234375
tensor(12402.2109, grad_fn=<NegBackward0>) tensor(12402.1523, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12402.091796875
tensor(12402.1523, grad_fn=<NegBackward0>) tensor(12402.0918, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12402.0283203125
tensor(12402.0918, grad_fn=<NegBackward0>) tensor(12402.0283, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12401.962890625
tensor(12402.0283, grad_fn=<NegBackward0>) tensor(12401.9629, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12401.8984375
tensor(12401.9629, grad_fn=<NegBackward0>) tensor(12401.8984, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12401.837890625
tensor(12401.8984, grad_fn=<NegBackward0>) tensor(12401.8379, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12401.78125
tensor(12401.8379, grad_fn=<NegBackward0>) tensor(12401.7812, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12401.734375
tensor(12401.7812, grad_fn=<NegBackward0>) tensor(12401.7344, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12401.6923828125
tensor(12401.7344, grad_fn=<NegBackward0>) tensor(12401.6924, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12401.65234375
tensor(12401.6924, grad_fn=<NegBackward0>) tensor(12401.6523, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12401.6123046875
tensor(12401.6523, grad_fn=<NegBackward0>) tensor(12401.6123, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12401.56640625
tensor(12401.6123, grad_fn=<NegBackward0>) tensor(12401.5664, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12401.50390625
tensor(12401.5664, grad_fn=<NegBackward0>) tensor(12401.5039, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12401.4150390625
tensor(12401.5039, grad_fn=<NegBackward0>) tensor(12401.4150, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12401.27734375
tensor(12401.4150, grad_fn=<NegBackward0>) tensor(12401.2773, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12401.0537109375
tensor(12401.2773, grad_fn=<NegBackward0>) tensor(12401.0537, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12400.80859375
tensor(12401.0537, grad_fn=<NegBackward0>) tensor(12400.8086, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12400.6787109375
tensor(12400.8086, grad_fn=<NegBackward0>) tensor(12400.6787, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12400.63671875
tensor(12400.6787, grad_fn=<NegBackward0>) tensor(12400.6367, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12400.623046875
tensor(12400.6367, grad_fn=<NegBackward0>) tensor(12400.6230, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12400.6162109375
tensor(12400.6230, grad_fn=<NegBackward0>) tensor(12400.6162, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12400.611328125
tensor(12400.6162, grad_fn=<NegBackward0>) tensor(12400.6113, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12400.6103515625
tensor(12400.6113, grad_fn=<NegBackward0>) tensor(12400.6104, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12400.607421875
tensor(12400.6104, grad_fn=<NegBackward0>) tensor(12400.6074, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12400.6064453125
tensor(12400.6074, grad_fn=<NegBackward0>) tensor(12400.6064, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12400.60546875
tensor(12400.6064, grad_fn=<NegBackward0>) tensor(12400.6055, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12400.6025390625
tensor(12400.6055, grad_fn=<NegBackward0>) tensor(12400.6025, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12400.599609375
tensor(12400.6025, grad_fn=<NegBackward0>) tensor(12400.5996, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12400.59765625
tensor(12400.5996, grad_fn=<NegBackward0>) tensor(12400.5977, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12400.591796875
tensor(12400.5977, grad_fn=<NegBackward0>) tensor(12400.5918, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12400.583984375
tensor(12400.5918, grad_fn=<NegBackward0>) tensor(12400.5840, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12400.5751953125
tensor(12400.5840, grad_fn=<NegBackward0>) tensor(12400.5752, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12400.5712890625
tensor(12400.5752, grad_fn=<NegBackward0>) tensor(12400.5713, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12400.56640625
tensor(12400.5713, grad_fn=<NegBackward0>) tensor(12400.5664, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12400.5634765625
tensor(12400.5664, grad_fn=<NegBackward0>) tensor(12400.5635, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12400.5615234375
tensor(12400.5635, grad_fn=<NegBackward0>) tensor(12400.5615, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12400.5576171875
tensor(12400.5615, grad_fn=<NegBackward0>) tensor(12400.5576, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12400.5546875
tensor(12400.5576, grad_fn=<NegBackward0>) tensor(12400.5547, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12400.55078125
tensor(12400.5547, grad_fn=<NegBackward0>) tensor(12400.5508, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12400.544921875
tensor(12400.5508, grad_fn=<NegBackward0>) tensor(12400.5449, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12400.5380859375
tensor(12400.5449, grad_fn=<NegBackward0>) tensor(12400.5381, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12400.52734375
tensor(12400.5381, grad_fn=<NegBackward0>) tensor(12400.5273, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12400.505859375
tensor(12400.5273, grad_fn=<NegBackward0>) tensor(12400.5059, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12400.4736328125
tensor(12400.5059, grad_fn=<NegBackward0>) tensor(12400.4736, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12400.466796875
tensor(12400.4736, grad_fn=<NegBackward0>) tensor(12400.4668, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12400.46484375
tensor(12400.4668, grad_fn=<NegBackward0>) tensor(12400.4648, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12400.4677734375
tensor(12400.4648, grad_fn=<NegBackward0>) tensor(12400.4678, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12400.4638671875
tensor(12400.4648, grad_fn=<NegBackward0>) tensor(12400.4639, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12400.466796875
tensor(12400.4639, grad_fn=<NegBackward0>) tensor(12400.4668, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12400.46484375
tensor(12400.4639, grad_fn=<NegBackward0>) tensor(12400.4648, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12400.46875
tensor(12400.4639, grad_fn=<NegBackward0>) tensor(12400.4688, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12400.4658203125
tensor(12400.4639, grad_fn=<NegBackward0>) tensor(12400.4658, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -12400.4755859375
tensor(12400.4639, grad_fn=<NegBackward0>) tensor(12400.4756, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.1644, 0.8356],
        [0.9894, 0.0106]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8082, 0.1918], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1895, 0.2060],
         [0.6622, 0.2167]],

        [[0.6171, 0.2036],
         [0.5911, 0.5073]],

        [[0.6242, 0.1949],
         [0.6134, 0.6822]],

        [[0.6990, 0.1991],
         [0.5277, 0.6238]],

        [[0.5372, 0.2085],
         [0.7258, 0.6838]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.033448153944155117
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.005431979218977636
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.06651612798668921
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0009741485332976146
Global Adjusted Rand Index: 0.0017677847955903578
Average Adjusted Rand Index: 0.02088442252330487
[0.0364909343744948, 0.0017677847955903578] [0.9759959241956121, 0.02088442252330487] [11868.4248046875, 12400.4755859375]
-------------------------------------
This iteration is 54
True Objective function: Loss = -12069.930642290668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20299.673828125
inf tensor(20299.6738, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12551.6416015625
tensor(20299.6738, grad_fn=<NegBackward0>) tensor(12551.6416, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12550.9697265625
tensor(12551.6416, grad_fn=<NegBackward0>) tensor(12550.9697, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12550.265625
tensor(12550.9697, grad_fn=<NegBackward0>) tensor(12550.2656, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12549.33984375
tensor(12550.2656, grad_fn=<NegBackward0>) tensor(12549.3398, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12547.9638671875
tensor(12549.3398, grad_fn=<NegBackward0>) tensor(12547.9639, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12547.220703125
tensor(12547.9639, grad_fn=<NegBackward0>) tensor(12547.2207, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12546.7236328125
tensor(12547.2207, grad_fn=<NegBackward0>) tensor(12546.7236, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12546.345703125
tensor(12546.7236, grad_fn=<NegBackward0>) tensor(12546.3457, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12546.015625
tensor(12546.3457, grad_fn=<NegBackward0>) tensor(12546.0156, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12545.576171875
tensor(12546.0156, grad_fn=<NegBackward0>) tensor(12545.5762, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12367.3505859375
tensor(12545.5762, grad_fn=<NegBackward0>) tensor(12367.3506, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12241.5009765625
tensor(12367.3506, grad_fn=<NegBackward0>) tensor(12241.5010, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12225.04296875
tensor(12241.5010, grad_fn=<NegBackward0>) tensor(12225.0430, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12217.90234375
tensor(12225.0430, grad_fn=<NegBackward0>) tensor(12217.9023, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12217.7265625
tensor(12217.9023, grad_fn=<NegBackward0>) tensor(12217.7266, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12217.4892578125
tensor(12217.7266, grad_fn=<NegBackward0>) tensor(12217.4893, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12216.8525390625
tensor(12217.4893, grad_fn=<NegBackward0>) tensor(12216.8525, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12215.6337890625
tensor(12216.8525, grad_fn=<NegBackward0>) tensor(12215.6338, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12208.23828125
tensor(12215.6338, grad_fn=<NegBackward0>) tensor(12208.2383, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12205.0380859375
tensor(12208.2383, grad_fn=<NegBackward0>) tensor(12205.0381, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12203.53125
tensor(12205.0381, grad_fn=<NegBackward0>) tensor(12203.5312, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12186.7529296875
tensor(12203.5312, grad_fn=<NegBackward0>) tensor(12186.7529, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12150.2138671875
tensor(12186.7529, grad_fn=<NegBackward0>) tensor(12150.2139, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12134.646484375
tensor(12150.2139, grad_fn=<NegBackward0>) tensor(12134.6465, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12123.4423828125
tensor(12134.6465, grad_fn=<NegBackward0>) tensor(12123.4424, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12112.21875
tensor(12123.4424, grad_fn=<NegBackward0>) tensor(12112.2188, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12112.1904296875
tensor(12112.2188, grad_fn=<NegBackward0>) tensor(12112.1904, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12112.1689453125
tensor(12112.1904, grad_fn=<NegBackward0>) tensor(12112.1689, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12104.484375
tensor(12112.1689, grad_fn=<NegBackward0>) tensor(12104.4844, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12104.474609375
tensor(12104.4844, grad_fn=<NegBackward0>) tensor(12104.4746, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12104.4794921875
tensor(12104.4746, grad_fn=<NegBackward0>) tensor(12104.4795, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -12104.4619140625
tensor(12104.4746, grad_fn=<NegBackward0>) tensor(12104.4619, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12104.4560546875
tensor(12104.4619, grad_fn=<NegBackward0>) tensor(12104.4561, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12104.451171875
tensor(12104.4561, grad_fn=<NegBackward0>) tensor(12104.4512, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12104.4423828125
tensor(12104.4512, grad_fn=<NegBackward0>) tensor(12104.4424, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12101.638671875
tensor(12104.4424, grad_fn=<NegBackward0>) tensor(12101.6387, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12101.64453125
tensor(12101.6387, grad_fn=<NegBackward0>) tensor(12101.6445, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12101.6298828125
tensor(12101.6387, grad_fn=<NegBackward0>) tensor(12101.6299, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12101.62890625
tensor(12101.6299, grad_fn=<NegBackward0>) tensor(12101.6289, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12101.6259765625
tensor(12101.6289, grad_fn=<NegBackward0>) tensor(12101.6260, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12101.6240234375
tensor(12101.6260, grad_fn=<NegBackward0>) tensor(12101.6240, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12101.62109375
tensor(12101.6240, grad_fn=<NegBackward0>) tensor(12101.6211, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12101.6171875
tensor(12101.6211, grad_fn=<NegBackward0>) tensor(12101.6172, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12101.6142578125
tensor(12101.6172, grad_fn=<NegBackward0>) tensor(12101.6143, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12101.6123046875
tensor(12101.6143, grad_fn=<NegBackward0>) tensor(12101.6123, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12101.6279296875
tensor(12101.6123, grad_fn=<NegBackward0>) tensor(12101.6279, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12101.611328125
tensor(12101.6123, grad_fn=<NegBackward0>) tensor(12101.6113, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12101.6103515625
tensor(12101.6113, grad_fn=<NegBackward0>) tensor(12101.6104, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12101.609375
tensor(12101.6104, grad_fn=<NegBackward0>) tensor(12101.6094, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12101.607421875
tensor(12101.6094, grad_fn=<NegBackward0>) tensor(12101.6074, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12101.6142578125
tensor(12101.6074, grad_fn=<NegBackward0>) tensor(12101.6143, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12101.5634765625
tensor(12101.6074, grad_fn=<NegBackward0>) tensor(12101.5635, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12101.556640625
tensor(12101.5635, grad_fn=<NegBackward0>) tensor(12101.5566, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12101.556640625
tensor(12101.5566, grad_fn=<NegBackward0>) tensor(12101.5566, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12101.5537109375
tensor(12101.5566, grad_fn=<NegBackward0>) tensor(12101.5537, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12101.5576171875
tensor(12101.5537, grad_fn=<NegBackward0>) tensor(12101.5576, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12101.5546875
tensor(12101.5537, grad_fn=<NegBackward0>) tensor(12101.5547, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12101.5537109375
tensor(12101.5537, grad_fn=<NegBackward0>) tensor(12101.5537, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12101.5537109375
tensor(12101.5537, grad_fn=<NegBackward0>) tensor(12101.5537, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12101.5546875
tensor(12101.5537, grad_fn=<NegBackward0>) tensor(12101.5547, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12101.552734375
tensor(12101.5537, grad_fn=<NegBackward0>) tensor(12101.5527, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12101.5537109375
tensor(12101.5527, grad_fn=<NegBackward0>) tensor(12101.5537, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12101.5517578125
tensor(12101.5527, grad_fn=<NegBackward0>) tensor(12101.5518, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12101.552734375
tensor(12101.5518, grad_fn=<NegBackward0>) tensor(12101.5527, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12101.5517578125
tensor(12101.5518, grad_fn=<NegBackward0>) tensor(12101.5518, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12101.5576171875
tensor(12101.5518, grad_fn=<NegBackward0>) tensor(12101.5576, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12101.5576171875
tensor(12101.5518, grad_fn=<NegBackward0>) tensor(12101.5576, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12101.5517578125
tensor(12101.5518, grad_fn=<NegBackward0>) tensor(12101.5518, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12101.55078125
tensor(12101.5518, grad_fn=<NegBackward0>) tensor(12101.5508, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12101.5517578125
tensor(12101.5508, grad_fn=<NegBackward0>) tensor(12101.5518, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12101.5498046875
tensor(12101.5508, grad_fn=<NegBackward0>) tensor(12101.5498, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12101.55078125
tensor(12101.5498, grad_fn=<NegBackward0>) tensor(12101.5508, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12101.568359375
tensor(12101.5498, grad_fn=<NegBackward0>) tensor(12101.5684, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12101.552734375
tensor(12101.5498, grad_fn=<NegBackward0>) tensor(12101.5527, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12101.5537109375
tensor(12101.5498, grad_fn=<NegBackward0>) tensor(12101.5537, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -12101.548828125
tensor(12101.5498, grad_fn=<NegBackward0>) tensor(12101.5488, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12101.5478515625
tensor(12101.5488, grad_fn=<NegBackward0>) tensor(12101.5479, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12101.55078125
tensor(12101.5479, grad_fn=<NegBackward0>) tensor(12101.5508, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12101.5498046875
tensor(12101.5479, grad_fn=<NegBackward0>) tensor(12101.5498, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12101.548828125
tensor(12101.5479, grad_fn=<NegBackward0>) tensor(12101.5488, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -12101.5498046875
tensor(12101.5479, grad_fn=<NegBackward0>) tensor(12101.5498, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -12101.583984375
tensor(12101.5479, grad_fn=<NegBackward0>) tensor(12101.5840, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.5578, 0.4422],
        [0.3024, 0.6976]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5406, 0.4594], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2957, 0.1063],
         [0.7301, 0.3045]],

        [[0.7231, 0.1003],
         [0.6045, 0.6655]],

        [[0.6533, 0.1174],
         [0.6139, 0.5824]],

        [[0.5443, 0.0995],
         [0.5251, 0.6756]],

        [[0.7025, 0.0996],
         [0.5635, 0.6161]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.35854597160942825
Average Adjusted Rand Index: 0.9839773697219775
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20455.130859375
inf tensor(20455.1309, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12550.7099609375
tensor(20455.1309, grad_fn=<NegBackward0>) tensor(12550.7100, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12550.111328125
tensor(12550.7100, grad_fn=<NegBackward0>) tensor(12550.1113, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12548.970703125
tensor(12550.1113, grad_fn=<NegBackward0>) tensor(12548.9707, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12547.9814453125
tensor(12548.9707, grad_fn=<NegBackward0>) tensor(12547.9814, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12547.3671875
tensor(12547.9814, grad_fn=<NegBackward0>) tensor(12547.3672, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12546.8271484375
tensor(12547.3672, grad_fn=<NegBackward0>) tensor(12546.8271, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12546.3232421875
tensor(12546.8271, grad_fn=<NegBackward0>) tensor(12546.3232, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12545.9111328125
tensor(12546.3232, grad_fn=<NegBackward0>) tensor(12545.9111, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12482.5966796875
tensor(12545.9111, grad_fn=<NegBackward0>) tensor(12482.5967, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12218.0595703125
tensor(12482.5967, grad_fn=<NegBackward0>) tensor(12218.0596, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12216.3359375
tensor(12218.0596, grad_fn=<NegBackward0>) tensor(12216.3359, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12214.517578125
tensor(12216.3359, grad_fn=<NegBackward0>) tensor(12214.5176, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12214.3037109375
tensor(12214.5176, grad_fn=<NegBackward0>) tensor(12214.3037, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12214.2744140625
tensor(12214.3037, grad_fn=<NegBackward0>) tensor(12214.2744, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12214.1748046875
tensor(12214.2744, grad_fn=<NegBackward0>) tensor(12214.1748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12212.4365234375
tensor(12214.1748, grad_fn=<NegBackward0>) tensor(12212.4365, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12206.0458984375
tensor(12212.4365, grad_fn=<NegBackward0>) tensor(12206.0459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12204.203125
tensor(12206.0459, grad_fn=<NegBackward0>) tensor(12204.2031, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12203.3291015625
tensor(12204.2031, grad_fn=<NegBackward0>) tensor(12203.3291, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12203.3037109375
tensor(12203.3291, grad_fn=<NegBackward0>) tensor(12203.3037, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12200.76953125
tensor(12203.3037, grad_fn=<NegBackward0>) tensor(12200.7695, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12198.5166015625
tensor(12200.7695, grad_fn=<NegBackward0>) tensor(12198.5166, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12198.421875
tensor(12198.5166, grad_fn=<NegBackward0>) tensor(12198.4219, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12198.4169921875
tensor(12198.4219, grad_fn=<NegBackward0>) tensor(12198.4170, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12198.4150390625
tensor(12198.4170, grad_fn=<NegBackward0>) tensor(12198.4150, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12198.4052734375
tensor(12198.4150, grad_fn=<NegBackward0>) tensor(12198.4053, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12198.3798828125
tensor(12198.4053, grad_fn=<NegBackward0>) tensor(12198.3799, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12196.416015625
tensor(12198.3799, grad_fn=<NegBackward0>) tensor(12196.4160, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12196.4111328125
tensor(12196.4160, grad_fn=<NegBackward0>) tensor(12196.4111, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12196.3818359375
tensor(12196.4111, grad_fn=<NegBackward0>) tensor(12196.3818, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12196.3798828125
tensor(12196.3818, grad_fn=<NegBackward0>) tensor(12196.3799, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12196.37890625
tensor(12196.3799, grad_fn=<NegBackward0>) tensor(12196.3789, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12196.3056640625
tensor(12196.3789, grad_fn=<NegBackward0>) tensor(12196.3057, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12196.302734375
tensor(12196.3057, grad_fn=<NegBackward0>) tensor(12196.3027, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12196.3017578125
tensor(12196.3027, grad_fn=<NegBackward0>) tensor(12196.3018, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12196.2998046875
tensor(12196.3018, grad_fn=<NegBackward0>) tensor(12196.2998, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12196.2724609375
tensor(12196.2998, grad_fn=<NegBackward0>) tensor(12196.2725, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12196.26953125
tensor(12196.2725, grad_fn=<NegBackward0>) tensor(12196.2695, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12196.271484375
tensor(12196.2695, grad_fn=<NegBackward0>) tensor(12196.2715, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12196.267578125
tensor(12196.2695, grad_fn=<NegBackward0>) tensor(12196.2676, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12196.2685546875
tensor(12196.2676, grad_fn=<NegBackward0>) tensor(12196.2686, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12196.2666015625
tensor(12196.2676, grad_fn=<NegBackward0>) tensor(12196.2666, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12196.263671875
tensor(12196.2666, grad_fn=<NegBackward0>) tensor(12196.2637, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12194.7998046875
tensor(12196.2637, grad_fn=<NegBackward0>) tensor(12194.7998, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12194.798828125
tensor(12194.7998, grad_fn=<NegBackward0>) tensor(12194.7988, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12192.9599609375
tensor(12194.7988, grad_fn=<NegBackward0>) tensor(12192.9600, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12192.9462890625
tensor(12192.9600, grad_fn=<NegBackward0>) tensor(12192.9463, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12192.9453125
tensor(12192.9463, grad_fn=<NegBackward0>) tensor(12192.9453, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12192.9501953125
tensor(12192.9453, grad_fn=<NegBackward0>) tensor(12192.9502, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12192.9443359375
tensor(12192.9453, grad_fn=<NegBackward0>) tensor(12192.9443, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12192.9482421875
tensor(12192.9443, grad_fn=<NegBackward0>) tensor(12192.9482, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12192.943359375
tensor(12192.9443, grad_fn=<NegBackward0>) tensor(12192.9434, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12192.94140625
tensor(12192.9434, grad_fn=<NegBackward0>) tensor(12192.9414, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12192.93359375
tensor(12192.9414, grad_fn=<NegBackward0>) tensor(12192.9336, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12190.6767578125
tensor(12192.9336, grad_fn=<NegBackward0>) tensor(12190.6768, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12123.0556640625
tensor(12190.6768, grad_fn=<NegBackward0>) tensor(12123.0557, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12122.818359375
tensor(12123.0557, grad_fn=<NegBackward0>) tensor(12122.8184, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12109.392578125
tensor(12122.8184, grad_fn=<NegBackward0>) tensor(12109.3926, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12109.3916015625
tensor(12109.3926, grad_fn=<NegBackward0>) tensor(12109.3916, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12109.3916015625
tensor(12109.3916, grad_fn=<NegBackward0>) tensor(12109.3916, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12109.390625
tensor(12109.3916, grad_fn=<NegBackward0>) tensor(12109.3906, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12109.3896484375
tensor(12109.3906, grad_fn=<NegBackward0>) tensor(12109.3896, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12109.4013671875
tensor(12109.3896, grad_fn=<NegBackward0>) tensor(12109.4014, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12109.38671875
tensor(12109.3896, grad_fn=<NegBackward0>) tensor(12109.3867, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12109.384765625
tensor(12109.3867, grad_fn=<NegBackward0>) tensor(12109.3848, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12109.3828125
tensor(12109.3848, grad_fn=<NegBackward0>) tensor(12109.3828, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12109.3837890625
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3838, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12109.412109375
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.4121, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12109.384765625
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3848, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12109.3828125
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3828, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12109.390625
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3906, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12109.3876953125
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3877, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12109.3828125
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3828, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12109.3818359375
tensor(12109.3828, grad_fn=<NegBackward0>) tensor(12109.3818, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12109.3935546875
tensor(12109.3818, grad_fn=<NegBackward0>) tensor(12109.3936, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12109.3828125
tensor(12109.3818, grad_fn=<NegBackward0>) tensor(12109.3828, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12109.3828125
tensor(12109.3818, grad_fn=<NegBackward0>) tensor(12109.3828, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -12109.380859375
tensor(12109.3818, grad_fn=<NegBackward0>) tensor(12109.3809, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12109.38671875
tensor(12109.3809, grad_fn=<NegBackward0>) tensor(12109.3867, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12101.568359375
tensor(12109.3809, grad_fn=<NegBackward0>) tensor(12101.5684, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12101.5615234375
tensor(12101.5684, grad_fn=<NegBackward0>) tensor(12101.5615, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12101.5625
tensor(12101.5615, grad_fn=<NegBackward0>) tensor(12101.5625, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12101.564453125
tensor(12101.5615, grad_fn=<NegBackward0>) tensor(12101.5645, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12101.5625
tensor(12101.5615, grad_fn=<NegBackward0>) tensor(12101.5625, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12101.560546875
tensor(12101.5615, grad_fn=<NegBackward0>) tensor(12101.5605, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12101.5693359375
tensor(12101.5605, grad_fn=<NegBackward0>) tensor(12101.5693, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12101.5693359375
tensor(12101.5605, grad_fn=<NegBackward0>) tensor(12101.5693, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12101.619140625
tensor(12101.5605, grad_fn=<NegBackward0>) tensor(12101.6191, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12101.5625
tensor(12101.5605, grad_fn=<NegBackward0>) tensor(12101.5625, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -12101.5625
tensor(12101.5605, grad_fn=<NegBackward0>) tensor(12101.5625, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.5564, 0.4436],
        [0.3053, 0.6947]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5395, 0.4605], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2957, 0.1061],
         [0.5762, 0.3055]],

        [[0.5652, 0.1003],
         [0.6597, 0.7128]],

        [[0.5293, 0.1174],
         [0.6512, 0.5397]],

        [[0.6304, 0.0995],
         [0.5594, 0.5302]],

        [[0.5165, 0.0996],
         [0.5979, 0.6322]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.35854597160942825
Average Adjusted Rand Index: 0.9839773697219775
[0.35854597160942825, 0.35854597160942825] [0.9839773697219775, 0.9839773697219775] [12101.583984375, 12101.5625]
-------------------------------------
This iteration is 55
True Objective function: Loss = -11813.622696721859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19165.986328125
inf tensor(19165.9863, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12276.296875
tensor(19165.9863, grad_fn=<NegBackward0>) tensor(12276.2969, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12276.0068359375
tensor(12276.2969, grad_fn=<NegBackward0>) tensor(12276.0068, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12275.9228515625
tensor(12276.0068, grad_fn=<NegBackward0>) tensor(12275.9229, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12275.826171875
tensor(12275.9229, grad_fn=<NegBackward0>) tensor(12275.8262, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12275.6025390625
tensor(12275.8262, grad_fn=<NegBackward0>) tensor(12275.6025, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12273.654296875
tensor(12275.6025, grad_fn=<NegBackward0>) tensor(12273.6543, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12273.298828125
tensor(12273.6543, grad_fn=<NegBackward0>) tensor(12273.2988, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12273.013671875
tensor(12273.2988, grad_fn=<NegBackward0>) tensor(12273.0137, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12271.40625
tensor(12273.0137, grad_fn=<NegBackward0>) tensor(12271.4062, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12269.8544921875
tensor(12271.4062, grad_fn=<NegBackward0>) tensor(12269.8545, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12269.306640625
tensor(12269.8545, grad_fn=<NegBackward0>) tensor(12269.3066, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12269.13671875
tensor(12269.3066, grad_fn=<NegBackward0>) tensor(12269.1367, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12269.0556640625
tensor(12269.1367, grad_fn=<NegBackward0>) tensor(12269.0557, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12269.0078125
tensor(12269.0557, grad_fn=<NegBackward0>) tensor(12269.0078, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12268.9697265625
tensor(12269.0078, grad_fn=<NegBackward0>) tensor(12268.9697, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12268.9482421875
tensor(12268.9697, grad_fn=<NegBackward0>) tensor(12268.9482, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12268.921875
tensor(12268.9482, grad_fn=<NegBackward0>) tensor(12268.9219, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12268.91015625
tensor(12268.9219, grad_fn=<NegBackward0>) tensor(12268.9102, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12268.900390625
tensor(12268.9102, grad_fn=<NegBackward0>) tensor(12268.9004, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12268.8916015625
tensor(12268.9004, grad_fn=<NegBackward0>) tensor(12268.8916, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12268.8857421875
tensor(12268.8916, grad_fn=<NegBackward0>) tensor(12268.8857, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12268.8798828125
tensor(12268.8857, grad_fn=<NegBackward0>) tensor(12268.8799, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12268.8759765625
tensor(12268.8799, grad_fn=<NegBackward0>) tensor(12268.8760, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12268.8720703125
tensor(12268.8760, grad_fn=<NegBackward0>) tensor(12268.8721, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12268.8681640625
tensor(12268.8721, grad_fn=<NegBackward0>) tensor(12268.8682, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12268.86328125
tensor(12268.8682, grad_fn=<NegBackward0>) tensor(12268.8633, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12268.8623046875
tensor(12268.8633, grad_fn=<NegBackward0>) tensor(12268.8623, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12268.8603515625
tensor(12268.8623, grad_fn=<NegBackward0>) tensor(12268.8604, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12268.857421875
tensor(12268.8604, grad_fn=<NegBackward0>) tensor(12268.8574, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12268.85546875
tensor(12268.8574, grad_fn=<NegBackward0>) tensor(12268.8555, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12268.853515625
tensor(12268.8555, grad_fn=<NegBackward0>) tensor(12268.8535, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12268.8515625
tensor(12268.8535, grad_fn=<NegBackward0>) tensor(12268.8516, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12268.8515625
tensor(12268.8516, grad_fn=<NegBackward0>) tensor(12268.8516, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12268.849609375
tensor(12268.8516, grad_fn=<NegBackward0>) tensor(12268.8496, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12268.849609375
tensor(12268.8496, grad_fn=<NegBackward0>) tensor(12268.8496, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12268.84765625
tensor(12268.8496, grad_fn=<NegBackward0>) tensor(12268.8477, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12268.8466796875
tensor(12268.8477, grad_fn=<NegBackward0>) tensor(12268.8467, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12268.8466796875
tensor(12268.8467, grad_fn=<NegBackward0>) tensor(12268.8467, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12268.8447265625
tensor(12268.8467, grad_fn=<NegBackward0>) tensor(12268.8447, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12268.845703125
tensor(12268.8447, grad_fn=<NegBackward0>) tensor(12268.8457, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12268.84375
tensor(12268.8447, grad_fn=<NegBackward0>) tensor(12268.8438, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12268.84375
tensor(12268.8438, grad_fn=<NegBackward0>) tensor(12268.8438, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12268.84375
tensor(12268.8438, grad_fn=<NegBackward0>) tensor(12268.8438, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12268.841796875
tensor(12268.8438, grad_fn=<NegBackward0>) tensor(12268.8418, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12268.8408203125
tensor(12268.8418, grad_fn=<NegBackward0>) tensor(12268.8408, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12268.8408203125
tensor(12268.8408, grad_fn=<NegBackward0>) tensor(12268.8408, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12268.841796875
tensor(12268.8408, grad_fn=<NegBackward0>) tensor(12268.8418, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12268.8388671875
tensor(12268.8408, grad_fn=<NegBackward0>) tensor(12268.8389, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12268.8408203125
tensor(12268.8389, grad_fn=<NegBackward0>) tensor(12268.8408, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12268.8388671875
tensor(12268.8389, grad_fn=<NegBackward0>) tensor(12268.8389, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12268.83984375
tensor(12268.8389, grad_fn=<NegBackward0>) tensor(12268.8398, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12268.837890625
tensor(12268.8389, grad_fn=<NegBackward0>) tensor(12268.8379, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12268.837890625
tensor(12268.8379, grad_fn=<NegBackward0>) tensor(12268.8379, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12268.837890625
tensor(12268.8379, grad_fn=<NegBackward0>) tensor(12268.8379, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12268.8369140625
tensor(12268.8379, grad_fn=<NegBackward0>) tensor(12268.8369, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12268.8369140625
tensor(12268.8369, grad_fn=<NegBackward0>) tensor(12268.8369, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12268.837890625
tensor(12268.8369, grad_fn=<NegBackward0>) tensor(12268.8379, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12268.837890625
tensor(12268.8369, grad_fn=<NegBackward0>) tensor(12268.8379, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12268.8359375
tensor(12268.8369, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12268.83984375
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8398, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12268.8359375
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12268.8369140625
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8369, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12268.8359375
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12268.8359375
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12268.8369140625
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8369, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12268.890625
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8906, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12268.8349609375
tensor(12268.8359, grad_fn=<NegBackward0>) tensor(12268.8350, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12269.0400390625
tensor(12268.8350, grad_fn=<NegBackward0>) tensor(12269.0400, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12268.833984375
tensor(12268.8350, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12268.833984375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12268.833984375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12268.833984375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12268.8359375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12268.8349609375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8350, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12268.833984375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12268.8349609375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8350, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12268.8349609375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8350, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12268.833984375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12268.8359375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12268.833984375
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12268.83203125
tensor(12268.8340, grad_fn=<NegBackward0>) tensor(12268.8320, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12268.8349609375
tensor(12268.8320, grad_fn=<NegBackward0>) tensor(12268.8350, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12268.8359375
tensor(12268.8320, grad_fn=<NegBackward0>) tensor(12268.8359, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12268.833984375
tensor(12268.8320, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12268.833984375
tensor(12268.8320, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -12268.833984375
tensor(12268.8320, grad_fn=<NegBackward0>) tensor(12268.8340, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[9.9999e-01, 1.2187e-05],
        [3.3900e-01, 6.6100e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8249, 0.1751], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.2143],
         [0.6476, 0.2515]],

        [[0.5196, 0.2167],
         [0.5722, 0.6087]],

        [[0.6979, 0.2380],
         [0.7150, 0.5287]],

        [[0.5212, 0.2928],
         [0.5306, 0.6181]],

        [[0.7027, 0.1488],
         [0.7129, 0.7277]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.004682108332720131
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 59
Adjusted Rand Index: -0.020678759622205528
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.019124365424315677
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
Global Adjusted Rand Index: -0.004947426631957965
Average Adjusted Rand Index: -0.008401915417603056
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22398.02734375
inf tensor(22398.0273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12276.3466796875
tensor(22398.0273, grad_fn=<NegBackward0>) tensor(12276.3467, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12276.0615234375
tensor(12276.3467, grad_fn=<NegBackward0>) tensor(12276.0615, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12275.9619140625
tensor(12276.0615, grad_fn=<NegBackward0>) tensor(12275.9619, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12275.857421875
tensor(12275.9619, grad_fn=<NegBackward0>) tensor(12275.8574, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12275.6943359375
tensor(12275.8574, grad_fn=<NegBackward0>) tensor(12275.6943, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12275.4638671875
tensor(12275.6943, grad_fn=<NegBackward0>) tensor(12275.4639, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12275.26953125
tensor(12275.4639, grad_fn=<NegBackward0>) tensor(12275.2695, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12275.1572265625
tensor(12275.2695, grad_fn=<NegBackward0>) tensor(12275.1572, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12275.10546875
tensor(12275.1572, grad_fn=<NegBackward0>) tensor(12275.1055, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12275.0673828125
tensor(12275.1055, grad_fn=<NegBackward0>) tensor(12275.0674, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12275.037109375
tensor(12275.0674, grad_fn=<NegBackward0>) tensor(12275.0371, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12275.0087890625
tensor(12275.0371, grad_fn=<NegBackward0>) tensor(12275.0088, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12274.9814453125
tensor(12275.0088, grad_fn=<NegBackward0>) tensor(12274.9814, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12274.9521484375
tensor(12274.9814, grad_fn=<NegBackward0>) tensor(12274.9521, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12274.9228515625
tensor(12274.9521, grad_fn=<NegBackward0>) tensor(12274.9229, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12274.8857421875
tensor(12274.9229, grad_fn=<NegBackward0>) tensor(12274.8857, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12274.859375
tensor(12274.8857, grad_fn=<NegBackward0>) tensor(12274.8594, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12274.84765625
tensor(12274.8594, grad_fn=<NegBackward0>) tensor(12274.8477, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12274.84375
tensor(12274.8477, grad_fn=<NegBackward0>) tensor(12274.8438, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12274.8388671875
tensor(12274.8438, grad_fn=<NegBackward0>) tensor(12274.8389, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12274.8359375
tensor(12274.8389, grad_fn=<NegBackward0>) tensor(12274.8359, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12274.8349609375
tensor(12274.8359, grad_fn=<NegBackward0>) tensor(12274.8350, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12274.83203125
tensor(12274.8350, grad_fn=<NegBackward0>) tensor(12274.8320, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12274.8330078125
tensor(12274.8320, grad_fn=<NegBackward0>) tensor(12274.8330, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -12274.8310546875
tensor(12274.8320, grad_fn=<NegBackward0>) tensor(12274.8311, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12274.830078125
tensor(12274.8311, grad_fn=<NegBackward0>) tensor(12274.8301, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12274.828125
tensor(12274.8301, grad_fn=<NegBackward0>) tensor(12274.8281, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12274.8291015625
tensor(12274.8281, grad_fn=<NegBackward0>) tensor(12274.8291, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12274.828125
tensor(12274.8281, grad_fn=<NegBackward0>) tensor(12274.8281, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12274.826171875
tensor(12274.8281, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12274.8271484375
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8271, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -12274.8271484375
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8271, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -12274.828125
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8281, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -12274.8271484375
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8271, grad_fn=<NegBackward0>)
4
Iteration 3500: Loss = -12274.826171875
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12274.826171875
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12274.826171875
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12274.826171875
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12274.826171875
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12274.8251953125
tensor(12274.8262, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12274.826171875
tensor(12274.8252, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12274.8251953125
tensor(12274.8252, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12274.8251953125
tensor(12274.8252, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12274.82421875
tensor(12274.8252, grad_fn=<NegBackward0>) tensor(12274.8242, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12274.826171875
tensor(12274.8242, grad_fn=<NegBackward0>) tensor(12274.8262, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12274.8251953125
tensor(12274.8242, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12274.8251953125
tensor(12274.8242, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -12274.8251953125
tensor(12274.8242, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -12274.8251953125
tensor(12274.8242, grad_fn=<NegBackward0>) tensor(12274.8252, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4900 due to no improvement.
pi: tensor([[0.9487, 0.0513],
        [0.9975, 0.0025]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8215, 0.1785], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1981, 0.2084],
         [0.6124, 0.2206]],

        [[0.6403, 0.1721],
         [0.7186, 0.5761]],

        [[0.5221, 0.2184],
         [0.5430, 0.6590]],

        [[0.6385, 0.1093],
         [0.6423, 0.6816]],

        [[0.6810, 0.1813],
         [0.5738, 0.6034]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 66
Adjusted Rand Index: 0.0639954496618846
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0049617437634031656
Average Adjusted Rand Index: 0.01279908993237692
[-0.004947426631957965, 0.0049617437634031656] [-0.008401915417603056, 0.01279908993237692] [12268.833984375, 12274.8251953125]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11930.522254451182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21827.865234375
inf tensor(21827.8652, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12417.4189453125
tensor(21827.8652, grad_fn=<NegBackward0>) tensor(12417.4189, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12416.9404296875
tensor(12417.4189, grad_fn=<NegBackward0>) tensor(12416.9404, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12416.796875
tensor(12416.9404, grad_fn=<NegBackward0>) tensor(12416.7969, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12416.701171875
tensor(12416.7969, grad_fn=<NegBackward0>) tensor(12416.7012, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12416.619140625
tensor(12416.7012, grad_fn=<NegBackward0>) tensor(12416.6191, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12416.546875
tensor(12416.6191, grad_fn=<NegBackward0>) tensor(12416.5469, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12416.470703125
tensor(12416.5469, grad_fn=<NegBackward0>) tensor(12416.4707, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12416.388671875
tensor(12416.4707, grad_fn=<NegBackward0>) tensor(12416.3887, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12416.3056640625
tensor(12416.3887, grad_fn=<NegBackward0>) tensor(12416.3057, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12416.2392578125
tensor(12416.3057, grad_fn=<NegBackward0>) tensor(12416.2393, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12416.1865234375
tensor(12416.2393, grad_fn=<NegBackward0>) tensor(12416.1865, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12416.1416015625
tensor(12416.1865, grad_fn=<NegBackward0>) tensor(12416.1416, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12416.0986328125
tensor(12416.1416, grad_fn=<NegBackward0>) tensor(12416.0986, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12416.0537109375
tensor(12416.0986, grad_fn=<NegBackward0>) tensor(12416.0537, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12416.0068359375
tensor(12416.0537, grad_fn=<NegBackward0>) tensor(12416.0068, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12415.95703125
tensor(12416.0068, grad_fn=<NegBackward0>) tensor(12415.9570, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12415.8994140625
tensor(12415.9570, grad_fn=<NegBackward0>) tensor(12415.8994, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12415.8369140625
tensor(12415.8994, grad_fn=<NegBackward0>) tensor(12415.8369, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12415.7626953125
tensor(12415.8369, grad_fn=<NegBackward0>) tensor(12415.7627, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12415.6787109375
tensor(12415.7627, grad_fn=<NegBackward0>) tensor(12415.6787, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12415.5947265625
tensor(12415.6787, grad_fn=<NegBackward0>) tensor(12415.5947, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12415.51953125
tensor(12415.5947, grad_fn=<NegBackward0>) tensor(12415.5195, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12415.466796875
tensor(12415.5195, grad_fn=<NegBackward0>) tensor(12415.4668, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12415.427734375
tensor(12415.4668, grad_fn=<NegBackward0>) tensor(12415.4277, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12415.4033203125
tensor(12415.4277, grad_fn=<NegBackward0>) tensor(12415.4033, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12415.384765625
tensor(12415.4033, grad_fn=<NegBackward0>) tensor(12415.3848, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12415.373046875
tensor(12415.3848, grad_fn=<NegBackward0>) tensor(12415.3730, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12415.36328125
tensor(12415.3730, grad_fn=<NegBackward0>) tensor(12415.3633, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12415.3564453125
tensor(12415.3633, grad_fn=<NegBackward0>) tensor(12415.3564, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12415.3515625
tensor(12415.3564, grad_fn=<NegBackward0>) tensor(12415.3516, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12415.3466796875
tensor(12415.3516, grad_fn=<NegBackward0>) tensor(12415.3467, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12415.3447265625
tensor(12415.3467, grad_fn=<NegBackward0>) tensor(12415.3447, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12415.341796875
tensor(12415.3447, grad_fn=<NegBackward0>) tensor(12415.3418, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12415.3408203125
tensor(12415.3418, grad_fn=<NegBackward0>) tensor(12415.3408, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12415.3388671875
tensor(12415.3408, grad_fn=<NegBackward0>) tensor(12415.3389, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12415.3359375
tensor(12415.3389, grad_fn=<NegBackward0>) tensor(12415.3359, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12415.3349609375
tensor(12415.3359, grad_fn=<NegBackward0>) tensor(12415.3350, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12415.3349609375
tensor(12415.3350, grad_fn=<NegBackward0>) tensor(12415.3350, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12415.3330078125
tensor(12415.3350, grad_fn=<NegBackward0>) tensor(12415.3330, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12415.33203125
tensor(12415.3330, grad_fn=<NegBackward0>) tensor(12415.3320, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12415.3330078125
tensor(12415.3320, grad_fn=<NegBackward0>) tensor(12415.3330, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12415.33203125
tensor(12415.3320, grad_fn=<NegBackward0>) tensor(12415.3320, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12415.330078125
tensor(12415.3320, grad_fn=<NegBackward0>) tensor(12415.3301, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12415.328125
tensor(12415.3301, grad_fn=<NegBackward0>) tensor(12415.3281, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12415.3291015625
tensor(12415.3281, grad_fn=<NegBackward0>) tensor(12415.3291, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12415.328125
tensor(12415.3281, grad_fn=<NegBackward0>) tensor(12415.3281, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12415.3271484375
tensor(12415.3281, grad_fn=<NegBackward0>) tensor(12415.3271, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12415.326171875
tensor(12415.3271, grad_fn=<NegBackward0>) tensor(12415.3262, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12415.326171875
tensor(12415.3262, grad_fn=<NegBackward0>) tensor(12415.3262, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12415.326171875
tensor(12415.3262, grad_fn=<NegBackward0>) tensor(12415.3262, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12415.32421875
tensor(12415.3262, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12415.3251953125
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12415.32421875
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12415.3232421875
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12415.3251953125
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12415.32421875
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12415.32421875
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12415.32421875
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -12415.3232421875
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12415.3232421875
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12415.3251953125
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12415.32421875
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12415.322265625
tensor(12415.3232, grad_fn=<NegBackward0>) tensor(12415.3223, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12415.330078125
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3301, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12415.322265625
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3223, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12415.3232421875
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12415.32421875
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12415.32421875
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12415.32421875
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
4
Iteration 7000: Loss = -12415.3232421875
tensor(12415.3223, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.9316, 0.0684],
        [0.9905, 0.0095]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0466, 0.9534], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.2054],
         [0.6647, 0.2070]],

        [[0.6754, 0.1517],
         [0.5382, 0.5261]],

        [[0.6974, 0.2127],
         [0.5037, 0.5365]],

        [[0.5801, 0.1654],
         [0.7292, 0.6344]],

        [[0.5928, 0.2195],
         [0.6680, 0.5531]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012838829882453615
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21543.12890625
inf tensor(21543.1289, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12417.595703125
tensor(21543.1289, grad_fn=<NegBackward0>) tensor(12417.5957, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12417.068359375
tensor(12417.5957, grad_fn=<NegBackward0>) tensor(12417.0684, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12416.921875
tensor(12417.0684, grad_fn=<NegBackward0>) tensor(12416.9219, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12416.8408203125
tensor(12416.9219, grad_fn=<NegBackward0>) tensor(12416.8408, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12416.78125
tensor(12416.8408, grad_fn=<NegBackward0>) tensor(12416.7812, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12416.7333984375
tensor(12416.7812, grad_fn=<NegBackward0>) tensor(12416.7334, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12416.6875
tensor(12416.7334, grad_fn=<NegBackward0>) tensor(12416.6875, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12416.6376953125
tensor(12416.6875, grad_fn=<NegBackward0>) tensor(12416.6377, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12416.5712890625
tensor(12416.6377, grad_fn=<NegBackward0>) tensor(12416.5713, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12416.4892578125
tensor(12416.5713, grad_fn=<NegBackward0>) tensor(12416.4893, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12416.39453125
tensor(12416.4893, grad_fn=<NegBackward0>) tensor(12416.3945, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12416.298828125
tensor(12416.3945, grad_fn=<NegBackward0>) tensor(12416.2988, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12416.21484375
tensor(12416.2988, grad_fn=<NegBackward0>) tensor(12416.2148, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12416.150390625
tensor(12416.2148, grad_fn=<NegBackward0>) tensor(12416.1504, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12416.1015625
tensor(12416.1504, grad_fn=<NegBackward0>) tensor(12416.1016, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12416.0517578125
tensor(12416.1016, grad_fn=<NegBackward0>) tensor(12416.0518, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12416.0029296875
tensor(12416.0518, grad_fn=<NegBackward0>) tensor(12416.0029, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12415.9521484375
tensor(12416.0029, grad_fn=<NegBackward0>) tensor(12415.9521, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12415.900390625
tensor(12415.9521, grad_fn=<NegBackward0>) tensor(12415.9004, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12415.8447265625
tensor(12415.9004, grad_fn=<NegBackward0>) tensor(12415.8447, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12415.78125
tensor(12415.8447, grad_fn=<NegBackward0>) tensor(12415.7812, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12415.705078125
tensor(12415.7812, grad_fn=<NegBackward0>) tensor(12415.7051, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12415.6181640625
tensor(12415.7051, grad_fn=<NegBackward0>) tensor(12415.6182, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12415.5341796875
tensor(12415.6182, grad_fn=<NegBackward0>) tensor(12415.5342, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12415.4697265625
tensor(12415.5342, grad_fn=<NegBackward0>) tensor(12415.4697, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12415.427734375
tensor(12415.4697, grad_fn=<NegBackward0>) tensor(12415.4277, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12415.3994140625
tensor(12415.4277, grad_fn=<NegBackward0>) tensor(12415.3994, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12415.3798828125
tensor(12415.3994, grad_fn=<NegBackward0>) tensor(12415.3799, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12415.369140625
tensor(12415.3799, grad_fn=<NegBackward0>) tensor(12415.3691, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12415.357421875
tensor(12415.3691, grad_fn=<NegBackward0>) tensor(12415.3574, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12415.3515625
tensor(12415.3574, grad_fn=<NegBackward0>) tensor(12415.3516, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12415.345703125
tensor(12415.3516, grad_fn=<NegBackward0>) tensor(12415.3457, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12415.34375
tensor(12415.3457, grad_fn=<NegBackward0>) tensor(12415.3438, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12415.3408203125
tensor(12415.3438, grad_fn=<NegBackward0>) tensor(12415.3408, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12415.337890625
tensor(12415.3408, grad_fn=<NegBackward0>) tensor(12415.3379, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12415.3369140625
tensor(12415.3379, grad_fn=<NegBackward0>) tensor(12415.3369, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12415.333984375
tensor(12415.3369, grad_fn=<NegBackward0>) tensor(12415.3340, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12415.3310546875
tensor(12415.3340, grad_fn=<NegBackward0>) tensor(12415.3311, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12415.3310546875
tensor(12415.3311, grad_fn=<NegBackward0>) tensor(12415.3311, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12415.330078125
tensor(12415.3311, grad_fn=<NegBackward0>) tensor(12415.3301, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12415.3291015625
tensor(12415.3301, grad_fn=<NegBackward0>) tensor(12415.3291, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12415.3291015625
tensor(12415.3291, grad_fn=<NegBackward0>) tensor(12415.3291, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12415.3271484375
tensor(12415.3291, grad_fn=<NegBackward0>) tensor(12415.3271, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12415.326171875
tensor(12415.3271, grad_fn=<NegBackward0>) tensor(12415.3262, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12415.326171875
tensor(12415.3262, grad_fn=<NegBackward0>) tensor(12415.3262, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12415.3251953125
tensor(12415.3262, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12415.32421875
tensor(12415.3252, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12415.3251953125
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12415.32421875
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12415.3251953125
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12415.3251953125
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3252, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12415.32421875
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12415.3212890625
tensor(12415.3242, grad_fn=<NegBackward0>) tensor(12415.3213, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12415.3232421875
tensor(12415.3213, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12415.3232421875
tensor(12415.3213, grad_fn=<NegBackward0>) tensor(12415.3232, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12415.32421875
tensor(12415.3213, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12415.32421875
tensor(12415.3213, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12415.32421875
tensor(12415.3213, grad_fn=<NegBackward0>) tensor(12415.3242, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.0084, 0.9916],
        [0.0688, 0.9312]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9371, 0.0629], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2070, 0.2053],
         [0.6613, 0.1997]],

        [[0.5848, 0.1512],
         [0.5966, 0.5001]],

        [[0.6645, 0.2127],
         [0.5205, 0.7217]],

        [[0.5182, 0.1655],
         [0.6940, 0.6295]],

        [[0.5931, 0.2195],
         [0.7226, 0.6135]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0012838829882453615
Average Adjusted Rand Index: 0.0
[-0.0012838829882453615, -0.0012838829882453615] [0.0, 0.0] [12415.3232421875, 12415.32421875]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11882.859168513682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22155.720703125
inf tensor(22155.7207, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11867.705078125
tensor(22155.7207, grad_fn=<NegBackward0>) tensor(11867.7051, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11864.673828125
tensor(11867.7051, grad_fn=<NegBackward0>) tensor(11864.6738, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11864.5126953125
tensor(11864.6738, grad_fn=<NegBackward0>) tensor(11864.5127, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11864.4423828125
tensor(11864.5127, grad_fn=<NegBackward0>) tensor(11864.4424, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11864.4052734375
tensor(11864.4424, grad_fn=<NegBackward0>) tensor(11864.4053, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11864.384765625
tensor(11864.4053, grad_fn=<NegBackward0>) tensor(11864.3848, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11864.3681640625
tensor(11864.3848, grad_fn=<NegBackward0>) tensor(11864.3682, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11864.3583984375
tensor(11864.3682, grad_fn=<NegBackward0>) tensor(11864.3584, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11864.349609375
tensor(11864.3584, grad_fn=<NegBackward0>) tensor(11864.3496, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11864.3466796875
tensor(11864.3496, grad_fn=<NegBackward0>) tensor(11864.3467, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11864.33984375
tensor(11864.3467, grad_fn=<NegBackward0>) tensor(11864.3398, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11864.3369140625
tensor(11864.3398, grad_fn=<NegBackward0>) tensor(11864.3369, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11864.3349609375
tensor(11864.3369, grad_fn=<NegBackward0>) tensor(11864.3350, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11864.33203125
tensor(11864.3350, grad_fn=<NegBackward0>) tensor(11864.3320, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11864.3310546875
tensor(11864.3320, grad_fn=<NegBackward0>) tensor(11864.3311, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11864.3291015625
tensor(11864.3311, grad_fn=<NegBackward0>) tensor(11864.3291, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11864.3291015625
tensor(11864.3291, grad_fn=<NegBackward0>) tensor(11864.3291, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11864.3271484375
tensor(11864.3291, grad_fn=<NegBackward0>) tensor(11864.3271, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11864.3251953125
tensor(11864.3271, grad_fn=<NegBackward0>) tensor(11864.3252, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11864.328125
tensor(11864.3252, grad_fn=<NegBackward0>) tensor(11864.3281, grad_fn=<NegBackward0>)
1
Iteration 2100: Loss = -11864.3251953125
tensor(11864.3252, grad_fn=<NegBackward0>) tensor(11864.3252, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11864.32421875
tensor(11864.3252, grad_fn=<NegBackward0>) tensor(11864.3242, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11864.3232421875
tensor(11864.3242, grad_fn=<NegBackward0>) tensor(11864.3232, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11864.322265625
tensor(11864.3232, grad_fn=<NegBackward0>) tensor(11864.3223, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11864.32421875
tensor(11864.3223, grad_fn=<NegBackward0>) tensor(11864.3242, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11864.322265625
tensor(11864.3223, grad_fn=<NegBackward0>) tensor(11864.3223, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11864.322265625
tensor(11864.3223, grad_fn=<NegBackward0>) tensor(11864.3223, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11864.3212890625
tensor(11864.3223, grad_fn=<NegBackward0>) tensor(11864.3213, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11864.3212890625
tensor(11864.3213, grad_fn=<NegBackward0>) tensor(11864.3213, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11864.3212890625
tensor(11864.3213, grad_fn=<NegBackward0>) tensor(11864.3213, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11864.3203125
tensor(11864.3213, grad_fn=<NegBackward0>) tensor(11864.3203, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11864.3203125
tensor(11864.3203, grad_fn=<NegBackward0>) tensor(11864.3203, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11864.3193359375
tensor(11864.3203, grad_fn=<NegBackward0>) tensor(11864.3193, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11864.3193359375
tensor(11864.3193, grad_fn=<NegBackward0>) tensor(11864.3193, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11864.3203125
tensor(11864.3193, grad_fn=<NegBackward0>) tensor(11864.3203, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11864.318359375
tensor(11864.3193, grad_fn=<NegBackward0>) tensor(11864.3184, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11864.3203125
tensor(11864.3184, grad_fn=<NegBackward0>) tensor(11864.3203, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11864.3193359375
tensor(11864.3184, grad_fn=<NegBackward0>) tensor(11864.3193, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11864.3173828125
tensor(11864.3184, grad_fn=<NegBackward0>) tensor(11864.3174, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11864.3193359375
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3193, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -11864.3173828125
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3174, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11864.318359375
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3184, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11864.318359375
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3184, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -11864.318359375
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3184, grad_fn=<NegBackward0>)
3
Iteration 4500: Loss = -11864.3173828125
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3174, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11864.3173828125
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3174, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11864.3154296875
tensor(11864.3174, grad_fn=<NegBackward0>) tensor(11864.3154, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11864.31640625
tensor(11864.3154, grad_fn=<NegBackward0>) tensor(11864.3164, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11864.318359375
tensor(11864.3154, grad_fn=<NegBackward0>) tensor(11864.3184, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -11864.322265625
tensor(11864.3154, grad_fn=<NegBackward0>) tensor(11864.3223, grad_fn=<NegBackward0>)
3
Iteration 5100: Loss = -11864.3173828125
tensor(11864.3154, grad_fn=<NegBackward0>) tensor(11864.3174, grad_fn=<NegBackward0>)
4
Iteration 5200: Loss = -11864.31640625
tensor(11864.3154, grad_fn=<NegBackward0>) tensor(11864.3164, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5200 due to no improvement.
pi: tensor([[0.2402, 0.7598],
        [0.7630, 0.2370]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4890, 0.5110], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2946, 0.0976],
         [0.6354, 0.3010]],

        [[0.7273, 0.0994],
         [0.7001, 0.5644]],

        [[0.5450, 0.1054],
         [0.6251, 0.7249]],

        [[0.5199, 0.0979],
         [0.6199, 0.5027]],

        [[0.6034, 0.1065],
         [0.6892, 0.6731]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.03493891201588048
Average Adjusted Rand Index: 0.9526443416567867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20710.244140625
inf tensor(20710.2441, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12358.4541015625
tensor(20710.2441, grad_fn=<NegBackward0>) tensor(12358.4541, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12357.9794921875
tensor(12358.4541, grad_fn=<NegBackward0>) tensor(12357.9795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12357.021484375
tensor(12357.9795, grad_fn=<NegBackward0>) tensor(12357.0215, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12356.24609375
tensor(12357.0215, grad_fn=<NegBackward0>) tensor(12356.2461, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12356.013671875
tensor(12356.2461, grad_fn=<NegBackward0>) tensor(12356.0137, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12355.64453125
tensor(12356.0137, grad_fn=<NegBackward0>) tensor(12355.6445, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12355.056640625
tensor(12355.6445, grad_fn=<NegBackward0>) tensor(12355.0566, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12354.630859375
tensor(12355.0566, grad_fn=<NegBackward0>) tensor(12354.6309, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12354.431640625
tensor(12354.6309, grad_fn=<NegBackward0>) tensor(12354.4316, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12354.31640625
tensor(12354.4316, grad_fn=<NegBackward0>) tensor(12354.3164, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12354.2333984375
tensor(12354.3164, grad_fn=<NegBackward0>) tensor(12354.2334, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12354.1669921875
tensor(12354.2334, grad_fn=<NegBackward0>) tensor(12354.1670, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12354.1083984375
tensor(12354.1670, grad_fn=<NegBackward0>) tensor(12354.1084, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12354.060546875
tensor(12354.1084, grad_fn=<NegBackward0>) tensor(12354.0605, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12354.0224609375
tensor(12354.0605, grad_fn=<NegBackward0>) tensor(12354.0225, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12353.994140625
tensor(12354.0225, grad_fn=<NegBackward0>) tensor(12353.9941, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12353.9716796875
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12353.9717, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12353.9541015625
tensor(12353.9717, grad_fn=<NegBackward0>) tensor(12353.9541, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12353.939453125
tensor(12353.9541, grad_fn=<NegBackward0>) tensor(12353.9395, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12353.927734375
tensor(12353.9395, grad_fn=<NegBackward0>) tensor(12353.9277, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12353.919921875
tensor(12353.9277, grad_fn=<NegBackward0>) tensor(12353.9199, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12353.9111328125
tensor(12353.9199, grad_fn=<NegBackward0>) tensor(12353.9111, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12353.904296875
tensor(12353.9111, grad_fn=<NegBackward0>) tensor(12353.9043, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12353.90234375
tensor(12353.9043, grad_fn=<NegBackward0>) tensor(12353.9023, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12353.896484375
tensor(12353.9023, grad_fn=<NegBackward0>) tensor(12353.8965, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12353.8935546875
tensor(12353.8965, grad_fn=<NegBackward0>) tensor(12353.8936, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12353.8916015625
tensor(12353.8936, grad_fn=<NegBackward0>) tensor(12353.8916, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12353.8896484375
tensor(12353.8916, grad_fn=<NegBackward0>) tensor(12353.8896, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12353.8876953125
tensor(12353.8896, grad_fn=<NegBackward0>) tensor(12353.8877, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12353.88671875
tensor(12353.8877, grad_fn=<NegBackward0>) tensor(12353.8867, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12353.8837890625
tensor(12353.8867, grad_fn=<NegBackward0>) tensor(12353.8838, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12353.8828125
tensor(12353.8838, grad_fn=<NegBackward0>) tensor(12353.8828, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12353.8828125
tensor(12353.8828, grad_fn=<NegBackward0>) tensor(12353.8828, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12353.8818359375
tensor(12353.8828, grad_fn=<NegBackward0>) tensor(12353.8818, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12353.8828125
tensor(12353.8818, grad_fn=<NegBackward0>) tensor(12353.8828, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12353.880859375
tensor(12353.8818, grad_fn=<NegBackward0>) tensor(12353.8809, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12353.8798828125
tensor(12353.8809, grad_fn=<NegBackward0>) tensor(12353.8799, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12353.8798828125
tensor(12353.8799, grad_fn=<NegBackward0>) tensor(12353.8799, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12353.880859375
tensor(12353.8799, grad_fn=<NegBackward0>) tensor(12353.8809, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12353.87890625
tensor(12353.8799, grad_fn=<NegBackward0>) tensor(12353.8789, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12353.8798828125
tensor(12353.8789, grad_fn=<NegBackward0>) tensor(12353.8799, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12353.8798828125
tensor(12353.8789, grad_fn=<NegBackward0>) tensor(12353.8799, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -12353.8798828125
tensor(12353.8789, grad_fn=<NegBackward0>) tensor(12353.8799, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -12353.8798828125
tensor(12353.8789, grad_fn=<NegBackward0>) tensor(12353.8799, grad_fn=<NegBackward0>)
4
Iteration 4500: Loss = -12353.8779296875
tensor(12353.8789, grad_fn=<NegBackward0>) tensor(12353.8779, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12353.8779296875
tensor(12353.8779, grad_fn=<NegBackward0>) tensor(12353.8779, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12353.8779296875
tensor(12353.8779, grad_fn=<NegBackward0>) tensor(12353.8779, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12353.8779296875
tensor(12353.8779, grad_fn=<NegBackward0>) tensor(12353.8779, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12353.876953125
tensor(12353.8779, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12353.87890625
tensor(12353.8770, grad_fn=<NegBackward0>) tensor(12353.8789, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12353.87890625
tensor(12353.8770, grad_fn=<NegBackward0>) tensor(12353.8789, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12353.8779296875
tensor(12353.8770, grad_fn=<NegBackward0>) tensor(12353.8779, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -12353.8759765625
tensor(12353.8770, grad_fn=<NegBackward0>) tensor(12353.8760, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12353.876953125
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12353.876953125
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12353.8759765625
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8760, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12353.8759765625
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8760, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12353.876953125
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12353.876953125
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12353.8759765625
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8760, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12353.875
tensor(12353.8760, grad_fn=<NegBackward0>) tensor(12353.8750, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12353.8759765625
tensor(12353.8750, grad_fn=<NegBackward0>) tensor(12353.8760, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12353.8759765625
tensor(12353.8750, grad_fn=<NegBackward0>) tensor(12353.8760, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12353.876953125
tensor(12353.8750, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12353.8779296875
tensor(12353.8750, grad_fn=<NegBackward0>) tensor(12353.8779, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -12353.876953125
tensor(12353.8750, grad_fn=<NegBackward0>) tensor(12353.8770, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.6335, 0.3665],
        [0.0079, 0.9921]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0138, 0.9862], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1530, 0.3205],
         [0.5532, 0.1986]],

        [[0.6720, 0.3081],
         [0.6644, 0.5065]],

        [[0.5853, 0.2058],
         [0.5453, 0.7188]],

        [[0.5836, 0.2361],
         [0.7180, 0.7016]],

        [[0.5225, 0.0946],
         [0.6274, 0.5667]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 0.000125253510288723
Average Adjusted Rand Index: 0.000117190084965842
[0.03493891201588048, 0.000125253510288723] [0.9526443416567867, 0.000117190084965842] [11864.31640625, 12353.876953125]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11910.266620838333
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23879.892578125
inf tensor(23879.8926, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12465.4970703125
tensor(23879.8926, grad_fn=<NegBackward0>) tensor(12465.4971, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12465.1748046875
tensor(12465.4971, grad_fn=<NegBackward0>) tensor(12465.1748, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12465.0830078125
tensor(12465.1748, grad_fn=<NegBackward0>) tensor(12465.0830, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12465.0361328125
tensor(12465.0830, grad_fn=<NegBackward0>) tensor(12465.0361, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12465.0048828125
tensor(12465.0361, grad_fn=<NegBackward0>) tensor(12465.0049, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12464.982421875
tensor(12465.0049, grad_fn=<NegBackward0>) tensor(12464.9824, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12464.9638671875
tensor(12464.9824, grad_fn=<NegBackward0>) tensor(12464.9639, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12464.94921875
tensor(12464.9639, grad_fn=<NegBackward0>) tensor(12464.9492, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12464.9375
tensor(12464.9492, grad_fn=<NegBackward0>) tensor(12464.9375, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12464.927734375
tensor(12464.9375, grad_fn=<NegBackward0>) tensor(12464.9277, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12464.91796875
tensor(12464.9277, grad_fn=<NegBackward0>) tensor(12464.9180, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12464.91015625
tensor(12464.9180, grad_fn=<NegBackward0>) tensor(12464.9102, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12464.9013671875
tensor(12464.9102, grad_fn=<NegBackward0>) tensor(12464.9014, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12464.8955078125
tensor(12464.9014, grad_fn=<NegBackward0>) tensor(12464.8955, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12464.8876953125
tensor(12464.8955, grad_fn=<NegBackward0>) tensor(12464.8877, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12464.876953125
tensor(12464.8877, grad_fn=<NegBackward0>) tensor(12464.8770, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12464.8466796875
tensor(12464.8770, grad_fn=<NegBackward0>) tensor(12464.8467, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12464.31640625
tensor(12464.8467, grad_fn=<NegBackward0>) tensor(12464.3164, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12464.15234375
tensor(12464.3164, grad_fn=<NegBackward0>) tensor(12464.1523, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12464.0849609375
tensor(12464.1523, grad_fn=<NegBackward0>) tensor(12464.0850, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12464.0546875
tensor(12464.0850, grad_fn=<NegBackward0>) tensor(12464.0547, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12464.041015625
tensor(12464.0547, grad_fn=<NegBackward0>) tensor(12464.0410, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12464.0302734375
tensor(12464.0410, grad_fn=<NegBackward0>) tensor(12464.0303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12464.025390625
tensor(12464.0303, grad_fn=<NegBackward0>) tensor(12464.0254, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12464.01953125
tensor(12464.0254, grad_fn=<NegBackward0>) tensor(12464.0195, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12464.015625
tensor(12464.0195, grad_fn=<NegBackward0>) tensor(12464.0156, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12464.0107421875
tensor(12464.0156, grad_fn=<NegBackward0>) tensor(12464.0107, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12464.0078125
tensor(12464.0107, grad_fn=<NegBackward0>) tensor(12464.0078, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12464.00390625
tensor(12464.0078, grad_fn=<NegBackward0>) tensor(12464.0039, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12464.0
tensor(12464.0039, grad_fn=<NegBackward0>) tensor(12464., grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12463.994140625
tensor(12464., grad_fn=<NegBackward0>) tensor(12463.9941, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12463.990234375
tensor(12463.9941, grad_fn=<NegBackward0>) tensor(12463.9902, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12463.9833984375
tensor(12463.9902, grad_fn=<NegBackward0>) tensor(12463.9834, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12463.974609375
tensor(12463.9834, grad_fn=<NegBackward0>) tensor(12463.9746, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12463.9619140625
tensor(12463.9746, grad_fn=<NegBackward0>) tensor(12463.9619, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12463.9267578125
tensor(12463.9619, grad_fn=<NegBackward0>) tensor(12463.9268, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12463.541015625
tensor(12463.9268, grad_fn=<NegBackward0>) tensor(12463.5410, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12462.5009765625
tensor(12463.5410, grad_fn=<NegBackward0>) tensor(12462.5010, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12462.365234375
tensor(12462.5010, grad_fn=<NegBackward0>) tensor(12462.3652, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12462.32421875
tensor(12462.3652, grad_fn=<NegBackward0>) tensor(12462.3242, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12462.306640625
tensor(12462.3242, grad_fn=<NegBackward0>) tensor(12462.3066, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12462.296875
tensor(12462.3066, grad_fn=<NegBackward0>) tensor(12462.2969, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12462.2919921875
tensor(12462.2969, grad_fn=<NegBackward0>) tensor(12462.2920, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12462.287109375
tensor(12462.2920, grad_fn=<NegBackward0>) tensor(12462.2871, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12462.2841796875
tensor(12462.2871, grad_fn=<NegBackward0>) tensor(12462.2842, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12462.2822265625
tensor(12462.2842, grad_fn=<NegBackward0>) tensor(12462.2822, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12462.28125
tensor(12462.2822, grad_fn=<NegBackward0>) tensor(12462.2812, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12462.2822265625
tensor(12462.2812, grad_fn=<NegBackward0>) tensor(12462.2822, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12462.28125
tensor(12462.2812, grad_fn=<NegBackward0>) tensor(12462.2812, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12462.279296875
tensor(12462.2812, grad_fn=<NegBackward0>) tensor(12462.2793, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12462.279296875
tensor(12462.2793, grad_fn=<NegBackward0>) tensor(12462.2793, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12462.2802734375
tensor(12462.2793, grad_fn=<NegBackward0>) tensor(12462.2803, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12462.2783203125
tensor(12462.2793, grad_fn=<NegBackward0>) tensor(12462.2783, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12462.2783203125
tensor(12462.2783, grad_fn=<NegBackward0>) tensor(12462.2783, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12462.2783203125
tensor(12462.2783, grad_fn=<NegBackward0>) tensor(12462.2783, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12462.2783203125
tensor(12462.2783, grad_fn=<NegBackward0>) tensor(12462.2783, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12462.27734375
tensor(12462.2783, grad_fn=<NegBackward0>) tensor(12462.2773, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12462.2763671875
tensor(12462.2773, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12462.275390625
tensor(12462.2764, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12462.2763671875
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12462.275390625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12462.2763671875
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12462.275390625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12462.2763671875
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12462.275390625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12462.275390625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12462.275390625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12462.2744140625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2744, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12462.2734375
tensor(12462.2744, grad_fn=<NegBackward0>) tensor(12462.2734, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12462.275390625
tensor(12462.2734, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12462.275390625
tensor(12462.2734, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12462.27734375
tensor(12462.2734, grad_fn=<NegBackward0>) tensor(12462.2773, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -12462.2890625
tensor(12462.2734, grad_fn=<NegBackward0>) tensor(12462.2891, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -12462.275390625
tensor(12462.2734, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.9904, 0.0096],
        [0.2339, 0.7661]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9856, 0.0144], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.3257],
         [0.6285, 0.3287]],

        [[0.5227, 0.2574],
         [0.7127, 0.6724]],

        [[0.5420, 0.2568],
         [0.5384, 0.5363]],

        [[0.6186, 0.2550],
         [0.7261, 0.5280]],

        [[0.6858, 0.2558],
         [0.6975, 0.6389]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -5.7831832162466576e-05
Average Adjusted Rand Index: -0.001167928327375977
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23310.9453125
inf tensor(23310.9453, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12465.6240234375
tensor(23310.9453, grad_fn=<NegBackward0>) tensor(12465.6240, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12465.1640625
tensor(12465.6240, grad_fn=<NegBackward0>) tensor(12465.1641, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12465.0703125
tensor(12465.1641, grad_fn=<NegBackward0>) tensor(12465.0703, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12465.0224609375
tensor(12465.0703, grad_fn=<NegBackward0>) tensor(12465.0225, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12464.9912109375
tensor(12465.0225, grad_fn=<NegBackward0>) tensor(12464.9912, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12464.9677734375
tensor(12464.9912, grad_fn=<NegBackward0>) tensor(12464.9678, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12464.9501953125
tensor(12464.9678, grad_fn=<NegBackward0>) tensor(12464.9502, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12464.9345703125
tensor(12464.9502, grad_fn=<NegBackward0>) tensor(12464.9346, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12464.9248046875
tensor(12464.9346, grad_fn=<NegBackward0>) tensor(12464.9248, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12464.9130859375
tensor(12464.9248, grad_fn=<NegBackward0>) tensor(12464.9131, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12464.9052734375
tensor(12464.9131, grad_fn=<NegBackward0>) tensor(12464.9053, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12464.896484375
tensor(12464.9053, grad_fn=<NegBackward0>) tensor(12464.8965, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12464.8896484375
tensor(12464.8965, grad_fn=<NegBackward0>) tensor(12464.8896, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12464.8828125
tensor(12464.8896, grad_fn=<NegBackward0>) tensor(12464.8828, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12464.8759765625
tensor(12464.8828, grad_fn=<NegBackward0>) tensor(12464.8760, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12464.8681640625
tensor(12464.8760, grad_fn=<NegBackward0>) tensor(12464.8682, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12464.861328125
tensor(12464.8682, grad_fn=<NegBackward0>) tensor(12464.8613, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12464.8515625
tensor(12464.8613, grad_fn=<NegBackward0>) tensor(12464.8516, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12464.7958984375
tensor(12464.8516, grad_fn=<NegBackward0>) tensor(12464.7959, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12464.255859375
tensor(12464.7959, grad_fn=<NegBackward0>) tensor(12464.2559, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12464.1142578125
tensor(12464.2559, grad_fn=<NegBackward0>) tensor(12464.1143, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12464.0576171875
tensor(12464.1143, grad_fn=<NegBackward0>) tensor(12464.0576, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12464.0302734375
tensor(12464.0576, grad_fn=<NegBackward0>) tensor(12464.0303, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12464.013671875
tensor(12464.0303, grad_fn=<NegBackward0>) tensor(12464.0137, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12464.001953125
tensor(12464.0137, grad_fn=<NegBackward0>) tensor(12464.0020, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12463.9951171875
tensor(12464.0020, grad_fn=<NegBackward0>) tensor(12463.9951, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12463.986328125
tensor(12463.9951, grad_fn=<NegBackward0>) tensor(12463.9863, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12463.978515625
tensor(12463.9863, grad_fn=<NegBackward0>) tensor(12463.9785, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12463.966796875
tensor(12463.9785, grad_fn=<NegBackward0>) tensor(12463.9668, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12463.9453125
tensor(12463.9668, grad_fn=<NegBackward0>) tensor(12463.9453, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12463.8505859375
tensor(12463.9453, grad_fn=<NegBackward0>) tensor(12463.8506, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12462.8984375
tensor(12463.8506, grad_fn=<NegBackward0>) tensor(12462.8984, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12462.439453125
tensor(12462.8984, grad_fn=<NegBackward0>) tensor(12462.4395, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12462.3505859375
tensor(12462.4395, grad_fn=<NegBackward0>) tensor(12462.3506, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12462.3203125
tensor(12462.3506, grad_fn=<NegBackward0>) tensor(12462.3203, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12462.3056640625
tensor(12462.3203, grad_fn=<NegBackward0>) tensor(12462.3057, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12462.2958984375
tensor(12462.3057, grad_fn=<NegBackward0>) tensor(12462.2959, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12462.2900390625
tensor(12462.2959, grad_fn=<NegBackward0>) tensor(12462.2900, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12462.2880859375
tensor(12462.2900, grad_fn=<NegBackward0>) tensor(12462.2881, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12462.28515625
tensor(12462.2881, grad_fn=<NegBackward0>) tensor(12462.2852, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12462.283203125
tensor(12462.2852, grad_fn=<NegBackward0>) tensor(12462.2832, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12462.283203125
tensor(12462.2832, grad_fn=<NegBackward0>) tensor(12462.2832, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12462.28125
tensor(12462.2832, grad_fn=<NegBackward0>) tensor(12462.2812, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12462.2802734375
tensor(12462.2812, grad_fn=<NegBackward0>) tensor(12462.2803, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12462.2802734375
tensor(12462.2803, grad_fn=<NegBackward0>) tensor(12462.2803, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12462.279296875
tensor(12462.2803, grad_fn=<NegBackward0>) tensor(12462.2793, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12462.27734375
tensor(12462.2793, grad_fn=<NegBackward0>) tensor(12462.2773, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12462.27734375
tensor(12462.2773, grad_fn=<NegBackward0>) tensor(12462.2773, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12462.2763671875
tensor(12462.2773, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12462.2763671875
tensor(12462.2764, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12462.2763671875
tensor(12462.2764, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12462.275390625
tensor(12462.2764, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12462.2744140625
tensor(12462.2754, grad_fn=<NegBackward0>) tensor(12462.2744, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12462.27734375
tensor(12462.2744, grad_fn=<NegBackward0>) tensor(12462.2773, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12462.2763671875
tensor(12462.2744, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12462.2763671875
tensor(12462.2744, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12462.275390625
tensor(12462.2744, grad_fn=<NegBackward0>) tensor(12462.2754, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12462.2763671875
tensor(12462.2744, grad_fn=<NegBackward0>) tensor(12462.2764, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.7650, 0.2350],
        [0.0096, 0.9904]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0145, 0.9855], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3291, 0.3256],
         [0.6022, 0.1993]],

        [[0.5236, 0.2575],
         [0.5312, 0.7310]],

        [[0.6920, 0.2569],
         [0.6648, 0.7243]],

        [[0.6239, 0.2552],
         [0.7016, 0.6970]],

        [[0.6026, 0.2560],
         [0.5817, 0.7239]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -5.7831832162466576e-05
Average Adjusted Rand Index: -0.001167928327375977
[-5.7831832162466576e-05, -5.7831832162466576e-05] [-0.001167928327375977, -0.001167928327375977] [12462.275390625, 12462.2763671875]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11946.793615683191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20361.046875
inf tensor(20361.0469, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12381.3134765625
tensor(20361.0469, grad_fn=<NegBackward0>) tensor(12381.3135, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12380.943359375
tensor(12381.3135, grad_fn=<NegBackward0>) tensor(12380.9434, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12380.857421875
tensor(12380.9434, grad_fn=<NegBackward0>) tensor(12380.8574, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12380.810546875
tensor(12380.8574, grad_fn=<NegBackward0>) tensor(12380.8105, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12380.7841796875
tensor(12380.8105, grad_fn=<NegBackward0>) tensor(12380.7842, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12380.765625
tensor(12380.7842, grad_fn=<NegBackward0>) tensor(12380.7656, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12380.7529296875
tensor(12380.7656, grad_fn=<NegBackward0>) tensor(12380.7529, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12380.7431640625
tensor(12380.7529, grad_fn=<NegBackward0>) tensor(12380.7432, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12380.734375
tensor(12380.7432, grad_fn=<NegBackward0>) tensor(12380.7344, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12380.7275390625
tensor(12380.7344, grad_fn=<NegBackward0>) tensor(12380.7275, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12380.720703125
tensor(12380.7275, grad_fn=<NegBackward0>) tensor(12380.7207, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12380.7138671875
tensor(12380.7207, grad_fn=<NegBackward0>) tensor(12380.7139, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12380.703125
tensor(12380.7139, grad_fn=<NegBackward0>) tensor(12380.7031, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12380.6953125
tensor(12380.7031, grad_fn=<NegBackward0>) tensor(12380.6953, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12380.6806640625
tensor(12380.6953, grad_fn=<NegBackward0>) tensor(12380.6807, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12380.66796875
tensor(12380.6807, grad_fn=<NegBackward0>) tensor(12380.6680, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12380.650390625
tensor(12380.6680, grad_fn=<NegBackward0>) tensor(12380.6504, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12380.630859375
tensor(12380.6504, grad_fn=<NegBackward0>) tensor(12380.6309, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12380.6083984375
tensor(12380.6309, grad_fn=<NegBackward0>) tensor(12380.6084, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12380.5869140625
tensor(12380.6084, grad_fn=<NegBackward0>) tensor(12380.5869, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12380.5625
tensor(12380.5869, grad_fn=<NegBackward0>) tensor(12380.5625, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12380.5400390625
tensor(12380.5625, grad_fn=<NegBackward0>) tensor(12380.5400, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12380.5205078125
tensor(12380.5400, grad_fn=<NegBackward0>) tensor(12380.5205, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12380.501953125
tensor(12380.5205, grad_fn=<NegBackward0>) tensor(12380.5020, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12380.4873046875
tensor(12380.5020, grad_fn=<NegBackward0>) tensor(12380.4873, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12380.474609375
tensor(12380.4873, grad_fn=<NegBackward0>) tensor(12380.4746, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12380.4619140625
tensor(12380.4746, grad_fn=<NegBackward0>) tensor(12380.4619, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12380.4482421875
tensor(12380.4619, grad_fn=<NegBackward0>) tensor(12380.4482, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12380.4375
tensor(12380.4482, grad_fn=<NegBackward0>) tensor(12380.4375, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12380.427734375
tensor(12380.4375, grad_fn=<NegBackward0>) tensor(12380.4277, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12380.416015625
tensor(12380.4277, grad_fn=<NegBackward0>) tensor(12380.4160, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12380.404296875
tensor(12380.4160, grad_fn=<NegBackward0>) tensor(12380.4043, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12380.392578125
tensor(12380.4043, grad_fn=<NegBackward0>) tensor(12380.3926, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12380.3828125
tensor(12380.3926, grad_fn=<NegBackward0>) tensor(12380.3828, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12380.3759765625
tensor(12380.3828, grad_fn=<NegBackward0>) tensor(12380.3760, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12380.3720703125
tensor(12380.3760, grad_fn=<NegBackward0>) tensor(12380.3721, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12380.369140625
tensor(12380.3721, grad_fn=<NegBackward0>) tensor(12380.3691, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12380.3681640625
tensor(12380.3691, grad_fn=<NegBackward0>) tensor(12380.3682, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12380.365234375
tensor(12380.3682, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12380.369140625
tensor(12380.3652, grad_fn=<NegBackward0>) tensor(12380.3691, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12380.3642578125
tensor(12380.3652, grad_fn=<NegBackward0>) tensor(12380.3643, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12380.365234375
tensor(12380.3643, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12380.36328125
tensor(12380.3643, grad_fn=<NegBackward0>) tensor(12380.3633, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12380.365234375
tensor(12380.3633, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12380.365234375
tensor(12380.3633, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12380.365234375
tensor(12380.3633, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12380.3720703125
tensor(12380.3633, grad_fn=<NegBackward0>) tensor(12380.3721, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -12380.365234375
tensor(12380.3633, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.3440, 0.6560],
        [0.9681, 0.0319]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9931, 0.0069], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.1985],
         [0.7143, 0.2031]],

        [[0.7045, 0.2029],
         [0.5577, 0.5586]],

        [[0.6724, 0.1959],
         [0.6354, 0.5047]],

        [[0.5594, 0.1982],
         [0.7247, 0.6735]],

        [[0.5842, 0.2045],
         [0.6792, 0.6021]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0085796864645491
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001558188928143107
Average Adjusted Rand Index: -0.00171593729290982
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20863.9609375
inf tensor(20863.9609, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12381.2529296875
tensor(20863.9609, grad_fn=<NegBackward0>) tensor(12381.2529, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12380.8505859375
tensor(12381.2529, grad_fn=<NegBackward0>) tensor(12380.8506, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12380.755859375
tensor(12380.8506, grad_fn=<NegBackward0>) tensor(12380.7559, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12380.7109375
tensor(12380.7559, grad_fn=<NegBackward0>) tensor(12380.7109, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12380.6845703125
tensor(12380.7109, grad_fn=<NegBackward0>) tensor(12380.6846, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12380.6669921875
tensor(12380.6846, grad_fn=<NegBackward0>) tensor(12380.6670, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12380.6513671875
tensor(12380.6670, grad_fn=<NegBackward0>) tensor(12380.6514, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12380.6357421875
tensor(12380.6514, grad_fn=<NegBackward0>) tensor(12380.6357, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12380.6201171875
tensor(12380.6357, grad_fn=<NegBackward0>) tensor(12380.6201, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12380.6025390625
tensor(12380.6201, grad_fn=<NegBackward0>) tensor(12380.6025, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12380.5849609375
tensor(12380.6025, grad_fn=<NegBackward0>) tensor(12380.5850, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12380.56640625
tensor(12380.5850, grad_fn=<NegBackward0>) tensor(12380.5664, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12380.546875
tensor(12380.5664, grad_fn=<NegBackward0>) tensor(12380.5469, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12380.529296875
tensor(12380.5469, grad_fn=<NegBackward0>) tensor(12380.5293, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12380.513671875
tensor(12380.5293, grad_fn=<NegBackward0>) tensor(12380.5137, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12380.5
tensor(12380.5137, grad_fn=<NegBackward0>) tensor(12380.5000, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12380.484375
tensor(12380.5000, grad_fn=<NegBackward0>) tensor(12380.4844, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12380.474609375
tensor(12380.4844, grad_fn=<NegBackward0>) tensor(12380.4746, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12380.46484375
tensor(12380.4746, grad_fn=<NegBackward0>) tensor(12380.4648, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12380.4541015625
tensor(12380.4648, grad_fn=<NegBackward0>) tensor(12380.4541, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12380.4453125
tensor(12380.4541, grad_fn=<NegBackward0>) tensor(12380.4453, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12380.4345703125
tensor(12380.4453, grad_fn=<NegBackward0>) tensor(12380.4346, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12380.4248046875
tensor(12380.4346, grad_fn=<NegBackward0>) tensor(12380.4248, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12380.4150390625
tensor(12380.4248, grad_fn=<NegBackward0>) tensor(12380.4150, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12380.4033203125
tensor(12380.4150, grad_fn=<NegBackward0>) tensor(12380.4033, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12380.392578125
tensor(12380.4033, grad_fn=<NegBackward0>) tensor(12380.3926, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12380.380859375
tensor(12380.3926, grad_fn=<NegBackward0>) tensor(12380.3809, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12380.373046875
tensor(12380.3809, grad_fn=<NegBackward0>) tensor(12380.3730, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12380.3681640625
tensor(12380.3730, grad_fn=<NegBackward0>) tensor(12380.3682, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12380.365234375
tensor(12380.3682, grad_fn=<NegBackward0>) tensor(12380.3652, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12380.36328125
tensor(12380.3652, grad_fn=<NegBackward0>) tensor(12380.3633, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12380.361328125
tensor(12380.3633, grad_fn=<NegBackward0>) tensor(12380.3613, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12380.3623046875
tensor(12380.3613, grad_fn=<NegBackward0>) tensor(12380.3623, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12380.361328125
tensor(12380.3613, grad_fn=<NegBackward0>) tensor(12380.3613, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12380.3623046875
tensor(12380.3613, grad_fn=<NegBackward0>) tensor(12380.3623, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12380.361328125
tensor(12380.3613, grad_fn=<NegBackward0>) tensor(12380.3613, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12380.359375
tensor(12380.3613, grad_fn=<NegBackward0>) tensor(12380.3594, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12380.359375
tensor(12380.3594, grad_fn=<NegBackward0>) tensor(12380.3594, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12380.359375
tensor(12380.3594, grad_fn=<NegBackward0>) tensor(12380.3594, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12380.3583984375
tensor(12380.3594, grad_fn=<NegBackward0>) tensor(12380.3584, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12380.3583984375
tensor(12380.3584, grad_fn=<NegBackward0>) tensor(12380.3584, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12380.357421875
tensor(12380.3584, grad_fn=<NegBackward0>) tensor(12380.3574, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12380.3583984375
tensor(12380.3574, grad_fn=<NegBackward0>) tensor(12380.3584, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12380.35546875
tensor(12380.3574, grad_fn=<NegBackward0>) tensor(12380.3555, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12380.35546875
tensor(12380.3555, grad_fn=<NegBackward0>) tensor(12380.3555, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12380.353515625
tensor(12380.3555, grad_fn=<NegBackward0>) tensor(12380.3535, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12380.3525390625
tensor(12380.3535, grad_fn=<NegBackward0>) tensor(12380.3525, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12380.353515625
tensor(12380.3525, grad_fn=<NegBackward0>) tensor(12380.3535, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12380.3525390625
tensor(12380.3525, grad_fn=<NegBackward0>) tensor(12380.3525, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12380.3515625
tensor(12380.3525, grad_fn=<NegBackward0>) tensor(12380.3516, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12380.349609375
tensor(12380.3516, grad_fn=<NegBackward0>) tensor(12380.3496, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12380.3515625
tensor(12380.3496, grad_fn=<NegBackward0>) tensor(12380.3516, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12380.3486328125
tensor(12380.3496, grad_fn=<NegBackward0>) tensor(12380.3486, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12380.3505859375
tensor(12380.3486, grad_fn=<NegBackward0>) tensor(12380.3506, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12380.3486328125
tensor(12380.3486, grad_fn=<NegBackward0>) tensor(12380.3486, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12380.34765625
tensor(12380.3486, grad_fn=<NegBackward0>) tensor(12380.3477, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12380.3486328125
tensor(12380.3477, grad_fn=<NegBackward0>) tensor(12380.3486, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12380.34765625
tensor(12380.3477, grad_fn=<NegBackward0>) tensor(12380.3477, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12380.34765625
tensor(12380.3477, grad_fn=<NegBackward0>) tensor(12380.3477, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12380.3466796875
tensor(12380.3477, grad_fn=<NegBackward0>) tensor(12380.3467, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12380.34765625
tensor(12380.3467, grad_fn=<NegBackward0>) tensor(12380.3477, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12380.345703125
tensor(12380.3467, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12380.421875
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.4219, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12380.345703125
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3457, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12380.3466796875
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3467, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12380.34375
tensor(12380.3457, grad_fn=<NegBackward0>) tensor(12380.3438, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12380.3447265625
tensor(12380.3438, grad_fn=<NegBackward0>) tensor(12380.3447, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12380.3466796875
tensor(12380.3438, grad_fn=<NegBackward0>) tensor(12380.3467, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12380.3466796875
tensor(12380.3438, grad_fn=<NegBackward0>) tensor(12380.3467, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -12380.3466796875
tensor(12380.3438, grad_fn=<NegBackward0>) tensor(12380.3467, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -12380.3447265625
tensor(12380.3438, grad_fn=<NegBackward0>) tensor(12380.3447, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.3466, 0.6534],
        [0.9365, 0.0635]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0029, 0.9971], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2018, 0.2003],
         [0.5819, 0.1980]],

        [[0.5207, 0.2078],
         [0.6507, 0.7165]],

        [[0.7252, 0.1960],
         [0.6898, 0.7088]],

        [[0.6586, 0.1971],
         [0.6581, 0.5728]],

        [[0.6822, 0.2036],
         [0.5907, 0.7075]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.001983822866076629
Global Adjusted Rand Index: -0.0013964162689525453
Average Adjusted Rand Index: -0.00039676457321532576
[-0.001558188928143107, -0.0013964162689525453] [-0.00171593729290982, -0.00039676457321532576] [12380.365234375, 12380.3447265625]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11664.226479156672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22006.8671875
inf tensor(22006.8672, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12129.5546875
tensor(22006.8672, grad_fn=<NegBackward0>) tensor(12129.5547, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12128.806640625
tensor(12129.5547, grad_fn=<NegBackward0>) tensor(12128.8066, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12128.634765625
tensor(12128.8066, grad_fn=<NegBackward0>) tensor(12128.6348, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12128.5361328125
tensor(12128.6348, grad_fn=<NegBackward0>) tensor(12128.5361, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12128.47265625
tensor(12128.5361, grad_fn=<NegBackward0>) tensor(12128.4727, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12128.423828125
tensor(12128.4727, grad_fn=<NegBackward0>) tensor(12128.4238, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12128.3837890625
tensor(12128.4238, grad_fn=<NegBackward0>) tensor(12128.3838, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12128.3486328125
tensor(12128.3838, grad_fn=<NegBackward0>) tensor(12128.3486, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12128.322265625
tensor(12128.3486, grad_fn=<NegBackward0>) tensor(12128.3223, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12128.2998046875
tensor(12128.3223, grad_fn=<NegBackward0>) tensor(12128.2998, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12128.2783203125
tensor(12128.2998, grad_fn=<NegBackward0>) tensor(12128.2783, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12128.2587890625
tensor(12128.2783, grad_fn=<NegBackward0>) tensor(12128.2588, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12128.240234375
tensor(12128.2588, grad_fn=<NegBackward0>) tensor(12128.2402, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12128.2177734375
tensor(12128.2402, grad_fn=<NegBackward0>) tensor(12128.2178, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12128.1904296875
tensor(12128.2178, grad_fn=<NegBackward0>) tensor(12128.1904, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12128.142578125
tensor(12128.1904, grad_fn=<NegBackward0>) tensor(12128.1426, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12128.0458984375
tensor(12128.1426, grad_fn=<NegBackward0>) tensor(12128.0459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12127.8369140625
tensor(12128.0459, grad_fn=<NegBackward0>) tensor(12127.8369, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12127.5947265625
tensor(12127.8369, grad_fn=<NegBackward0>) tensor(12127.5947, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12127.44921875
tensor(12127.5947, grad_fn=<NegBackward0>) tensor(12127.4492, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12127.3310546875
tensor(12127.4492, grad_fn=<NegBackward0>) tensor(12127.3311, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12127.20703125
tensor(12127.3311, grad_fn=<NegBackward0>) tensor(12127.2070, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12127.04296875
tensor(12127.2070, grad_fn=<NegBackward0>) tensor(12127.0430, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12126.4345703125
tensor(12127.0430, grad_fn=<NegBackward0>) tensor(12126.4346, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12125.697265625
tensor(12126.4346, grad_fn=<NegBackward0>) tensor(12125.6973, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12125.3818359375
tensor(12125.6973, grad_fn=<NegBackward0>) tensor(12125.3818, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12125.2119140625
tensor(12125.3818, grad_fn=<NegBackward0>) tensor(12125.2119, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12125.1083984375
tensor(12125.2119, grad_fn=<NegBackward0>) tensor(12125.1084, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12125.0400390625
tensor(12125.1084, grad_fn=<NegBackward0>) tensor(12125.0400, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12124.9921875
tensor(12125.0400, grad_fn=<NegBackward0>) tensor(12124.9922, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12124.958984375
tensor(12124.9922, grad_fn=<NegBackward0>) tensor(12124.9590, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12124.93359375
tensor(12124.9590, grad_fn=<NegBackward0>) tensor(12124.9336, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12124.9150390625
tensor(12124.9336, grad_fn=<NegBackward0>) tensor(12124.9150, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12124.8984375
tensor(12124.9150, grad_fn=<NegBackward0>) tensor(12124.8984, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12124.888671875
tensor(12124.8984, grad_fn=<NegBackward0>) tensor(12124.8887, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12124.8779296875
tensor(12124.8887, grad_fn=<NegBackward0>) tensor(12124.8779, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12124.8701171875
tensor(12124.8779, grad_fn=<NegBackward0>) tensor(12124.8701, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12124.861328125
tensor(12124.8701, grad_fn=<NegBackward0>) tensor(12124.8613, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12124.85546875
tensor(12124.8613, grad_fn=<NegBackward0>) tensor(12124.8555, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12124.8515625
tensor(12124.8555, grad_fn=<NegBackward0>) tensor(12124.8516, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12124.8466796875
tensor(12124.8516, grad_fn=<NegBackward0>) tensor(12124.8467, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12124.84375
tensor(12124.8467, grad_fn=<NegBackward0>) tensor(12124.8438, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12124.8388671875
tensor(12124.8438, grad_fn=<NegBackward0>) tensor(12124.8389, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12124.837890625
tensor(12124.8389, grad_fn=<NegBackward0>) tensor(12124.8379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12124.8330078125
tensor(12124.8379, grad_fn=<NegBackward0>) tensor(12124.8330, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12124.8310546875
tensor(12124.8330, grad_fn=<NegBackward0>) tensor(12124.8311, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12124.828125
tensor(12124.8311, grad_fn=<NegBackward0>) tensor(12124.8281, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12124.828125
tensor(12124.8281, grad_fn=<NegBackward0>) tensor(12124.8281, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12124.8251953125
tensor(12124.8281, grad_fn=<NegBackward0>) tensor(12124.8252, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12124.826171875
tensor(12124.8252, grad_fn=<NegBackward0>) tensor(12124.8262, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12124.822265625
tensor(12124.8252, grad_fn=<NegBackward0>) tensor(12124.8223, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12124.8232421875
tensor(12124.8223, grad_fn=<NegBackward0>) tensor(12124.8232, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12124.8203125
tensor(12124.8223, grad_fn=<NegBackward0>) tensor(12124.8203, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12124.818359375
tensor(12124.8203, grad_fn=<NegBackward0>) tensor(12124.8184, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12124.8193359375
tensor(12124.8184, grad_fn=<NegBackward0>) tensor(12124.8193, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12124.818359375
tensor(12124.8184, grad_fn=<NegBackward0>) tensor(12124.8184, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12124.8193359375
tensor(12124.8184, grad_fn=<NegBackward0>) tensor(12124.8193, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12124.81640625
tensor(12124.8184, grad_fn=<NegBackward0>) tensor(12124.8164, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12124.8154296875
tensor(12124.8164, grad_fn=<NegBackward0>) tensor(12124.8154, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12124.8154296875
tensor(12124.8154, grad_fn=<NegBackward0>) tensor(12124.8154, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12124.8134765625
tensor(12124.8154, grad_fn=<NegBackward0>) tensor(12124.8135, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12124.814453125
tensor(12124.8135, grad_fn=<NegBackward0>) tensor(12124.8145, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12124.8134765625
tensor(12124.8135, grad_fn=<NegBackward0>) tensor(12124.8135, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12124.8125
tensor(12124.8135, grad_fn=<NegBackward0>) tensor(12124.8125, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12124.8125
tensor(12124.8125, grad_fn=<NegBackward0>) tensor(12124.8125, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12124.810546875
tensor(12124.8125, grad_fn=<NegBackward0>) tensor(12124.8105, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12124.8095703125
tensor(12124.8105, grad_fn=<NegBackward0>) tensor(12124.8096, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12124.810546875
tensor(12124.8096, grad_fn=<NegBackward0>) tensor(12124.8105, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12124.828125
tensor(12124.8096, grad_fn=<NegBackward0>) tensor(12124.8281, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12124.8095703125
tensor(12124.8096, grad_fn=<NegBackward0>) tensor(12124.8096, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12124.80859375
tensor(12124.8096, grad_fn=<NegBackward0>) tensor(12124.8086, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12124.80859375
tensor(12124.8086, grad_fn=<NegBackward0>) tensor(12124.8086, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12124.80859375
tensor(12124.8086, grad_fn=<NegBackward0>) tensor(12124.8086, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12124.8076171875
tensor(12124.8086, grad_fn=<NegBackward0>) tensor(12124.8076, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12124.806640625
tensor(12124.8076, grad_fn=<NegBackward0>) tensor(12124.8066, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12124.8134765625
tensor(12124.8066, grad_fn=<NegBackward0>) tensor(12124.8135, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12124.806640625
tensor(12124.8066, grad_fn=<NegBackward0>) tensor(12124.8066, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12124.8056640625
tensor(12124.8066, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12124.8056640625
tensor(12124.8057, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12124.8056640625
tensor(12124.8057, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12124.8056640625
tensor(12124.8057, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12124.9794921875
tensor(12124.8057, grad_fn=<NegBackward0>) tensor(12124.9795, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12124.8056640625
tensor(12124.8057, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12124.8046875
tensor(12124.8057, grad_fn=<NegBackward0>) tensor(12124.8047, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12124.8056640625
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12124.806640625
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8066, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12124.8046875
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8047, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12124.8046875
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8047, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12124.8046875
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8047, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12125.1123046875
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12125.1123, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12124.8056640625
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12124.8037109375
tensor(12124.8047, grad_fn=<NegBackward0>) tensor(12124.8037, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12124.8046875
tensor(12124.8037, grad_fn=<NegBackward0>) tensor(12124.8047, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12124.8046875
tensor(12124.8037, grad_fn=<NegBackward0>) tensor(12124.8047, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -12124.8095703125
tensor(12124.8037, grad_fn=<NegBackward0>) tensor(12124.8096, grad_fn=<NegBackward0>)
3
Iteration 9600: Loss = -12124.8056640625
tensor(12124.8037, grad_fn=<NegBackward0>) tensor(12124.8057, grad_fn=<NegBackward0>)
4
Iteration 9700: Loss = -12124.978515625
tensor(12124.8037, grad_fn=<NegBackward0>) tensor(12124.9785, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[9.9999e-01, 7.5422e-06],
        [2.2489e-01, 7.7511e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9607, 0.0393], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1939, 0.2365],
         [0.6124, 0.2493]],

        [[0.7132, 0.1168],
         [0.5606, 0.6354]],

        [[0.6785, 0.1988],
         [0.6860, 0.6674]],

        [[0.6440, 0.2583],
         [0.7232, 0.6782]],

        [[0.7016, 0.1069],
         [0.6851, 0.6351]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: 0.0036693409817386774
Average Adjusted Rand Index: 0.003962819798794847
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21437.873046875
inf tensor(21437.8730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12128.9794921875
tensor(21437.8730, grad_fn=<NegBackward0>) tensor(12128.9795, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12128.560546875
tensor(12128.9795, grad_fn=<NegBackward0>) tensor(12128.5605, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12128.4658203125
tensor(12128.5605, grad_fn=<NegBackward0>) tensor(12128.4658, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12128.408203125
tensor(12128.4658, grad_fn=<NegBackward0>) tensor(12128.4082, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12128.3623046875
tensor(12128.4082, grad_fn=<NegBackward0>) tensor(12128.3623, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12128.3232421875
tensor(12128.3623, grad_fn=<NegBackward0>) tensor(12128.3232, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12128.2900390625
tensor(12128.3232, grad_fn=<NegBackward0>) tensor(12128.2900, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12128.2626953125
tensor(12128.2900, grad_fn=<NegBackward0>) tensor(12128.2627, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12128.240234375
tensor(12128.2627, grad_fn=<NegBackward0>) tensor(12128.2402, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12128.224609375
tensor(12128.2402, grad_fn=<NegBackward0>) tensor(12128.2246, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12128.212890625
tensor(12128.2246, grad_fn=<NegBackward0>) tensor(12128.2129, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12128.2001953125
tensor(12128.2129, grad_fn=<NegBackward0>) tensor(12128.2002, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12128.1865234375
tensor(12128.2002, grad_fn=<NegBackward0>) tensor(12128.1865, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12128.1748046875
tensor(12128.1865, grad_fn=<NegBackward0>) tensor(12128.1748, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12128.1630859375
tensor(12128.1748, grad_fn=<NegBackward0>) tensor(12128.1631, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12128.1484375
tensor(12128.1631, grad_fn=<NegBackward0>) tensor(12128.1484, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12128.1337890625
tensor(12128.1484, grad_fn=<NegBackward0>) tensor(12128.1338, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12128.119140625
tensor(12128.1338, grad_fn=<NegBackward0>) tensor(12128.1191, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12128.1025390625
tensor(12128.1191, grad_fn=<NegBackward0>) tensor(12128.1025, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12128.0810546875
tensor(12128.1025, grad_fn=<NegBackward0>) tensor(12128.0811, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12128.0595703125
tensor(12128.0811, grad_fn=<NegBackward0>) tensor(12128.0596, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12128.0341796875
tensor(12128.0596, grad_fn=<NegBackward0>) tensor(12128.0342, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12128.00390625
tensor(12128.0342, grad_fn=<NegBackward0>) tensor(12128.0039, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12127.970703125
tensor(12128.0039, grad_fn=<NegBackward0>) tensor(12127.9707, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12127.931640625
tensor(12127.9707, grad_fn=<NegBackward0>) tensor(12127.9316, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12127.888671875
tensor(12127.9316, grad_fn=<NegBackward0>) tensor(12127.8887, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12127.83984375
tensor(12127.8887, grad_fn=<NegBackward0>) tensor(12127.8398, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12127.791015625
tensor(12127.8398, grad_fn=<NegBackward0>) tensor(12127.7910, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12127.7451171875
tensor(12127.7910, grad_fn=<NegBackward0>) tensor(12127.7451, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12127.7041015625
tensor(12127.7451, grad_fn=<NegBackward0>) tensor(12127.7041, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12127.666015625
tensor(12127.7041, grad_fn=<NegBackward0>) tensor(12127.6660, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12127.626953125
tensor(12127.6660, grad_fn=<NegBackward0>) tensor(12127.6270, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12127.587890625
tensor(12127.6270, grad_fn=<NegBackward0>) tensor(12127.5879, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12127.5546875
tensor(12127.5879, grad_fn=<NegBackward0>) tensor(12127.5547, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12127.5244140625
tensor(12127.5547, grad_fn=<NegBackward0>) tensor(12127.5244, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12127.5009765625
tensor(12127.5244, grad_fn=<NegBackward0>) tensor(12127.5010, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12127.478515625
tensor(12127.5010, grad_fn=<NegBackward0>) tensor(12127.4785, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12127.4599609375
tensor(12127.4785, grad_fn=<NegBackward0>) tensor(12127.4600, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12127.4453125
tensor(12127.4600, grad_fn=<NegBackward0>) tensor(12127.4453, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12127.4326171875
tensor(12127.4453, grad_fn=<NegBackward0>) tensor(12127.4326, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12127.4228515625
tensor(12127.4326, grad_fn=<NegBackward0>) tensor(12127.4229, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12127.4150390625
tensor(12127.4229, grad_fn=<NegBackward0>) tensor(12127.4150, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12127.412109375
tensor(12127.4150, grad_fn=<NegBackward0>) tensor(12127.4121, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12127.408203125
tensor(12127.4121, grad_fn=<NegBackward0>) tensor(12127.4082, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12127.4072265625
tensor(12127.4082, grad_fn=<NegBackward0>) tensor(12127.4072, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12127.40625
tensor(12127.4072, grad_fn=<NegBackward0>) tensor(12127.4062, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12127.4052734375
tensor(12127.4062, grad_fn=<NegBackward0>) tensor(12127.4053, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12127.40625
tensor(12127.4053, grad_fn=<NegBackward0>) tensor(12127.4062, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12127.404296875
tensor(12127.4053, grad_fn=<NegBackward0>) tensor(12127.4043, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12127.40234375
tensor(12127.4043, grad_fn=<NegBackward0>) tensor(12127.4023, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12127.400390625
tensor(12127.4023, grad_fn=<NegBackward0>) tensor(12127.4004, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12127.3994140625
tensor(12127.4004, grad_fn=<NegBackward0>) tensor(12127.3994, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12127.3994140625
tensor(12127.3994, grad_fn=<NegBackward0>) tensor(12127.3994, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12127.396484375
tensor(12127.3994, grad_fn=<NegBackward0>) tensor(12127.3965, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12127.3955078125
tensor(12127.3965, grad_fn=<NegBackward0>) tensor(12127.3955, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12127.3935546875
tensor(12127.3955, grad_fn=<NegBackward0>) tensor(12127.3936, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12127.390625
tensor(12127.3936, grad_fn=<NegBackward0>) tensor(12127.3906, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12127.3896484375
tensor(12127.3906, grad_fn=<NegBackward0>) tensor(12127.3896, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12127.3857421875
tensor(12127.3896, grad_fn=<NegBackward0>) tensor(12127.3857, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12127.3818359375
tensor(12127.3857, grad_fn=<NegBackward0>) tensor(12127.3818, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12127.37890625
tensor(12127.3818, grad_fn=<NegBackward0>) tensor(12127.3789, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12127.376953125
tensor(12127.3789, grad_fn=<NegBackward0>) tensor(12127.3770, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12127.3740234375
tensor(12127.3770, grad_fn=<NegBackward0>) tensor(12127.3740, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12127.3701171875
tensor(12127.3740, grad_fn=<NegBackward0>) tensor(12127.3701, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12127.37109375
tensor(12127.3701, grad_fn=<NegBackward0>) tensor(12127.3711, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12127.3671875
tensor(12127.3701, grad_fn=<NegBackward0>) tensor(12127.3672, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12127.3671875
tensor(12127.3672, grad_fn=<NegBackward0>) tensor(12127.3672, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12127.3671875
tensor(12127.3672, grad_fn=<NegBackward0>) tensor(12127.3672, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12127.365234375
tensor(12127.3672, grad_fn=<NegBackward0>) tensor(12127.3652, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12127.3642578125
tensor(12127.3652, grad_fn=<NegBackward0>) tensor(12127.3643, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12127.365234375
tensor(12127.3643, grad_fn=<NegBackward0>) tensor(12127.3652, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12127.36328125
tensor(12127.3643, grad_fn=<NegBackward0>) tensor(12127.3633, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12127.42578125
tensor(12127.3633, grad_fn=<NegBackward0>) tensor(12127.4258, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12127.3623046875
tensor(12127.3633, grad_fn=<NegBackward0>) tensor(12127.3623, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12127.390625
tensor(12127.3623, grad_fn=<NegBackward0>) tensor(12127.3906, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12127.3623046875
tensor(12127.3623, grad_fn=<NegBackward0>) tensor(12127.3623, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12127.3623046875
tensor(12127.3623, grad_fn=<NegBackward0>) tensor(12127.3623, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12127.365234375
tensor(12127.3623, grad_fn=<NegBackward0>) tensor(12127.3652, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12127.361328125
tensor(12127.3623, grad_fn=<NegBackward0>) tensor(12127.3613, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12127.361328125
tensor(12127.3613, grad_fn=<NegBackward0>) tensor(12127.3613, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12127.361328125
tensor(12127.3613, grad_fn=<NegBackward0>) tensor(12127.3613, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12127.3623046875
tensor(12127.3613, grad_fn=<NegBackward0>) tensor(12127.3623, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12127.4326171875
tensor(12127.3613, grad_fn=<NegBackward0>) tensor(12127.4326, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12127.361328125
tensor(12127.3613, grad_fn=<NegBackward0>) tensor(12127.3613, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12127.3603515625
tensor(12127.3613, grad_fn=<NegBackward0>) tensor(12127.3604, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12127.38671875
tensor(12127.3604, grad_fn=<NegBackward0>) tensor(12127.3867, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12127.3623046875
tensor(12127.3604, grad_fn=<NegBackward0>) tensor(12127.3623, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12127.4228515625
tensor(12127.3604, grad_fn=<NegBackward0>) tensor(12127.4229, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12127.3642578125
tensor(12127.3604, grad_fn=<NegBackward0>) tensor(12127.3643, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -12127.369140625
tensor(12127.3604, grad_fn=<NegBackward0>) tensor(12127.3691, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9000 due to no improvement.
pi: tensor([[0.3918, 0.6082],
        [0.8621, 0.1379]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0077, 0.9923], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1906, 0.1941],
         [0.5735, 0.1949]],

        [[0.5051, 0.1924],
         [0.5549, 0.6736]],

        [[0.5550, 0.1984],
         [0.7019, 0.5641]],

        [[0.6037, 0.1952],
         [0.7092, 0.7012]],

        [[0.7134, 0.1839],
         [0.6324, 0.5968]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000849596896156477
Average Adjusted Rand Index: 0.0
[0.0036693409817386774, -0.000849596896156477] [0.003962819798794847, 0.0] [12124.978515625, 12127.369140625]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11993.249998752
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21437.49609375
inf tensor(21437.4961, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12404.392578125
tensor(21437.4961, grad_fn=<NegBackward0>) tensor(12404.3926, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12401.4580078125
tensor(12404.3926, grad_fn=<NegBackward0>) tensor(12401.4580, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12401.271484375
tensor(12401.4580, grad_fn=<NegBackward0>) tensor(12401.2715, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12401.0830078125
tensor(12401.2715, grad_fn=<NegBackward0>) tensor(12401.0830, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12400.7509765625
tensor(12401.0830, grad_fn=<NegBackward0>) tensor(12400.7510, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12400.25
tensor(12400.7510, grad_fn=<NegBackward0>) tensor(12400.2500, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12399.8681640625
tensor(12400.2500, grad_fn=<NegBackward0>) tensor(12399.8682, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12399.7568359375
tensor(12399.8682, grad_fn=<NegBackward0>) tensor(12399.7568, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12399.697265625
tensor(12399.7568, grad_fn=<NegBackward0>) tensor(12399.6973, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12399.6220703125
tensor(12399.6973, grad_fn=<NegBackward0>) tensor(12399.6221, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12399.46875
tensor(12399.6221, grad_fn=<NegBackward0>) tensor(12399.4688, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12399.2138671875
tensor(12399.4688, grad_fn=<NegBackward0>) tensor(12399.2139, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12399.072265625
tensor(12399.2139, grad_fn=<NegBackward0>) tensor(12399.0723, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12399.0234375
tensor(12399.0723, grad_fn=<NegBackward0>) tensor(12399.0234, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12399.0029296875
tensor(12399.0234, grad_fn=<NegBackward0>) tensor(12399.0029, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12398.994140625
tensor(12399.0029, grad_fn=<NegBackward0>) tensor(12398.9941, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12398.9873046875
tensor(12398.9941, grad_fn=<NegBackward0>) tensor(12398.9873, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12398.982421875
tensor(12398.9873, grad_fn=<NegBackward0>) tensor(12398.9824, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12398.9794921875
tensor(12398.9824, grad_fn=<NegBackward0>) tensor(12398.9795, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12398.9775390625
tensor(12398.9795, grad_fn=<NegBackward0>) tensor(12398.9775, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12398.9755859375
tensor(12398.9775, grad_fn=<NegBackward0>) tensor(12398.9756, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12398.974609375
tensor(12398.9756, grad_fn=<NegBackward0>) tensor(12398.9746, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12398.97265625
tensor(12398.9746, grad_fn=<NegBackward0>) tensor(12398.9727, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12398.9716796875
tensor(12398.9727, grad_fn=<NegBackward0>) tensor(12398.9717, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12398.9697265625
tensor(12398.9717, grad_fn=<NegBackward0>) tensor(12398.9697, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12398.970703125
tensor(12398.9697, grad_fn=<NegBackward0>) tensor(12398.9707, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -12398.96875
tensor(12398.9697, grad_fn=<NegBackward0>) tensor(12398.9688, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12398.96875
tensor(12398.9688, grad_fn=<NegBackward0>) tensor(12398.9688, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12398.96875
tensor(12398.9688, grad_fn=<NegBackward0>) tensor(12398.9688, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12398.9658203125
tensor(12398.9688, grad_fn=<NegBackward0>) tensor(12398.9658, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12398.9658203125
tensor(12398.9658, grad_fn=<NegBackward0>) tensor(12398.9658, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12398.9677734375
tensor(12398.9658, grad_fn=<NegBackward0>) tensor(12398.9678, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12398.966796875
tensor(12398.9658, grad_fn=<NegBackward0>) tensor(12398.9668, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -12398.966796875
tensor(12398.9658, grad_fn=<NegBackward0>) tensor(12398.9668, grad_fn=<NegBackward0>)
3
Iteration 3500: Loss = -12398.96484375
tensor(12398.9658, grad_fn=<NegBackward0>) tensor(12398.9648, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12398.9736328125
tensor(12398.9648, grad_fn=<NegBackward0>) tensor(12398.9736, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12398.9658203125
tensor(12398.9648, grad_fn=<NegBackward0>) tensor(12398.9658, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -12398.96484375
tensor(12398.9648, grad_fn=<NegBackward0>) tensor(12398.9648, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12398.9658203125
tensor(12398.9648, grad_fn=<NegBackward0>) tensor(12398.9658, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12398.9638671875
tensor(12398.9648, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12398.9638671875
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12398.9638671875
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12398.96484375
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9648, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12398.966796875
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9668, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -12398.9638671875
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12398.9638671875
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12398.962890625
tensor(12398.9639, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12398.962890625
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12398.962890625
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12398.9638671875
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12398.9638671875
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12398.962890625
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12398.9638671875
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9639, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12398.96484375
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9648, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -12398.9697265625
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9697, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -12398.962890625
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12398.9619140625
tensor(12398.9629, grad_fn=<NegBackward0>) tensor(12398.9619, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12398.962890625
tensor(12398.9619, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12398.9609375
tensor(12398.9619, grad_fn=<NegBackward0>) tensor(12398.9609, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12398.9619140625
tensor(12398.9609, grad_fn=<NegBackward0>) tensor(12398.9619, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12398.9619140625
tensor(12398.9609, grad_fn=<NegBackward0>) tensor(12398.9619, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -12398.962890625
tensor(12398.9609, grad_fn=<NegBackward0>) tensor(12398.9629, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -12398.96484375
tensor(12398.9609, grad_fn=<NegBackward0>) tensor(12398.9648, grad_fn=<NegBackward0>)
4
Iteration 6400: Loss = -12398.9619140625
tensor(12398.9609, grad_fn=<NegBackward0>) tensor(12398.9619, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6400 due to no improvement.
pi: tensor([[9.6675e-01, 3.3251e-02],
        [9.9999e-01, 9.8033e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5387, 0.4613], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2131, 0.1018],
         [0.6474, 0.3009]],

        [[0.6054, 0.1226],
         [0.6308, 0.6231]],

        [[0.7210, 0.1150],
         [0.5292, 0.6189]],

        [[0.6840, 0.1417],
         [0.5314, 0.5276]],

        [[0.5098, 0.1580],
         [0.5769, 0.6032]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05475233666028976
Average Adjusted Rand Index: 0.1829707181426203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22100.26171875
inf tensor(22100.2617, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12154.998046875
tensor(22100.2617, grad_fn=<NegBackward0>) tensor(12154.9980, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12087.1416015625
tensor(12154.9980, grad_fn=<NegBackward0>) tensor(12087.1416, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12030.96484375
tensor(12087.1416, grad_fn=<NegBackward0>) tensor(12030.9648, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12030.2470703125
tensor(12030.9648, grad_fn=<NegBackward0>) tensor(12030.2471, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12030.1103515625
tensor(12030.2471, grad_fn=<NegBackward0>) tensor(12030.1104, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12030.046875
tensor(12030.1104, grad_fn=<NegBackward0>) tensor(12030.0469, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12029.9169921875
tensor(12030.0469, grad_fn=<NegBackward0>) tensor(12029.9170, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12029.8955078125
tensor(12029.9170, grad_fn=<NegBackward0>) tensor(12029.8955, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12029.8798828125
tensor(12029.8955, grad_fn=<NegBackward0>) tensor(12029.8799, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12029.8681640625
tensor(12029.8799, grad_fn=<NegBackward0>) tensor(12029.8682, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12029.859375
tensor(12029.8682, grad_fn=<NegBackward0>) tensor(12029.8594, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12029.853515625
tensor(12029.8594, grad_fn=<NegBackward0>) tensor(12029.8535, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12029.84765625
tensor(12029.8535, grad_fn=<NegBackward0>) tensor(12029.8477, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12029.8427734375
tensor(12029.8477, grad_fn=<NegBackward0>) tensor(12029.8428, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12029.83984375
tensor(12029.8428, grad_fn=<NegBackward0>) tensor(12029.8398, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12029.8369140625
tensor(12029.8398, grad_fn=<NegBackward0>) tensor(12029.8369, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12029.833984375
tensor(12029.8369, grad_fn=<NegBackward0>) tensor(12029.8340, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12029.8310546875
tensor(12029.8340, grad_fn=<NegBackward0>) tensor(12029.8311, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12029.830078125
tensor(12029.8311, grad_fn=<NegBackward0>) tensor(12029.8301, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12029.8291015625
tensor(12029.8301, grad_fn=<NegBackward0>) tensor(12029.8291, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12029.826171875
tensor(12029.8291, grad_fn=<NegBackward0>) tensor(12029.8262, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12029.818359375
tensor(12029.8262, grad_fn=<NegBackward0>) tensor(12029.8184, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12029.81640625
tensor(12029.8184, grad_fn=<NegBackward0>) tensor(12029.8164, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12029.8154296875
tensor(12029.8164, grad_fn=<NegBackward0>) tensor(12029.8154, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12029.814453125
tensor(12029.8154, grad_fn=<NegBackward0>) tensor(12029.8145, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12029.814453125
tensor(12029.8145, grad_fn=<NegBackward0>) tensor(12029.8145, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12029.8115234375
tensor(12029.8145, grad_fn=<NegBackward0>) tensor(12029.8115, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12029.8125
tensor(12029.8115, grad_fn=<NegBackward0>) tensor(12029.8125, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12029.8115234375
tensor(12029.8115, grad_fn=<NegBackward0>) tensor(12029.8115, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12029.8115234375
tensor(12029.8115, grad_fn=<NegBackward0>) tensor(12029.8115, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12029.8095703125
tensor(12029.8115, grad_fn=<NegBackward0>) tensor(12029.8096, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12029.8076171875
tensor(12029.8096, grad_fn=<NegBackward0>) tensor(12029.8076, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12029.693359375
tensor(12029.8076, grad_fn=<NegBackward0>) tensor(12029.6934, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12029.6923828125
tensor(12029.6934, grad_fn=<NegBackward0>) tensor(12029.6924, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12029.69140625
tensor(12029.6924, grad_fn=<NegBackward0>) tensor(12029.6914, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12029.69140625
tensor(12029.6914, grad_fn=<NegBackward0>) tensor(12029.6914, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12029.69140625
tensor(12029.6914, grad_fn=<NegBackward0>) tensor(12029.6914, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12029.69140625
tensor(12029.6914, grad_fn=<NegBackward0>) tensor(12029.6914, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12029.6904296875
tensor(12029.6914, grad_fn=<NegBackward0>) tensor(12029.6904, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12029.69140625
tensor(12029.6904, grad_fn=<NegBackward0>) tensor(12029.6914, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12029.69140625
tensor(12029.6904, grad_fn=<NegBackward0>) tensor(12029.6914, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12029.701171875
tensor(12029.6904, grad_fn=<NegBackward0>) tensor(12029.7012, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -12029.689453125
tensor(12029.6904, grad_fn=<NegBackward0>) tensor(12029.6895, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12029.689453125
tensor(12029.6895, grad_fn=<NegBackward0>) tensor(12029.6895, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12029.685546875
tensor(12029.6895, grad_fn=<NegBackward0>) tensor(12029.6855, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12029.6806640625
tensor(12029.6855, grad_fn=<NegBackward0>) tensor(12029.6807, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12029.6796875
tensor(12029.6807, grad_fn=<NegBackward0>) tensor(12029.6797, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12029.6796875
tensor(12029.6797, grad_fn=<NegBackward0>) tensor(12029.6797, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12029.6787109375
tensor(12029.6797, grad_fn=<NegBackward0>) tensor(12029.6787, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12029.677734375
tensor(12029.6787, grad_fn=<NegBackward0>) tensor(12029.6777, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12029.6787109375
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6787, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12029.677734375
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6777, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12029.677734375
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6777, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12029.677734375
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6777, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12029.689453125
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6895, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12029.6787109375
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6787, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12029.6845703125
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6846, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12029.6787109375
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6787, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -12029.6826171875
tensor(12029.6777, grad_fn=<NegBackward0>) tensor(12029.6826, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.3698, 0.6302],
        [0.6384, 0.3616]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5198, 0.4802], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2917, 0.1004],
         [0.6065, 0.3076]],

        [[0.6246, 0.1165],
         [0.6039, 0.5238]],

        [[0.6417, 0.1010],
         [0.6909, 0.5513]],

        [[0.5751, 0.1027],
         [0.5811, 0.5024]],

        [[0.6991, 0.1049],
         [0.6727, 0.6840]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.039698377346181674
Average Adjusted Rand Index: 0.9761598395607173
[0.05475233666028976, 0.039698377346181674] [0.1829707181426203, 0.9761598395607173] [12398.9619140625, 12029.6826171875]
-------------------------------------
This iteration is 62
True Objective function: Loss = -11801.215983255155
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19023.787109375
inf tensor(19023.7871, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12252.244140625
tensor(19023.7871, grad_fn=<NegBackward0>) tensor(12252.2441, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12251.943359375
tensor(12252.2441, grad_fn=<NegBackward0>) tensor(12251.9434, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12251.705078125
tensor(12251.9434, grad_fn=<NegBackward0>) tensor(12251.7051, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12251.3271484375
tensor(12251.7051, grad_fn=<NegBackward0>) tensor(12251.3271, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12251.123046875
tensor(12251.3271, grad_fn=<NegBackward0>) tensor(12251.1230, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12251.087890625
tensor(12251.1230, grad_fn=<NegBackward0>) tensor(12251.0879, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12251.07421875
tensor(12251.0879, grad_fn=<NegBackward0>) tensor(12251.0742, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12251.0693359375
tensor(12251.0742, grad_fn=<NegBackward0>) tensor(12251.0693, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12251.06640625
tensor(12251.0693, grad_fn=<NegBackward0>) tensor(12251.0664, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12251.0625
tensor(12251.0664, grad_fn=<NegBackward0>) tensor(12251.0625, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12251.0615234375
tensor(12251.0625, grad_fn=<NegBackward0>) tensor(12251.0615, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12251.0595703125
tensor(12251.0615, grad_fn=<NegBackward0>) tensor(12251.0596, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12251.05859375
tensor(12251.0596, grad_fn=<NegBackward0>) tensor(12251.0586, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12251.0537109375
tensor(12251.0586, grad_fn=<NegBackward0>) tensor(12251.0537, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12251.0517578125
tensor(12251.0537, grad_fn=<NegBackward0>) tensor(12251.0518, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12251.0517578125
tensor(12251.0518, grad_fn=<NegBackward0>) tensor(12251.0518, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12251.0517578125
tensor(12251.0518, grad_fn=<NegBackward0>) tensor(12251.0518, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12251.05078125
tensor(12251.0518, grad_fn=<NegBackward0>) tensor(12251.0508, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12251.05078125
tensor(12251.0508, grad_fn=<NegBackward0>) tensor(12251.0508, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12251.0498046875
tensor(12251.0508, grad_fn=<NegBackward0>) tensor(12251.0498, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12251.0498046875
tensor(12251.0498, grad_fn=<NegBackward0>) tensor(12251.0498, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12251.05078125
tensor(12251.0498, grad_fn=<NegBackward0>) tensor(12251.0508, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -12251.05078125
tensor(12251.0498, grad_fn=<NegBackward0>) tensor(12251.0508, grad_fn=<NegBackward0>)
2
Iteration 2400: Loss = -12251.0498046875
tensor(12251.0498, grad_fn=<NegBackward0>) tensor(12251.0498, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12251.0498046875
tensor(12251.0498, grad_fn=<NegBackward0>) tensor(12251.0498, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12251.048828125
tensor(12251.0498, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12251.048828125
tensor(12251.0488, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12251.0498046875
tensor(12251.0488, grad_fn=<NegBackward0>) tensor(12251.0498, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12251.048828125
tensor(12251.0488, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12251.0478515625
tensor(12251.0488, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12251.0498046875
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0498, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12251.048828125
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12251.048828125
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -12251.048828125
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12251.048828125
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12251.048828125
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12251.048828125
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12251.0478515625
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12251.046875
tensor(12251.0479, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12251.048828125
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12251.0478515625
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12251.0478515625
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12251.046875
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0469, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12251.048828125
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12251.048828125
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0488, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12251.0478515625
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -12251.0478515625
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -12251.0478515625
tensor(12251.0469, grad_fn=<NegBackward0>) tensor(12251.0479, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.0228, 0.9772],
        [0.0383, 0.9617]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0314, 0.9686], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2341, 0.2101],
         [0.6995, 0.1950]],

        [[0.5227, 0.1790],
         [0.5101, 0.5642]],

        [[0.6410, 0.1842],
         [0.6447, 0.5541]],

        [[0.6435, 0.2180],
         [0.6998, 0.6769]],

        [[0.5983, 0.2645],
         [0.6114, 0.5365]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
Global Adjusted Rand Index: -0.00039543502922895045
Average Adjusted Rand Index: -0.0013316687473990845
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24600.4921875
inf tensor(24600.4922, grad_fn=<NegBackward0>)
Iteration 100: Loss = -11940.4912109375
tensor(24600.4922, grad_fn=<NegBackward0>) tensor(11940.4912, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11836.5537109375
tensor(11940.4912, grad_fn=<NegBackward0>) tensor(11836.5537, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11836.267578125
tensor(11836.5537, grad_fn=<NegBackward0>) tensor(11836.2676, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11836.1572265625
tensor(11836.2676, grad_fn=<NegBackward0>) tensor(11836.1572, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11836.1005859375
tensor(11836.1572, grad_fn=<NegBackward0>) tensor(11836.1006, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11836.06640625
tensor(11836.1006, grad_fn=<NegBackward0>) tensor(11836.0664, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11836.044921875
tensor(11836.0664, grad_fn=<NegBackward0>) tensor(11836.0449, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11836.029296875
tensor(11836.0449, grad_fn=<NegBackward0>) tensor(11836.0293, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11836.0185546875
tensor(11836.0293, grad_fn=<NegBackward0>) tensor(11836.0186, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11836.0087890625
tensor(11836.0186, grad_fn=<NegBackward0>) tensor(11836.0088, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11836.0029296875
tensor(11836.0088, grad_fn=<NegBackward0>) tensor(11836.0029, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11835.9970703125
tensor(11836.0029, grad_fn=<NegBackward0>) tensor(11835.9971, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11835.994140625
tensor(11835.9971, grad_fn=<NegBackward0>) tensor(11835.9941, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11835.9912109375
tensor(11835.9941, grad_fn=<NegBackward0>) tensor(11835.9912, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11835.9892578125
tensor(11835.9912, grad_fn=<NegBackward0>) tensor(11835.9893, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11835.984375
tensor(11835.9893, grad_fn=<NegBackward0>) tensor(11835.9844, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11835.984375
tensor(11835.9844, grad_fn=<NegBackward0>) tensor(11835.9844, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11835.982421875
tensor(11835.9844, grad_fn=<NegBackward0>) tensor(11835.9824, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11835.9814453125
tensor(11835.9824, grad_fn=<NegBackward0>) tensor(11835.9814, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11835.978515625
tensor(11835.9814, grad_fn=<NegBackward0>) tensor(11835.9785, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11835.978515625
tensor(11835.9785, grad_fn=<NegBackward0>) tensor(11835.9785, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11835.9765625
tensor(11835.9785, grad_fn=<NegBackward0>) tensor(11835.9766, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11835.9775390625
tensor(11835.9766, grad_fn=<NegBackward0>) tensor(11835.9775, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -11835.9755859375
tensor(11835.9766, grad_fn=<NegBackward0>) tensor(11835.9756, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11835.978515625
tensor(11835.9756, grad_fn=<NegBackward0>) tensor(11835.9785, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11835.9755859375
tensor(11835.9756, grad_fn=<NegBackward0>) tensor(11835.9756, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11835.9736328125
tensor(11835.9756, grad_fn=<NegBackward0>) tensor(11835.9736, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11835.9736328125
tensor(11835.9736, grad_fn=<NegBackward0>) tensor(11835.9736, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11835.98046875
tensor(11835.9736, grad_fn=<NegBackward0>) tensor(11835.9805, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -11835.97265625
tensor(11835.9736, grad_fn=<NegBackward0>) tensor(11835.9727, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11835.9814453125
tensor(11835.9727, grad_fn=<NegBackward0>) tensor(11835.9814, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -11835.9775390625
tensor(11835.9727, grad_fn=<NegBackward0>) tensor(11835.9775, grad_fn=<NegBackward0>)
2
Iteration 3300: Loss = -11835.978515625
tensor(11835.9727, grad_fn=<NegBackward0>) tensor(11835.9785, grad_fn=<NegBackward0>)
3
Iteration 3400: Loss = -11835.970703125
tensor(11835.9727, grad_fn=<NegBackward0>) tensor(11835.9707, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11835.970703125
tensor(11835.9707, grad_fn=<NegBackward0>) tensor(11835.9707, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11835.9697265625
tensor(11835.9707, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11835.97265625
tensor(11835.9697, grad_fn=<NegBackward0>) tensor(11835.9727, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11835.970703125
tensor(11835.9697, grad_fn=<NegBackward0>) tensor(11835.9707, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -11835.9697265625
tensor(11835.9697, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11835.96875
tensor(11835.9697, grad_fn=<NegBackward0>) tensor(11835.9688, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11835.9716796875
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9717, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11835.9697265625
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -11835.9697265625
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
3
Iteration 4400: Loss = -11835.96875
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9688, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11835.96875
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9688, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11835.9697265625
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11835.9677734375
tensor(11835.9688, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11835.96875
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9688, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11835.9677734375
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11835.9716796875
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9717, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11835.96875
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9688, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11835.9677734375
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11835.9697265625
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11835.9677734375
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11835.9697265625
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11835.9697265625
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9697, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11835.966796875
tensor(11835.9678, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11835.978515625
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9785, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11835.9677734375
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11835.9677734375
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11835.96875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9688, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11835.9677734375
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11835.966796875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9668, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11835.9794921875
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9795, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11835.9677734375
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11835.9912109375
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9912, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -11835.9677734375
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9678, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -11835.9736328125
tensor(11835.9668, grad_fn=<NegBackward0>) tensor(11835.9736, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7400 due to no improvement.
pi: tensor([[0.3134, 0.6866],
        [0.5959, 0.4041]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5240, 0.4760], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2913, 0.1066],
         [0.6399, 0.2904]],

        [[0.5178, 0.0939],
         [0.7146, 0.6808]],

        [[0.6630, 0.0992],
         [0.6383, 0.5830]],

        [[0.5316, 0.1033],
         [0.5680, 0.6681]],

        [[0.6641, 0.1014],
         [0.6787, 0.6597]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
Global Adjusted Rand Index: 0.03191874485772568
Average Adjusted Rand Index: 0.9367788296366564
[-0.00039543502922895045, 0.03191874485772568] [-0.0013316687473990845, 0.9367788296366564] [12251.0478515625, 11835.9736328125]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11829.455991868004
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20217.38671875
inf tensor(20217.3867, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12376.8916015625
tensor(20217.3867, grad_fn=<NegBackward0>) tensor(12376.8916, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12376.681640625
tensor(12376.8916, grad_fn=<NegBackward0>) tensor(12376.6816, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12376.6337890625
tensor(12376.6816, grad_fn=<NegBackward0>) tensor(12376.6338, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12376.6005859375
tensor(12376.6338, grad_fn=<NegBackward0>) tensor(12376.6006, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12376.5751953125
tensor(12376.6006, grad_fn=<NegBackward0>) tensor(12376.5752, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12376.55078125
tensor(12376.5752, grad_fn=<NegBackward0>) tensor(12376.5508, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12376.525390625
tensor(12376.5508, grad_fn=<NegBackward0>) tensor(12376.5254, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12376.4951171875
tensor(12376.5254, grad_fn=<NegBackward0>) tensor(12376.4951, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12376.44921875
tensor(12376.4951, grad_fn=<NegBackward0>) tensor(12376.4492, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12376.375
tensor(12376.4492, grad_fn=<NegBackward0>) tensor(12376.3750, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12376.26171875
tensor(12376.3750, grad_fn=<NegBackward0>) tensor(12376.2617, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12376.1845703125
tensor(12376.2617, grad_fn=<NegBackward0>) tensor(12376.1846, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12376.115234375
tensor(12376.1846, grad_fn=<NegBackward0>) tensor(12376.1152, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12376.037109375
tensor(12376.1152, grad_fn=<NegBackward0>) tensor(12376.0371, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12375.966796875
tensor(12376.0371, grad_fn=<NegBackward0>) tensor(12375.9668, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12375.8896484375
tensor(12375.9668, grad_fn=<NegBackward0>) tensor(12375.8896, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12375.365234375
tensor(12375.8896, grad_fn=<NegBackward0>) tensor(12375.3652, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12371.958984375
tensor(12375.3652, grad_fn=<NegBackward0>) tensor(12371.9590, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12370.76171875
tensor(12371.9590, grad_fn=<NegBackward0>) tensor(12370.7617, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12266.9091796875
tensor(12370.7617, grad_fn=<NegBackward0>) tensor(12266.9092, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11883.62890625
tensor(12266.9092, grad_fn=<NegBackward0>) tensor(11883.6289, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11849.046875
tensor(11883.6289, grad_fn=<NegBackward0>) tensor(11849.0469, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11839.30078125
tensor(11849.0469, grad_fn=<NegBackward0>) tensor(11839.3008, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11839.26171875
tensor(11839.3008, grad_fn=<NegBackward0>) tensor(11839.2617, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11839.2373046875
tensor(11839.2617, grad_fn=<NegBackward0>) tensor(11839.2373, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11833.4326171875
tensor(11839.2373, grad_fn=<NegBackward0>) tensor(11833.4326, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11833.0771484375
tensor(11833.4326, grad_fn=<NegBackward0>) tensor(11833.0771, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11832.919921875
tensor(11833.0771, grad_fn=<NegBackward0>) tensor(11832.9199, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11832.9140625
tensor(11832.9199, grad_fn=<NegBackward0>) tensor(11832.9141, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11832.91015625
tensor(11832.9141, grad_fn=<NegBackward0>) tensor(11832.9102, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11832.90625
tensor(11832.9102, grad_fn=<NegBackward0>) tensor(11832.9062, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11832.9033203125
tensor(11832.9062, grad_fn=<NegBackward0>) tensor(11832.9033, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11832.900390625
tensor(11832.9033, grad_fn=<NegBackward0>) tensor(11832.9004, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11832.8974609375
tensor(11832.9004, grad_fn=<NegBackward0>) tensor(11832.8975, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11832.896484375
tensor(11832.8975, grad_fn=<NegBackward0>) tensor(11832.8965, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11832.8955078125
tensor(11832.8965, grad_fn=<NegBackward0>) tensor(11832.8955, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11832.892578125
tensor(11832.8955, grad_fn=<NegBackward0>) tensor(11832.8926, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11832.8916015625
tensor(11832.8926, grad_fn=<NegBackward0>) tensor(11832.8916, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11832.890625
tensor(11832.8916, grad_fn=<NegBackward0>) tensor(11832.8906, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11832.888671875
tensor(11832.8906, grad_fn=<NegBackward0>) tensor(11832.8887, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11832.8603515625
tensor(11832.8887, grad_fn=<NegBackward0>) tensor(11832.8604, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11832.8740234375
tensor(11832.8604, grad_fn=<NegBackward0>) tensor(11832.8740, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11832.857421875
tensor(11832.8604, grad_fn=<NegBackward0>) tensor(11832.8574, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11832.8583984375
tensor(11832.8574, grad_fn=<NegBackward0>) tensor(11832.8584, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11832.86328125
tensor(11832.8574, grad_fn=<NegBackward0>) tensor(11832.8633, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -11832.8583984375
tensor(11832.8574, grad_fn=<NegBackward0>) tensor(11832.8584, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -11832.857421875
tensor(11832.8574, grad_fn=<NegBackward0>) tensor(11832.8574, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11832.8603515625
tensor(11832.8574, grad_fn=<NegBackward0>) tensor(11832.8604, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11832.8564453125
tensor(11832.8574, grad_fn=<NegBackward0>) tensor(11832.8564, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11832.8564453125
tensor(11832.8564, grad_fn=<NegBackward0>) tensor(11832.8564, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11832.8544921875
tensor(11832.8564, grad_fn=<NegBackward0>) tensor(11832.8545, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11832.8544921875
tensor(11832.8545, grad_fn=<NegBackward0>) tensor(11832.8545, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11832.853515625
tensor(11832.8545, grad_fn=<NegBackward0>) tensor(11832.8535, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11832.8564453125
tensor(11832.8535, grad_fn=<NegBackward0>) tensor(11832.8564, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11832.853515625
tensor(11832.8535, grad_fn=<NegBackward0>) tensor(11832.8535, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11832.853515625
tensor(11832.8535, grad_fn=<NegBackward0>) tensor(11832.8535, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11832.8544921875
tensor(11832.8535, grad_fn=<NegBackward0>) tensor(11832.8545, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11832.8525390625
tensor(11832.8535, grad_fn=<NegBackward0>) tensor(11832.8525, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11832.8515625
tensor(11832.8525, grad_fn=<NegBackward0>) tensor(11832.8516, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11832.8525390625
tensor(11832.8516, grad_fn=<NegBackward0>) tensor(11832.8525, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11832.8515625
tensor(11832.8516, grad_fn=<NegBackward0>) tensor(11832.8516, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11832.8525390625
tensor(11832.8516, grad_fn=<NegBackward0>) tensor(11832.8525, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11832.8525390625
tensor(11832.8516, grad_fn=<NegBackward0>) tensor(11832.8525, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11832.8515625
tensor(11832.8516, grad_fn=<NegBackward0>) tensor(11832.8516, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11832.8505859375
tensor(11832.8516, grad_fn=<NegBackward0>) tensor(11832.8506, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11832.849609375
tensor(11832.8506, grad_fn=<NegBackward0>) tensor(11832.8496, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11832.84765625
tensor(11832.8496, grad_fn=<NegBackward0>) tensor(11832.8477, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11832.8486328125
tensor(11832.8477, grad_fn=<NegBackward0>) tensor(11832.8486, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11832.8486328125
tensor(11832.8477, grad_fn=<NegBackward0>) tensor(11832.8486, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11832.8505859375
tensor(11832.8477, grad_fn=<NegBackward0>) tensor(11832.8506, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11832.8564453125
tensor(11832.8477, grad_fn=<NegBackward0>) tensor(11832.8564, grad_fn=<NegBackward0>)
4
Iteration 7200: Loss = -11832.84765625
tensor(11832.8477, grad_fn=<NegBackward0>) tensor(11832.8477, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11832.8466796875
tensor(11832.8477, grad_fn=<NegBackward0>) tensor(11832.8467, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11832.8486328125
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8486, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11832.8505859375
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8506, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11832.84765625
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8477, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -11832.84765625
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8477, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -11832.8466796875
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8467, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11832.943359375
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.9434, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11832.8466796875
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8467, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11832.861328125
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8613, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11832.8447265625
tensor(11832.8467, grad_fn=<NegBackward0>) tensor(11832.8447, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11832.8447265625
tensor(11832.8447, grad_fn=<NegBackward0>) tensor(11832.8447, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11832.8447265625
tensor(11832.8447, grad_fn=<NegBackward0>) tensor(11832.8447, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11832.8447265625
tensor(11832.8447, grad_fn=<NegBackward0>) tensor(11832.8447, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11832.841796875
tensor(11832.8447, grad_fn=<NegBackward0>) tensor(11832.8418, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11832.7734375
tensor(11832.8418, grad_fn=<NegBackward0>) tensor(11832.7734, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11832.7734375
tensor(11832.7734, grad_fn=<NegBackward0>) tensor(11832.7734, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11832.7939453125
tensor(11832.7734, grad_fn=<NegBackward0>) tensor(11832.7939, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11832.7734375
tensor(11832.7734, grad_fn=<NegBackward0>) tensor(11832.7734, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11832.7900390625
tensor(11832.7734, grad_fn=<NegBackward0>) tensor(11832.7900, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11832.771484375
tensor(11832.7734, grad_fn=<NegBackward0>) tensor(11832.7715, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11832.939453125
tensor(11832.7715, grad_fn=<NegBackward0>) tensor(11832.9395, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11832.771484375
tensor(11832.7715, grad_fn=<NegBackward0>) tensor(11832.7715, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11832.7900390625
tensor(11832.7715, grad_fn=<NegBackward0>) tensor(11832.7900, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11832.7705078125
tensor(11832.7715, grad_fn=<NegBackward0>) tensor(11832.7705, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11832.775390625
tensor(11832.7705, grad_fn=<NegBackward0>) tensor(11832.7754, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11832.7734375
tensor(11832.7705, grad_fn=<NegBackward0>) tensor(11832.7734, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -11832.7783203125
tensor(11832.7705, grad_fn=<NegBackward0>) tensor(11832.7783, grad_fn=<NegBackward0>)
3
pi: tensor([[0.7707, 0.2293],
        [0.2012, 0.7988]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5797, 0.4203], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3070, 0.0893],
         [0.5691, 0.2934]],

        [[0.5941, 0.0964],
         [0.6759, 0.7156]],

        [[0.7269, 0.1045],
         [0.6142, 0.5203]],

        [[0.6657, 0.1005],
         [0.7197, 0.5706]],

        [[0.5378, 0.1077],
         [0.5888, 0.6265]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9681923689419211
Average Adjusted Rand Index: 0.967995265922902
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22226.89453125
inf tensor(22226.8945, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12376.9853515625
tensor(22226.8945, grad_fn=<NegBackward0>) tensor(12376.9854, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12376.685546875
tensor(12376.9854, grad_fn=<NegBackward0>) tensor(12376.6855, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12376.6337890625
tensor(12376.6855, grad_fn=<NegBackward0>) tensor(12376.6338, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12376.607421875
tensor(12376.6338, grad_fn=<NegBackward0>) tensor(12376.6074, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12376.5859375
tensor(12376.6074, grad_fn=<NegBackward0>) tensor(12376.5859, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12376.56640625
tensor(12376.5859, grad_fn=<NegBackward0>) tensor(12376.5664, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12376.544921875
tensor(12376.5664, grad_fn=<NegBackward0>) tensor(12376.5449, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12376.5234375
tensor(12376.5449, grad_fn=<NegBackward0>) tensor(12376.5234, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12376.5
tensor(12376.5234, grad_fn=<NegBackward0>) tensor(12376.5000, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12376.474609375
tensor(12376.5000, grad_fn=<NegBackward0>) tensor(12376.4746, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12376.447265625
tensor(12376.4746, grad_fn=<NegBackward0>) tensor(12376.4473, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12376.4189453125
tensor(12376.4473, grad_fn=<NegBackward0>) tensor(12376.4189, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12376.3876953125
tensor(12376.4189, grad_fn=<NegBackward0>) tensor(12376.3877, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12376.3486328125
tensor(12376.3877, grad_fn=<NegBackward0>) tensor(12376.3486, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12376.23046875
tensor(12376.3486, grad_fn=<NegBackward0>) tensor(12376.2305, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12375.416015625
tensor(12376.2305, grad_fn=<NegBackward0>) tensor(12375.4160, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12374.2822265625
tensor(12375.4160, grad_fn=<NegBackward0>) tensor(12374.2822, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12373.56640625
tensor(12374.2822, grad_fn=<NegBackward0>) tensor(12373.5664, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12371.783203125
tensor(12373.5664, grad_fn=<NegBackward0>) tensor(12371.7832, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12371.0966796875
tensor(12371.7832, grad_fn=<NegBackward0>) tensor(12371.0967, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12370.873046875
tensor(12371.0967, grad_fn=<NegBackward0>) tensor(12370.8730, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12370.689453125
tensor(12370.8730, grad_fn=<NegBackward0>) tensor(12370.6895, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12370.5849609375
tensor(12370.6895, grad_fn=<NegBackward0>) tensor(12370.5850, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12370.5146484375
tensor(12370.5850, grad_fn=<NegBackward0>) tensor(12370.5146, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12370.4384765625
tensor(12370.5146, grad_fn=<NegBackward0>) tensor(12370.4385, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12370.349609375
tensor(12370.4385, grad_fn=<NegBackward0>) tensor(12370.3496, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12370.25
tensor(12370.3496, grad_fn=<NegBackward0>) tensor(12370.2500, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12370.1494140625
tensor(12370.2500, grad_fn=<NegBackward0>) tensor(12370.1494, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12370.056640625
tensor(12370.1494, grad_fn=<NegBackward0>) tensor(12370.0566, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12369.9697265625
tensor(12370.0566, grad_fn=<NegBackward0>) tensor(12369.9697, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12369.890625
tensor(12369.9697, grad_fn=<NegBackward0>) tensor(12369.8906, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12369.8134765625
tensor(12369.8906, grad_fn=<NegBackward0>) tensor(12369.8135, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12369.7412109375
tensor(12369.8135, grad_fn=<NegBackward0>) tensor(12369.7412, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12369.669921875
tensor(12369.7412, grad_fn=<NegBackward0>) tensor(12369.6699, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12369.6015625
tensor(12369.6699, grad_fn=<NegBackward0>) tensor(12369.6016, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12369.537109375
tensor(12369.6016, grad_fn=<NegBackward0>) tensor(12369.5371, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12369.4794921875
tensor(12369.5371, grad_fn=<NegBackward0>) tensor(12369.4795, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12369.427734375
tensor(12369.4795, grad_fn=<NegBackward0>) tensor(12369.4277, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12369.3798828125
tensor(12369.4277, grad_fn=<NegBackward0>) tensor(12369.3799, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12369.3388671875
tensor(12369.3799, grad_fn=<NegBackward0>) tensor(12369.3389, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12369.302734375
tensor(12369.3389, grad_fn=<NegBackward0>) tensor(12369.3027, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12369.2734375
tensor(12369.3027, grad_fn=<NegBackward0>) tensor(12369.2734, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12369.2451171875
tensor(12369.2734, grad_fn=<NegBackward0>) tensor(12369.2451, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12369.22265625
tensor(12369.2451, grad_fn=<NegBackward0>) tensor(12369.2227, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12369.201171875
tensor(12369.2227, grad_fn=<NegBackward0>) tensor(12369.2012, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12369.1826171875
tensor(12369.2012, grad_fn=<NegBackward0>) tensor(12369.1826, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12369.1669921875
tensor(12369.1826, grad_fn=<NegBackward0>) tensor(12369.1670, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12369.15234375
tensor(12369.1670, grad_fn=<NegBackward0>) tensor(12369.1523, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12369.1396484375
tensor(12369.1523, grad_fn=<NegBackward0>) tensor(12369.1396, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12369.1298828125
tensor(12369.1396, grad_fn=<NegBackward0>) tensor(12369.1299, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12369.119140625
tensor(12369.1299, grad_fn=<NegBackward0>) tensor(12369.1191, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12369.1103515625
tensor(12369.1191, grad_fn=<NegBackward0>) tensor(12369.1104, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12369.1025390625
tensor(12369.1104, grad_fn=<NegBackward0>) tensor(12369.1025, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12369.0947265625
tensor(12369.1025, grad_fn=<NegBackward0>) tensor(12369.0947, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12369.087890625
tensor(12369.0947, grad_fn=<NegBackward0>) tensor(12369.0879, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12369.0810546875
tensor(12369.0879, grad_fn=<NegBackward0>) tensor(12369.0811, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12369.078125
tensor(12369.0811, grad_fn=<NegBackward0>) tensor(12369.0781, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12369.072265625
tensor(12369.0781, grad_fn=<NegBackward0>) tensor(12369.0723, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12369.06640625
tensor(12369.0723, grad_fn=<NegBackward0>) tensor(12369.0664, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12369.0634765625
tensor(12369.0664, grad_fn=<NegBackward0>) tensor(12369.0635, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12369.05859375
tensor(12369.0635, grad_fn=<NegBackward0>) tensor(12369.0586, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12369.056640625
tensor(12369.0586, grad_fn=<NegBackward0>) tensor(12369.0566, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12369.052734375
tensor(12369.0566, grad_fn=<NegBackward0>) tensor(12369.0527, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12369.048828125
tensor(12369.0527, grad_fn=<NegBackward0>) tensor(12369.0488, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12369.0458984375
tensor(12369.0488, grad_fn=<NegBackward0>) tensor(12369.0459, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12369.0439453125
tensor(12369.0459, grad_fn=<NegBackward0>) tensor(12369.0439, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12369.0419921875
tensor(12369.0439, grad_fn=<NegBackward0>) tensor(12369.0420, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12369.0400390625
tensor(12369.0420, grad_fn=<NegBackward0>) tensor(12369.0400, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12369.0361328125
tensor(12369.0400, grad_fn=<NegBackward0>) tensor(12369.0361, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12369.037109375
tensor(12369.0361, grad_fn=<NegBackward0>) tensor(12369.0371, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12369.0341796875
tensor(12369.0361, grad_fn=<NegBackward0>) tensor(12369.0342, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12369.0302734375
tensor(12369.0342, grad_fn=<NegBackward0>) tensor(12369.0303, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12369.03125
tensor(12369.0303, grad_fn=<NegBackward0>) tensor(12369.0312, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12369.0283203125
tensor(12369.0303, grad_fn=<NegBackward0>) tensor(12369.0283, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12369.02734375
tensor(12369.0283, grad_fn=<NegBackward0>) tensor(12369.0273, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12369.025390625
tensor(12369.0273, grad_fn=<NegBackward0>) tensor(12369.0254, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12369.0244140625
tensor(12369.0254, grad_fn=<NegBackward0>) tensor(12369.0244, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12369.0234375
tensor(12369.0244, grad_fn=<NegBackward0>) tensor(12369.0234, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12369.0224609375
tensor(12369.0234, grad_fn=<NegBackward0>) tensor(12369.0225, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12369.021484375
tensor(12369.0225, grad_fn=<NegBackward0>) tensor(12369.0215, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12369.01953125
tensor(12369.0215, grad_fn=<NegBackward0>) tensor(12369.0195, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12369.0185546875
tensor(12369.0195, grad_fn=<NegBackward0>) tensor(12369.0186, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12369.0185546875
tensor(12369.0186, grad_fn=<NegBackward0>) tensor(12369.0186, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12369.0166015625
tensor(12369.0186, grad_fn=<NegBackward0>) tensor(12369.0166, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12369.01953125
tensor(12369.0166, grad_fn=<NegBackward0>) tensor(12369.0195, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12369.0146484375
tensor(12369.0166, grad_fn=<NegBackward0>) tensor(12369.0146, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12369.015625
tensor(12369.0146, grad_fn=<NegBackward0>) tensor(12369.0156, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12369.0126953125
tensor(12369.0146, grad_fn=<NegBackward0>) tensor(12369.0127, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12369.01171875
tensor(12369.0127, grad_fn=<NegBackward0>) tensor(12369.0117, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12369.013671875
tensor(12369.0117, grad_fn=<NegBackward0>) tensor(12369.0137, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12369.01171875
tensor(12369.0117, grad_fn=<NegBackward0>) tensor(12369.0117, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12369.015625
tensor(12369.0117, grad_fn=<NegBackward0>) tensor(12369.0156, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12369.01171875
tensor(12369.0117, grad_fn=<NegBackward0>) tensor(12369.0117, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12369.01171875
tensor(12369.0117, grad_fn=<NegBackward0>) tensor(12369.0117, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12369.0107421875
tensor(12369.0117, grad_fn=<NegBackward0>) tensor(12369.0107, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12369.009765625
tensor(12369.0107, grad_fn=<NegBackward0>) tensor(12369.0098, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12369.0087890625
tensor(12369.0098, grad_fn=<NegBackward0>) tensor(12369.0088, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12369.0087890625
tensor(12369.0088, grad_fn=<NegBackward0>) tensor(12369.0088, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12369.0087890625
tensor(12369.0088, grad_fn=<NegBackward0>) tensor(12369.0088, grad_fn=<NegBackward0>)
pi: tensor([[9.9994e-01, 6.0081e-05],
        [1.6111e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0289, 0.9711], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[3.4729e-04, 1.1965e-01],
         [7.2942e-01, 2.0119e-01]],

        [[6.5785e-01, 2.2294e-01],
         [5.4752e-01, 5.6427e-01]],

        [[5.0865e-01, 1.2373e-01],
         [7.2174e-01, 6.8867e-01]],

        [[5.9944e-01, 2.6158e-01],
         [5.4212e-01, 5.6395e-01]],

        [[5.6564e-01, 1.6141e-01],
         [5.2540e-01, 7.1852e-01]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.02868762140155424
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
Global Adjusted Rand Index: 0.00272164090041111
Average Adjusted Rand Index: 0.008416863637822701
[0.9681923689419211, 0.00272164090041111] [0.967995265922902, 0.008416863637822701] [11832.7802734375, 12369.0087890625]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11883.747532531203
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21175.068359375
inf tensor(21175.0684, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12382.8076171875
tensor(21175.0684, grad_fn=<NegBackward0>) tensor(12382.8076, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12382.2861328125
tensor(12382.8076, grad_fn=<NegBackward0>) tensor(12382.2861, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12382.1611328125
tensor(12382.2861, grad_fn=<NegBackward0>) tensor(12382.1611, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12382.076171875
tensor(12382.1611, grad_fn=<NegBackward0>) tensor(12382.0762, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12382.01171875
tensor(12382.0762, grad_fn=<NegBackward0>) tensor(12382.0117, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12381.9541015625
tensor(12382.0117, grad_fn=<NegBackward0>) tensor(12381.9541, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12381.8916015625
tensor(12381.9541, grad_fn=<NegBackward0>) tensor(12381.8916, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12381.7900390625
tensor(12381.8916, grad_fn=<NegBackward0>) tensor(12381.7900, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12381.3896484375
tensor(12381.7900, grad_fn=<NegBackward0>) tensor(12381.3896, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12380.3017578125
tensor(12381.3896, grad_fn=<NegBackward0>) tensor(12380.3018, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12379.80078125
tensor(12380.3018, grad_fn=<NegBackward0>) tensor(12379.8008, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12379.5849609375
tensor(12379.8008, grad_fn=<NegBackward0>) tensor(12379.5850, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12379.46484375
tensor(12379.5850, grad_fn=<NegBackward0>) tensor(12379.4648, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12379.3876953125
tensor(12379.4648, grad_fn=<NegBackward0>) tensor(12379.3877, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12379.3330078125
tensor(12379.3877, grad_fn=<NegBackward0>) tensor(12379.3330, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12379.283203125
tensor(12379.3330, grad_fn=<NegBackward0>) tensor(12379.2832, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12379.232421875
tensor(12379.2832, grad_fn=<NegBackward0>) tensor(12379.2324, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12379.1669921875
tensor(12379.2324, grad_fn=<NegBackward0>) tensor(12379.1670, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12379.0791015625
tensor(12379.1670, grad_fn=<NegBackward0>) tensor(12379.0791, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12378.9775390625
tensor(12379.0791, grad_fn=<NegBackward0>) tensor(12378.9775, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12378.8623046875
tensor(12378.9775, grad_fn=<NegBackward0>) tensor(12378.8623, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12378.6748046875
tensor(12378.8623, grad_fn=<NegBackward0>) tensor(12378.6748, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12378.4033203125
tensor(12378.6748, grad_fn=<NegBackward0>) tensor(12378.4033, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12378.2041015625
tensor(12378.4033, grad_fn=<NegBackward0>) tensor(12378.2041, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12378.1044921875
tensor(12378.2041, grad_fn=<NegBackward0>) tensor(12378.1045, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12378.0380859375
tensor(12378.1045, grad_fn=<NegBackward0>) tensor(12378.0381, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12377.9814453125
tensor(12378.0381, grad_fn=<NegBackward0>) tensor(12377.9814, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12377.923828125
tensor(12377.9814, grad_fn=<NegBackward0>) tensor(12377.9238, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12377.865234375
tensor(12377.9238, grad_fn=<NegBackward0>) tensor(12377.8652, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12377.798828125
tensor(12377.8652, grad_fn=<NegBackward0>) tensor(12377.7988, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12377.7314453125
tensor(12377.7988, grad_fn=<NegBackward0>) tensor(12377.7314, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12377.6640625
tensor(12377.7314, grad_fn=<NegBackward0>) tensor(12377.6641, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12377.6044921875
tensor(12377.6641, grad_fn=<NegBackward0>) tensor(12377.6045, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12377.556640625
tensor(12377.6045, grad_fn=<NegBackward0>) tensor(12377.5566, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12377.5166015625
tensor(12377.5566, grad_fn=<NegBackward0>) tensor(12377.5166, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12377.490234375
tensor(12377.5166, grad_fn=<NegBackward0>) tensor(12377.4902, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12377.46875
tensor(12377.4902, grad_fn=<NegBackward0>) tensor(12377.4688, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12377.4541015625
tensor(12377.4688, grad_fn=<NegBackward0>) tensor(12377.4541, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12377.44140625
tensor(12377.4541, grad_fn=<NegBackward0>) tensor(12377.4414, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12377.4306640625
tensor(12377.4414, grad_fn=<NegBackward0>) tensor(12377.4307, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12377.423828125
tensor(12377.4307, grad_fn=<NegBackward0>) tensor(12377.4238, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12377.419921875
tensor(12377.4238, grad_fn=<NegBackward0>) tensor(12377.4199, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12377.4140625
tensor(12377.4199, grad_fn=<NegBackward0>) tensor(12377.4141, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12377.41015625
tensor(12377.4141, grad_fn=<NegBackward0>) tensor(12377.4102, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12377.40625
tensor(12377.4102, grad_fn=<NegBackward0>) tensor(12377.4062, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12377.40234375
tensor(12377.4062, grad_fn=<NegBackward0>) tensor(12377.4023, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12377.400390625
tensor(12377.4023, grad_fn=<NegBackward0>) tensor(12377.4004, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12377.396484375
tensor(12377.4004, grad_fn=<NegBackward0>) tensor(12377.3965, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12377.39453125
tensor(12377.3965, grad_fn=<NegBackward0>) tensor(12377.3945, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12377.392578125
tensor(12377.3945, grad_fn=<NegBackward0>) tensor(12377.3926, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12377.3916015625
tensor(12377.3926, grad_fn=<NegBackward0>) tensor(12377.3916, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12377.3896484375
tensor(12377.3916, grad_fn=<NegBackward0>) tensor(12377.3896, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12377.3876953125
tensor(12377.3896, grad_fn=<NegBackward0>) tensor(12377.3877, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12377.38671875
tensor(12377.3877, grad_fn=<NegBackward0>) tensor(12377.3867, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12377.3837890625
tensor(12377.3867, grad_fn=<NegBackward0>) tensor(12377.3838, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12377.3837890625
tensor(12377.3838, grad_fn=<NegBackward0>) tensor(12377.3838, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12377.3837890625
tensor(12377.3838, grad_fn=<NegBackward0>) tensor(12377.3838, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12377.3818359375
tensor(12377.3838, grad_fn=<NegBackward0>) tensor(12377.3818, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12377.380859375
tensor(12377.3818, grad_fn=<NegBackward0>) tensor(12377.3809, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12377.380859375
tensor(12377.3809, grad_fn=<NegBackward0>) tensor(12377.3809, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12377.37890625
tensor(12377.3809, grad_fn=<NegBackward0>) tensor(12377.3789, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12377.380859375
tensor(12377.3789, grad_fn=<NegBackward0>) tensor(12377.3809, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12377.376953125
tensor(12377.3789, grad_fn=<NegBackward0>) tensor(12377.3770, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12377.376953125
tensor(12377.3770, grad_fn=<NegBackward0>) tensor(12377.3770, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12377.3759765625
tensor(12377.3770, grad_fn=<NegBackward0>) tensor(12377.3760, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12377.3740234375
tensor(12377.3760, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12377.375
tensor(12377.3740, grad_fn=<NegBackward0>) tensor(12377.3750, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12377.47265625
tensor(12377.3740, grad_fn=<NegBackward0>) tensor(12377.4727, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12377.3759765625
tensor(12377.3740, grad_fn=<NegBackward0>) tensor(12377.3760, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12377.3740234375
tensor(12377.3740, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12377.3740234375
tensor(12377.3740, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12377.373046875
tensor(12377.3740, grad_fn=<NegBackward0>) tensor(12377.3730, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12377.373046875
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3730, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12377.373046875
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3730, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12377.39453125
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3945, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12377.3720703125
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3721, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12377.42578125
tensor(12377.3721, grad_fn=<NegBackward0>) tensor(12377.4258, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12377.37109375
tensor(12377.3721, grad_fn=<NegBackward0>) tensor(12377.3711, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12377.740234375
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.7402, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12377.3720703125
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.3721, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12377.37109375
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.3711, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12377.3701171875
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12377.37109375
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.3711, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12377.3779296875
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.3779, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -12377.369140625
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12377.3701171875
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12377.3857421875
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3857, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12377.369140625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12377.369140625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12377.3701171875
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12377.369140625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12377.423828125
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.4238, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12377.3681640625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3682, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12377.369140625
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12377.369140625
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12377.3681640625
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3682, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12377.3935546875
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3936, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12377.3681640625
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3682, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12377.3720703125
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3721, grad_fn=<NegBackward0>)
1
pi: tensor([[1.0000e+00, 3.5174e-06],
        [1.6983e-02, 9.8302e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9789, 0.0211], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1793],
         [0.6483, 0.3960]],

        [[0.7044, 0.2334],
         [0.5435, 0.5162]],

        [[0.5182, 0.2444],
         [0.6767, 0.6723]],

        [[0.6321, 0.0787],
         [0.6822, 0.7193]],

        [[0.5952, 0.2201],
         [0.5608, 0.6267]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: -0.013574768645372375
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
Global Adjusted Rand Index: -0.001037814036071472
Average Adjusted Rand Index: -0.004191209707380607
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20163.251953125
inf tensor(20163.2520, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12382.69921875
tensor(20163.2520, grad_fn=<NegBackward0>) tensor(12382.6992, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12382.1865234375
tensor(12382.6992, grad_fn=<NegBackward0>) tensor(12382.1865, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12382.064453125
tensor(12382.1865, grad_fn=<NegBackward0>) tensor(12382.0645, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12381.9736328125
tensor(12382.0645, grad_fn=<NegBackward0>) tensor(12381.9736, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12381.8984375
tensor(12381.9736, grad_fn=<NegBackward0>) tensor(12381.8984, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12381.8232421875
tensor(12381.8984, grad_fn=<NegBackward0>) tensor(12381.8232, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12381.73828125
tensor(12381.8232, grad_fn=<NegBackward0>) tensor(12381.7383, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12381.5712890625
tensor(12381.7383, grad_fn=<NegBackward0>) tensor(12381.5713, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12380.779296875
tensor(12381.5713, grad_fn=<NegBackward0>) tensor(12380.7793, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12379.91015625
tensor(12380.7793, grad_fn=<NegBackward0>) tensor(12379.9102, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12379.5712890625
tensor(12379.9102, grad_fn=<NegBackward0>) tensor(12379.5713, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12379.4130859375
tensor(12379.5713, grad_fn=<NegBackward0>) tensor(12379.4131, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12379.326171875
tensor(12379.4131, grad_fn=<NegBackward0>) tensor(12379.3262, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12379.2646484375
tensor(12379.3262, grad_fn=<NegBackward0>) tensor(12379.2646, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12379.203125
tensor(12379.2646, grad_fn=<NegBackward0>) tensor(12379.2031, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12379.123046875
tensor(12379.2031, grad_fn=<NegBackward0>) tensor(12379.1230, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12378.9912109375
tensor(12379.1230, grad_fn=<NegBackward0>) tensor(12378.9912, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12378.818359375
tensor(12378.9912, grad_fn=<NegBackward0>) tensor(12378.8184, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12378.671875
tensor(12378.8184, grad_fn=<NegBackward0>) tensor(12378.6719, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12378.529296875
tensor(12378.6719, grad_fn=<NegBackward0>) tensor(12378.5293, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12378.412109375
tensor(12378.5293, grad_fn=<NegBackward0>) tensor(12378.4121, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12378.34765625
tensor(12378.4121, grad_fn=<NegBackward0>) tensor(12378.3477, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12378.310546875
tensor(12378.3477, grad_fn=<NegBackward0>) tensor(12378.3105, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12378.287109375
tensor(12378.3105, grad_fn=<NegBackward0>) tensor(12378.2871, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12378.267578125
tensor(12378.2871, grad_fn=<NegBackward0>) tensor(12378.2676, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12378.25
tensor(12378.2676, grad_fn=<NegBackward0>) tensor(12378.2500, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12378.232421875
tensor(12378.2500, grad_fn=<NegBackward0>) tensor(12378.2324, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12378.2099609375
tensor(12378.2324, grad_fn=<NegBackward0>) tensor(12378.2100, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12378.1806640625
tensor(12378.2100, grad_fn=<NegBackward0>) tensor(12378.1807, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12378.1396484375
tensor(12378.1807, grad_fn=<NegBackward0>) tensor(12378.1396, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12378.0859375
tensor(12378.1396, grad_fn=<NegBackward0>) tensor(12378.0859, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12378.0205078125
tensor(12378.0859, grad_fn=<NegBackward0>) tensor(12378.0205, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12377.9541015625
tensor(12378.0205, grad_fn=<NegBackward0>) tensor(12377.9541, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12377.87890625
tensor(12377.9541, grad_fn=<NegBackward0>) tensor(12377.8789, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12377.787109375
tensor(12377.8789, grad_fn=<NegBackward0>) tensor(12377.7871, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12377.681640625
tensor(12377.7871, grad_fn=<NegBackward0>) tensor(12377.6816, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12377.5830078125
tensor(12377.6816, grad_fn=<NegBackward0>) tensor(12377.5830, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12377.51171875
tensor(12377.5830, grad_fn=<NegBackward0>) tensor(12377.5117, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12377.46875
tensor(12377.5117, grad_fn=<NegBackward0>) tensor(12377.4688, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12377.44140625
tensor(12377.4688, grad_fn=<NegBackward0>) tensor(12377.4414, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12377.4248046875
tensor(12377.4414, grad_fn=<NegBackward0>) tensor(12377.4248, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12377.416015625
tensor(12377.4248, grad_fn=<NegBackward0>) tensor(12377.4160, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12377.4072265625
tensor(12377.4160, grad_fn=<NegBackward0>) tensor(12377.4072, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12377.4013671875
tensor(12377.4072, grad_fn=<NegBackward0>) tensor(12377.4014, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12377.3974609375
tensor(12377.4014, grad_fn=<NegBackward0>) tensor(12377.3975, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12377.392578125
tensor(12377.3975, grad_fn=<NegBackward0>) tensor(12377.3926, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12377.392578125
tensor(12377.3926, grad_fn=<NegBackward0>) tensor(12377.3926, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12377.388671875
tensor(12377.3926, grad_fn=<NegBackward0>) tensor(12377.3887, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12377.3876953125
tensor(12377.3887, grad_fn=<NegBackward0>) tensor(12377.3877, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12377.384765625
tensor(12377.3877, grad_fn=<NegBackward0>) tensor(12377.3848, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12377.384765625
tensor(12377.3848, grad_fn=<NegBackward0>) tensor(12377.3848, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12377.380859375
tensor(12377.3848, grad_fn=<NegBackward0>) tensor(12377.3809, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12377.3798828125
tensor(12377.3809, grad_fn=<NegBackward0>) tensor(12377.3799, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12377.3798828125
tensor(12377.3799, grad_fn=<NegBackward0>) tensor(12377.3799, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12377.3779296875
tensor(12377.3799, grad_fn=<NegBackward0>) tensor(12377.3779, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12377.37890625
tensor(12377.3779, grad_fn=<NegBackward0>) tensor(12377.3789, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12377.376953125
tensor(12377.3779, grad_fn=<NegBackward0>) tensor(12377.3770, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12377.376953125
tensor(12377.3770, grad_fn=<NegBackward0>) tensor(12377.3770, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12377.3759765625
tensor(12377.3770, grad_fn=<NegBackward0>) tensor(12377.3760, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12377.3759765625
tensor(12377.3760, grad_fn=<NegBackward0>) tensor(12377.3760, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12377.3759765625
tensor(12377.3760, grad_fn=<NegBackward0>) tensor(12377.3760, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12377.373046875
tensor(12377.3760, grad_fn=<NegBackward0>) tensor(12377.3730, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12377.3740234375
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12377.375
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3750, grad_fn=<NegBackward0>)
2
Iteration 6500: Loss = -12377.373046875
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3730, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12377.3720703125
tensor(12377.3730, grad_fn=<NegBackward0>) tensor(12377.3721, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12377.3740234375
tensor(12377.3721, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12377.578125
tensor(12377.3721, grad_fn=<NegBackward0>) tensor(12377.5781, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12377.37109375
tensor(12377.3721, grad_fn=<NegBackward0>) tensor(12377.3711, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12377.37109375
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.3711, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12377.3740234375
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12377.3701171875
tensor(12377.3711, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12377.3828125
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.3828, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12377.3701171875
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12377.71875
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.7188, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12377.369140625
tensor(12377.3701, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12377.369140625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12377.3701171875
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12377.369140625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12377.3701171875
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3701, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12377.37109375
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3711, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -12377.3857421875
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3857, grad_fn=<NegBackward0>)
3
Iteration 8300: Loss = -12377.369140625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12377.3740234375
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3740, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12377.3681640625
tensor(12377.3691, grad_fn=<NegBackward0>) tensor(12377.3682, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12377.3671875
tensor(12377.3682, grad_fn=<NegBackward0>) tensor(12377.3672, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12377.369140625
tensor(12377.3672, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12377.3681640625
tensor(12377.3672, grad_fn=<NegBackward0>) tensor(12377.3682, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12377.3681640625
tensor(12377.3672, grad_fn=<NegBackward0>) tensor(12377.3682, grad_fn=<NegBackward0>)
3
Iteration 9000: Loss = -12377.369140625
tensor(12377.3672, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
4
Iteration 9100: Loss = -12377.369140625
tensor(12377.3672, grad_fn=<NegBackward0>) tensor(12377.3691, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[1.0000e+00, 3.5791e-06],
        [1.7029e-02, 9.8297e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9789, 0.0211], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1793],
         [0.7121, 0.3959]],

        [[0.6943, 0.2334],
         [0.5510, 0.6099]],

        [[0.6792, 0.2444],
         [0.6457, 0.5988]],

        [[0.6390, 0.0787],
         [0.5849, 0.6580]],

        [[0.5011, 0.2201],
         [0.6481, 0.7270]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: -0.013574768645372375
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
Global Adjusted Rand Index: -0.001037814036071472
Average Adjusted Rand Index: -0.004191209707380607
[-0.001037814036071472, -0.001037814036071472] [-0.004191209707380607, -0.004191209707380607] [12377.3671875, 12377.369140625]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11907.594253266367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19900.548828125
inf tensor(19900.5488, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12395.09765625
tensor(19900.5488, grad_fn=<NegBackward0>) tensor(12395.0977, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12394.8056640625
tensor(12395.0977, grad_fn=<NegBackward0>) tensor(12394.8057, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12394.7431640625
tensor(12394.8057, grad_fn=<NegBackward0>) tensor(12394.7432, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12394.7041015625
tensor(12394.7432, grad_fn=<NegBackward0>) tensor(12394.7041, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12394.677734375
tensor(12394.7041, grad_fn=<NegBackward0>) tensor(12394.6777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12394.6572265625
tensor(12394.6777, grad_fn=<NegBackward0>) tensor(12394.6572, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12394.640625
tensor(12394.6572, grad_fn=<NegBackward0>) tensor(12394.6406, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12394.626953125
tensor(12394.6406, grad_fn=<NegBackward0>) tensor(12394.6270, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12394.6171875
tensor(12394.6270, grad_fn=<NegBackward0>) tensor(12394.6172, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12394.60546875
tensor(12394.6172, grad_fn=<NegBackward0>) tensor(12394.6055, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12394.5927734375
tensor(12394.6055, grad_fn=<NegBackward0>) tensor(12394.5928, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12394.5791015625
tensor(12394.5928, grad_fn=<NegBackward0>) tensor(12394.5791, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12394.5546875
tensor(12394.5791, grad_fn=<NegBackward0>) tensor(12394.5547, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12394.505859375
tensor(12394.5547, grad_fn=<NegBackward0>) tensor(12394.5059, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12394.421875
tensor(12394.5059, grad_fn=<NegBackward0>) tensor(12394.4219, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12394.365234375
tensor(12394.4219, grad_fn=<NegBackward0>) tensor(12394.3652, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12394.3447265625
tensor(12394.3652, grad_fn=<NegBackward0>) tensor(12394.3447, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12394.333984375
tensor(12394.3447, grad_fn=<NegBackward0>) tensor(12394.3340, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12394.326171875
tensor(12394.3340, grad_fn=<NegBackward0>) tensor(12394.3262, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12394.3203125
tensor(12394.3262, grad_fn=<NegBackward0>) tensor(12394.3203, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12394.3173828125
tensor(12394.3203, grad_fn=<NegBackward0>) tensor(12394.3174, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12394.31640625
tensor(12394.3174, grad_fn=<NegBackward0>) tensor(12394.3164, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12394.314453125
tensor(12394.3164, grad_fn=<NegBackward0>) tensor(12394.3145, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12394.3115234375
tensor(12394.3145, grad_fn=<NegBackward0>) tensor(12394.3115, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12394.310546875
tensor(12394.3115, grad_fn=<NegBackward0>) tensor(12394.3105, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12394.310546875
tensor(12394.3105, grad_fn=<NegBackward0>) tensor(12394.3105, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12394.3095703125
tensor(12394.3105, grad_fn=<NegBackward0>) tensor(12394.3096, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12394.30859375
tensor(12394.3096, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12394.30859375
tensor(12394.3086, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12394.3076171875
tensor(12394.3086, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12394.3076171875
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12394.306640625
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12394.3076171875
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12394.30859375
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -12394.306640625
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12394.3056640625
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12394.306640625
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12394.3076171875
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
2
Iteration 3900: Loss = -12394.3056640625
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12394.3076171875
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12394.3046875
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12394.3046875
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12394.3046875
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12394.3056640625
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12394.3056640625
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12394.3056640625
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12394.3046875
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12394.302734375
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3027, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12394.3046875
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12394.3037109375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12394.3037109375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -12394.3037109375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -12394.302734375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3027, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12394.3046875
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12394.3017578125
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12394.30078125
tensor(12394.3018, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12394.3037109375
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12394.3017578125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12394.3037109375
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -12394.3017578125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -12394.30078125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12394.3017578125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12394.3017578125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12394.3017578125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12394.3076171875
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -12394.30078125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12394.3017578125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12394.30078125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12394.30078125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12394.3037109375
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12394.298828125
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.2988, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12394.30078125
tensor(12394.2988, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12394.306640625
tensor(12394.2988, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12394.2998046875
tensor(12394.2988, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12394.2998046875
tensor(12394.2988, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -12394.30078125
tensor(12394.2988, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.0071, 0.9929],
        [0.0177, 0.9823]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0372, 0.9628], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2305, 0.2126],
         [0.6243, 0.1992]],

        [[0.5575, 0.2929],
         [0.6059, 0.5951]],

        [[0.7191, 0.2093],
         [0.6458, 0.5580]],

        [[0.6534, 0.2296],
         [0.5965, 0.6043]],

        [[0.6278, 0.1943],
         [0.6681, 0.5373]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007272257147012719
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24044.68359375
inf tensor(24044.6836, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12395.4501953125
tensor(24044.6836, grad_fn=<NegBackward0>) tensor(12395.4502, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12394.962890625
tensor(12395.4502, grad_fn=<NegBackward0>) tensor(12394.9629, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12394.869140625
tensor(12394.9629, grad_fn=<NegBackward0>) tensor(12394.8691, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12394.8076171875
tensor(12394.8691, grad_fn=<NegBackward0>) tensor(12394.8076, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12394.7666015625
tensor(12394.8076, grad_fn=<NegBackward0>) tensor(12394.7666, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12394.7353515625
tensor(12394.7666, grad_fn=<NegBackward0>) tensor(12394.7354, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12394.712890625
tensor(12394.7354, grad_fn=<NegBackward0>) tensor(12394.7129, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12394.6953125
tensor(12394.7129, grad_fn=<NegBackward0>) tensor(12394.6953, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12394.68359375
tensor(12394.6953, grad_fn=<NegBackward0>) tensor(12394.6836, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12394.671875
tensor(12394.6836, grad_fn=<NegBackward0>) tensor(12394.6719, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12394.6630859375
tensor(12394.6719, grad_fn=<NegBackward0>) tensor(12394.6631, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12394.654296875
tensor(12394.6631, grad_fn=<NegBackward0>) tensor(12394.6543, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12394.646484375
tensor(12394.6543, grad_fn=<NegBackward0>) tensor(12394.6465, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12394.63671875
tensor(12394.6465, grad_fn=<NegBackward0>) tensor(12394.6367, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12394.6201171875
tensor(12394.6367, grad_fn=<NegBackward0>) tensor(12394.6201, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12394.5888671875
tensor(12394.6201, grad_fn=<NegBackward0>) tensor(12394.5889, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12394.5087890625
tensor(12394.5889, grad_fn=<NegBackward0>) tensor(12394.5088, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12394.4150390625
tensor(12394.5088, grad_fn=<NegBackward0>) tensor(12394.4150, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12394.3759765625
tensor(12394.4150, grad_fn=<NegBackward0>) tensor(12394.3760, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12394.359375
tensor(12394.3760, grad_fn=<NegBackward0>) tensor(12394.3594, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12394.349609375
tensor(12394.3594, grad_fn=<NegBackward0>) tensor(12394.3496, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12394.33984375
tensor(12394.3496, grad_fn=<NegBackward0>) tensor(12394.3398, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12394.333984375
tensor(12394.3398, grad_fn=<NegBackward0>) tensor(12394.3340, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12394.328125
tensor(12394.3340, grad_fn=<NegBackward0>) tensor(12394.3281, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12394.3251953125
tensor(12394.3281, grad_fn=<NegBackward0>) tensor(12394.3252, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12394.3212890625
tensor(12394.3252, grad_fn=<NegBackward0>) tensor(12394.3213, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12394.318359375
tensor(12394.3213, grad_fn=<NegBackward0>) tensor(12394.3184, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12394.3154296875
tensor(12394.3184, grad_fn=<NegBackward0>) tensor(12394.3154, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12394.314453125
tensor(12394.3154, grad_fn=<NegBackward0>) tensor(12394.3145, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12394.3125
tensor(12394.3145, grad_fn=<NegBackward0>) tensor(12394.3125, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12394.3115234375
tensor(12394.3125, grad_fn=<NegBackward0>) tensor(12394.3115, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12394.3095703125
tensor(12394.3115, grad_fn=<NegBackward0>) tensor(12394.3096, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12394.30859375
tensor(12394.3096, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12394.30859375
tensor(12394.3086, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12394.30859375
tensor(12394.3086, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12394.3076171875
tensor(12394.3086, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12394.3095703125
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3096, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12394.3076171875
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -12394.3076171875
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12394.3076171875
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12394.3076171875
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -12394.30859375
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
4
Iteration 4900: Loss = -12394.306640625
tensor(12394.3076, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12394.3076171875
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12394.30859375
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12394.30859375
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3086, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -12394.3076171875
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -12394.306640625
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12394.306640625
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12394.306640625
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12394.3056640625
tensor(12394.3066, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12394.3076171875
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12394.3076171875
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3076, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12394.306640625
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3066, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -12394.3046875
tensor(12394.3057, grad_fn=<NegBackward0>) tensor(12394.3047, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12394.3056640625
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3057, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12394.3037109375
tensor(12394.3047, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12394.302734375
tensor(12394.3037, grad_fn=<NegBackward0>) tensor(12394.3027, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12394.302734375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3027, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12394.302734375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3027, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12394.3037109375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3037, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12394.302734375
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3027, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12394.3017578125
tensor(12394.3027, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12394.3017578125
tensor(12394.3018, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12394.30078125
tensor(12394.3018, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12394.2998046875
tensor(12394.3008, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12394.30078125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12394.2998046875
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12394.30078125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12394.3125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3125, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12394.3017578125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -12394.3017578125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3018, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -12394.2998046875
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12394.2998046875
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12394.30078125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12394.30078125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3008, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12394.3173828125
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.3174, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12394.2978515625
tensor(12394.2998, grad_fn=<NegBackward0>) tensor(12394.2979, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12394.4775390625
tensor(12394.2979, grad_fn=<NegBackward0>) tensor(12394.4775, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12394.2998046875
tensor(12394.2979, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12394.298828125
tensor(12394.2979, grad_fn=<NegBackward0>) tensor(12394.2988, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12394.2998046875
tensor(12394.2979, grad_fn=<NegBackward0>) tensor(12394.2998, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -12394.298828125
tensor(12394.2979, grad_fn=<NegBackward0>) tensor(12394.2988, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.9822, 0.0178],
        [0.9958, 0.0042]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9616, 0.0384], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.2126],
         [0.6797, 0.2303]],

        [[0.7237, 0.2930],
         [0.7299, 0.6019]],

        [[0.6216, 0.2094],
         [0.6634, 0.6117]],

        [[0.6865, 0.2296],
         [0.5452, 0.5237]],

        [[0.6339, 0.1943],
         [0.5774, 0.6640]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007272257147012719
Average Adjusted Rand Index: -0.0008569898232458489
[-0.0007272257147012719, -0.0007272257147012719] [-0.0008569898232458489, -0.0008569898232458489] [12394.30078125, 12394.298828125]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11936.087499424195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20577.685546875
inf tensor(20577.6855, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12417.30078125
tensor(20577.6855, grad_fn=<NegBackward0>) tensor(12417.3008, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12416.8828125
tensor(12417.3008, grad_fn=<NegBackward0>) tensor(12416.8828, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12416.7705078125
tensor(12416.8828, grad_fn=<NegBackward0>) tensor(12416.7705, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12416.666015625
tensor(12416.7705, grad_fn=<NegBackward0>) tensor(12416.6660, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12416.5556640625
tensor(12416.6660, grad_fn=<NegBackward0>) tensor(12416.5557, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12416.4560546875
tensor(12416.5557, grad_fn=<NegBackward0>) tensor(12416.4561, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12416.376953125
tensor(12416.4561, grad_fn=<NegBackward0>) tensor(12416.3770, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12416.3134765625
tensor(12416.3770, grad_fn=<NegBackward0>) tensor(12416.3135, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12416.259765625
tensor(12416.3135, grad_fn=<NegBackward0>) tensor(12416.2598, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12416.205078125
tensor(12416.2598, grad_fn=<NegBackward0>) tensor(12416.2051, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12416.1494140625
tensor(12416.2051, grad_fn=<NegBackward0>) tensor(12416.1494, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12416.09765625
tensor(12416.1494, grad_fn=<NegBackward0>) tensor(12416.0977, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12416.048828125
tensor(12416.0977, grad_fn=<NegBackward0>) tensor(12416.0488, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12416.01171875
tensor(12416.0488, grad_fn=<NegBackward0>) tensor(12416.0117, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12415.9814453125
tensor(12416.0117, grad_fn=<NegBackward0>) tensor(12415.9814, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12415.9580078125
tensor(12415.9814, grad_fn=<NegBackward0>) tensor(12415.9580, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12415.9423828125
tensor(12415.9580, grad_fn=<NegBackward0>) tensor(12415.9424, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12415.9287109375
tensor(12415.9424, grad_fn=<NegBackward0>) tensor(12415.9287, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12415.9189453125
tensor(12415.9287, grad_fn=<NegBackward0>) tensor(12415.9189, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12415.9111328125
tensor(12415.9189, grad_fn=<NegBackward0>) tensor(12415.9111, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12415.904296875
tensor(12415.9111, grad_fn=<NegBackward0>) tensor(12415.9043, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12415.9013671875
tensor(12415.9043, grad_fn=<NegBackward0>) tensor(12415.9014, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12415.8955078125
tensor(12415.9014, grad_fn=<NegBackward0>) tensor(12415.8955, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12415.8916015625
tensor(12415.8955, grad_fn=<NegBackward0>) tensor(12415.8916, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12415.888671875
tensor(12415.8916, grad_fn=<NegBackward0>) tensor(12415.8887, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12415.8837890625
tensor(12415.8887, grad_fn=<NegBackward0>) tensor(12415.8838, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12415.87890625
tensor(12415.8838, grad_fn=<NegBackward0>) tensor(12415.8789, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12415.8720703125
tensor(12415.8789, grad_fn=<NegBackward0>) tensor(12415.8721, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12415.8623046875
tensor(12415.8721, grad_fn=<NegBackward0>) tensor(12415.8623, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12415.8466796875
tensor(12415.8623, grad_fn=<NegBackward0>) tensor(12415.8467, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12415.8125
tensor(12415.8467, grad_fn=<NegBackward0>) tensor(12415.8125, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12415.7158203125
tensor(12415.8125, grad_fn=<NegBackward0>) tensor(12415.7158, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12415.421875
tensor(12415.7158, grad_fn=<NegBackward0>) tensor(12415.4219, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12005.7236328125
tensor(12415.4219, grad_fn=<NegBackward0>) tensor(12005.7236, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11995.984375
tensor(12005.7236, grad_fn=<NegBackward0>) tensor(11995.9844, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11980.0166015625
tensor(11995.9844, grad_fn=<NegBackward0>) tensor(11980.0166, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11975.619140625
tensor(11980.0166, grad_fn=<NegBackward0>) tensor(11975.6191, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11975.611328125
tensor(11975.6191, grad_fn=<NegBackward0>) tensor(11975.6113, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11975.6005859375
tensor(11975.6113, grad_fn=<NegBackward0>) tensor(11975.6006, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11969.3759765625
tensor(11975.6006, grad_fn=<NegBackward0>) tensor(11969.3760, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11967.0380859375
tensor(11969.3760, grad_fn=<NegBackward0>) tensor(11967.0381, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11967.0361328125
tensor(11967.0381, grad_fn=<NegBackward0>) tensor(11967.0361, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11966.884765625
tensor(11967.0361, grad_fn=<NegBackward0>) tensor(11966.8848, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11952.1953125
tensor(11966.8848, grad_fn=<NegBackward0>) tensor(11952.1953, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11952.1865234375
tensor(11952.1953, grad_fn=<NegBackward0>) tensor(11952.1865, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11952.1845703125
tensor(11952.1865, grad_fn=<NegBackward0>) tensor(11952.1846, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11952.185546875
tensor(11952.1846, grad_fn=<NegBackward0>) tensor(11952.1855, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11952.169921875
tensor(11952.1846, grad_fn=<NegBackward0>) tensor(11952.1699, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11942.6650390625
tensor(11952.1699, grad_fn=<NegBackward0>) tensor(11942.6650, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11939.9990234375
tensor(11942.6650, grad_fn=<NegBackward0>) tensor(11939.9990, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11930.888671875
tensor(11939.9990, grad_fn=<NegBackward0>) tensor(11930.8887, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11930.884765625
tensor(11930.8887, grad_fn=<NegBackward0>) tensor(11930.8848, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11930.8798828125
tensor(11930.8848, grad_fn=<NegBackward0>) tensor(11930.8799, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11930.87890625
tensor(11930.8799, grad_fn=<NegBackward0>) tensor(11930.8789, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11930.8759765625
tensor(11930.8789, grad_fn=<NegBackward0>) tensor(11930.8760, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11930.7900390625
tensor(11930.8760, grad_fn=<NegBackward0>) tensor(11930.7900, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11930.7998046875
tensor(11930.7900, grad_fn=<NegBackward0>) tensor(11930.7998, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -11930.7861328125
tensor(11930.7900, grad_fn=<NegBackward0>) tensor(11930.7861, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11930.7890625
tensor(11930.7861, grad_fn=<NegBackward0>) tensor(11930.7891, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11930.7841796875
tensor(11930.7861, grad_fn=<NegBackward0>) tensor(11930.7842, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11930.7841796875
tensor(11930.7842, grad_fn=<NegBackward0>) tensor(11930.7842, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11930.7841796875
tensor(11930.7842, grad_fn=<NegBackward0>) tensor(11930.7842, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11930.78125
tensor(11930.7842, grad_fn=<NegBackward0>) tensor(11930.7812, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11930.78125
tensor(11930.7812, grad_fn=<NegBackward0>) tensor(11930.7812, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11930.783203125
tensor(11930.7812, grad_fn=<NegBackward0>) tensor(11930.7832, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11930.779296875
tensor(11930.7812, grad_fn=<NegBackward0>) tensor(11930.7793, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11930.775390625
tensor(11930.7793, grad_fn=<NegBackward0>) tensor(11930.7754, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11930.7744140625
tensor(11930.7754, grad_fn=<NegBackward0>) tensor(11930.7744, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11930.7724609375
tensor(11930.7744, grad_fn=<NegBackward0>) tensor(11930.7725, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11930.759765625
tensor(11930.7725, grad_fn=<NegBackward0>) tensor(11930.7598, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11930.7587890625
tensor(11930.7598, grad_fn=<NegBackward0>) tensor(11930.7588, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11930.767578125
tensor(11930.7588, grad_fn=<NegBackward0>) tensor(11930.7676, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11930.7548828125
tensor(11930.7588, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11930.7548828125
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11930.75
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7500, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11930.7490234375
tensor(11930.7500, grad_fn=<NegBackward0>) tensor(11930.7490, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11930.748046875
tensor(11930.7490, grad_fn=<NegBackward0>) tensor(11930.7480, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11930.7490234375
tensor(11930.7480, grad_fn=<NegBackward0>) tensor(11930.7490, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11930.7451171875
tensor(11930.7480, grad_fn=<NegBackward0>) tensor(11930.7451, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11930.744140625
tensor(11930.7451, grad_fn=<NegBackward0>) tensor(11930.7441, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11930.7412109375
tensor(11930.7441, grad_fn=<NegBackward0>) tensor(11930.7412, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11930.7373046875
tensor(11930.7412, grad_fn=<NegBackward0>) tensor(11930.7373, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11930.7509765625
tensor(11930.7373, grad_fn=<NegBackward0>) tensor(11930.7510, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11930.736328125
tensor(11930.7373, grad_fn=<NegBackward0>) tensor(11930.7363, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11930.744140625
tensor(11930.7363, grad_fn=<NegBackward0>) tensor(11930.7441, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11930.736328125
tensor(11930.7363, grad_fn=<NegBackward0>) tensor(11930.7363, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11930.7373046875
tensor(11930.7363, grad_fn=<NegBackward0>) tensor(11930.7373, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11930.736328125
tensor(11930.7363, grad_fn=<NegBackward0>) tensor(11930.7363, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11930.74609375
tensor(11930.7363, grad_fn=<NegBackward0>) tensor(11930.7461, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11930.7353515625
tensor(11930.7363, grad_fn=<NegBackward0>) tensor(11930.7354, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11930.7353515625
tensor(11930.7354, grad_fn=<NegBackward0>) tensor(11930.7354, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11930.7646484375
tensor(11930.7354, grad_fn=<NegBackward0>) tensor(11930.7646, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11930.736328125
tensor(11930.7354, grad_fn=<NegBackward0>) tensor(11930.7363, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -11930.7353515625
tensor(11930.7354, grad_fn=<NegBackward0>) tensor(11930.7354, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11930.732421875
tensor(11930.7354, grad_fn=<NegBackward0>) tensor(11930.7324, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11930.7392578125
tensor(11930.7324, grad_fn=<NegBackward0>) tensor(11930.7393, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11930.7294921875
tensor(11930.7324, grad_fn=<NegBackward0>) tensor(11930.7295, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11930.73046875
tensor(11930.7295, grad_fn=<NegBackward0>) tensor(11930.7305, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11930.7392578125
tensor(11930.7295, grad_fn=<NegBackward0>) tensor(11930.7393, grad_fn=<NegBackward0>)
2
pi: tensor([[0.2796, 0.7204],
        [0.7659, 0.2341]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4478, 0.5522], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2921, 0.1022],
         [0.6794, 0.3067]],

        [[0.5122, 0.0954],
         [0.5772, 0.7253]],

        [[0.7307, 0.1096],
         [0.6356, 0.6472]],

        [[0.6995, 0.0980],
         [0.5964, 0.5417]],

        [[0.6689, 0.1084],
         [0.5235, 0.5825]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.03340768036284783
Average Adjusted Rand Index: 0.9451304454226589
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20358.615234375
inf tensor(20358.6152, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12417.4013671875
tensor(20358.6152, grad_fn=<NegBackward0>) tensor(12417.4014, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12416.9794921875
tensor(12417.4014, grad_fn=<NegBackward0>) tensor(12416.9795, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12416.853515625
tensor(12416.9795, grad_fn=<NegBackward0>) tensor(12416.8535, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12416.7373046875
tensor(12416.8535, grad_fn=<NegBackward0>) tensor(12416.7373, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12416.6318359375
tensor(12416.7373, grad_fn=<NegBackward0>) tensor(12416.6318, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12416.5556640625
tensor(12416.6318, grad_fn=<NegBackward0>) tensor(12416.5557, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12416.4970703125
tensor(12416.5557, grad_fn=<NegBackward0>) tensor(12416.4971, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12416.4423828125
tensor(12416.4971, grad_fn=<NegBackward0>) tensor(12416.4424, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12416.384765625
tensor(12416.4424, grad_fn=<NegBackward0>) tensor(12416.3848, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12416.3212890625
tensor(12416.3848, grad_fn=<NegBackward0>) tensor(12416.3213, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12416.255859375
tensor(12416.3213, grad_fn=<NegBackward0>) tensor(12416.2559, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12416.19921875
tensor(12416.2559, grad_fn=<NegBackward0>) tensor(12416.1992, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12416.1494140625
tensor(12416.1992, grad_fn=<NegBackward0>) tensor(12416.1494, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12416.1103515625
tensor(12416.1494, grad_fn=<NegBackward0>) tensor(12416.1104, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12416.07421875
tensor(12416.1104, grad_fn=<NegBackward0>) tensor(12416.0742, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12416.0439453125
tensor(12416.0742, grad_fn=<NegBackward0>) tensor(12416.0439, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12416.021484375
tensor(12416.0439, grad_fn=<NegBackward0>) tensor(12416.0215, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12416.0
tensor(12416.0215, grad_fn=<NegBackward0>) tensor(12416., grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12415.984375
tensor(12416., grad_fn=<NegBackward0>) tensor(12415.9844, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12415.96875
tensor(12415.9844, grad_fn=<NegBackward0>) tensor(12415.9688, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12415.95703125
tensor(12415.9688, grad_fn=<NegBackward0>) tensor(12415.9570, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12415.943359375
tensor(12415.9570, grad_fn=<NegBackward0>) tensor(12415.9434, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12415.9345703125
tensor(12415.9434, grad_fn=<NegBackward0>) tensor(12415.9346, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12415.9287109375
tensor(12415.9346, grad_fn=<NegBackward0>) tensor(12415.9287, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12415.9208984375
tensor(12415.9287, grad_fn=<NegBackward0>) tensor(12415.9209, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12415.9140625
tensor(12415.9209, grad_fn=<NegBackward0>) tensor(12415.9141, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12415.91015625
tensor(12415.9141, grad_fn=<NegBackward0>) tensor(12415.9102, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12415.90625
tensor(12415.9102, grad_fn=<NegBackward0>) tensor(12415.9062, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12415.9033203125
tensor(12415.9062, grad_fn=<NegBackward0>) tensor(12415.9033, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12415.8994140625
tensor(12415.9033, grad_fn=<NegBackward0>) tensor(12415.8994, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12415.8974609375
tensor(12415.8994, grad_fn=<NegBackward0>) tensor(12415.8975, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12415.892578125
tensor(12415.8975, grad_fn=<NegBackward0>) tensor(12415.8926, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12415.890625
tensor(12415.8926, grad_fn=<NegBackward0>) tensor(12415.8906, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12415.88671875
tensor(12415.8906, grad_fn=<NegBackward0>) tensor(12415.8867, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12415.8837890625
tensor(12415.8867, grad_fn=<NegBackward0>) tensor(12415.8838, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12415.87890625
tensor(12415.8838, grad_fn=<NegBackward0>) tensor(12415.8789, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12415.8740234375
tensor(12415.8789, grad_fn=<NegBackward0>) tensor(12415.8740, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12415.8681640625
tensor(12415.8740, grad_fn=<NegBackward0>) tensor(12415.8682, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12415.8603515625
tensor(12415.8682, grad_fn=<NegBackward0>) tensor(12415.8604, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12415.8486328125
tensor(12415.8604, grad_fn=<NegBackward0>) tensor(12415.8486, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12415.8271484375
tensor(12415.8486, grad_fn=<NegBackward0>) tensor(12415.8271, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12415.7802734375
tensor(12415.8271, grad_fn=<NegBackward0>) tensor(12415.7803, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12415.6474609375
tensor(12415.7803, grad_fn=<NegBackward0>) tensor(12415.6475, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12415.353515625
tensor(12415.6475, grad_fn=<NegBackward0>) tensor(12415.3535, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11983.4462890625
tensor(12415.3535, grad_fn=<NegBackward0>) tensor(11983.4463, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11982.7109375
tensor(11983.4463, grad_fn=<NegBackward0>) tensor(11982.7109, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11975.673828125
tensor(11982.7109, grad_fn=<NegBackward0>) tensor(11975.6738, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11975.65234375
tensor(11975.6738, grad_fn=<NegBackward0>) tensor(11975.6523, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11967.0048828125
tensor(11975.6523, grad_fn=<NegBackward0>) tensor(11967.0049, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11966.99609375
tensor(11967.0049, grad_fn=<NegBackward0>) tensor(11966.9961, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11959.2919921875
tensor(11966.9961, grad_fn=<NegBackward0>) tensor(11959.2920, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11959.2880859375
tensor(11959.2920, grad_fn=<NegBackward0>) tensor(11959.2881, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11959.28515625
tensor(11959.2881, grad_fn=<NegBackward0>) tensor(11959.2852, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11959.251953125
tensor(11959.2852, grad_fn=<NegBackward0>) tensor(11959.2520, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11959.24609375
tensor(11959.2520, grad_fn=<NegBackward0>) tensor(11959.2461, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11959.240234375
tensor(11959.2461, grad_fn=<NegBackward0>) tensor(11959.2402, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11943.0625
tensor(11959.2402, grad_fn=<NegBackward0>) tensor(11943.0625, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11943.060546875
tensor(11943.0625, grad_fn=<NegBackward0>) tensor(11943.0605, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11943.0576171875
tensor(11943.0605, grad_fn=<NegBackward0>) tensor(11943.0576, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11943.048828125
tensor(11943.0576, grad_fn=<NegBackward0>) tensor(11943.0488, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11935.341796875
tensor(11943.0488, grad_fn=<NegBackward0>) tensor(11935.3418, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11933.736328125
tensor(11935.3418, grad_fn=<NegBackward0>) tensor(11933.7363, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11933.73046875
tensor(11933.7363, grad_fn=<NegBackward0>) tensor(11933.7305, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11933.728515625
tensor(11933.7305, grad_fn=<NegBackward0>) tensor(11933.7285, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11933.7275390625
tensor(11933.7285, grad_fn=<NegBackward0>) tensor(11933.7275, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11933.734375
tensor(11933.7275, grad_fn=<NegBackward0>) tensor(11933.7344, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11933.724609375
tensor(11933.7275, grad_fn=<NegBackward0>) tensor(11933.7246, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11933.73046875
tensor(11933.7246, grad_fn=<NegBackward0>) tensor(11933.7305, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11933.724609375
tensor(11933.7246, grad_fn=<NegBackward0>) tensor(11933.7246, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11933.6591796875
tensor(11933.7246, grad_fn=<NegBackward0>) tensor(11933.6592, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11933.6494140625
tensor(11933.6592, grad_fn=<NegBackward0>) tensor(11933.6494, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11930.818359375
tensor(11933.6494, grad_fn=<NegBackward0>) tensor(11930.8184, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11930.7822265625
tensor(11930.8184, grad_fn=<NegBackward0>) tensor(11930.7822, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11930.7880859375
tensor(11930.7822, grad_fn=<NegBackward0>) tensor(11930.7881, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11930.779296875
tensor(11930.7822, grad_fn=<NegBackward0>) tensor(11930.7793, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11930.7724609375
tensor(11930.7793, grad_fn=<NegBackward0>) tensor(11930.7725, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11930.798828125
tensor(11930.7725, grad_fn=<NegBackward0>) tensor(11930.7988, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11930.76171875
tensor(11930.7725, grad_fn=<NegBackward0>) tensor(11930.7617, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11930.759765625
tensor(11930.7617, grad_fn=<NegBackward0>) tensor(11930.7598, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11930.7783203125
tensor(11930.7598, grad_fn=<NegBackward0>) tensor(11930.7783, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11930.7587890625
tensor(11930.7598, grad_fn=<NegBackward0>) tensor(11930.7588, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11930.7548828125
tensor(11930.7588, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11930.7548828125
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11930.755859375
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7559, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11930.755859375
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7559, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11930.7548828125
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11930.76171875
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7617, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11930.75390625
tensor(11930.7549, grad_fn=<NegBackward0>) tensor(11930.7539, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11930.7529296875
tensor(11930.7539, grad_fn=<NegBackward0>) tensor(11930.7529, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11930.75390625
tensor(11930.7529, grad_fn=<NegBackward0>) tensor(11930.7539, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11930.7548828125
tensor(11930.7529, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11930.7548828125
tensor(11930.7529, grad_fn=<NegBackward0>) tensor(11930.7549, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -11930.7431640625
tensor(11930.7529, grad_fn=<NegBackward0>) tensor(11930.7432, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11930.865234375
tensor(11930.7432, grad_fn=<NegBackward0>) tensor(11930.8652, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11930.73828125
tensor(11930.7432, grad_fn=<NegBackward0>) tensor(11930.7383, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11930.7392578125
tensor(11930.7383, grad_fn=<NegBackward0>) tensor(11930.7393, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11930.73828125
tensor(11930.7383, grad_fn=<NegBackward0>) tensor(11930.7383, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11930.7373046875
tensor(11930.7383, grad_fn=<NegBackward0>) tensor(11930.7373, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11930.7392578125
tensor(11930.7373, grad_fn=<NegBackward0>) tensor(11930.7393, grad_fn=<NegBackward0>)
1
pi: tensor([[0.2790, 0.7210],
        [0.7659, 0.2341]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4475, 0.5525], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2921, 0.1020],
         [0.7021, 0.3067]],

        [[0.6055, 0.0953],
         [0.6245, 0.6586]],

        [[0.5296, 0.1097],
         [0.5336, 0.7297]],

        [[0.6081, 0.0979],
         [0.6121, 0.5376]],

        [[0.6439, 0.1079],
         [0.5432, 0.6985]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.03340768036284783
Average Adjusted Rand Index: 0.9451304454226589
[0.03340768036284783, 0.03340768036284783] [0.9451304454226589, 0.9451304454226589] [11930.7353515625, 11930.7353515625]
-------------------------------------
This iteration is 67
True Objective function: Loss = -11759.892371638682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21842.3828125
inf tensor(21842.3828, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12304.541015625
tensor(21842.3828, grad_fn=<NegBackward0>) tensor(12304.5410, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12304.2255859375
tensor(12304.5410, grad_fn=<NegBackward0>) tensor(12304.2256, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12304.095703125
tensor(12304.2256, grad_fn=<NegBackward0>) tensor(12304.0957, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12303.9931640625
tensor(12304.0957, grad_fn=<NegBackward0>) tensor(12303.9932, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12303.9140625
tensor(12303.9932, grad_fn=<NegBackward0>) tensor(12303.9141, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12303.859375
tensor(12303.9141, grad_fn=<NegBackward0>) tensor(12303.8594, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12303.8125
tensor(12303.8594, grad_fn=<NegBackward0>) tensor(12303.8125, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12303.7724609375
tensor(12303.8125, grad_fn=<NegBackward0>) tensor(12303.7725, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12303.73046875
tensor(12303.7725, grad_fn=<NegBackward0>) tensor(12303.7305, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12303.685546875
tensor(12303.7305, grad_fn=<NegBackward0>) tensor(12303.6855, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12303.6337890625
tensor(12303.6855, grad_fn=<NegBackward0>) tensor(12303.6338, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12303.568359375
tensor(12303.6338, grad_fn=<NegBackward0>) tensor(12303.5684, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12303.482421875
tensor(12303.5684, grad_fn=<NegBackward0>) tensor(12303.4824, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12303.3642578125
tensor(12303.4824, grad_fn=<NegBackward0>) tensor(12303.3643, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12303.2080078125
tensor(12303.3643, grad_fn=<NegBackward0>) tensor(12303.2080, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12303.009765625
tensor(12303.2080, grad_fn=<NegBackward0>) tensor(12303.0098, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12302.7626953125
tensor(12303.0098, grad_fn=<NegBackward0>) tensor(12302.7627, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12302.69921875
tensor(12302.7627, grad_fn=<NegBackward0>) tensor(12302.6992, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12302.67578125
tensor(12302.6992, grad_fn=<NegBackward0>) tensor(12302.6758, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12302.658203125
tensor(12302.6758, grad_fn=<NegBackward0>) tensor(12302.6582, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12302.63671875
tensor(12302.6582, grad_fn=<NegBackward0>) tensor(12302.6367, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12302.609375
tensor(12302.6367, grad_fn=<NegBackward0>) tensor(12302.6094, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12302.5732421875
tensor(12302.6094, grad_fn=<NegBackward0>) tensor(12302.5732, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12302.5263671875
tensor(12302.5732, grad_fn=<NegBackward0>) tensor(12302.5264, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12302.486328125
tensor(12302.5264, grad_fn=<NegBackward0>) tensor(12302.4863, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12302.4541015625
tensor(12302.4863, grad_fn=<NegBackward0>) tensor(12302.4541, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12302.43359375
tensor(12302.4541, grad_fn=<NegBackward0>) tensor(12302.4336, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12302.4208984375
tensor(12302.4336, grad_fn=<NegBackward0>) tensor(12302.4209, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12302.4111328125
tensor(12302.4209, grad_fn=<NegBackward0>) tensor(12302.4111, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12302.4033203125
tensor(12302.4111, grad_fn=<NegBackward0>) tensor(12302.4033, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12302.3955078125
tensor(12302.4033, grad_fn=<NegBackward0>) tensor(12302.3955, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12302.38671875
tensor(12302.3955, grad_fn=<NegBackward0>) tensor(12302.3867, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12302.3779296875
tensor(12302.3867, grad_fn=<NegBackward0>) tensor(12302.3779, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12302.41796875
tensor(12302.3779, grad_fn=<NegBackward0>) tensor(12302.4180, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12302.357421875
tensor(12302.3779, grad_fn=<NegBackward0>) tensor(12302.3574, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12302.353515625
tensor(12302.3574, grad_fn=<NegBackward0>) tensor(12302.3535, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12302.3486328125
tensor(12302.3535, grad_fn=<NegBackward0>) tensor(12302.3486, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12302.34765625
tensor(12302.3486, grad_fn=<NegBackward0>) tensor(12302.3477, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12302.3447265625
tensor(12302.3477, grad_fn=<NegBackward0>) tensor(12302.3447, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12302.3427734375
tensor(12302.3447, grad_fn=<NegBackward0>) tensor(12302.3428, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12302.3408203125
tensor(12302.3428, grad_fn=<NegBackward0>) tensor(12302.3408, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12302.3388671875
tensor(12302.3408, grad_fn=<NegBackward0>) tensor(12302.3389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12302.337890625
tensor(12302.3389, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12302.3369140625
tensor(12302.3379, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12302.3388671875
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3389, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12302.3369140625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12302.3388671875
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3389, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12302.337890625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -12302.3369140625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12302.3369140625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12302.353515625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3535, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12302.3369140625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12302.3369140625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12302.3447265625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3447, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12302.3359375
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3359, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12302.337890625
tensor(12302.3359, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12302.3583984375
tensor(12302.3359, grad_fn=<NegBackward0>) tensor(12302.3584, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12302.3369140625
tensor(12302.3359, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12302.337890625
tensor(12302.3359, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -12302.3369140625
tensor(12302.3359, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.2985, 0.7015],
        [0.3427, 0.6573]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9886, 0.0114], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1931, 0.1577],
         [0.5064, 0.2013]],

        [[0.7113, 0.1995],
         [0.6097, 0.5956]],

        [[0.6707, 0.2084],
         [0.5018, 0.5402]],

        [[0.6629, 0.1934],
         [0.5631, 0.5892]],

        [[0.5790, 0.1905],
         [0.5206, 0.5391]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014914073562566407
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21233.236328125
inf tensor(21233.2363, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12304.3642578125
tensor(21233.2363, grad_fn=<NegBackward0>) tensor(12304.3643, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12304.0947265625
tensor(12304.3643, grad_fn=<NegBackward0>) tensor(12304.0947, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12304.0263671875
tensor(12304.0947, grad_fn=<NegBackward0>) tensor(12304.0264, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12303.9716796875
tensor(12304.0264, grad_fn=<NegBackward0>) tensor(12303.9717, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12303.9140625
tensor(12303.9717, grad_fn=<NegBackward0>) tensor(12303.9141, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12303.85546875
tensor(12303.9141, grad_fn=<NegBackward0>) tensor(12303.8555, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12303.787109375
tensor(12303.8555, grad_fn=<NegBackward0>) tensor(12303.7871, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12303.70703125
tensor(12303.7871, grad_fn=<NegBackward0>) tensor(12303.7070, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12303.611328125
tensor(12303.7070, grad_fn=<NegBackward0>) tensor(12303.6113, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12303.505859375
tensor(12303.6113, grad_fn=<NegBackward0>) tensor(12303.5059, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12303.41015625
tensor(12303.5059, grad_fn=<NegBackward0>) tensor(12303.4102, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12303.3134765625
tensor(12303.4102, grad_fn=<NegBackward0>) tensor(12303.3135, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12303.21875
tensor(12303.3135, grad_fn=<NegBackward0>) tensor(12303.2188, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12303.13671875
tensor(12303.2188, grad_fn=<NegBackward0>) tensor(12303.1367, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12303.076171875
tensor(12303.1367, grad_fn=<NegBackward0>) tensor(12303.0762, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12303.0263671875
tensor(12303.0762, grad_fn=<NegBackward0>) tensor(12303.0264, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12302.982421875
tensor(12303.0264, grad_fn=<NegBackward0>) tensor(12302.9824, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12302.9326171875
tensor(12302.9824, grad_fn=<NegBackward0>) tensor(12302.9326, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12302.826171875
tensor(12302.9326, grad_fn=<NegBackward0>) tensor(12302.8262, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12302.6904296875
tensor(12302.8262, grad_fn=<NegBackward0>) tensor(12302.6904, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12302.642578125
tensor(12302.6904, grad_fn=<NegBackward0>) tensor(12302.6426, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12302.6162109375
tensor(12302.6426, grad_fn=<NegBackward0>) tensor(12302.6162, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12302.58984375
tensor(12302.6162, grad_fn=<NegBackward0>) tensor(12302.5898, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12302.560546875
tensor(12302.5898, grad_fn=<NegBackward0>) tensor(12302.5605, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12302.5263671875
tensor(12302.5605, grad_fn=<NegBackward0>) tensor(12302.5264, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12302.4921875
tensor(12302.5264, grad_fn=<NegBackward0>) tensor(12302.4922, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12302.462890625
tensor(12302.4922, grad_fn=<NegBackward0>) tensor(12302.4629, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12302.4365234375
tensor(12302.4629, grad_fn=<NegBackward0>) tensor(12302.4365, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12302.4150390625
tensor(12302.4365, grad_fn=<NegBackward0>) tensor(12302.4150, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12302.3994140625
tensor(12302.4150, grad_fn=<NegBackward0>) tensor(12302.3994, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12302.3876953125
tensor(12302.3994, grad_fn=<NegBackward0>) tensor(12302.3877, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12302.3779296875
tensor(12302.3877, grad_fn=<NegBackward0>) tensor(12302.3779, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12302.37109375
tensor(12302.3779, grad_fn=<NegBackward0>) tensor(12302.3711, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12302.3662109375
tensor(12302.3711, grad_fn=<NegBackward0>) tensor(12302.3662, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12302.3623046875
tensor(12302.3662, grad_fn=<NegBackward0>) tensor(12302.3623, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12302.3603515625
tensor(12302.3623, grad_fn=<NegBackward0>) tensor(12302.3604, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12302.3583984375
tensor(12302.3604, grad_fn=<NegBackward0>) tensor(12302.3584, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12302.35546875
tensor(12302.3584, grad_fn=<NegBackward0>) tensor(12302.3555, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12302.353515625
tensor(12302.3555, grad_fn=<NegBackward0>) tensor(12302.3535, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12302.3525390625
tensor(12302.3535, grad_fn=<NegBackward0>) tensor(12302.3525, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12302.3515625
tensor(12302.3525, grad_fn=<NegBackward0>) tensor(12302.3516, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12302.3515625
tensor(12302.3516, grad_fn=<NegBackward0>) tensor(12302.3516, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12302.3486328125
tensor(12302.3516, grad_fn=<NegBackward0>) tensor(12302.3486, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12302.349609375
tensor(12302.3486, grad_fn=<NegBackward0>) tensor(12302.3496, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12302.3466796875
tensor(12302.3486, grad_fn=<NegBackward0>) tensor(12302.3467, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12302.3447265625
tensor(12302.3467, grad_fn=<NegBackward0>) tensor(12302.3447, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12302.3427734375
tensor(12302.3447, grad_fn=<NegBackward0>) tensor(12302.3428, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12302.3427734375
tensor(12302.3428, grad_fn=<NegBackward0>) tensor(12302.3428, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12302.33984375
tensor(12302.3428, grad_fn=<NegBackward0>) tensor(12302.3398, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12302.33984375
tensor(12302.3398, grad_fn=<NegBackward0>) tensor(12302.3398, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12302.337890625
tensor(12302.3398, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12302.33984375
tensor(12302.3379, grad_fn=<NegBackward0>) tensor(12302.3398, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12302.3369140625
tensor(12302.3379, grad_fn=<NegBackward0>) tensor(12302.3369, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12302.3388671875
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3389, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12302.337890625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12302.376953125
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3770, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12302.337890625
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3379, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12302.341796875
tensor(12302.3369, grad_fn=<NegBackward0>) tensor(12302.3418, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.6565, 0.3435],
        [0.7031, 0.2969]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0118, 0.9882], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.1582],
         [0.5728, 0.1931]],

        [[0.6429, 0.1995],
         [0.6033, 0.5824]],

        [[0.6412, 0.2084],
         [0.5266, 0.6359]],

        [[0.5218, 0.1934],
         [0.6280, 0.7195]],

        [[0.6684, 0.1905],
         [0.6636, 0.7176]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014914073562566407
Average Adjusted Rand Index: 0.0
[-0.0014914073562566407, -0.0014914073562566407] [0.0, 0.0] [12302.3369140625, 12302.341796875]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11973.964821978168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21619.044921875
inf tensor(21619.0449, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12532.408203125
tensor(21619.0449, grad_fn=<NegBackward0>) tensor(12532.4082, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12531.9130859375
tensor(12532.4082, grad_fn=<NegBackward0>) tensor(12531.9131, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12531.76953125
tensor(12531.9131, grad_fn=<NegBackward0>) tensor(12531.7695, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12531.669921875
tensor(12531.7695, grad_fn=<NegBackward0>) tensor(12531.6699, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12531.5791015625
tensor(12531.6699, grad_fn=<NegBackward0>) tensor(12531.5791, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12531.4931640625
tensor(12531.5791, grad_fn=<NegBackward0>) tensor(12531.4932, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12531.41796875
tensor(12531.4932, grad_fn=<NegBackward0>) tensor(12531.4180, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12531.3525390625
tensor(12531.4180, grad_fn=<NegBackward0>) tensor(12531.3525, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12531.28515625
tensor(12531.3525, grad_fn=<NegBackward0>) tensor(12531.2852, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12531.2138671875
tensor(12531.2852, grad_fn=<NegBackward0>) tensor(12531.2139, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12531.140625
tensor(12531.2139, grad_fn=<NegBackward0>) tensor(12531.1406, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12531.0673828125
tensor(12531.1406, grad_fn=<NegBackward0>) tensor(12531.0674, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12530.9931640625
tensor(12531.0674, grad_fn=<NegBackward0>) tensor(12530.9932, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12530.93359375
tensor(12530.9932, grad_fn=<NegBackward0>) tensor(12530.9336, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12530.861328125
tensor(12530.9336, grad_fn=<NegBackward0>) tensor(12530.8613, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12530.806640625
tensor(12530.8613, grad_fn=<NegBackward0>) tensor(12530.8066, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12530.7548828125
tensor(12530.8066, grad_fn=<NegBackward0>) tensor(12530.7549, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12530.70703125
tensor(12530.7549, grad_fn=<NegBackward0>) tensor(12530.7070, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12530.6640625
tensor(12530.7070, grad_fn=<NegBackward0>) tensor(12530.6641, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12530.6201171875
tensor(12530.6641, grad_fn=<NegBackward0>) tensor(12530.6201, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12530.576171875
tensor(12530.6201, grad_fn=<NegBackward0>) tensor(12530.5762, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12530.53125
tensor(12530.5762, grad_fn=<NegBackward0>) tensor(12530.5312, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12530.486328125
tensor(12530.5312, grad_fn=<NegBackward0>) tensor(12530.4863, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12530.4375
tensor(12530.4863, grad_fn=<NegBackward0>) tensor(12530.4375, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12530.3857421875
tensor(12530.4375, grad_fn=<NegBackward0>) tensor(12530.3857, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12530.3310546875
tensor(12530.3857, grad_fn=<NegBackward0>) tensor(12530.3311, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12530.27734375
tensor(12530.3311, grad_fn=<NegBackward0>) tensor(12530.2773, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12530.2255859375
tensor(12530.2773, grad_fn=<NegBackward0>) tensor(12530.2256, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12530.1748046875
tensor(12530.2256, grad_fn=<NegBackward0>) tensor(12530.1748, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12530.1298828125
tensor(12530.1748, grad_fn=<NegBackward0>) tensor(12530.1299, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12530.0908203125
tensor(12530.1299, grad_fn=<NegBackward0>) tensor(12530.0908, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12530.05859375
tensor(12530.0908, grad_fn=<NegBackward0>) tensor(12530.0586, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12530.0283203125
tensor(12530.0586, grad_fn=<NegBackward0>) tensor(12530.0283, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12530.0048828125
tensor(12530.0283, grad_fn=<NegBackward0>) tensor(12530.0049, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12529.986328125
tensor(12530.0049, grad_fn=<NegBackward0>) tensor(12529.9863, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12529.9697265625
tensor(12529.9863, grad_fn=<NegBackward0>) tensor(12529.9697, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12529.953125
tensor(12529.9697, grad_fn=<NegBackward0>) tensor(12529.9531, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12529.939453125
tensor(12529.9531, grad_fn=<NegBackward0>) tensor(12529.9395, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12529.92578125
tensor(12529.9395, grad_fn=<NegBackward0>) tensor(12529.9258, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12529.9130859375
tensor(12529.9258, grad_fn=<NegBackward0>) tensor(12529.9131, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12529.900390625
tensor(12529.9131, grad_fn=<NegBackward0>) tensor(12529.9004, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12529.88671875
tensor(12529.9004, grad_fn=<NegBackward0>) tensor(12529.8867, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12529.875
tensor(12529.8867, grad_fn=<NegBackward0>) tensor(12529.8750, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12529.8662109375
tensor(12529.8750, grad_fn=<NegBackward0>) tensor(12529.8662, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12529.8564453125
tensor(12529.8662, grad_fn=<NegBackward0>) tensor(12529.8564, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12529.849609375
tensor(12529.8564, grad_fn=<NegBackward0>) tensor(12529.8496, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12529.8447265625
tensor(12529.8496, grad_fn=<NegBackward0>) tensor(12529.8447, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12529.8408203125
tensor(12529.8447, grad_fn=<NegBackward0>) tensor(12529.8408, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12529.8388671875
tensor(12529.8408, grad_fn=<NegBackward0>) tensor(12529.8389, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12529.8369140625
tensor(12529.8389, grad_fn=<NegBackward0>) tensor(12529.8369, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12529.8349609375
tensor(12529.8369, grad_fn=<NegBackward0>) tensor(12529.8350, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12529.8330078125
tensor(12529.8350, grad_fn=<NegBackward0>) tensor(12529.8330, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12529.8310546875
tensor(12529.8330, grad_fn=<NegBackward0>) tensor(12529.8311, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12529.830078125
tensor(12529.8311, grad_fn=<NegBackward0>) tensor(12529.8301, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12529.8291015625
tensor(12529.8301, grad_fn=<NegBackward0>) tensor(12529.8291, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12529.8271484375
tensor(12529.8291, grad_fn=<NegBackward0>) tensor(12529.8271, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12529.8251953125
tensor(12529.8271, grad_fn=<NegBackward0>) tensor(12529.8252, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12529.8212890625
tensor(12529.8252, grad_fn=<NegBackward0>) tensor(12529.8213, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12529.8203125
tensor(12529.8213, grad_fn=<NegBackward0>) tensor(12529.8203, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12529.8173828125
tensor(12529.8203, grad_fn=<NegBackward0>) tensor(12529.8174, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12529.81640625
tensor(12529.8174, grad_fn=<NegBackward0>) tensor(12529.8164, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12529.814453125
tensor(12529.8164, grad_fn=<NegBackward0>) tensor(12529.8145, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12529.8115234375
tensor(12529.8145, grad_fn=<NegBackward0>) tensor(12529.8115, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12529.8115234375
tensor(12529.8115, grad_fn=<NegBackward0>) tensor(12529.8115, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12529.8115234375
tensor(12529.8115, grad_fn=<NegBackward0>) tensor(12529.8115, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12529.80859375
tensor(12529.8115, grad_fn=<NegBackward0>) tensor(12529.8086, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12529.81640625
tensor(12529.8086, grad_fn=<NegBackward0>) tensor(12529.8164, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12529.814453125
tensor(12529.8086, grad_fn=<NegBackward0>) tensor(12529.8145, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12529.8046875
tensor(12529.8086, grad_fn=<NegBackward0>) tensor(12529.8047, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12529.8076171875
tensor(12529.8047, grad_fn=<NegBackward0>) tensor(12529.8076, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12529.869140625
tensor(12529.8047, grad_fn=<NegBackward0>) tensor(12529.8691, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12529.8017578125
tensor(12529.8047, grad_fn=<NegBackward0>) tensor(12529.8018, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12529.8017578125
tensor(12529.8018, grad_fn=<NegBackward0>) tensor(12529.8018, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12529.8330078125
tensor(12529.8018, grad_fn=<NegBackward0>) tensor(12529.8330, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12529.80078125
tensor(12529.8018, grad_fn=<NegBackward0>) tensor(12529.8008, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12529.8076171875
tensor(12529.8008, grad_fn=<NegBackward0>) tensor(12529.8076, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12529.798828125
tensor(12529.8008, grad_fn=<NegBackward0>) tensor(12529.7988, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12529.798828125
tensor(12529.7988, grad_fn=<NegBackward0>) tensor(12529.7988, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12529.8544921875
tensor(12529.7988, grad_fn=<NegBackward0>) tensor(12529.8545, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12529.798828125
tensor(12529.7988, grad_fn=<NegBackward0>) tensor(12529.7988, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12529.7978515625
tensor(12529.7988, grad_fn=<NegBackward0>) tensor(12529.7979, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12529.9501953125
tensor(12529.7979, grad_fn=<NegBackward0>) tensor(12529.9502, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12529.7978515625
tensor(12529.7979, grad_fn=<NegBackward0>) tensor(12529.7979, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12529.8076171875
tensor(12529.7979, grad_fn=<NegBackward0>) tensor(12529.8076, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12529.796875
tensor(12529.7979, grad_fn=<NegBackward0>) tensor(12529.7969, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12529.8017578125
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.8018, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12529.7958984375
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.7959, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12529.8046875
tensor(12529.7959, grad_fn=<NegBackward0>) tensor(12529.8047, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12529.796875
tensor(12529.7959, grad_fn=<NegBackward0>) tensor(12529.7969, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -12529.7998046875
tensor(12529.7959, grad_fn=<NegBackward0>) tensor(12529.7998, grad_fn=<NegBackward0>)
3
Iteration 9100: Loss = -12529.796875
tensor(12529.7959, grad_fn=<NegBackward0>) tensor(12529.7969, grad_fn=<NegBackward0>)
4
Iteration 9200: Loss = -12529.7998046875
tensor(12529.7959, grad_fn=<NegBackward0>) tensor(12529.7998, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.0948, 0.9052],
        [0.9962, 0.0038]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9301, 0.0699], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2090, 0.2152],
         [0.6243, 0.1971]],

        [[0.6030, 0.2007],
         [0.5045, 0.7056]],

        [[0.6548, 0.1880],
         [0.5432, 0.6973]],

        [[0.5949, 0.2053],
         [0.5523, 0.6599]],

        [[0.6958, 0.2127],
         [0.6207, 0.5742]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009040259804344876
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21603.6328125
inf tensor(21603.6328, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12532.2421875
tensor(21603.6328, grad_fn=<NegBackward0>) tensor(12532.2422, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12531.7998046875
tensor(12532.2422, grad_fn=<NegBackward0>) tensor(12531.7998, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12531.646484375
tensor(12531.7998, grad_fn=<NegBackward0>) tensor(12531.6465, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12531.4736328125
tensor(12531.6465, grad_fn=<NegBackward0>) tensor(12531.4736, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12531.330078125
tensor(12531.4736, grad_fn=<NegBackward0>) tensor(12531.3301, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12531.220703125
tensor(12531.3301, grad_fn=<NegBackward0>) tensor(12531.2207, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12531.12109375
tensor(12531.2207, grad_fn=<NegBackward0>) tensor(12531.1211, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12531.0361328125
tensor(12531.1211, grad_fn=<NegBackward0>) tensor(12531.0361, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12530.9638671875
tensor(12531.0361, grad_fn=<NegBackward0>) tensor(12530.9639, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12530.9033203125
tensor(12530.9639, grad_fn=<NegBackward0>) tensor(12530.9033, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12530.8505859375
tensor(12530.9033, grad_fn=<NegBackward0>) tensor(12530.8506, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12530.8056640625
tensor(12530.8506, grad_fn=<NegBackward0>) tensor(12530.8057, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12530.7666015625
tensor(12530.8057, grad_fn=<NegBackward0>) tensor(12530.7666, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12530.73046875
tensor(12530.7666, grad_fn=<NegBackward0>) tensor(12530.7305, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12530.6923828125
tensor(12530.7305, grad_fn=<NegBackward0>) tensor(12530.6924, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12530.6533203125
tensor(12530.6924, grad_fn=<NegBackward0>) tensor(12530.6533, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12530.61328125
tensor(12530.6533, grad_fn=<NegBackward0>) tensor(12530.6133, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12530.56640625
tensor(12530.6133, grad_fn=<NegBackward0>) tensor(12530.5664, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12530.51953125
tensor(12530.5664, grad_fn=<NegBackward0>) tensor(12530.5195, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12530.4677734375
tensor(12530.5195, grad_fn=<NegBackward0>) tensor(12530.4678, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12530.4130859375
tensor(12530.4678, grad_fn=<NegBackward0>) tensor(12530.4131, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12530.3564453125
tensor(12530.4131, grad_fn=<NegBackward0>) tensor(12530.3564, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12530.294921875
tensor(12530.3564, grad_fn=<NegBackward0>) tensor(12530.2949, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12530.2373046875
tensor(12530.2949, grad_fn=<NegBackward0>) tensor(12530.2373, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12530.1796875
tensor(12530.2373, grad_fn=<NegBackward0>) tensor(12530.1797, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12530.12890625
tensor(12530.1797, grad_fn=<NegBackward0>) tensor(12530.1289, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12530.083984375
tensor(12530.1289, grad_fn=<NegBackward0>) tensor(12530.0840, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12530.046875
tensor(12530.0840, grad_fn=<NegBackward0>) tensor(12530.0469, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12530.017578125
tensor(12530.0469, grad_fn=<NegBackward0>) tensor(12530.0176, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12529.994140625
tensor(12530.0176, grad_fn=<NegBackward0>) tensor(12529.9941, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12529.97265625
tensor(12529.9941, grad_fn=<NegBackward0>) tensor(12529.9727, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12529.9560546875
tensor(12529.9727, grad_fn=<NegBackward0>) tensor(12529.9561, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12529.94140625
tensor(12529.9561, grad_fn=<NegBackward0>) tensor(12529.9414, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12529.9267578125
tensor(12529.9414, grad_fn=<NegBackward0>) tensor(12529.9268, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12529.9150390625
tensor(12529.9268, grad_fn=<NegBackward0>) tensor(12529.9150, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12529.900390625
tensor(12529.9150, grad_fn=<NegBackward0>) tensor(12529.9004, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12529.8896484375
tensor(12529.9004, grad_fn=<NegBackward0>) tensor(12529.8896, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12529.8759765625
tensor(12529.8896, grad_fn=<NegBackward0>) tensor(12529.8760, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12529.8662109375
tensor(12529.8760, grad_fn=<NegBackward0>) tensor(12529.8662, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12529.859375
tensor(12529.8662, grad_fn=<NegBackward0>) tensor(12529.8594, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12529.8515625
tensor(12529.8594, grad_fn=<NegBackward0>) tensor(12529.8516, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12529.8486328125
tensor(12529.8516, grad_fn=<NegBackward0>) tensor(12529.8486, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12529.8447265625
tensor(12529.8486, grad_fn=<NegBackward0>) tensor(12529.8447, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12529.8408203125
tensor(12529.8447, grad_fn=<NegBackward0>) tensor(12529.8408, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12529.8388671875
tensor(12529.8408, grad_fn=<NegBackward0>) tensor(12529.8389, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12529.8369140625
tensor(12529.8389, grad_fn=<NegBackward0>) tensor(12529.8369, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12529.833984375
tensor(12529.8369, grad_fn=<NegBackward0>) tensor(12529.8340, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12529.8330078125
tensor(12529.8340, grad_fn=<NegBackward0>) tensor(12529.8330, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12529.8310546875
tensor(12529.8330, grad_fn=<NegBackward0>) tensor(12529.8311, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12529.828125
tensor(12529.8311, grad_fn=<NegBackward0>) tensor(12529.8281, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12529.826171875
tensor(12529.8281, grad_fn=<NegBackward0>) tensor(12529.8262, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12529.8232421875
tensor(12529.8262, grad_fn=<NegBackward0>) tensor(12529.8232, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12529.822265625
tensor(12529.8232, grad_fn=<NegBackward0>) tensor(12529.8223, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12529.8203125
tensor(12529.8223, grad_fn=<NegBackward0>) tensor(12529.8203, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12529.8173828125
tensor(12529.8203, grad_fn=<NegBackward0>) tensor(12529.8174, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12529.8154296875
tensor(12529.8174, grad_fn=<NegBackward0>) tensor(12529.8154, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12529.814453125
tensor(12529.8154, grad_fn=<NegBackward0>) tensor(12529.8145, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12529.8134765625
tensor(12529.8145, grad_fn=<NegBackward0>) tensor(12529.8135, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12529.810546875
tensor(12529.8135, grad_fn=<NegBackward0>) tensor(12529.8105, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12529.8095703125
tensor(12529.8105, grad_fn=<NegBackward0>) tensor(12529.8096, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12529.8076171875
tensor(12529.8096, grad_fn=<NegBackward0>) tensor(12529.8076, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12529.8056640625
tensor(12529.8076, grad_fn=<NegBackward0>) tensor(12529.8057, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12529.8056640625
tensor(12529.8057, grad_fn=<NegBackward0>) tensor(12529.8057, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12529.8046875
tensor(12529.8057, grad_fn=<NegBackward0>) tensor(12529.8047, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12529.802734375
tensor(12529.8047, grad_fn=<NegBackward0>) tensor(12529.8027, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12529.802734375
tensor(12529.8027, grad_fn=<NegBackward0>) tensor(12529.8027, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12529.8046875
tensor(12529.8027, grad_fn=<NegBackward0>) tensor(12529.8047, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12529.810546875
tensor(12529.8027, grad_fn=<NegBackward0>) tensor(12529.8105, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12529.8037109375
tensor(12529.8027, grad_fn=<NegBackward0>) tensor(12529.8037, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12529.7998046875
tensor(12529.8027, grad_fn=<NegBackward0>) tensor(12529.7998, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12529.7998046875
tensor(12529.7998, grad_fn=<NegBackward0>) tensor(12529.7998, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12529.7998046875
tensor(12529.7998, grad_fn=<NegBackward0>) tensor(12529.7998, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12529.802734375
tensor(12529.7998, grad_fn=<NegBackward0>) tensor(12529.8027, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12529.7978515625
tensor(12529.7998, grad_fn=<NegBackward0>) tensor(12529.7979, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12529.796875
tensor(12529.7979, grad_fn=<NegBackward0>) tensor(12529.7969, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12529.876953125
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.8770, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12529.7978515625
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.7979, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12529.8232421875
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.8232, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -12529.7978515625
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.7979, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -12529.8056640625
tensor(12529.7969, grad_fn=<NegBackward0>) tensor(12529.8057, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.0061, 0.9939],
        [0.9019, 0.0981]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0690, 0.9310], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1972, 0.2152],
         [0.5093, 0.2083]],

        [[0.7068, 0.2005],
         [0.5717, 0.6607]],

        [[0.5435, 0.1883],
         [0.7184, 0.7090]],

        [[0.6381, 0.2051],
         [0.6797, 0.5227]],

        [[0.5947, 0.2127],
         [0.6718, 0.6861]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009040259804344876
Average Adjusted Rand Index: 0.0
[-0.0009040259804344876, -0.0009040259804344876] [0.0, 0.0] [12529.7998046875, 12529.8056640625]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11935.519345287514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23675.2109375
inf tensor(23675.2109, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12462.6337890625
tensor(23675.2109, grad_fn=<NegBackward0>) tensor(12462.6338, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12462.115234375
tensor(12462.6338, grad_fn=<NegBackward0>) tensor(12462.1152, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12461.955078125
tensor(12462.1152, grad_fn=<NegBackward0>) tensor(12461.9551, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12461.7763671875
tensor(12461.9551, grad_fn=<NegBackward0>) tensor(12461.7764, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12461.6396484375
tensor(12461.7764, grad_fn=<NegBackward0>) tensor(12461.6396, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12461.572265625
tensor(12461.6396, grad_fn=<NegBackward0>) tensor(12461.5723, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12461.5302734375
tensor(12461.5723, grad_fn=<NegBackward0>) tensor(12461.5303, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12461.494140625
tensor(12461.5303, grad_fn=<NegBackward0>) tensor(12461.4941, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12461.4375
tensor(12461.4941, grad_fn=<NegBackward0>) tensor(12461.4375, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12461.3671875
tensor(12461.4375, grad_fn=<NegBackward0>) tensor(12461.3672, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12461.294921875
tensor(12461.3672, grad_fn=<NegBackward0>) tensor(12461.2949, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12461.2119140625
tensor(12461.2949, grad_fn=<NegBackward0>) tensor(12461.2119, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12461.1435546875
tensor(12461.2119, grad_fn=<NegBackward0>) tensor(12461.1436, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12461.0966796875
tensor(12461.1436, grad_fn=<NegBackward0>) tensor(12461.0967, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12461.068359375
tensor(12461.0967, grad_fn=<NegBackward0>) tensor(12461.0684, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12461.048828125
tensor(12461.0684, grad_fn=<NegBackward0>) tensor(12461.0488, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12461.0322265625
tensor(12461.0488, grad_fn=<NegBackward0>) tensor(12461.0322, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12461.0244140625
tensor(12461.0322, grad_fn=<NegBackward0>) tensor(12461.0244, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12461.0146484375
tensor(12461.0244, grad_fn=<NegBackward0>) tensor(12461.0146, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12461.0078125
tensor(12461.0146, grad_fn=<NegBackward0>) tensor(12461.0078, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12461.0029296875
tensor(12461.0078, grad_fn=<NegBackward0>) tensor(12461.0029, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12460.9990234375
tensor(12461.0029, grad_fn=<NegBackward0>) tensor(12460.9990, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12460.99609375
tensor(12460.9990, grad_fn=<NegBackward0>) tensor(12460.9961, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12460.9912109375
tensor(12460.9961, grad_fn=<NegBackward0>) tensor(12460.9912, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12460.990234375
tensor(12460.9912, grad_fn=<NegBackward0>) tensor(12460.9902, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12460.9873046875
tensor(12460.9902, grad_fn=<NegBackward0>) tensor(12460.9873, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12460.984375
tensor(12460.9873, grad_fn=<NegBackward0>) tensor(12460.9844, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12460.9833984375
tensor(12460.9844, grad_fn=<NegBackward0>) tensor(12460.9834, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12460.98046875
tensor(12460.9834, grad_fn=<NegBackward0>) tensor(12460.9805, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12460.978515625
tensor(12460.9805, grad_fn=<NegBackward0>) tensor(12460.9785, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12460.978515625
tensor(12460.9785, grad_fn=<NegBackward0>) tensor(12460.9785, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12460.9775390625
tensor(12460.9785, grad_fn=<NegBackward0>) tensor(12460.9775, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12460.9775390625
tensor(12460.9775, grad_fn=<NegBackward0>) tensor(12460.9775, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12460.9775390625
tensor(12460.9775, grad_fn=<NegBackward0>) tensor(12460.9775, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12460.9765625
tensor(12460.9775, grad_fn=<NegBackward0>) tensor(12460.9766, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12460.9755859375
tensor(12460.9766, grad_fn=<NegBackward0>) tensor(12460.9756, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12460.974609375
tensor(12460.9756, grad_fn=<NegBackward0>) tensor(12460.9746, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12460.974609375
tensor(12460.9746, grad_fn=<NegBackward0>) tensor(12460.9746, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12460.9755859375
tensor(12460.9746, grad_fn=<NegBackward0>) tensor(12460.9756, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12460.97265625
tensor(12460.9746, grad_fn=<NegBackward0>) tensor(12460.9727, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12460.9716796875
tensor(12460.9727, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12460.9736328125
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9736, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12460.9716796875
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12460.974609375
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9746, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12460.9716796875
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12460.9716796875
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12460.9697265625
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12460.970703125
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12460.9697265625
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12460.970703125
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12460.970703125
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12460.9716796875
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -12460.96875
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12460.96875
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12460.970703125
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12460.970703125
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12460.9697265625
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12460.96875
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12460.9697265625
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12460.9677734375
tensor(12460.9688, grad_fn=<NegBackward0>) tensor(12460.9678, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12460.9677734375
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9678, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12460.96875
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12460.96875
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12460.96875
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12460.96875
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -12460.9697265625
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[9.4896e-01, 5.1042e-02],
        [9.9902e-01, 9.7546e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9815, 0.0185], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.3044],
         [0.7186, 0.2512]],

        [[0.6931, 0.2308],
         [0.7104, 0.5203]],

        [[0.5768, 0.2596],
         [0.5391, 0.6167]],

        [[0.7212, 0.2114],
         [0.5969, 0.6727]],

        [[0.6750, 0.1910],
         [0.5313, 0.6761]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00044770900403200164
Average Adjusted Rand Index: -5.4546790352242226e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18519.166015625
inf tensor(18519.1660, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12462.2939453125
tensor(18519.1660, grad_fn=<NegBackward0>) tensor(12462.2939, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12462.009765625
tensor(12462.2939, grad_fn=<NegBackward0>) tensor(12462.0098, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12461.9091796875
tensor(12462.0098, grad_fn=<NegBackward0>) tensor(12461.9092, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12461.8017578125
tensor(12461.9092, grad_fn=<NegBackward0>) tensor(12461.8018, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12461.6591796875
tensor(12461.8018, grad_fn=<NegBackward0>) tensor(12461.6592, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12461.5517578125
tensor(12461.6592, grad_fn=<NegBackward0>) tensor(12461.5518, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12461.51171875
tensor(12461.5518, grad_fn=<NegBackward0>) tensor(12461.5117, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12461.4912109375
tensor(12461.5117, grad_fn=<NegBackward0>) tensor(12461.4912, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12461.478515625
tensor(12461.4912, grad_fn=<NegBackward0>) tensor(12461.4785, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12461.462890625
tensor(12461.4785, grad_fn=<NegBackward0>) tensor(12461.4629, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12461.447265625
tensor(12461.4629, grad_fn=<NegBackward0>) tensor(12461.4473, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12461.4296875
tensor(12461.4473, grad_fn=<NegBackward0>) tensor(12461.4297, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12461.404296875
tensor(12461.4297, grad_fn=<NegBackward0>) tensor(12461.4043, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12461.33984375
tensor(12461.4043, grad_fn=<NegBackward0>) tensor(12461.3398, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12461.20703125
tensor(12461.3398, grad_fn=<NegBackward0>) tensor(12461.2070, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12461.095703125
tensor(12461.2070, grad_fn=<NegBackward0>) tensor(12461.0957, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12461.0478515625
tensor(12461.0957, grad_fn=<NegBackward0>) tensor(12461.0479, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12461.025390625
tensor(12461.0479, grad_fn=<NegBackward0>) tensor(12461.0254, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12461.0126953125
tensor(12461.0254, grad_fn=<NegBackward0>) tensor(12461.0127, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12461.005859375
tensor(12461.0127, grad_fn=<NegBackward0>) tensor(12461.0059, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12461.001953125
tensor(12461.0059, grad_fn=<NegBackward0>) tensor(12461.0020, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12460.99609375
tensor(12461.0020, grad_fn=<NegBackward0>) tensor(12460.9961, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12460.9931640625
tensor(12460.9961, grad_fn=<NegBackward0>) tensor(12460.9932, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12460.990234375
tensor(12460.9932, grad_fn=<NegBackward0>) tensor(12460.9902, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12460.98828125
tensor(12460.9902, grad_fn=<NegBackward0>) tensor(12460.9883, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12460.986328125
tensor(12460.9883, grad_fn=<NegBackward0>) tensor(12460.9863, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12460.9853515625
tensor(12460.9863, grad_fn=<NegBackward0>) tensor(12460.9854, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12460.984375
tensor(12460.9854, grad_fn=<NegBackward0>) tensor(12460.9844, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12460.9814453125
tensor(12460.9844, grad_fn=<NegBackward0>) tensor(12460.9814, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12460.9794921875
tensor(12460.9814, grad_fn=<NegBackward0>) tensor(12460.9795, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12460.9794921875
tensor(12460.9795, grad_fn=<NegBackward0>) tensor(12460.9795, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12460.978515625
tensor(12460.9795, grad_fn=<NegBackward0>) tensor(12460.9785, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12460.978515625
tensor(12460.9785, grad_fn=<NegBackward0>) tensor(12460.9785, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12460.978515625
tensor(12460.9785, grad_fn=<NegBackward0>) tensor(12460.9785, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12460.9765625
tensor(12460.9785, grad_fn=<NegBackward0>) tensor(12460.9766, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12460.9755859375
tensor(12460.9766, grad_fn=<NegBackward0>) tensor(12460.9756, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12460.9736328125
tensor(12460.9756, grad_fn=<NegBackward0>) tensor(12460.9736, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12460.974609375
tensor(12460.9736, grad_fn=<NegBackward0>) tensor(12460.9746, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12460.974609375
tensor(12460.9736, grad_fn=<NegBackward0>) tensor(12460.9746, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -12460.9736328125
tensor(12460.9736, grad_fn=<NegBackward0>) tensor(12460.9736, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12460.97265625
tensor(12460.9736, grad_fn=<NegBackward0>) tensor(12460.9727, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12460.9736328125
tensor(12460.9727, grad_fn=<NegBackward0>) tensor(12460.9736, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12460.97265625
tensor(12460.9727, grad_fn=<NegBackward0>) tensor(12460.9727, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12460.97265625
tensor(12460.9727, grad_fn=<NegBackward0>) tensor(12460.9727, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12460.97265625
tensor(12460.9727, grad_fn=<NegBackward0>) tensor(12460.9727, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12460.9716796875
tensor(12460.9727, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12460.970703125
tensor(12460.9717, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12460.9716796875
tensor(12460.9707, grad_fn=<NegBackward0>) tensor(12460.9717, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12460.9697265625
tensor(12460.9707, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12460.970703125
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12460.970703125
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12460.9697265625
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12460.9697265625
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12460.9697265625
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12460.9677734375
tensor(12460.9697, grad_fn=<NegBackward0>) tensor(12460.9678, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12460.9697265625
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12460.970703125
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9707, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12460.96875
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9688, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12460.9697265625
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -12460.9697265625
tensor(12460.9678, grad_fn=<NegBackward0>) tensor(12460.9697, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.9490, 0.0510],
        [0.9985, 0.0015]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9815, 0.0185], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.3044],
         [0.5931, 0.2511]],

        [[0.5415, 0.2308],
         [0.6000, 0.6832]],

        [[0.5387, 0.2596],
         [0.5134, 0.5354]],

        [[0.6012, 0.2113],
         [0.6849, 0.5414]],

        [[0.5832, 0.1910],
         [0.5199, 0.6598]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0012662455124815629
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00044770900403200164
Average Adjusted Rand Index: -5.4546790352242226e-05
[-0.00044770900403200164, -0.00044770900403200164] [-5.4546790352242226e-05, -5.4546790352242226e-05] [12460.9697265625, 12460.9697265625]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11947.625729028518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21008.798828125
inf tensor(21008.7988, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12496.806640625
tensor(21008.7988, grad_fn=<NegBackward0>) tensor(12496.8066, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12496.5126953125
tensor(12496.8066, grad_fn=<NegBackward0>) tensor(12496.5127, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12496.474609375
tensor(12496.5127, grad_fn=<NegBackward0>) tensor(12496.4746, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12496.4501953125
tensor(12496.4746, grad_fn=<NegBackward0>) tensor(12496.4502, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12496.4287109375
tensor(12496.4502, grad_fn=<NegBackward0>) tensor(12496.4287, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12496.41015625
tensor(12496.4287, grad_fn=<NegBackward0>) tensor(12496.4102, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12496.388671875
tensor(12496.4102, grad_fn=<NegBackward0>) tensor(12496.3887, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12496.3486328125
tensor(12496.3887, grad_fn=<NegBackward0>) tensor(12496.3486, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12496.2451171875
tensor(12496.3486, grad_fn=<NegBackward0>) tensor(12496.2451, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12495.4794921875
tensor(12496.2451, grad_fn=<NegBackward0>) tensor(12495.4795, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12493.8251953125
tensor(12495.4795, grad_fn=<NegBackward0>) tensor(12493.8252, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12493.5302734375
tensor(12493.8252, grad_fn=<NegBackward0>) tensor(12493.5303, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12493.412109375
tensor(12493.5303, grad_fn=<NegBackward0>) tensor(12493.4121, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12493.34765625
tensor(12493.4121, grad_fn=<NegBackward0>) tensor(12493.3477, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12493.3095703125
tensor(12493.3477, grad_fn=<NegBackward0>) tensor(12493.3096, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12493.2841796875
tensor(12493.3096, grad_fn=<NegBackward0>) tensor(12493.2842, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12493.2666015625
tensor(12493.2842, grad_fn=<NegBackward0>) tensor(12493.2666, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12493.251953125
tensor(12493.2666, grad_fn=<NegBackward0>) tensor(12493.2520, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12493.2421875
tensor(12493.2520, grad_fn=<NegBackward0>) tensor(12493.2422, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12493.2265625
tensor(12493.2422, grad_fn=<NegBackward0>) tensor(12493.2266, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12493.2099609375
tensor(12493.2266, grad_fn=<NegBackward0>) tensor(12493.2100, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12493.19140625
tensor(12493.2100, grad_fn=<NegBackward0>) tensor(12493.1914, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12493.1787109375
tensor(12493.1914, grad_fn=<NegBackward0>) tensor(12493.1787, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12493.1669921875
tensor(12493.1787, grad_fn=<NegBackward0>) tensor(12493.1670, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12493.158203125
tensor(12493.1670, grad_fn=<NegBackward0>) tensor(12493.1582, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12493.1484375
tensor(12493.1582, grad_fn=<NegBackward0>) tensor(12493.1484, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12493.1357421875
tensor(12493.1484, grad_fn=<NegBackward0>) tensor(12493.1357, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12493.1123046875
tensor(12493.1357, grad_fn=<NegBackward0>) tensor(12493.1123, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12493.0087890625
tensor(12493.1123, grad_fn=<NegBackward0>) tensor(12493.0088, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12491.2666015625
tensor(12493.0088, grad_fn=<NegBackward0>) tensor(12491.2666, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12490.134765625
tensor(12491.2666, grad_fn=<NegBackward0>) tensor(12490.1348, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12489.8583984375
tensor(12490.1348, grad_fn=<NegBackward0>) tensor(12489.8584, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12489.75
tensor(12489.8584, grad_fn=<NegBackward0>) tensor(12489.7500, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12489.69921875
tensor(12489.7500, grad_fn=<NegBackward0>) tensor(12489.6992, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12489.6689453125
tensor(12489.6992, grad_fn=<NegBackward0>) tensor(12489.6689, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12489.65234375
tensor(12489.6689, grad_fn=<NegBackward0>) tensor(12489.6523, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12489.6416015625
tensor(12489.6523, grad_fn=<NegBackward0>) tensor(12489.6416, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12489.6318359375
tensor(12489.6416, grad_fn=<NegBackward0>) tensor(12489.6318, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12489.62890625
tensor(12489.6318, grad_fn=<NegBackward0>) tensor(12489.6289, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12489.6259765625
tensor(12489.6289, grad_fn=<NegBackward0>) tensor(12489.6260, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12489.6201171875
tensor(12489.6260, grad_fn=<NegBackward0>) tensor(12489.6201, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12489.6181640625
tensor(12489.6201, grad_fn=<NegBackward0>) tensor(12489.6182, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12489.6162109375
tensor(12489.6182, grad_fn=<NegBackward0>) tensor(12489.6162, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12489.6142578125
tensor(12489.6162, grad_fn=<NegBackward0>) tensor(12489.6143, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12489.61328125
tensor(12489.6143, grad_fn=<NegBackward0>) tensor(12489.6133, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12489.611328125
tensor(12489.6133, grad_fn=<NegBackward0>) tensor(12489.6113, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12489.6103515625
tensor(12489.6113, grad_fn=<NegBackward0>) tensor(12489.6104, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12489.609375
tensor(12489.6104, grad_fn=<NegBackward0>) tensor(12489.6094, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12489.607421875
tensor(12489.6094, grad_fn=<NegBackward0>) tensor(12489.6074, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12489.607421875
tensor(12489.6074, grad_fn=<NegBackward0>) tensor(12489.6074, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12489.607421875
tensor(12489.6074, grad_fn=<NegBackward0>) tensor(12489.6074, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12489.6064453125
tensor(12489.6074, grad_fn=<NegBackward0>) tensor(12489.6064, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12489.60546875
tensor(12489.6064, grad_fn=<NegBackward0>) tensor(12489.6055, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12489.6044921875
tensor(12489.6055, grad_fn=<NegBackward0>) tensor(12489.6045, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12489.603515625
tensor(12489.6045, grad_fn=<NegBackward0>) tensor(12489.6035, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12489.6025390625
tensor(12489.6035, grad_fn=<NegBackward0>) tensor(12489.6025, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12489.6015625
tensor(12489.6025, grad_fn=<NegBackward0>) tensor(12489.6016, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12489.6025390625
tensor(12489.6016, grad_fn=<NegBackward0>) tensor(12489.6025, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12489.6015625
tensor(12489.6016, grad_fn=<NegBackward0>) tensor(12489.6016, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12489.6005859375
tensor(12489.6016, grad_fn=<NegBackward0>) tensor(12489.6006, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12489.6005859375
tensor(12489.6006, grad_fn=<NegBackward0>) tensor(12489.6006, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12489.6015625
tensor(12489.6006, grad_fn=<NegBackward0>) tensor(12489.6016, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12489.6005859375
tensor(12489.6006, grad_fn=<NegBackward0>) tensor(12489.6006, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12489.599609375
tensor(12489.6006, grad_fn=<NegBackward0>) tensor(12489.5996, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12489.599609375
tensor(12489.5996, grad_fn=<NegBackward0>) tensor(12489.5996, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12489.5986328125
tensor(12489.5996, grad_fn=<NegBackward0>) tensor(12489.5986, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12489.5986328125
tensor(12489.5986, grad_fn=<NegBackward0>) tensor(12489.5986, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12489.5986328125
tensor(12489.5986, grad_fn=<NegBackward0>) tensor(12489.5986, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12489.5986328125
tensor(12489.5986, grad_fn=<NegBackward0>) tensor(12489.5986, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12489.5986328125
tensor(12489.5986, grad_fn=<NegBackward0>) tensor(12489.5986, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12489.822265625
tensor(12489.5986, grad_fn=<NegBackward0>) tensor(12489.8223, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12489.59765625
tensor(12489.5986, grad_fn=<NegBackward0>) tensor(12489.5977, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12489.6064453125
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.6064, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12489.59765625
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.5977, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12489.6015625
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.6016, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12489.5966796875
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.5967, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12489.630859375
tensor(12489.5967, grad_fn=<NegBackward0>) tensor(12489.6309, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12489.59375
tensor(12489.5967, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12489.625
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.6250, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12489.59375
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12489.59375
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12489.6103515625
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.6104, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12489.5927734375
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12489.5927734375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12489.6298828125
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.6299, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12489.59375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12489.59375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12489.8955078125
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.8955, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -12489.5927734375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12489.5927734375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12489.595703125
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5957, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12489.59375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -12489.5947265625
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5947, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -12489.611328125
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.6113, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -12489.59375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9500 due to no improvement.
pi: tensor([[9.9979e-01, 2.0784e-04],
        [1.0465e-04, 9.9990e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9679, 0.0321], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2061, 0.1751],
         [0.6332, 0.0782]],

        [[0.5353, 0.0943],
         [0.5986, 0.6941]],

        [[0.6128, 0.1672],
         [0.6696, 0.7105]],

        [[0.6395, 0.2141],
         [0.5600, 0.7196]],

        [[0.5680, 0.1627],
         [0.6904, 0.5153]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: 9.826492412255229e-05
Average Adjusted Rand Index: -0.0018905175248184046
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22779.328125
inf tensor(22779.3281, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12495.8583984375
tensor(22779.3281, grad_fn=<NegBackward0>) tensor(12495.8584, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12494.23046875
tensor(12495.8584, grad_fn=<NegBackward0>) tensor(12494.2305, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12493.677734375
tensor(12494.2305, grad_fn=<NegBackward0>) tensor(12493.6777, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12493.451171875
tensor(12493.6777, grad_fn=<NegBackward0>) tensor(12493.4512, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12493.3623046875
tensor(12493.4512, grad_fn=<NegBackward0>) tensor(12493.3623, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12493.3203125
tensor(12493.3623, grad_fn=<NegBackward0>) tensor(12493.3203, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12493.296875
tensor(12493.3203, grad_fn=<NegBackward0>) tensor(12493.2969, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12493.2802734375
tensor(12493.2969, grad_fn=<NegBackward0>) tensor(12493.2803, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12493.26171875
tensor(12493.2803, grad_fn=<NegBackward0>) tensor(12493.2617, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12493.2294921875
tensor(12493.2617, grad_fn=<NegBackward0>) tensor(12493.2295, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12493.0751953125
tensor(12493.2295, grad_fn=<NegBackward0>) tensor(12493.0752, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12491.67578125
tensor(12493.0752, grad_fn=<NegBackward0>) tensor(12491.6758, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12490.41796875
tensor(12491.6758, grad_fn=<NegBackward0>) tensor(12490.4180, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12490.0771484375
tensor(12490.4180, grad_fn=<NegBackward0>) tensor(12490.0771, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12489.94140625
tensor(12490.0771, grad_fn=<NegBackward0>) tensor(12489.9414, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12489.8544921875
tensor(12489.9414, grad_fn=<NegBackward0>) tensor(12489.8545, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12489.7919921875
tensor(12489.8545, grad_fn=<NegBackward0>) tensor(12489.7920, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12489.7470703125
tensor(12489.7920, grad_fn=<NegBackward0>) tensor(12489.7471, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12489.7119140625
tensor(12489.7471, grad_fn=<NegBackward0>) tensor(12489.7119, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12489.685546875
tensor(12489.7119, grad_fn=<NegBackward0>) tensor(12489.6855, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12489.66796875
tensor(12489.6855, grad_fn=<NegBackward0>) tensor(12489.6680, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12489.6513671875
tensor(12489.6680, grad_fn=<NegBackward0>) tensor(12489.6514, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12489.6396484375
tensor(12489.6514, grad_fn=<NegBackward0>) tensor(12489.6396, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12489.6298828125
tensor(12489.6396, grad_fn=<NegBackward0>) tensor(12489.6299, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12489.6240234375
tensor(12489.6299, grad_fn=<NegBackward0>) tensor(12489.6240, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12489.619140625
tensor(12489.6240, grad_fn=<NegBackward0>) tensor(12489.6191, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12489.61328125
tensor(12489.6191, grad_fn=<NegBackward0>) tensor(12489.6133, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12489.6123046875
tensor(12489.6133, grad_fn=<NegBackward0>) tensor(12489.6123, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12489.6083984375
tensor(12489.6123, grad_fn=<NegBackward0>) tensor(12489.6084, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12489.6064453125
tensor(12489.6084, grad_fn=<NegBackward0>) tensor(12489.6064, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12489.60546875
tensor(12489.6064, grad_fn=<NegBackward0>) tensor(12489.6055, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12489.6025390625
tensor(12489.6055, grad_fn=<NegBackward0>) tensor(12489.6025, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12489.6015625
tensor(12489.6025, grad_fn=<NegBackward0>) tensor(12489.6016, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12489.6005859375
tensor(12489.6016, grad_fn=<NegBackward0>) tensor(12489.6006, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12489.599609375
tensor(12489.6006, grad_fn=<NegBackward0>) tensor(12489.5996, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12489.59765625
tensor(12489.5996, grad_fn=<NegBackward0>) tensor(12489.5977, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12489.59765625
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.5977, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12489.59765625
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.5977, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12489.5966796875
tensor(12489.5977, grad_fn=<NegBackward0>) tensor(12489.5967, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12489.595703125
tensor(12489.5967, grad_fn=<NegBackward0>) tensor(12489.5957, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12489.5966796875
tensor(12489.5957, grad_fn=<NegBackward0>) tensor(12489.5967, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12489.595703125
tensor(12489.5957, grad_fn=<NegBackward0>) tensor(12489.5957, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12489.59375
tensor(12489.5957, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12489.59375
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12489.59375
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.5938, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12489.5927734375
tensor(12489.5938, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12489.5927734375
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12489.591796875
tensor(12489.5928, grad_fn=<NegBackward0>) tensor(12489.5918, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12489.5927734375
tensor(12489.5918, grad_fn=<NegBackward0>) tensor(12489.5928, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12489.591796875
tensor(12489.5918, grad_fn=<NegBackward0>) tensor(12489.5918, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12489.5908203125
tensor(12489.5918, grad_fn=<NegBackward0>) tensor(12489.5908, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12489.5908203125
tensor(12489.5908, grad_fn=<NegBackward0>) tensor(12489.5908, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12489.5908203125
tensor(12489.5908, grad_fn=<NegBackward0>) tensor(12489.5908, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12489.58984375
tensor(12489.5908, grad_fn=<NegBackward0>) tensor(12489.5898, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12489.591796875
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5918, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12489.591796875
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5918, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12489.5908203125
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5908, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12489.58984375
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5898, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12489.58984375
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5898, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12489.58984375
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5898, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12489.5908203125
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5908, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12489.5888671875
tensor(12489.5898, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12489.5888671875
tensor(12489.5889, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12489.5888671875
tensor(12489.5889, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12489.587890625
tensor(12489.5889, grad_fn=<NegBackward0>) tensor(12489.5879, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12489.58984375
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5898, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12489.587890625
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5879, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12489.587890625
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5879, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12489.5888671875
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12489.587890625
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5879, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12489.5888671875
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12489.587890625
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5879, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12489.8271484375
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.8271, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12489.5888671875
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12489.6201171875
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.6201, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12489.5888671875
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -12489.5869140625
tensor(12489.5879, grad_fn=<NegBackward0>) tensor(12489.5869, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12489.5888671875
tensor(12489.5869, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12489.587890625
tensor(12489.5869, grad_fn=<NegBackward0>) tensor(12489.5879, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12489.5888671875
tensor(12489.5869, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -12489.5888671875
tensor(12489.5869, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -12489.5888671875
tensor(12489.5869, grad_fn=<NegBackward0>) tensor(12489.5889, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[9.9992e-01, 8.4047e-05],
        [1.9137e-04, 9.9981e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0322, 0.9678], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0786, 0.1751],
         [0.6417, 0.2061]],

        [[0.5453, 0.0945],
         [0.5563, 0.6448]],

        [[0.5342, 0.1673],
         [0.6568, 0.5801]],

        [[0.6856, 0.2140],
         [0.5627, 0.6270]],

        [[0.6308, 0.1627],
         [0.5022, 0.6827]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.003937327268695544
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
Global Adjusted Rand Index: 9.826492412255229e-05
Average Adjusted Rand Index: -0.0018905175248184046
[9.826492412255229e-05, 9.826492412255229e-05] [-0.0018905175248184046, -0.0018905175248184046] [12489.59375, 12489.5888671875]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11953.471350058191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23413.310546875
inf tensor(23413.3105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12452.8447265625
tensor(23413.3105, grad_fn=<NegBackward0>) tensor(12452.8447, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12452.107421875
tensor(12452.8447, grad_fn=<NegBackward0>) tensor(12452.1074, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12451.693359375
tensor(12452.1074, grad_fn=<NegBackward0>) tensor(12451.6934, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12451.35546875
tensor(12451.6934, grad_fn=<NegBackward0>) tensor(12451.3555, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12451.1962890625
tensor(12451.3555, grad_fn=<NegBackward0>) tensor(12451.1963, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12451.1005859375
tensor(12451.1963, grad_fn=<NegBackward0>) tensor(12451.1006, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12450.9951171875
tensor(12451.1006, grad_fn=<NegBackward0>) tensor(12450.9951, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12450.87890625
tensor(12450.9951, grad_fn=<NegBackward0>) tensor(12450.8789, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12450.818359375
tensor(12450.8789, grad_fn=<NegBackward0>) tensor(12450.8184, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12450.7978515625
tensor(12450.8184, grad_fn=<NegBackward0>) tensor(12450.7979, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12450.7880859375
tensor(12450.7979, grad_fn=<NegBackward0>) tensor(12450.7881, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12450.7802734375
tensor(12450.7881, grad_fn=<NegBackward0>) tensor(12450.7803, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12450.7744140625
tensor(12450.7803, grad_fn=<NegBackward0>) tensor(12450.7744, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12450.7705078125
tensor(12450.7744, grad_fn=<NegBackward0>) tensor(12450.7705, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12450.765625
tensor(12450.7705, grad_fn=<NegBackward0>) tensor(12450.7656, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12450.7607421875
tensor(12450.7656, grad_fn=<NegBackward0>) tensor(12450.7607, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12450.7578125
tensor(12450.7607, grad_fn=<NegBackward0>) tensor(12450.7578, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12450.7548828125
tensor(12450.7578, grad_fn=<NegBackward0>) tensor(12450.7549, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12450.751953125
tensor(12450.7549, grad_fn=<NegBackward0>) tensor(12450.7520, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12450.75
tensor(12450.7520, grad_fn=<NegBackward0>) tensor(12450.7500, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12450.7470703125
tensor(12450.7500, grad_fn=<NegBackward0>) tensor(12450.7471, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12450.74609375
tensor(12450.7471, grad_fn=<NegBackward0>) tensor(12450.7461, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12450.744140625
tensor(12450.7461, grad_fn=<NegBackward0>) tensor(12450.7441, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12450.744140625
tensor(12450.7441, grad_fn=<NegBackward0>) tensor(12450.7441, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12450.7431640625
tensor(12450.7441, grad_fn=<NegBackward0>) tensor(12450.7432, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12450.7412109375
tensor(12450.7432, grad_fn=<NegBackward0>) tensor(12450.7412, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12450.7421875
tensor(12450.7412, grad_fn=<NegBackward0>) tensor(12450.7422, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12450.7392578125
tensor(12450.7412, grad_fn=<NegBackward0>) tensor(12450.7393, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12450.740234375
tensor(12450.7393, grad_fn=<NegBackward0>) tensor(12450.7402, grad_fn=<NegBackward0>)
1
Iteration 3000: Loss = -12450.73828125
tensor(12450.7393, grad_fn=<NegBackward0>) tensor(12450.7383, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12450.73828125
tensor(12450.7383, grad_fn=<NegBackward0>) tensor(12450.7383, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12450.7373046875
tensor(12450.7383, grad_fn=<NegBackward0>) tensor(12450.7373, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12450.736328125
tensor(12450.7373, grad_fn=<NegBackward0>) tensor(12450.7363, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12450.7373046875
tensor(12450.7363, grad_fn=<NegBackward0>) tensor(12450.7373, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12450.7353515625
tensor(12450.7363, grad_fn=<NegBackward0>) tensor(12450.7354, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12450.7353515625
tensor(12450.7354, grad_fn=<NegBackward0>) tensor(12450.7354, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12450.734375
tensor(12450.7354, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12450.7353515625
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7354, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12450.734375
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12450.7333984375
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12450.732421875
tensor(12450.7334, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12450.7333984375
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12450.7333984375
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
2
Iteration 4400: Loss = -12450.732421875
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12450.7333984375
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12450.7314453125
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 72%|███████▏  | 72/100 [17:29:44<6:37:17, 851.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 73%|███████▎  | 73/100 [17:47:47<6:54:27, 921.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 74%|███████▍  | 74/100 [18:04:00<6:45:46, 936.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 75%|███████▌  | 75/100 [18:17:38<6:15:22, 900.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 76%|███████▌  | 76/100 [18:32:47<6:01:18, 903.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 77%|███████▋  | 77/100 [18:46:57<5:40:12, 887.51s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 78%|███████▊  | 78/100 [19:01:10<5:21:36, 877.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 79%|███████▉  | 79/100 [19:14:48<5:00:44, 859.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 80%|████████  | 80/100 [19:32:47<5:08:25, 925.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 81%|████████  | 81/100 [19:50:47<5:07:43, 971.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 82%|████████▏ | 82/100 [20:08:53<5:01:44, 1005.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 83%|████████▎ | 83/100 [20:17:35<4:03:52, 860.74s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 84%|████████▍ | 84/100 [20:35:52<4:08:27, 931.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 85%|████████▌ | 85/100 [20:52:17<3:56:53, 947.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 86%|████████▌ | 86/100 [21:02:58<3:19:40, 855.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 87%|████████▋ | 87/100 [21:19:04<3:12:32, 888.66s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 88%|████████▊ | 88/100 [21:27:47<2:35:50, 779.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 89%|████████▉ | 89/100 [21:42:37<2:28:54, 812.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 90%|█████████ | 90/100 [21:56:59<2:17:52, 827.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 91%|█████████ | 91/100 [22:13:20<2:11:00, 873.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 92%|█████████▏| 92/100 [22:26:50<1:53:54, 854.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 93%|█████████▎| 93/100 [22:43:20<1:44:25, 895.04s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 94%|█████████▍| 94/100 [22:54:11<1:22:11, 821.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 95%|█████████▌| 95/100 [23:02:37<1:00:34, 726.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 96%|█████████▌| 96/100 [23:12:29<45:46, 686.59s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 97%|█████████▋| 97/100 [23:28:42<38:37, 772.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 98%|█████████▊| 98/100 [23:43:26<26:51, 805.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
 99%|█████████▉| 99/100 [23:55:25<12:59, 779.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  str_stability = str(stability).replace('0.', '0p')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
100%|██████████| 100/100 [24:07:53<00:00, 770.22s/it]100%|██████████| 100/100 [24:07:53<00:00, 868.73s/it]
Iteration 5000: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12450.73046875
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12450.73046875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12450.732421875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12450.732421875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -12450.73046875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12450.7294921875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7295, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12450.7314453125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12450.732421875
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -12450.73046875
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -12450.73046875
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -12450.7314453125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.9619, 0.0381],
        [0.9990, 0.0010]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9525, 0.0475], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2001, 0.2174],
         [0.7055, 0.2448]],

        [[0.6904, 0.2207],
         [0.5183, 0.6193]],

        [[0.5373, 0.2548],
         [0.6777, 0.5566]],

        [[0.7057, 0.2810],
         [0.5319, 0.7093]],

        [[0.6077, 0.1551],
         [0.5478, 0.5381]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009327450565879662
Average Adjusted Rand Index: -0.0014402981316412348
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23291.345703125
inf tensor(23291.3457, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12453.50390625
tensor(23291.3457, grad_fn=<NegBackward0>) tensor(12453.5039, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12452.8837890625
tensor(12453.5039, grad_fn=<NegBackward0>) tensor(12452.8838, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12452.708984375
tensor(12452.8838, grad_fn=<NegBackward0>) tensor(12452.7090, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12452.56640625
tensor(12452.7090, grad_fn=<NegBackward0>) tensor(12452.5664, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12452.3427734375
tensor(12452.5664, grad_fn=<NegBackward0>) tensor(12452.3428, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12451.9169921875
tensor(12452.3428, grad_fn=<NegBackward0>) tensor(12451.9170, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12451.4482421875
tensor(12451.9170, grad_fn=<NegBackward0>) tensor(12451.4482, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12451.1142578125
tensor(12451.4482, grad_fn=<NegBackward0>) tensor(12451.1143, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12450.9912109375
tensor(12451.1143, grad_fn=<NegBackward0>) tensor(12450.9912, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12450.9189453125
tensor(12450.9912, grad_fn=<NegBackward0>) tensor(12450.9189, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12450.873046875
tensor(12450.9189, grad_fn=<NegBackward0>) tensor(12450.8730, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12450.84375
tensor(12450.8730, grad_fn=<NegBackward0>) tensor(12450.8438, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12450.822265625
tensor(12450.8438, grad_fn=<NegBackward0>) tensor(12450.8223, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12450.80859375
tensor(12450.8223, grad_fn=<NegBackward0>) tensor(12450.8086, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12450.798828125
tensor(12450.8086, grad_fn=<NegBackward0>) tensor(12450.7988, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12450.791015625
tensor(12450.7988, grad_fn=<NegBackward0>) tensor(12450.7910, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12450.7861328125
tensor(12450.7910, grad_fn=<NegBackward0>) tensor(12450.7861, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12450.7802734375
tensor(12450.7861, grad_fn=<NegBackward0>) tensor(12450.7803, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12450.7763671875
tensor(12450.7803, grad_fn=<NegBackward0>) tensor(12450.7764, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12450.7734375
tensor(12450.7764, grad_fn=<NegBackward0>) tensor(12450.7734, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12450.771484375
tensor(12450.7734, grad_fn=<NegBackward0>) tensor(12450.7715, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12450.767578125
tensor(12450.7715, grad_fn=<NegBackward0>) tensor(12450.7676, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12450.765625
tensor(12450.7676, grad_fn=<NegBackward0>) tensor(12450.7656, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12450.763671875
tensor(12450.7656, grad_fn=<NegBackward0>) tensor(12450.7637, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12450.7607421875
tensor(12450.7637, grad_fn=<NegBackward0>) tensor(12450.7607, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12450.7587890625
tensor(12450.7607, grad_fn=<NegBackward0>) tensor(12450.7588, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12450.7587890625
tensor(12450.7588, grad_fn=<NegBackward0>) tensor(12450.7588, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12450.755859375
tensor(12450.7588, grad_fn=<NegBackward0>) tensor(12450.7559, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12450.7529296875
tensor(12450.7559, grad_fn=<NegBackward0>) tensor(12450.7529, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12450.751953125
tensor(12450.7529, grad_fn=<NegBackward0>) tensor(12450.7520, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12450.751953125
tensor(12450.7520, grad_fn=<NegBackward0>) tensor(12450.7520, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12450.748046875
tensor(12450.7520, grad_fn=<NegBackward0>) tensor(12450.7480, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12450.7470703125
tensor(12450.7480, grad_fn=<NegBackward0>) tensor(12450.7471, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12450.7451171875
tensor(12450.7471, grad_fn=<NegBackward0>) tensor(12450.7451, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12450.7431640625
tensor(12450.7451, grad_fn=<NegBackward0>) tensor(12450.7432, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12450.7421875
tensor(12450.7432, grad_fn=<NegBackward0>) tensor(12450.7422, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12450.740234375
tensor(12450.7422, grad_fn=<NegBackward0>) tensor(12450.7402, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12450.7392578125
tensor(12450.7402, grad_fn=<NegBackward0>) tensor(12450.7393, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12450.7412109375
tensor(12450.7393, grad_fn=<NegBackward0>) tensor(12450.7412, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12450.73828125
tensor(12450.7393, grad_fn=<NegBackward0>) tensor(12450.7383, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12450.7373046875
tensor(12450.7383, grad_fn=<NegBackward0>) tensor(12450.7373, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12450.736328125
tensor(12450.7373, grad_fn=<NegBackward0>) tensor(12450.7363, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12450.7353515625
tensor(12450.7363, grad_fn=<NegBackward0>) tensor(12450.7354, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12450.734375
tensor(12450.7354, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12450.734375
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12450.734375
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12450.734375
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12450.7353515625
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7354, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12450.7333984375
tensor(12450.7344, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12450.734375
tensor(12450.7334, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12450.734375
tensor(12450.7334, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12450.734375
tensor(12450.7334, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -12450.734375
tensor(12450.7334, grad_fn=<NegBackward0>) tensor(12450.7344, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -12450.732421875
tensor(12450.7334, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12450.732421875
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12450.732421875
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7324, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12450.7314453125
tensor(12450.7324, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12450.7333984375
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12450.7314453125
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12450.7333984375
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12450.73046875
tensor(12450.7314, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12450.7333984375
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7334, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12450.73046875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7305, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -12450.7314453125
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -12450.7294921875
tensor(12450.7305, grad_fn=<NegBackward0>) tensor(12450.7295, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12450.7314453125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12450.7314453125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12450.7314453125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7314, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12450.814453125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.8145, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -12450.736328125
tensor(12450.7295, grad_fn=<NegBackward0>) tensor(12450.7363, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[9.6202e-01, 3.7976e-02],
        [9.9908e-01, 9.2251e-04]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9527, 0.0473], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2004, 0.2173],
         [0.5670, 0.2448]],

        [[0.7306, 0.2207],
         [0.6259, 0.6299]],

        [[0.6270, 0.2548],
         [0.6252, 0.6573]],

        [[0.5730, 0.2811],
         [0.6991, 0.6218]],

        [[0.6115, 0.1551],
         [0.6874, 0.6205]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009327450565879662
Average Adjusted Rand Index: -0.0014402981316412348
[-0.0009327450565879662, -0.0009327450565879662] [-0.0014402981316412348, -0.0014402981316412348] [12450.7314453125, 12450.736328125]
-------------------------------------
This iteration is 72
True Objective function: Loss = -12019.502641105853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21067.84375
inf tensor(21067.8438, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12519.876953125
tensor(21067.8438, grad_fn=<NegBackward0>) tensor(12519.8770, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12519.634765625
tensor(12519.8770, grad_fn=<NegBackward0>) tensor(12519.6348, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12519.5595703125
tensor(12519.6348, grad_fn=<NegBackward0>) tensor(12519.5596, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12519.498046875
tensor(12519.5596, grad_fn=<NegBackward0>) tensor(12519.4980, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12519.421875
tensor(12519.4980, grad_fn=<NegBackward0>) tensor(12519.4219, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12519.330078125
tensor(12519.4219, grad_fn=<NegBackward0>) tensor(12519.3301, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12519.2822265625
tensor(12519.3301, grad_fn=<NegBackward0>) tensor(12519.2822, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12519.2626953125
tensor(12519.2822, grad_fn=<NegBackward0>) tensor(12519.2627, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12519.2470703125
tensor(12519.2627, grad_fn=<NegBackward0>) tensor(12519.2471, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12519.2333984375
tensor(12519.2471, grad_fn=<NegBackward0>) tensor(12519.2334, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12519.2197265625
tensor(12519.2334, grad_fn=<NegBackward0>) tensor(12519.2197, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12519.205078125
tensor(12519.2197, grad_fn=<NegBackward0>) tensor(12519.2051, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12519.1953125
tensor(12519.2051, grad_fn=<NegBackward0>) tensor(12519.1953, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12519.185546875
tensor(12519.1953, grad_fn=<NegBackward0>) tensor(12519.1855, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12519.17578125
tensor(12519.1855, grad_fn=<NegBackward0>) tensor(12519.1758, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12519.1669921875
tensor(12519.1758, grad_fn=<NegBackward0>) tensor(12519.1670, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12519.1611328125
tensor(12519.1670, grad_fn=<NegBackward0>) tensor(12519.1611, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12519.1552734375
tensor(12519.1611, grad_fn=<NegBackward0>) tensor(12519.1553, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12519.150390625
tensor(12519.1553, grad_fn=<NegBackward0>) tensor(12519.1504, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12519.1474609375
tensor(12519.1504, grad_fn=<NegBackward0>) tensor(12519.1475, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12519.140625
tensor(12519.1475, grad_fn=<NegBackward0>) tensor(12519.1406, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12519.1318359375
tensor(12519.1406, grad_fn=<NegBackward0>) tensor(12519.1318, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12519.111328125
tensor(12519.1318, grad_fn=<NegBackward0>) tensor(12519.1113, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12519.0458984375
tensor(12519.1113, grad_fn=<NegBackward0>) tensor(12519.0459, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12518.84375
tensor(12519.0459, grad_fn=<NegBackward0>) tensor(12518.8438, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12518.560546875
tensor(12518.8438, grad_fn=<NegBackward0>) tensor(12518.5605, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12518.439453125
tensor(12518.5605, grad_fn=<NegBackward0>) tensor(12518.4395, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12518.390625
tensor(12518.4395, grad_fn=<NegBackward0>) tensor(12518.3906, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12518.361328125
tensor(12518.3906, grad_fn=<NegBackward0>) tensor(12518.3613, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12518.34375
tensor(12518.3613, grad_fn=<NegBackward0>) tensor(12518.3438, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12518.333984375
tensor(12518.3438, grad_fn=<NegBackward0>) tensor(12518.3340, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12518.32421875
tensor(12518.3340, grad_fn=<NegBackward0>) tensor(12518.3242, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12518.318359375
tensor(12518.3242, grad_fn=<NegBackward0>) tensor(12518.3184, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12518.3134765625
tensor(12518.3184, grad_fn=<NegBackward0>) tensor(12518.3135, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12518.3076171875
tensor(12518.3135, grad_fn=<NegBackward0>) tensor(12518.3076, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12518.306640625
tensor(12518.3076, grad_fn=<NegBackward0>) tensor(12518.3066, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12518.3037109375
tensor(12518.3066, grad_fn=<NegBackward0>) tensor(12518.3037, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12518.3017578125
tensor(12518.3037, grad_fn=<NegBackward0>) tensor(12518.3018, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12518.30078125
tensor(12518.3018, grad_fn=<NegBackward0>) tensor(12518.3008, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12518.2978515625
tensor(12518.3008, grad_fn=<NegBackward0>) tensor(12518.2979, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12518.2958984375
tensor(12518.2979, grad_fn=<NegBackward0>) tensor(12518.2959, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12518.294921875
tensor(12518.2959, grad_fn=<NegBackward0>) tensor(12518.2949, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12518.29296875
tensor(12518.2949, grad_fn=<NegBackward0>) tensor(12518.2930, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12518.2919921875
tensor(12518.2930, grad_fn=<NegBackward0>) tensor(12518.2920, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12518.2890625
tensor(12518.2920, grad_fn=<NegBackward0>) tensor(12518.2891, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12518.2861328125
tensor(12518.2891, grad_fn=<NegBackward0>) tensor(12518.2861, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12518.28125
tensor(12518.2861, grad_fn=<NegBackward0>) tensor(12518.2812, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12518.2705078125
tensor(12518.2812, grad_fn=<NegBackward0>) tensor(12518.2705, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12518.25
tensor(12518.2705, grad_fn=<NegBackward0>) tensor(12518.2500, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12518.21484375
tensor(12518.2500, grad_fn=<NegBackward0>) tensor(12518.2148, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12518.1669921875
tensor(12518.2148, grad_fn=<NegBackward0>) tensor(12518.1670, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12518.1171875
tensor(12518.1670, grad_fn=<NegBackward0>) tensor(12518.1172, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12518.072265625
tensor(12518.1172, grad_fn=<NegBackward0>) tensor(12518.0723, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12518.0341796875
tensor(12518.0723, grad_fn=<NegBackward0>) tensor(12518.0342, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12518.0029296875
tensor(12518.0342, grad_fn=<NegBackward0>) tensor(12518.0029, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12517.9794921875
tensor(12518.0029, grad_fn=<NegBackward0>) tensor(12517.9795, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12517.9580078125
tensor(12517.9795, grad_fn=<NegBackward0>) tensor(12517.9580, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12517.94140625
tensor(12517.9580, grad_fn=<NegBackward0>) tensor(12517.9414, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12517.9287109375
tensor(12517.9414, grad_fn=<NegBackward0>) tensor(12517.9287, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12517.9140625
tensor(12517.9287, grad_fn=<NegBackward0>) tensor(12517.9141, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12517.90625
tensor(12517.9141, grad_fn=<NegBackward0>) tensor(12517.9062, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12517.8974609375
tensor(12517.9062, grad_fn=<NegBackward0>) tensor(12517.8975, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12517.8896484375
tensor(12517.8975, grad_fn=<NegBackward0>) tensor(12517.8896, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12517.8828125
tensor(12517.8896, grad_fn=<NegBackward0>) tensor(12517.8828, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12517.876953125
tensor(12517.8828, grad_fn=<NegBackward0>) tensor(12517.8770, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12517.873046875
tensor(12517.8770, grad_fn=<NegBackward0>) tensor(12517.8730, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12517.8671875
tensor(12517.8730, grad_fn=<NegBackward0>) tensor(12517.8672, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12517.865234375
tensor(12517.8672, grad_fn=<NegBackward0>) tensor(12517.8652, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12517.8623046875
tensor(12517.8652, grad_fn=<NegBackward0>) tensor(12517.8623, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12517.8583984375
tensor(12517.8623, grad_fn=<NegBackward0>) tensor(12517.8584, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12517.85546875
tensor(12517.8584, grad_fn=<NegBackward0>) tensor(12517.8555, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12517.853515625
tensor(12517.8555, grad_fn=<NegBackward0>) tensor(12517.8535, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12517.8505859375
tensor(12517.8535, grad_fn=<NegBackward0>) tensor(12517.8506, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12517.8955078125
tensor(12517.8506, grad_fn=<NegBackward0>) tensor(12517.8955, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12517.845703125
tensor(12517.8506, grad_fn=<NegBackward0>) tensor(12517.8457, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12517.84765625
tensor(12517.8457, grad_fn=<NegBackward0>) tensor(12517.8477, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12517.8525390625
tensor(12517.8457, grad_fn=<NegBackward0>) tensor(12517.8525, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12517.84375
tensor(12517.8457, grad_fn=<NegBackward0>) tensor(12517.8438, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12517.841796875
tensor(12517.8438, grad_fn=<NegBackward0>) tensor(12517.8418, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12517.8427734375
tensor(12517.8418, grad_fn=<NegBackward0>) tensor(12517.8428, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12517.8388671875
tensor(12517.8418, grad_fn=<NegBackward0>) tensor(12517.8389, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12517.83984375
tensor(12517.8389, grad_fn=<NegBackward0>) tensor(12517.8398, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12517.8369140625
tensor(12517.8389, grad_fn=<NegBackward0>) tensor(12517.8369, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12517.8349609375
tensor(12517.8369, grad_fn=<NegBackward0>) tensor(12517.8350, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12517.8583984375
tensor(12517.8350, grad_fn=<NegBackward0>) tensor(12517.8584, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12517.8330078125
tensor(12517.8350, grad_fn=<NegBackward0>) tensor(12517.8330, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12517.841796875
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8418, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12517.8330078125
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8330, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12517.8310546875
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12517.8310546875
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12517.8310546875
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12517.83203125
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8320, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12517.8310546875
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12517.8310546875
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12517.8291015625
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8291, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12517.8310546875
tensor(12517.8291, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12517.8291015625
tensor(12517.8291, grad_fn=<NegBackward0>) tensor(12517.8291, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12517.8837890625
tensor(12517.8291, grad_fn=<NegBackward0>) tensor(12517.8838, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12517.8291015625
tensor(12517.8291, grad_fn=<NegBackward0>) tensor(12517.8291, grad_fn=<NegBackward0>)
pi: tensor([[1.0000e+00, 2.7990e-06],
        [3.1903e-04, 9.9968e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9815, 0.0185], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.2267],
         [0.6876, 0.0022]],

        [[0.6902, 0.1955],
         [0.7209, 0.5810]],

        [[0.5306, 0.1569],
         [0.5576, 0.6254]],

        [[0.6839, 0.3245],
         [0.6837, 0.6931]],

        [[0.6609, 0.1835],
         [0.6411, 0.7018]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 0.0005854869625796734
Average Adjusted Rand Index: 0.00017014714790078886
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22615.44921875
inf tensor(22615.4492, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12520.1884765625
tensor(22615.4492, grad_fn=<NegBackward0>) tensor(12520.1885, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12519.8115234375
tensor(12520.1885, grad_fn=<NegBackward0>) tensor(12519.8115, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12519.6953125
tensor(12519.8115, grad_fn=<NegBackward0>) tensor(12519.6953, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12519.6142578125
tensor(12519.6953, grad_fn=<NegBackward0>) tensor(12519.6143, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12519.5478515625
tensor(12519.6143, grad_fn=<NegBackward0>) tensor(12519.5479, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12519.48828125
tensor(12519.5479, grad_fn=<NegBackward0>) tensor(12519.4883, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12519.4296875
tensor(12519.4883, grad_fn=<NegBackward0>) tensor(12519.4297, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12519.388671875
tensor(12519.4297, grad_fn=<NegBackward0>) tensor(12519.3887, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12519.3642578125
tensor(12519.3887, grad_fn=<NegBackward0>) tensor(12519.3643, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12519.3427734375
tensor(12519.3643, grad_fn=<NegBackward0>) tensor(12519.3428, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12519.3271484375
tensor(12519.3428, grad_fn=<NegBackward0>) tensor(12519.3271, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12519.3134765625
tensor(12519.3271, grad_fn=<NegBackward0>) tensor(12519.3135, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12519.2978515625
tensor(12519.3135, grad_fn=<NegBackward0>) tensor(12519.2979, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12519.2861328125
tensor(12519.2979, grad_fn=<NegBackward0>) tensor(12519.2861, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12519.2744140625
tensor(12519.2861, grad_fn=<NegBackward0>) tensor(12519.2744, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12519.2626953125
tensor(12519.2744, grad_fn=<NegBackward0>) tensor(12519.2627, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12519.25
tensor(12519.2627, grad_fn=<NegBackward0>) tensor(12519.2500, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12519.2373046875
tensor(12519.2500, grad_fn=<NegBackward0>) tensor(12519.2373, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12519.224609375
tensor(12519.2373, grad_fn=<NegBackward0>) tensor(12519.2246, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12519.2119140625
tensor(12519.2246, grad_fn=<NegBackward0>) tensor(12519.2119, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12519.197265625
tensor(12519.2119, grad_fn=<NegBackward0>) tensor(12519.1973, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12519.1845703125
tensor(12519.1973, grad_fn=<NegBackward0>) tensor(12519.1846, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12519.1689453125
tensor(12519.1846, grad_fn=<NegBackward0>) tensor(12519.1689, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12519.1533203125
tensor(12519.1689, grad_fn=<NegBackward0>) tensor(12519.1533, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12519.1376953125
tensor(12519.1533, grad_fn=<NegBackward0>) tensor(12519.1377, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12519.1220703125
tensor(12519.1377, grad_fn=<NegBackward0>) tensor(12519.1221, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12519.1025390625
tensor(12519.1221, grad_fn=<NegBackward0>) tensor(12519.1025, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12519.0810546875
tensor(12519.1025, grad_fn=<NegBackward0>) tensor(12519.0811, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12519.052734375
tensor(12519.0811, grad_fn=<NegBackward0>) tensor(12519.0527, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12519.001953125
tensor(12519.0527, grad_fn=<NegBackward0>) tensor(12519.0020, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12518.8564453125
tensor(12519.0020, grad_fn=<NegBackward0>) tensor(12518.8564, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12518.626953125
tensor(12518.8564, grad_fn=<NegBackward0>) tensor(12518.6270, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12518.5126953125
tensor(12518.6270, grad_fn=<NegBackward0>) tensor(12518.5127, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12518.451171875
tensor(12518.5127, grad_fn=<NegBackward0>) tensor(12518.4512, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12518.4140625
tensor(12518.4512, grad_fn=<NegBackward0>) tensor(12518.4141, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12518.3876953125
tensor(12518.4141, grad_fn=<NegBackward0>) tensor(12518.3877, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12518.369140625
tensor(12518.3877, grad_fn=<NegBackward0>) tensor(12518.3691, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12518.35546875
tensor(12518.3691, grad_fn=<NegBackward0>) tensor(12518.3555, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12518.34375
tensor(12518.3555, grad_fn=<NegBackward0>) tensor(12518.3438, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12518.3359375
tensor(12518.3438, grad_fn=<NegBackward0>) tensor(12518.3359, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12518.330078125
tensor(12518.3359, grad_fn=<NegBackward0>) tensor(12518.3301, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12518.322265625
tensor(12518.3301, grad_fn=<NegBackward0>) tensor(12518.3223, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12518.318359375
tensor(12518.3223, grad_fn=<NegBackward0>) tensor(12518.3184, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12518.3134765625
tensor(12518.3184, grad_fn=<NegBackward0>) tensor(12518.3135, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12518.3115234375
tensor(12518.3135, grad_fn=<NegBackward0>) tensor(12518.3115, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12518.3076171875
tensor(12518.3115, grad_fn=<NegBackward0>) tensor(12518.3076, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12518.306640625
tensor(12518.3076, grad_fn=<NegBackward0>) tensor(12518.3066, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12518.3046875
tensor(12518.3066, grad_fn=<NegBackward0>) tensor(12518.3047, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12518.3017578125
tensor(12518.3047, grad_fn=<NegBackward0>) tensor(12518.3018, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12518.2998046875
tensor(12518.3018, grad_fn=<NegBackward0>) tensor(12518.2998, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12518.2998046875
tensor(12518.2998, grad_fn=<NegBackward0>) tensor(12518.2998, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12518.2978515625
tensor(12518.2998, grad_fn=<NegBackward0>) tensor(12518.2979, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12518.2958984375
tensor(12518.2979, grad_fn=<NegBackward0>) tensor(12518.2959, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12518.294921875
tensor(12518.2959, grad_fn=<NegBackward0>) tensor(12518.2949, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12518.2919921875
tensor(12518.2949, grad_fn=<NegBackward0>) tensor(12518.2920, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12518.2880859375
tensor(12518.2920, grad_fn=<NegBackward0>) tensor(12518.2881, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12518.2861328125
tensor(12518.2881, grad_fn=<NegBackward0>) tensor(12518.2861, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12518.2724609375
tensor(12518.2861, grad_fn=<NegBackward0>) tensor(12518.2725, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12518.25
tensor(12518.2725, grad_fn=<NegBackward0>) tensor(12518.2500, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12518.2060546875
tensor(12518.2500, grad_fn=<NegBackward0>) tensor(12518.2061, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12518.1474609375
tensor(12518.2061, grad_fn=<NegBackward0>) tensor(12518.1475, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12518.087890625
tensor(12518.1475, grad_fn=<NegBackward0>) tensor(12518.0879, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12518.041015625
tensor(12518.0879, grad_fn=<NegBackward0>) tensor(12518.0410, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12518.0029296875
tensor(12518.0410, grad_fn=<NegBackward0>) tensor(12518.0029, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12517.974609375
tensor(12518.0029, grad_fn=<NegBackward0>) tensor(12517.9746, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12517.951171875
tensor(12517.9746, grad_fn=<NegBackward0>) tensor(12517.9512, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12517.93359375
tensor(12517.9512, grad_fn=<NegBackward0>) tensor(12517.9336, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12517.91796875
tensor(12517.9336, grad_fn=<NegBackward0>) tensor(12517.9180, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12517.90625
tensor(12517.9180, grad_fn=<NegBackward0>) tensor(12517.9062, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12517.8974609375
tensor(12517.9062, grad_fn=<NegBackward0>) tensor(12517.8975, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12517.890625
tensor(12517.8975, grad_fn=<NegBackward0>) tensor(12517.8906, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12517.919921875
tensor(12517.8906, grad_fn=<NegBackward0>) tensor(12517.9199, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12517.876953125
tensor(12517.8906, grad_fn=<NegBackward0>) tensor(12517.8770, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12517.8720703125
tensor(12517.8770, grad_fn=<NegBackward0>) tensor(12517.8721, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12517.8759765625
tensor(12517.8721, grad_fn=<NegBackward0>) tensor(12517.8760, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12517.8623046875
tensor(12517.8721, grad_fn=<NegBackward0>) tensor(12517.8623, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12517.859375
tensor(12517.8623, grad_fn=<NegBackward0>) tensor(12517.8594, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12517.8603515625
tensor(12517.8594, grad_fn=<NegBackward0>) tensor(12517.8604, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12517.853515625
tensor(12517.8594, grad_fn=<NegBackward0>) tensor(12517.8535, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12517.8515625
tensor(12517.8535, grad_fn=<NegBackward0>) tensor(12517.8516, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12517.849609375
tensor(12517.8516, grad_fn=<NegBackward0>) tensor(12517.8496, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12517.84765625
tensor(12517.8496, grad_fn=<NegBackward0>) tensor(12517.8477, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12517.8466796875
tensor(12517.8477, grad_fn=<NegBackward0>) tensor(12517.8467, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12517.8447265625
tensor(12517.8467, grad_fn=<NegBackward0>) tensor(12517.8447, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12517.8408203125
tensor(12517.8447, grad_fn=<NegBackward0>) tensor(12517.8408, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12517.849609375
tensor(12517.8408, grad_fn=<NegBackward0>) tensor(12517.8496, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12517.83984375
tensor(12517.8408, grad_fn=<NegBackward0>) tensor(12517.8398, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12517.8369140625
tensor(12517.8398, grad_fn=<NegBackward0>) tensor(12517.8369, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12517.837890625
tensor(12517.8369, grad_fn=<NegBackward0>) tensor(12517.8379, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12517.8359375
tensor(12517.8369, grad_fn=<NegBackward0>) tensor(12517.8359, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12517.8349609375
tensor(12517.8359, grad_fn=<NegBackward0>) tensor(12517.8350, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12517.8330078125
tensor(12517.8350, grad_fn=<NegBackward0>) tensor(12517.8330, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12517.833984375
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8340, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12517.8408203125
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8408, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -12517.8330078125
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8330, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12518.048828125
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12518.0488, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -12517.83203125
tensor(12517.8330, grad_fn=<NegBackward0>) tensor(12517.8320, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12517.8310546875
tensor(12517.8320, grad_fn=<NegBackward0>) tensor(12517.8311, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12517.8486328125
tensor(12517.8311, grad_fn=<NegBackward0>) tensor(12517.8486, grad_fn=<NegBackward0>)
1
pi: tensor([[1.0000e+00, 3.2853e-06],
        [4.3670e-04, 9.9956e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9820, 0.0180], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2035, 0.2267],
         [0.6028, 0.0026]],

        [[0.5828, 0.1955],
         [0.5754, 0.5638]],

        [[0.5947, 0.1569],
         [0.5332, 0.5696]],

        [[0.5174, 0.3245],
         [0.6834, 0.6003]],

        [[0.6436, 0.1835],
         [0.6340, 0.5385]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 0.0005854869625796734
Average Adjusted Rand Index: 0.00017014714790078886
[0.0005854869625796734, 0.0005854869625796734] [0.00017014714790078886, 0.00017014714790078886] [12518.078125, 12517.8310546875]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11810.198909644521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21327.54296875
inf tensor(21327.5430, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12280.5966796875
tensor(21327.5430, grad_fn=<NegBackward0>) tensor(12280.5967, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12280.1259765625
tensor(12280.5967, grad_fn=<NegBackward0>) tensor(12280.1260, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12279.986328125
tensor(12280.1260, grad_fn=<NegBackward0>) tensor(12279.9863, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12279.85546875
tensor(12279.9863, grad_fn=<NegBackward0>) tensor(12279.8555, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12279.7158203125
tensor(12279.8555, grad_fn=<NegBackward0>) tensor(12279.7158, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12279.5556640625
tensor(12279.7158, grad_fn=<NegBackward0>) tensor(12279.5557, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12279.357421875
tensor(12279.5557, grad_fn=<NegBackward0>) tensor(12279.3574, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12279.130859375
tensor(12279.3574, grad_fn=<NegBackward0>) tensor(12279.1309, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12278.8935546875
tensor(12279.1309, grad_fn=<NegBackward0>) tensor(12278.8936, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12278.6611328125
tensor(12278.8936, grad_fn=<NegBackward0>) tensor(12278.6611, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12278.4677734375
tensor(12278.6611, grad_fn=<NegBackward0>) tensor(12278.4678, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12278.3271484375
tensor(12278.4678, grad_fn=<NegBackward0>) tensor(12278.3271, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12278.23828125
tensor(12278.3271, grad_fn=<NegBackward0>) tensor(12278.2383, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12278.1845703125
tensor(12278.2383, grad_fn=<NegBackward0>) tensor(12278.1846, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12278.140625
tensor(12278.1846, grad_fn=<NegBackward0>) tensor(12278.1406, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12278.0966796875
tensor(12278.1406, grad_fn=<NegBackward0>) tensor(12278.0967, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12278.0478515625
tensor(12278.0967, grad_fn=<NegBackward0>) tensor(12278.0479, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12277.9970703125
tensor(12278.0479, grad_fn=<NegBackward0>) tensor(12277.9971, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12277.9404296875
tensor(12277.9971, grad_fn=<NegBackward0>) tensor(12277.9404, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12277.8896484375
tensor(12277.9404, grad_fn=<NegBackward0>) tensor(12277.8896, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12277.8486328125
tensor(12277.8896, grad_fn=<NegBackward0>) tensor(12277.8486, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12277.8115234375
tensor(12277.8486, grad_fn=<NegBackward0>) tensor(12277.8115, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12277.7802734375
tensor(12277.8115, grad_fn=<NegBackward0>) tensor(12277.7803, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12277.75
tensor(12277.7803, grad_fn=<NegBackward0>) tensor(12277.7500, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12277.71484375
tensor(12277.7500, grad_fn=<NegBackward0>) tensor(12277.7148, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12277.6640625
tensor(12277.7148, grad_fn=<NegBackward0>) tensor(12277.6641, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12277.546875
tensor(12277.6641, grad_fn=<NegBackward0>) tensor(12277.5469, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12277.107421875
tensor(12277.5469, grad_fn=<NegBackward0>) tensor(12277.1074, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12276.23828125
tensor(12277.1074, grad_fn=<NegBackward0>) tensor(12276.2383, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12275.94140625
tensor(12276.2383, grad_fn=<NegBackward0>) tensor(12275.9414, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12275.841796875
tensor(12275.9414, grad_fn=<NegBackward0>) tensor(12275.8418, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12275.7802734375
tensor(12275.8418, grad_fn=<NegBackward0>) tensor(12275.7803, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12275.732421875
tensor(12275.7803, grad_fn=<NegBackward0>) tensor(12275.7324, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12275.6923828125
tensor(12275.7324, grad_fn=<NegBackward0>) tensor(12275.6924, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12275.6650390625
tensor(12275.6924, grad_fn=<NegBackward0>) tensor(12275.6650, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12275.6435546875
tensor(12275.6650, grad_fn=<NegBackward0>) tensor(12275.6436, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12275.62890625
tensor(12275.6436, grad_fn=<NegBackward0>) tensor(12275.6289, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12275.619140625
tensor(12275.6289, grad_fn=<NegBackward0>) tensor(12275.6191, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12275.609375
tensor(12275.6191, grad_fn=<NegBackward0>) tensor(12275.6094, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12275.6025390625
tensor(12275.6094, grad_fn=<NegBackward0>) tensor(12275.6025, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12275.595703125
tensor(12275.6025, grad_fn=<NegBackward0>) tensor(12275.5957, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12275.58984375
tensor(12275.5957, grad_fn=<NegBackward0>) tensor(12275.5898, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12275.583984375
tensor(12275.5898, grad_fn=<NegBackward0>) tensor(12275.5840, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12275.5224609375
tensor(12275.5840, grad_fn=<NegBackward0>) tensor(12275.5225, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12274.9130859375
tensor(12275.5225, grad_fn=<NegBackward0>) tensor(12274.9131, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12274.7451171875
tensor(12274.9131, grad_fn=<NegBackward0>) tensor(12274.7451, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12274.677734375
tensor(12274.7451, grad_fn=<NegBackward0>) tensor(12274.6777, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12274.6416015625
tensor(12274.6777, grad_fn=<NegBackward0>) tensor(12274.6416, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12274.62109375
tensor(12274.6416, grad_fn=<NegBackward0>) tensor(12274.6211, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12274.6083984375
tensor(12274.6211, grad_fn=<NegBackward0>) tensor(12274.6084, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12274.5947265625
tensor(12274.6084, grad_fn=<NegBackward0>) tensor(12274.5947, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12274.587890625
tensor(12274.5947, grad_fn=<NegBackward0>) tensor(12274.5879, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12274.583984375
tensor(12274.5879, grad_fn=<NegBackward0>) tensor(12274.5840, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12274.578125
tensor(12274.5840, grad_fn=<NegBackward0>) tensor(12274.5781, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12274.5771484375
tensor(12274.5781, grad_fn=<NegBackward0>) tensor(12274.5771, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12274.57421875
tensor(12274.5771, grad_fn=<NegBackward0>) tensor(12274.5742, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12274.5712890625
tensor(12274.5742, grad_fn=<NegBackward0>) tensor(12274.5713, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12274.5703125
tensor(12274.5713, grad_fn=<NegBackward0>) tensor(12274.5703, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12274.5693359375
tensor(12274.5703, grad_fn=<NegBackward0>) tensor(12274.5693, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12274.5673828125
tensor(12274.5693, grad_fn=<NegBackward0>) tensor(12274.5674, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12274.564453125
tensor(12274.5674, grad_fn=<NegBackward0>) tensor(12274.5645, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12274.564453125
tensor(12274.5645, grad_fn=<NegBackward0>) tensor(12274.5645, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12274.5634765625
tensor(12274.5645, grad_fn=<NegBackward0>) tensor(12274.5635, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12274.5634765625
tensor(12274.5635, grad_fn=<NegBackward0>) tensor(12274.5635, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12274.5615234375
tensor(12274.5635, grad_fn=<NegBackward0>) tensor(12274.5615, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12274.5615234375
tensor(12274.5615, grad_fn=<NegBackward0>) tensor(12274.5615, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12274.5595703125
tensor(12274.5615, grad_fn=<NegBackward0>) tensor(12274.5596, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12274.560546875
tensor(12274.5596, grad_fn=<NegBackward0>) tensor(12274.5605, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12274.55859375
tensor(12274.5596, grad_fn=<NegBackward0>) tensor(12274.5586, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12274.55859375
tensor(12274.5586, grad_fn=<NegBackward0>) tensor(12274.5586, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12274.55859375
tensor(12274.5586, grad_fn=<NegBackward0>) tensor(12274.5586, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12274.55859375
tensor(12274.5586, grad_fn=<NegBackward0>) tensor(12274.5586, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12274.5576171875
tensor(12274.5586, grad_fn=<NegBackward0>) tensor(12274.5576, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12274.556640625
tensor(12274.5576, grad_fn=<NegBackward0>) tensor(12274.5566, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12274.556640625
tensor(12274.5566, grad_fn=<NegBackward0>) tensor(12274.5566, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12274.5576171875
tensor(12274.5566, grad_fn=<NegBackward0>) tensor(12274.5576, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12274.5546875
tensor(12274.5566, grad_fn=<NegBackward0>) tensor(12274.5547, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12274.55859375
tensor(12274.5547, grad_fn=<NegBackward0>) tensor(12274.5586, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12274.5556640625
tensor(12274.5547, grad_fn=<NegBackward0>) tensor(12274.5557, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12274.556640625
tensor(12274.5547, grad_fn=<NegBackward0>) tensor(12274.5566, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -12274.5576171875
tensor(12274.5547, grad_fn=<NegBackward0>) tensor(12274.5576, grad_fn=<NegBackward0>)
4
Iteration 8200: Loss = -12274.5595703125
tensor(12274.5547, grad_fn=<NegBackward0>) tensor(12274.5596, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[1.7491e-05, 9.9998e-01],
        [9.9998e-01, 2.4397e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0303, 0.9697], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2064, 0.2667],
         [0.5898, 0.1879]],

        [[0.5922, 0.1958],
         [0.5609, 0.7112]],

        [[0.6859, 0.2187],
         [0.6627, 0.6161]],

        [[0.7204, 0.2020],
         [0.7113, 0.7302]],

        [[0.5963, 0.2455],
         [0.5778, 0.6360]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: 0.018335442091456804
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0012662455124815629
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002653835847620666
Average Adjusted Rand Index: 0.000584717664866145
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23207.84375
inf tensor(23207.8438, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12281.0078125
tensor(23207.8438, grad_fn=<NegBackward0>) tensor(12281.0078, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12280.318359375
tensor(12281.0078, grad_fn=<NegBackward0>) tensor(12280.3184, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12280.0546875
tensor(12280.3184, grad_fn=<NegBackward0>) tensor(12280.0547, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12279.7607421875
tensor(12280.0547, grad_fn=<NegBackward0>) tensor(12279.7607, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12279.2666015625
tensor(12279.7607, grad_fn=<NegBackward0>) tensor(12279.2666, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12278.7783203125
tensor(12279.2666, grad_fn=<NegBackward0>) tensor(12278.7783, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12278.6005859375
tensor(12278.7783, grad_fn=<NegBackward0>) tensor(12278.6006, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12278.4697265625
tensor(12278.6006, grad_fn=<NegBackward0>) tensor(12278.4697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12278.365234375
tensor(12278.4697, grad_fn=<NegBackward0>) tensor(12278.3652, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12278.283203125
tensor(12278.3652, grad_fn=<NegBackward0>) tensor(12278.2832, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12278.2109375
tensor(12278.2832, grad_fn=<NegBackward0>) tensor(12278.2109, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12278.142578125
tensor(12278.2109, grad_fn=<NegBackward0>) tensor(12278.1426, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12278.0634765625
tensor(12278.1426, grad_fn=<NegBackward0>) tensor(12278.0635, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12277.984375
tensor(12278.0635, grad_fn=<NegBackward0>) tensor(12277.9844, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12277.8955078125
tensor(12277.9844, grad_fn=<NegBackward0>) tensor(12277.8955, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12277.8232421875
tensor(12277.8955, grad_fn=<NegBackward0>) tensor(12277.8232, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12277.7705078125
tensor(12277.8232, grad_fn=<NegBackward0>) tensor(12277.7705, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12277.7548828125
tensor(12277.7705, grad_fn=<NegBackward0>) tensor(12277.7549, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12277.62890625
tensor(12277.7549, grad_fn=<NegBackward0>) tensor(12277.6289, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12277.2099609375
tensor(12277.6289, grad_fn=<NegBackward0>) tensor(12277.2100, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12276.1611328125
tensor(12277.2100, grad_fn=<NegBackward0>) tensor(12276.1611, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12275.91796875
tensor(12276.1611, grad_fn=<NegBackward0>) tensor(12275.9180, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12275.84765625
tensor(12275.9180, grad_fn=<NegBackward0>) tensor(12275.8477, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12275.8115234375
tensor(12275.8477, grad_fn=<NegBackward0>) tensor(12275.8115, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12275.7568359375
tensor(12275.8115, grad_fn=<NegBackward0>) tensor(12275.7568, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12275.4091796875
tensor(12275.7568, grad_fn=<NegBackward0>) tensor(12275.4092, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11977.126953125
tensor(12275.4092, grad_fn=<NegBackward0>) tensor(11977.1270, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11949.509765625
tensor(11977.1270, grad_fn=<NegBackward0>) tensor(11949.5098, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11938.70703125
tensor(11949.5098, grad_fn=<NegBackward0>) tensor(11938.7070, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11933.9833984375
tensor(11938.7070, grad_fn=<NegBackward0>) tensor(11933.9834, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11919.2470703125
tensor(11933.9834, grad_fn=<NegBackward0>) tensor(11919.2471, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11918.00390625
tensor(11919.2471, grad_fn=<NegBackward0>) tensor(11918.0039, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11906.55859375
tensor(11918.0039, grad_fn=<NegBackward0>) tensor(11906.5586, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11906.2958984375
tensor(11906.5586, grad_fn=<NegBackward0>) tensor(11906.2959, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11906.27734375
tensor(11906.2959, grad_fn=<NegBackward0>) tensor(11906.2773, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11905.462890625
tensor(11906.2773, grad_fn=<NegBackward0>) tensor(11905.4629, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11901.16015625
tensor(11905.4629, grad_fn=<NegBackward0>) tensor(11901.1602, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11887.72265625
tensor(11901.1602, grad_fn=<NegBackward0>) tensor(11887.7227, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11887.6953125
tensor(11887.7227, grad_fn=<NegBackward0>) tensor(11887.6953, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11885.4091796875
tensor(11887.6953, grad_fn=<NegBackward0>) tensor(11885.4092, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11885.4072265625
tensor(11885.4092, grad_fn=<NegBackward0>) tensor(11885.4072, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11875.33984375
tensor(11885.4072, grad_fn=<NegBackward0>) tensor(11875.3398, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11875.337890625
tensor(11875.3398, grad_fn=<NegBackward0>) tensor(11875.3379, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11875.328125
tensor(11875.3379, grad_fn=<NegBackward0>) tensor(11875.3281, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11868.80078125
tensor(11875.3281, grad_fn=<NegBackward0>) tensor(11868.8008, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11859.7392578125
tensor(11868.8008, grad_fn=<NegBackward0>) tensor(11859.7393, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11859.716796875
tensor(11859.7393, grad_fn=<NegBackward0>) tensor(11859.7168, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11859.7158203125
tensor(11859.7168, grad_fn=<NegBackward0>) tensor(11859.7158, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11859.71484375
tensor(11859.7158, grad_fn=<NegBackward0>) tensor(11859.7148, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11859.7138671875
tensor(11859.7148, grad_fn=<NegBackward0>) tensor(11859.7139, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11859.716796875
tensor(11859.7139, grad_fn=<NegBackward0>) tensor(11859.7168, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -11859.712890625
tensor(11859.7139, grad_fn=<NegBackward0>) tensor(11859.7129, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11859.6337890625
tensor(11859.7129, grad_fn=<NegBackward0>) tensor(11859.6338, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11854.3291015625
tensor(11859.6338, grad_fn=<NegBackward0>) tensor(11854.3291, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11854.3173828125
tensor(11854.3291, grad_fn=<NegBackward0>) tensor(11854.3174, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11845.01171875
tensor(11854.3174, grad_fn=<NegBackward0>) tensor(11845.0117, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11844.931640625
tensor(11845.0117, grad_fn=<NegBackward0>) tensor(11844.9316, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11844.9130859375
tensor(11844.9316, grad_fn=<NegBackward0>) tensor(11844.9131, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11838.64453125
tensor(11844.9131, grad_fn=<NegBackward0>) tensor(11838.6445, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11824.58203125
tensor(11838.6445, grad_fn=<NegBackward0>) tensor(11824.5820, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11812.34765625
tensor(11824.5820, grad_fn=<NegBackward0>) tensor(11812.3477, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11806.6328125
tensor(11812.3477, grad_fn=<NegBackward0>) tensor(11806.6328, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11806.6220703125
tensor(11806.6328, grad_fn=<NegBackward0>) tensor(11806.6221, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11806.609375
tensor(11806.6221, grad_fn=<NegBackward0>) tensor(11806.6094, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11806.6123046875
tensor(11806.6094, grad_fn=<NegBackward0>) tensor(11806.6123, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11806.6083984375
tensor(11806.6094, grad_fn=<NegBackward0>) tensor(11806.6084, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11806.609375
tensor(11806.6084, grad_fn=<NegBackward0>) tensor(11806.6094, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11806.607421875
tensor(11806.6084, grad_fn=<NegBackward0>) tensor(11806.6074, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11806.609375
tensor(11806.6074, grad_fn=<NegBackward0>) tensor(11806.6094, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11806.603515625
tensor(11806.6074, grad_fn=<NegBackward0>) tensor(11806.6035, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11806.6015625
tensor(11806.6035, grad_fn=<NegBackward0>) tensor(11806.6016, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11806.6025390625
tensor(11806.6016, grad_fn=<NegBackward0>) tensor(11806.6025, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11806.5927734375
tensor(11806.6016, grad_fn=<NegBackward0>) tensor(11806.5928, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11806.5908203125
tensor(11806.5928, grad_fn=<NegBackward0>) tensor(11806.5908, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11798.8154296875
tensor(11806.5908, grad_fn=<NegBackward0>) tensor(11798.8154, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11798.814453125
tensor(11798.8154, grad_fn=<NegBackward0>) tensor(11798.8145, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11798.80859375
tensor(11798.8145, grad_fn=<NegBackward0>) tensor(11798.8086, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11798.80859375
tensor(11798.8086, grad_fn=<NegBackward0>) tensor(11798.8086, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11798.8095703125
tensor(11798.8086, grad_fn=<NegBackward0>) tensor(11798.8096, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11798.810546875
tensor(11798.8086, grad_fn=<NegBackward0>) tensor(11798.8105, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11798.80859375
tensor(11798.8086, grad_fn=<NegBackward0>) tensor(11798.8086, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11798.9716796875
tensor(11798.8086, grad_fn=<NegBackward0>) tensor(11798.9717, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11798.8076171875
tensor(11798.8086, grad_fn=<NegBackward0>) tensor(11798.8076, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11798.8076171875
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8076, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11798.8125
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8125, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11798.8125
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8125, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11798.814453125
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8145, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11798.8203125
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8203, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -11798.8076171875
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8076, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11798.8056640625
tensor(11798.8076, grad_fn=<NegBackward0>) tensor(11798.8057, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11798.830078125
tensor(11798.8057, grad_fn=<NegBackward0>) tensor(11798.8301, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11798.8017578125
tensor(11798.8057, grad_fn=<NegBackward0>) tensor(11798.8018, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11798.7998046875
tensor(11798.8018, grad_fn=<NegBackward0>) tensor(11798.7998, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11798.814453125
tensor(11798.7998, grad_fn=<NegBackward0>) tensor(11798.8145, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11798.81640625
tensor(11798.7998, grad_fn=<NegBackward0>) tensor(11798.8164, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11798.8046875
tensor(11798.7998, grad_fn=<NegBackward0>) tensor(11798.8047, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11798.802734375
tensor(11798.7998, grad_fn=<NegBackward0>) tensor(11798.8027, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -11798.828125
tensor(11798.7998, grad_fn=<NegBackward0>) tensor(11798.8281, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.2074, 0.7926],
        [0.7050, 0.2950]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4208, 0.5792], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2908, 0.1031],
         [0.5033, 0.2953]],

        [[0.5837, 0.1092],
         [0.5646, 0.5250]],

        [[0.6303, 0.0945],
         [0.5092, 0.6277]],

        [[0.6813, 0.0980],
         [0.6120, 0.6502]],

        [[0.5812, 0.0894],
         [0.7047, 0.6734]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.04134925199609441
Average Adjusted Rand Index: 0.9841616161616162
[0.002653835847620666, 0.04134925199609441] [0.000584717664866145, 0.9841616161616162] [12274.5595703125, 11798.828125]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11814.895137135527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23228.5859375
inf tensor(23228.5859, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12304.7724609375
tensor(23228.5859, grad_fn=<NegBackward0>) tensor(12304.7725, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12304.3388671875
tensor(12304.7725, grad_fn=<NegBackward0>) tensor(12304.3389, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12304.2392578125
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.2393, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12304.1748046875
tensor(12304.2393, grad_fn=<NegBackward0>) tensor(12304.1748, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12304.1220703125
tensor(12304.1748, grad_fn=<NegBackward0>) tensor(12304.1221, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12304.0751953125
tensor(12304.1221, grad_fn=<NegBackward0>) tensor(12304.0752, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12304.02734375
tensor(12304.0752, grad_fn=<NegBackward0>) tensor(12304.0273, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12303.9697265625
tensor(12304.0273, grad_fn=<NegBackward0>) tensor(12303.9697, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12303.8984375
tensor(12303.9697, grad_fn=<NegBackward0>) tensor(12303.8984, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12303.8017578125
tensor(12303.8984, grad_fn=<NegBackward0>) tensor(12303.8018, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12303.68359375
tensor(12303.8018, grad_fn=<NegBackward0>) tensor(12303.6836, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12303.548828125
tensor(12303.6836, grad_fn=<NegBackward0>) tensor(12303.5488, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12303.4013671875
tensor(12303.5488, grad_fn=<NegBackward0>) tensor(12303.4014, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12303.2333984375
tensor(12303.4014, grad_fn=<NegBackward0>) tensor(12303.2334, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12303.025390625
tensor(12303.2334, grad_fn=<NegBackward0>) tensor(12303.0254, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12302.7548828125
tensor(12303.0254, grad_fn=<NegBackward0>) tensor(12302.7549, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12302.412109375
tensor(12302.7549, grad_fn=<NegBackward0>) tensor(12302.4121, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12302.0146484375
tensor(12302.4121, grad_fn=<NegBackward0>) tensor(12302.0146, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12298.8154296875
tensor(12302.0146, grad_fn=<NegBackward0>) tensor(12298.8154, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11981.5205078125
tensor(12298.8154, grad_fn=<NegBackward0>) tensor(11981.5205, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11880.9248046875
tensor(11981.5205, grad_fn=<NegBackward0>) tensor(11880.9248, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11871.9619140625
tensor(11880.9248, grad_fn=<NegBackward0>) tensor(11871.9619, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11861.625
tensor(11871.9619, grad_fn=<NegBackward0>) tensor(11861.6250, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11851.80859375
tensor(11861.6250, grad_fn=<NegBackward0>) tensor(11851.8086, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11838.544921875
tensor(11851.8086, grad_fn=<NegBackward0>) tensor(11838.5449, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11825.51953125
tensor(11838.5449, grad_fn=<NegBackward0>) tensor(11825.5195, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11825.408203125
tensor(11825.5195, grad_fn=<NegBackward0>) tensor(11825.4082, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11820.9345703125
tensor(11825.4082, grad_fn=<NegBackward0>) tensor(11820.9346, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11820.7958984375
tensor(11820.9346, grad_fn=<NegBackward0>) tensor(11820.7959, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11811.251953125
tensor(11820.7959, grad_fn=<NegBackward0>) tensor(11811.2520, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11811.2314453125
tensor(11811.2520, grad_fn=<NegBackward0>) tensor(11811.2314, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11811.1640625
tensor(11811.2314, grad_fn=<NegBackward0>) tensor(11811.1641, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11811.1455078125
tensor(11811.1641, grad_fn=<NegBackward0>) tensor(11811.1455, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11811.134765625
tensor(11811.1455, grad_fn=<NegBackward0>) tensor(11811.1348, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11811.12890625
tensor(11811.1348, grad_fn=<NegBackward0>) tensor(11811.1289, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11811.125
tensor(11811.1289, grad_fn=<NegBackward0>) tensor(11811.1250, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11811.119140625
tensor(11811.1250, grad_fn=<NegBackward0>) tensor(11811.1191, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11811.1259765625
tensor(11811.1191, grad_fn=<NegBackward0>) tensor(11811.1260, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11811.1083984375
tensor(11811.1191, grad_fn=<NegBackward0>) tensor(11811.1084, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11811.0869140625
tensor(11811.1084, grad_fn=<NegBackward0>) tensor(11811.0869, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11811.0849609375
tensor(11811.0869, grad_fn=<NegBackward0>) tensor(11811.0850, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11811.0810546875
tensor(11811.0850, grad_fn=<NegBackward0>) tensor(11811.0811, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11811.0830078125
tensor(11811.0811, grad_fn=<NegBackward0>) tensor(11811.0830, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11811.0771484375
tensor(11811.0811, grad_fn=<NegBackward0>) tensor(11811.0771, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11811.087890625
tensor(11811.0771, grad_fn=<NegBackward0>) tensor(11811.0879, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11811.068359375
tensor(11811.0771, grad_fn=<NegBackward0>) tensor(11811.0684, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11810.994140625
tensor(11811.0684, grad_fn=<NegBackward0>) tensor(11810.9941, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11810.984375
tensor(11810.9941, grad_fn=<NegBackward0>) tensor(11810.9844, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11810.978515625
tensor(11810.9844, grad_fn=<NegBackward0>) tensor(11810.9785, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11810.9755859375
tensor(11810.9785, grad_fn=<NegBackward0>) tensor(11810.9756, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11810.97265625
tensor(11810.9756, grad_fn=<NegBackward0>) tensor(11810.9727, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11810.970703125
tensor(11810.9727, grad_fn=<NegBackward0>) tensor(11810.9707, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11803.1708984375
tensor(11810.9707, grad_fn=<NegBackward0>) tensor(11803.1709, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11803.1494140625
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1494, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11803.146484375
tensor(11803.1494, grad_fn=<NegBackward0>) tensor(11803.1465, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11803.146484375
tensor(11803.1465, grad_fn=<NegBackward0>) tensor(11803.1465, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11803.1455078125
tensor(11803.1465, grad_fn=<NegBackward0>) tensor(11803.1455, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11803.162109375
tensor(11803.1455, grad_fn=<NegBackward0>) tensor(11803.1621, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11803.1435546875
tensor(11803.1455, grad_fn=<NegBackward0>) tensor(11803.1436, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11803.14453125
tensor(11803.1436, grad_fn=<NegBackward0>) tensor(11803.1445, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11803.1435546875
tensor(11803.1436, grad_fn=<NegBackward0>) tensor(11803.1436, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11803.146484375
tensor(11803.1436, grad_fn=<NegBackward0>) tensor(11803.1465, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11803.142578125
tensor(11803.1436, grad_fn=<NegBackward0>) tensor(11803.1426, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11803.1435546875
tensor(11803.1426, grad_fn=<NegBackward0>) tensor(11803.1436, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11803.142578125
tensor(11803.1426, grad_fn=<NegBackward0>) tensor(11803.1426, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11803.1416015625
tensor(11803.1426, grad_fn=<NegBackward0>) tensor(11803.1416, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11803.1396484375
tensor(11803.1416, grad_fn=<NegBackward0>) tensor(11803.1396, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11803.1513671875
tensor(11803.1396, grad_fn=<NegBackward0>) tensor(11803.1514, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11803.140625
tensor(11803.1396, grad_fn=<NegBackward0>) tensor(11803.1406, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11803.1416015625
tensor(11803.1396, grad_fn=<NegBackward0>) tensor(11803.1416, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11803.138671875
tensor(11803.1396, grad_fn=<NegBackward0>) tensor(11803.1387, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11803.1396484375
tensor(11803.1387, grad_fn=<NegBackward0>) tensor(11803.1396, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11803.1396484375
tensor(11803.1387, grad_fn=<NegBackward0>) tensor(11803.1396, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11803.140625
tensor(11803.1387, grad_fn=<NegBackward0>) tensor(11803.1406, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11803.140625
tensor(11803.1387, grad_fn=<NegBackward0>) tensor(11803.1406, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11803.1396484375
tensor(11803.1387, grad_fn=<NegBackward0>) tensor(11803.1396, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.2292, 0.7708],
        [0.7076, 0.2924]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4375, 0.5625], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2926, 0.1017],
         [0.5031, 0.2984]],

        [[0.5645, 0.0970],
         [0.6455, 0.5474]],

        [[0.5063, 0.0894],
         [0.5857, 0.5070]],

        [[0.5710, 0.1004],
         [0.6664, 0.6417]],

        [[0.5602, 0.1024],
         [0.5680, 0.5317]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03806254770490077
Average Adjusted Rand Index: 0.9839903977456617
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21088.810546875
inf tensor(21088.8105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12304.5625
tensor(21088.8105, grad_fn=<NegBackward0>) tensor(12304.5625, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12304.1708984375
tensor(12304.5625, grad_fn=<NegBackward0>) tensor(12304.1709, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12304.064453125
tensor(12304.1709, grad_fn=<NegBackward0>) tensor(12304.0645, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12303.99609375
tensor(12304.0645, grad_fn=<NegBackward0>) tensor(12303.9961, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12303.94921875
tensor(12303.9961, grad_fn=<NegBackward0>) tensor(12303.9492, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12303.91015625
tensor(12303.9492, grad_fn=<NegBackward0>) tensor(12303.9102, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12303.87109375
tensor(12303.9102, grad_fn=<NegBackward0>) tensor(12303.8711, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12303.8271484375
tensor(12303.8711, grad_fn=<NegBackward0>) tensor(12303.8271, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12303.7802734375
tensor(12303.8271, grad_fn=<NegBackward0>) tensor(12303.7803, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12303.724609375
tensor(12303.7803, grad_fn=<NegBackward0>) tensor(12303.7246, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12303.6552734375
tensor(12303.7246, grad_fn=<NegBackward0>) tensor(12303.6553, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12303.5810546875
tensor(12303.6553, grad_fn=<NegBackward0>) tensor(12303.5811, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12303.525390625
tensor(12303.5811, grad_fn=<NegBackward0>) tensor(12303.5254, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12303.4970703125
tensor(12303.5254, grad_fn=<NegBackward0>) tensor(12303.4971, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12303.486328125
tensor(12303.4971, grad_fn=<NegBackward0>) tensor(12303.4863, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12303.4775390625
tensor(12303.4863, grad_fn=<NegBackward0>) tensor(12303.4775, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12303.4677734375
tensor(12303.4775, grad_fn=<NegBackward0>) tensor(12303.4678, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12303.4541015625
tensor(12303.4678, grad_fn=<NegBackward0>) tensor(12303.4541, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12303.4228515625
tensor(12303.4541, grad_fn=<NegBackward0>) tensor(12303.4229, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12303.330078125
tensor(12303.4229, grad_fn=<NegBackward0>) tensor(12303.3301, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12302.7939453125
tensor(12303.3301, grad_fn=<NegBackward0>) tensor(12302.7939, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12112.6650390625
tensor(12302.7939, grad_fn=<NegBackward0>) tensor(12112.6650, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11879.9169921875
tensor(12112.6650, grad_fn=<NegBackward0>) tensor(11879.9170, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11864.1455078125
tensor(11879.9170, grad_fn=<NegBackward0>) tensor(11864.1455, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11851.630859375
tensor(11864.1455, grad_fn=<NegBackward0>) tensor(11851.6309, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11829.0830078125
tensor(11851.6309, grad_fn=<NegBackward0>) tensor(11829.0830, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11811.2822265625
tensor(11829.0830, grad_fn=<NegBackward0>) tensor(11811.2822, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11811.2685546875
tensor(11811.2822, grad_fn=<NegBackward0>) tensor(11811.2686, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11811.2587890625
tensor(11811.2686, grad_fn=<NegBackward0>) tensor(11811.2588, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11811.244140625
tensor(11811.2588, grad_fn=<NegBackward0>) tensor(11811.2441, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11811.1669921875
tensor(11811.2441, grad_fn=<NegBackward0>) tensor(11811.1670, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11811.1591796875
tensor(11811.1670, grad_fn=<NegBackward0>) tensor(11811.1592, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11811.1572265625
tensor(11811.1592, grad_fn=<NegBackward0>) tensor(11811.1572, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11811.1533203125
tensor(11811.1572, grad_fn=<NegBackward0>) tensor(11811.1533, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11811.15234375
tensor(11811.1533, grad_fn=<NegBackward0>) tensor(11811.1523, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11811.150390625
tensor(11811.1523, grad_fn=<NegBackward0>) tensor(11811.1504, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11811.1474609375
tensor(11811.1504, grad_fn=<NegBackward0>) tensor(11811.1475, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11804.7216796875
tensor(11811.1475, grad_fn=<NegBackward0>) tensor(11804.7217, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11803.3310546875
tensor(11804.7217, grad_fn=<NegBackward0>) tensor(11803.3311, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11803.19921875
tensor(11803.3311, grad_fn=<NegBackward0>) tensor(11803.1992, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11803.19921875
tensor(11803.1992, grad_fn=<NegBackward0>) tensor(11803.1992, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11803.197265625
tensor(11803.1992, grad_fn=<NegBackward0>) tensor(11803.1973, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11803.197265625
tensor(11803.1973, grad_fn=<NegBackward0>) tensor(11803.1973, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11803.193359375
tensor(11803.1973, grad_fn=<NegBackward0>) tensor(11803.1934, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11803.189453125
tensor(11803.1934, grad_fn=<NegBackward0>) tensor(11803.1895, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11803.189453125
tensor(11803.1895, grad_fn=<NegBackward0>) tensor(11803.1895, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11803.189453125
tensor(11803.1895, grad_fn=<NegBackward0>) tensor(11803.1895, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11803.189453125
tensor(11803.1895, grad_fn=<NegBackward0>) tensor(11803.1895, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11803.1875
tensor(11803.1895, grad_fn=<NegBackward0>) tensor(11803.1875, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11803.1884765625
tensor(11803.1875, grad_fn=<NegBackward0>) tensor(11803.1885, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11803.189453125
tensor(11803.1875, grad_fn=<NegBackward0>) tensor(11803.1895, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11803.1875
tensor(11803.1875, grad_fn=<NegBackward0>) tensor(11803.1875, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11803.185546875
tensor(11803.1875, grad_fn=<NegBackward0>) tensor(11803.1855, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11803.185546875
tensor(11803.1855, grad_fn=<NegBackward0>) tensor(11803.1855, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11803.185546875
tensor(11803.1855, grad_fn=<NegBackward0>) tensor(11803.1855, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11803.1904296875
tensor(11803.1855, grad_fn=<NegBackward0>) tensor(11803.1904, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11803.185546875
tensor(11803.1855, grad_fn=<NegBackward0>) tensor(11803.1855, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11803.1845703125
tensor(11803.1855, grad_fn=<NegBackward0>) tensor(11803.1846, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11803.1845703125
tensor(11803.1846, grad_fn=<NegBackward0>) tensor(11803.1846, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11803.18359375
tensor(11803.1846, grad_fn=<NegBackward0>) tensor(11803.1836, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11803.1845703125
tensor(11803.1836, grad_fn=<NegBackward0>) tensor(11803.1846, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11803.18359375
tensor(11803.1836, grad_fn=<NegBackward0>) tensor(11803.1836, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11803.1845703125
tensor(11803.1836, grad_fn=<NegBackward0>) tensor(11803.1846, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11803.181640625
tensor(11803.1836, grad_fn=<NegBackward0>) tensor(11803.1816, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11803.1708984375
tensor(11803.1816, grad_fn=<NegBackward0>) tensor(11803.1709, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11803.1728515625
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1729, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11803.171875
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1719, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11803.171875
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1719, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -11803.1708984375
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1709, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11803.1728515625
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1729, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11803.1708984375
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1709, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11803.171875
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1719, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11803.1728515625
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1729, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11803.1875
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1875, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -11803.1728515625
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1729, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -11803.177734375
tensor(11803.1709, grad_fn=<NegBackward0>) tensor(11803.1777, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.2289, 0.7711],
        [0.7057, 0.2943]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4386, 0.5614], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2935, 0.1017],
         [0.5995, 0.2984]],

        [[0.6470, 0.0970],
         [0.6793, 0.5796]],

        [[0.6645, 0.0894],
         [0.6498, 0.6742]],

        [[0.5941, 0.1003],
         [0.5797, 0.6805]],

        [[0.6859, 0.1024],
         [0.6555, 0.5891]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03806254770490077
Average Adjusted Rand Index: 0.9839903977456617
[0.03806254770490077, 0.03806254770490077] [0.9839903977456617, 0.9839903977456617] [11803.1396484375, 11803.177734375]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11682.329810067185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22654.181640625
inf tensor(22654.1816, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12237.21875
tensor(22654.1816, grad_fn=<NegBackward0>) tensor(12237.2188, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12236.8095703125
tensor(12237.2188, grad_fn=<NegBackward0>) tensor(12236.8096, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12236.6962890625
tensor(12236.8096, grad_fn=<NegBackward0>) tensor(12236.6963, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12236.6171875
tensor(12236.6963, grad_fn=<NegBackward0>) tensor(12236.6172, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12236.5478515625
tensor(12236.6172, grad_fn=<NegBackward0>) tensor(12236.5479, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12236.4755859375
tensor(12236.5479, grad_fn=<NegBackward0>) tensor(12236.4756, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12236.3837890625
tensor(12236.4756, grad_fn=<NegBackward0>) tensor(12236.3838, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12236.20703125
tensor(12236.3838, grad_fn=<NegBackward0>) tensor(12236.2070, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12235.83203125
tensor(12236.2070, grad_fn=<NegBackward0>) tensor(12235.8320, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12235.5986328125
tensor(12235.8320, grad_fn=<NegBackward0>) tensor(12235.5986, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12235.5224609375
tensor(12235.5986, grad_fn=<NegBackward0>) tensor(12235.5225, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12235.458984375
tensor(12235.5225, grad_fn=<NegBackward0>) tensor(12235.4590, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12235.35546875
tensor(12235.4590, grad_fn=<NegBackward0>) tensor(12235.3555, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12235.1962890625
tensor(12235.3555, grad_fn=<NegBackward0>) tensor(12235.1963, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12235.0185546875
tensor(12235.1963, grad_fn=<NegBackward0>) tensor(12235.0186, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12234.8271484375
tensor(12235.0186, grad_fn=<NegBackward0>) tensor(12234.8271, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12234.3388671875
tensor(12234.8271, grad_fn=<NegBackward0>) tensor(12234.3389, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11915.9609375
tensor(12234.3389, grad_fn=<NegBackward0>) tensor(11915.9609, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11863.470703125
tensor(11915.9609, grad_fn=<NegBackward0>) tensor(11863.4707, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11796.6396484375
tensor(11863.4707, grad_fn=<NegBackward0>) tensor(11796.6396, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11763.9189453125
tensor(11796.6396, grad_fn=<NegBackward0>) tensor(11763.9189, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11748.1923828125
tensor(11763.9189, grad_fn=<NegBackward0>) tensor(11748.1924, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11748.1494140625
tensor(11748.1924, grad_fn=<NegBackward0>) tensor(11748.1494, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11748.125
tensor(11748.1494, grad_fn=<NegBackward0>) tensor(11748.1250, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11748.0634765625
tensor(11748.1250, grad_fn=<NegBackward0>) tensor(11748.0635, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11731.5595703125
tensor(11748.0635, grad_fn=<NegBackward0>) tensor(11731.5596, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11722.765625
tensor(11731.5596, grad_fn=<NegBackward0>) tensor(11722.7656, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11716.5439453125
tensor(11722.7656, grad_fn=<NegBackward0>) tensor(11716.5439, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11716.5302734375
tensor(11716.5439, grad_fn=<NegBackward0>) tensor(11716.5303, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11716.525390625
tensor(11716.5303, grad_fn=<NegBackward0>) tensor(11716.5254, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11716.515625
tensor(11716.5254, grad_fn=<NegBackward0>) tensor(11716.5156, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11716.51171875
tensor(11716.5156, grad_fn=<NegBackward0>) tensor(11716.5117, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11716.509765625
tensor(11716.5117, grad_fn=<NegBackward0>) tensor(11716.5098, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11716.5068359375
tensor(11716.5098, grad_fn=<NegBackward0>) tensor(11716.5068, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11716.50390625
tensor(11716.5068, grad_fn=<NegBackward0>) tensor(11716.5039, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11703.98046875
tensor(11716.5039, grad_fn=<NegBackward0>) tensor(11703.9805, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11703.9072265625
tensor(11703.9805, grad_fn=<NegBackward0>) tensor(11703.9072, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11703.904296875
tensor(11703.9072, grad_fn=<NegBackward0>) tensor(11703.9043, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11703.90234375
tensor(11703.9043, grad_fn=<NegBackward0>) tensor(11703.9023, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11695.185546875
tensor(11703.9023, grad_fn=<NegBackward0>) tensor(11695.1855, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11695.1923828125
tensor(11695.1855, grad_fn=<NegBackward0>) tensor(11695.1924, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -11695.181640625
tensor(11695.1855, grad_fn=<NegBackward0>) tensor(11695.1816, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11695.1806640625
tensor(11695.1816, grad_fn=<NegBackward0>) tensor(11695.1807, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11695.1796875
tensor(11695.1807, grad_fn=<NegBackward0>) tensor(11695.1797, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11695.1787109375
tensor(11695.1797, grad_fn=<NegBackward0>) tensor(11695.1787, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11695.1767578125
tensor(11695.1787, grad_fn=<NegBackward0>) tensor(11695.1768, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11695.17578125
tensor(11695.1768, grad_fn=<NegBackward0>) tensor(11695.1758, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11695.17578125
tensor(11695.1758, grad_fn=<NegBackward0>) tensor(11695.1758, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11695.1748046875
tensor(11695.1758, grad_fn=<NegBackward0>) tensor(11695.1748, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11695.1865234375
tensor(11695.1748, grad_fn=<NegBackward0>) tensor(11695.1865, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11695.1748046875
tensor(11695.1748, grad_fn=<NegBackward0>) tensor(11695.1748, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11695.1767578125
tensor(11695.1748, grad_fn=<NegBackward0>) tensor(11695.1768, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11695.173828125
tensor(11695.1748, grad_fn=<NegBackward0>) tensor(11695.1738, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11695.1728515625
tensor(11695.1738, grad_fn=<NegBackward0>) tensor(11695.1729, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11695.173828125
tensor(11695.1729, grad_fn=<NegBackward0>) tensor(11695.1738, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11695.1728515625
tensor(11695.1729, grad_fn=<NegBackward0>) tensor(11695.1729, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11695.171875
tensor(11695.1729, grad_fn=<NegBackward0>) tensor(11695.1719, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11695.171875
tensor(11695.1719, grad_fn=<NegBackward0>) tensor(11695.1719, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11695.1728515625
tensor(11695.1719, grad_fn=<NegBackward0>) tensor(11695.1729, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11695.169921875
tensor(11695.1719, grad_fn=<NegBackward0>) tensor(11695.1699, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11695.171875
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1719, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11695.173828125
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1738, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11695.169921875
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1699, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11695.1708984375
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1709, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11695.169921875
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1699, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11695.1708984375
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1709, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11695.169921875
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1699, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11695.169921875
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1699, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11695.1689453125
tensor(11695.1699, grad_fn=<NegBackward0>) tensor(11695.1689, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11695.1748046875
tensor(11695.1689, grad_fn=<NegBackward0>) tensor(11695.1748, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11695.166015625
tensor(11695.1689, grad_fn=<NegBackward0>) tensor(11695.1660, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11695.1611328125
tensor(11695.1660, grad_fn=<NegBackward0>) tensor(11695.1611, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11695.17578125
tensor(11695.1611, grad_fn=<NegBackward0>) tensor(11695.1758, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11695.1611328125
tensor(11695.1611, grad_fn=<NegBackward0>) tensor(11695.1611, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11695.16015625
tensor(11695.1611, grad_fn=<NegBackward0>) tensor(11695.1602, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11695.1611328125
tensor(11695.1602, grad_fn=<NegBackward0>) tensor(11695.1611, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11695.169921875
tensor(11695.1602, grad_fn=<NegBackward0>) tensor(11695.1699, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11695.16015625
tensor(11695.1602, grad_fn=<NegBackward0>) tensor(11695.1602, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11695.1650390625
tensor(11695.1602, grad_fn=<NegBackward0>) tensor(11695.1650, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11695.1591796875
tensor(11695.1602, grad_fn=<NegBackward0>) tensor(11695.1592, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11695.1611328125
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1611, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11695.185546875
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1855, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11695.1591796875
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1592, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11695.162109375
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1621, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11695.16015625
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1602, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11695.32421875
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.3242, grad_fn=<NegBackward0>)
3
Iteration 8700: Loss = -11695.1591796875
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1592, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11695.158203125
tensor(11695.1592, grad_fn=<NegBackward0>) tensor(11695.1582, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11695.1611328125
tensor(11695.1582, grad_fn=<NegBackward0>) tensor(11695.1611, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11695.16015625
tensor(11695.1582, grad_fn=<NegBackward0>) tensor(11695.1602, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11695.1728515625
tensor(11695.1582, grad_fn=<NegBackward0>) tensor(11695.1729, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11695.1591796875
tensor(11695.1582, grad_fn=<NegBackward0>) tensor(11695.1592, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11695.16015625
tensor(11695.1582, grad_fn=<NegBackward0>) tensor(11695.1602, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.6417, 0.3583],
        [0.2623, 0.7377]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5471, 0.4529], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2930, 0.0914],
         [0.7221, 0.3002]],

        [[0.6720, 0.0976],
         [0.6636, 0.6299]],

        [[0.7234, 0.0905],
         [0.6717, 0.5747]],

        [[0.6225, 0.0959],
         [0.6820, 0.6487]],

        [[0.7059, 0.0915],
         [0.6078, 0.6280]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.3635234231024263
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19827.111328125
inf tensor(19827.1113, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12236.7705078125
tensor(19827.1113, grad_fn=<NegBackward0>) tensor(12236.7705, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12236.5126953125
tensor(12236.7705, grad_fn=<NegBackward0>) tensor(12236.5127, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12236.4296875
tensor(12236.5127, grad_fn=<NegBackward0>) tensor(12236.4297, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12236.3349609375
tensor(12236.4297, grad_fn=<NegBackward0>) tensor(12236.3350, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12236.189453125
tensor(12236.3350, grad_fn=<NegBackward0>) tensor(12236.1895, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12235.9931640625
tensor(12236.1895, grad_fn=<NegBackward0>) tensor(12235.9932, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12235.845703125
tensor(12235.9932, grad_fn=<NegBackward0>) tensor(12235.8457, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12235.7490234375
tensor(12235.8457, grad_fn=<NegBackward0>) tensor(12235.7490, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12235.673828125
tensor(12235.7490, grad_fn=<NegBackward0>) tensor(12235.6738, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12235.63671875
tensor(12235.6738, grad_fn=<NegBackward0>) tensor(12235.6367, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12235.61328125
tensor(12235.6367, grad_fn=<NegBackward0>) tensor(12235.6133, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12235.6005859375
tensor(12235.6133, grad_fn=<NegBackward0>) tensor(12235.6006, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12235.5908203125
tensor(12235.6006, grad_fn=<NegBackward0>) tensor(12235.5908, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12235.5810546875
tensor(12235.5908, grad_fn=<NegBackward0>) tensor(12235.5811, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12235.576171875
tensor(12235.5811, grad_fn=<NegBackward0>) tensor(12235.5762, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12235.5703125
tensor(12235.5762, grad_fn=<NegBackward0>) tensor(12235.5703, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12235.5625
tensor(12235.5703, grad_fn=<NegBackward0>) tensor(12235.5625, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12235.5556640625
tensor(12235.5625, grad_fn=<NegBackward0>) tensor(12235.5557, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12235.55078125
tensor(12235.5557, grad_fn=<NegBackward0>) tensor(12235.5508, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12235.544921875
tensor(12235.5508, grad_fn=<NegBackward0>) tensor(12235.5449, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12235.5380859375
tensor(12235.5449, grad_fn=<NegBackward0>) tensor(12235.5381, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12235.5302734375
tensor(12235.5381, grad_fn=<NegBackward0>) tensor(12235.5303, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12235.509765625
tensor(12235.5303, grad_fn=<NegBackward0>) tensor(12235.5098, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12235.3779296875
tensor(12235.5098, grad_fn=<NegBackward0>) tensor(12235.3779, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12234.69921875
tensor(12235.3779, grad_fn=<NegBackward0>) tensor(12234.6992, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11856.8330078125
tensor(12234.6992, grad_fn=<NegBackward0>) tensor(11856.8330, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11821.5146484375
tensor(11856.8330, grad_fn=<NegBackward0>) tensor(11821.5146, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11808.6787109375
tensor(11821.5146, grad_fn=<NegBackward0>) tensor(11808.6787, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11751.271484375
tensor(11808.6787, grad_fn=<NegBackward0>) tensor(11751.2715, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11751.0458984375
tensor(11751.2715, grad_fn=<NegBackward0>) tensor(11751.0459, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11716.78515625
tensor(11751.0459, grad_fn=<NegBackward0>) tensor(11716.7852, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11716.7236328125
tensor(11716.7852, grad_fn=<NegBackward0>) tensor(11716.7236, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11716.71875
tensor(11716.7236, grad_fn=<NegBackward0>) tensor(11716.7188, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11716.7119140625
tensor(11716.7188, grad_fn=<NegBackward0>) tensor(11716.7119, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11716.7041015625
tensor(11716.7119, grad_fn=<NegBackward0>) tensor(11716.7041, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11716.701171875
tensor(11716.7041, grad_fn=<NegBackward0>) tensor(11716.7012, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11716.7001953125
tensor(11716.7012, grad_fn=<NegBackward0>) tensor(11716.7002, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11716.69921875
tensor(11716.7002, grad_fn=<NegBackward0>) tensor(11716.6992, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11716.69921875
tensor(11716.6992, grad_fn=<NegBackward0>) tensor(11716.6992, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11716.6982421875
tensor(11716.6992, grad_fn=<NegBackward0>) tensor(11716.6982, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11716.697265625
tensor(11716.6982, grad_fn=<NegBackward0>) tensor(11716.6973, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11716.6943359375
tensor(11716.6973, grad_fn=<NegBackward0>) tensor(11716.6943, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11716.697265625
tensor(11716.6943, grad_fn=<NegBackward0>) tensor(11716.6973, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -11716.6943359375
tensor(11716.6943, grad_fn=<NegBackward0>) tensor(11716.6943, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11716.6923828125
tensor(11716.6943, grad_fn=<NegBackward0>) tensor(11716.6924, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11716.693359375
tensor(11716.6924, grad_fn=<NegBackward0>) tensor(11716.6934, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11716.69140625
tensor(11716.6924, grad_fn=<NegBackward0>) tensor(11716.6914, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11716.6904296875
tensor(11716.6914, grad_fn=<NegBackward0>) tensor(11716.6904, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11716.693359375
tensor(11716.6904, grad_fn=<NegBackward0>) tensor(11716.6934, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11716.689453125
tensor(11716.6904, grad_fn=<NegBackward0>) tensor(11716.6895, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11716.6884765625
tensor(11716.6895, grad_fn=<NegBackward0>) tensor(11716.6885, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11716.6865234375
tensor(11716.6885, grad_fn=<NegBackward0>) tensor(11716.6865, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11716.6875
tensor(11716.6865, grad_fn=<NegBackward0>) tensor(11716.6875, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11716.69140625
tensor(11716.6865, grad_fn=<NegBackward0>) tensor(11716.6914, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -11716.6875
tensor(11716.6865, grad_fn=<NegBackward0>) tensor(11716.6875, grad_fn=<NegBackward0>)
3
Iteration 5600: Loss = -11716.6845703125
tensor(11716.6865, grad_fn=<NegBackward0>) tensor(11716.6846, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11716.68359375
tensor(11716.6846, grad_fn=<NegBackward0>) tensor(11716.6836, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11716.685546875
tensor(11716.6836, grad_fn=<NegBackward0>) tensor(11716.6855, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11706.6748046875
tensor(11716.6836, grad_fn=<NegBackward0>) tensor(11706.6748, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11695.150390625
tensor(11706.6748, grad_fn=<NegBackward0>) tensor(11695.1504, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11695.1484375
tensor(11695.1504, grad_fn=<NegBackward0>) tensor(11695.1484, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11695.1474609375
tensor(11695.1484, grad_fn=<NegBackward0>) tensor(11695.1475, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11695.1474609375
tensor(11695.1475, grad_fn=<NegBackward0>) tensor(11695.1475, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11695.146484375
tensor(11695.1475, grad_fn=<NegBackward0>) tensor(11695.1465, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11695.146484375
tensor(11695.1465, grad_fn=<NegBackward0>) tensor(11695.1465, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11695.14453125
tensor(11695.1465, grad_fn=<NegBackward0>) tensor(11695.1445, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11695.146484375
tensor(11695.1445, grad_fn=<NegBackward0>) tensor(11695.1465, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11695.1455078125
tensor(11695.1445, grad_fn=<NegBackward0>) tensor(11695.1455, grad_fn=<NegBackward0>)
2
Iteration 6900: Loss = -11695.146484375
tensor(11695.1445, grad_fn=<NegBackward0>) tensor(11695.1465, grad_fn=<NegBackward0>)
3
Iteration 7000: Loss = -11695.1494140625
tensor(11695.1445, grad_fn=<NegBackward0>) tensor(11695.1494, grad_fn=<NegBackward0>)
4
Iteration 7100: Loss = -11695.146484375
tensor(11695.1445, grad_fn=<NegBackward0>) tensor(11695.1465, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.7359, 0.2641],
        [0.3597, 0.6403]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4525, 0.5475], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3003, 0.0914],
         [0.6807, 0.2930]],

        [[0.6495, 0.0977],
         [0.6487, 0.5107]],

        [[0.6028, 0.0905],
         [0.5645, 0.5020]],

        [[0.6116, 0.0959],
         [0.6834, 0.7009]],

        [[0.5397, 0.0915],
         [0.5049, 0.5033]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.3635234231024263
Average Adjusted Rand Index: 0.992
[0.3635234231024263, 0.3635234231024263] [0.992, 0.992] [11695.16015625, 11695.146484375]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11914.632544442185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20889.890625
inf tensor(20889.8906, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12440.6494140625
tensor(20889.8906, grad_fn=<NegBackward0>) tensor(12440.6494, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12440.29296875
tensor(12440.6494, grad_fn=<NegBackward0>) tensor(12440.2930, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12440.2255859375
tensor(12440.2930, grad_fn=<NegBackward0>) tensor(12440.2256, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12440.189453125
tensor(12440.2256, grad_fn=<NegBackward0>) tensor(12440.1895, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12440.1630859375
tensor(12440.1895, grad_fn=<NegBackward0>) tensor(12440.1631, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12440.1455078125
tensor(12440.1631, grad_fn=<NegBackward0>) tensor(12440.1455, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12440.1279296875
tensor(12440.1455, grad_fn=<NegBackward0>) tensor(12440.1279, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12440.1123046875
tensor(12440.1279, grad_fn=<NegBackward0>) tensor(12440.1123, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12440.095703125
tensor(12440.1123, grad_fn=<NegBackward0>) tensor(12440.0957, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12440.0771484375
tensor(12440.0957, grad_fn=<NegBackward0>) tensor(12440.0771, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12440.056640625
tensor(12440.0771, grad_fn=<NegBackward0>) tensor(12440.0566, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12440.0302734375
tensor(12440.0566, grad_fn=<NegBackward0>) tensor(12440.0303, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12439.99609375
tensor(12440.0303, grad_fn=<NegBackward0>) tensor(12439.9961, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12439.953125
tensor(12439.9961, grad_fn=<NegBackward0>) tensor(12439.9531, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12439.892578125
tensor(12439.9531, grad_fn=<NegBackward0>) tensor(12439.8926, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12439.8134765625
tensor(12439.8926, grad_fn=<NegBackward0>) tensor(12439.8135, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12439.7216796875
tensor(12439.8135, grad_fn=<NegBackward0>) tensor(12439.7217, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12439.6572265625
tensor(12439.7217, grad_fn=<NegBackward0>) tensor(12439.6572, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12439.5595703125
tensor(12439.6572, grad_fn=<NegBackward0>) tensor(12439.5596, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12439.5068359375
tensor(12439.5596, grad_fn=<NegBackward0>) tensor(12439.5068, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12439.4716796875
tensor(12439.5068, grad_fn=<NegBackward0>) tensor(12439.4717, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12439.44921875
tensor(12439.4717, grad_fn=<NegBackward0>) tensor(12439.4492, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12439.431640625
tensor(12439.4492, grad_fn=<NegBackward0>) tensor(12439.4316, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12439.41796875
tensor(12439.4316, grad_fn=<NegBackward0>) tensor(12439.4180, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12439.4091796875
tensor(12439.4180, grad_fn=<NegBackward0>) tensor(12439.4092, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12439.40234375
tensor(12439.4092, grad_fn=<NegBackward0>) tensor(12439.4023, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12439.3984375
tensor(12439.4023, grad_fn=<NegBackward0>) tensor(12439.3984, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12439.392578125
tensor(12439.3984, grad_fn=<NegBackward0>) tensor(12439.3926, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12439.3896484375
tensor(12439.3926, grad_fn=<NegBackward0>) tensor(12439.3896, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12439.38671875
tensor(12439.3896, grad_fn=<NegBackward0>) tensor(12439.3867, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12439.3837890625
tensor(12439.3867, grad_fn=<NegBackward0>) tensor(12439.3838, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12439.380859375
tensor(12439.3838, grad_fn=<NegBackward0>) tensor(12439.3809, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12439.380859375
tensor(12439.3809, grad_fn=<NegBackward0>) tensor(12439.3809, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12439.376953125
tensor(12439.3809, grad_fn=<NegBackward0>) tensor(12439.3770, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12439.375
tensor(12439.3770, grad_fn=<NegBackward0>) tensor(12439.3750, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12439.373046875
tensor(12439.3750, grad_fn=<NegBackward0>) tensor(12439.3730, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12439.373046875
tensor(12439.3730, grad_fn=<NegBackward0>) tensor(12439.3730, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12439.37109375
tensor(12439.3730, grad_fn=<NegBackward0>) tensor(12439.3711, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12439.3681640625
tensor(12439.3711, grad_fn=<NegBackward0>) tensor(12439.3682, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12439.3681640625
tensor(12439.3682, grad_fn=<NegBackward0>) tensor(12439.3682, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12439.3662109375
tensor(12439.3682, grad_fn=<NegBackward0>) tensor(12439.3662, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12439.365234375
tensor(12439.3662, grad_fn=<NegBackward0>) tensor(12439.3652, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12439.3642578125
tensor(12439.3652, grad_fn=<NegBackward0>) tensor(12439.3643, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12439.361328125
tensor(12439.3643, grad_fn=<NegBackward0>) tensor(12439.3613, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12439.361328125
tensor(12439.3613, grad_fn=<NegBackward0>) tensor(12439.3613, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12439.361328125
tensor(12439.3613, grad_fn=<NegBackward0>) tensor(12439.3613, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12439.361328125
tensor(12439.3613, grad_fn=<NegBackward0>) tensor(12439.3613, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12439.3583984375
tensor(12439.3613, grad_fn=<NegBackward0>) tensor(12439.3584, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12439.357421875
tensor(12439.3584, grad_fn=<NegBackward0>) tensor(12439.3574, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12439.35546875
tensor(12439.3574, grad_fn=<NegBackward0>) tensor(12439.3555, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12439.3544921875
tensor(12439.3555, grad_fn=<NegBackward0>) tensor(12439.3545, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12439.353515625
tensor(12439.3545, grad_fn=<NegBackward0>) tensor(12439.3535, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12439.3505859375
tensor(12439.3535, grad_fn=<NegBackward0>) tensor(12439.3506, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12439.349609375
tensor(12439.3506, grad_fn=<NegBackward0>) tensor(12439.3496, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12439.3486328125
tensor(12439.3496, grad_fn=<NegBackward0>) tensor(12439.3486, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12439.353515625
tensor(12439.3486, grad_fn=<NegBackward0>) tensor(12439.3535, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12439.3447265625
tensor(12439.3486, grad_fn=<NegBackward0>) tensor(12439.3447, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12439.341796875
tensor(12439.3447, grad_fn=<NegBackward0>) tensor(12439.3418, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12439.33984375
tensor(12439.3418, grad_fn=<NegBackward0>) tensor(12439.3398, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12439.3349609375
tensor(12439.3398, grad_fn=<NegBackward0>) tensor(12439.3350, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12439.33203125
tensor(12439.3350, grad_fn=<NegBackward0>) tensor(12439.3320, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12439.328125
tensor(12439.3320, grad_fn=<NegBackward0>) tensor(12439.3281, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12439.3232421875
tensor(12439.3281, grad_fn=<NegBackward0>) tensor(12439.3232, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12439.318359375
tensor(12439.3232, grad_fn=<NegBackward0>) tensor(12439.3184, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12439.341796875
tensor(12439.3184, grad_fn=<NegBackward0>) tensor(12439.3418, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12439.3115234375
tensor(12439.3184, grad_fn=<NegBackward0>) tensor(12439.3115, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12439.3125
tensor(12439.3115, grad_fn=<NegBackward0>) tensor(12439.3125, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12439.3095703125
tensor(12439.3115, grad_fn=<NegBackward0>) tensor(12439.3096, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12439.30859375
tensor(12439.3096, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12439.3095703125
tensor(12439.3086, grad_fn=<NegBackward0>) tensor(12439.3096, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12439.30859375
tensor(12439.3086, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12439.30859375
tensor(12439.3086, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12439.30859375
tensor(12439.3086, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12439.3076171875
tensor(12439.3086, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12439.30859375
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12439.30859375
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12439.3076171875
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12439.306640625
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3066, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12439.3076171875
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12439.3173828125
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3174, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12439.30859375
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -12439.3076171875
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
4
Iteration 8300: Loss = -12439.3125
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3125, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.7093, 0.2907],
        [0.9760, 0.0240]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0110, 0.9890], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.2076],
         [0.7072, 0.2078]],

        [[0.6774, 0.1887],
         [0.5081, 0.5847]],

        [[0.7055, 0.2031],
         [0.5258, 0.5841]],

        [[0.6246, 0.2056],
         [0.7119, 0.7150]],

        [[0.5405, 0.2011],
         [0.6789, 0.5222]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001302378301401357
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20559.646484375
inf tensor(20559.6465, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12440.654296875
tensor(20559.6465, grad_fn=<NegBackward0>) tensor(12440.6543, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12440.3173828125
tensor(12440.6543, grad_fn=<NegBackward0>) tensor(12440.3174, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12440.2236328125
tensor(12440.3174, grad_fn=<NegBackward0>) tensor(12440.2236, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12440.1689453125
tensor(12440.2236, grad_fn=<NegBackward0>) tensor(12440.1689, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12440.1298828125
tensor(12440.1689, grad_fn=<NegBackward0>) tensor(12440.1299, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12440.1005859375
tensor(12440.1299, grad_fn=<NegBackward0>) tensor(12440.1006, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12440.06640625
tensor(12440.1006, grad_fn=<NegBackward0>) tensor(12440.0664, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12440.0302734375
tensor(12440.0664, grad_fn=<NegBackward0>) tensor(12440.0303, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12439.9775390625
tensor(12440.0303, grad_fn=<NegBackward0>) tensor(12439.9775, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12439.90625
tensor(12439.9775, grad_fn=<NegBackward0>) tensor(12439.9062, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12439.80859375
tensor(12439.9062, grad_fn=<NegBackward0>) tensor(12439.8086, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12439.6796875
tensor(12439.8086, grad_fn=<NegBackward0>) tensor(12439.6797, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12439.5693359375
tensor(12439.6797, grad_fn=<NegBackward0>) tensor(12439.5693, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12439.49609375
tensor(12439.5693, grad_fn=<NegBackward0>) tensor(12439.4961, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12439.4541015625
tensor(12439.4961, grad_fn=<NegBackward0>) tensor(12439.4541, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12439.4267578125
tensor(12439.4541, grad_fn=<NegBackward0>) tensor(12439.4268, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12439.4111328125
tensor(12439.4268, grad_fn=<NegBackward0>) tensor(12439.4111, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12439.400390625
tensor(12439.4111, grad_fn=<NegBackward0>) tensor(12439.4004, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12439.392578125
tensor(12439.4004, grad_fn=<NegBackward0>) tensor(12439.3926, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12439.38671875
tensor(12439.3926, grad_fn=<NegBackward0>) tensor(12439.3867, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12439.3818359375
tensor(12439.3867, grad_fn=<NegBackward0>) tensor(12439.3818, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12439.37890625
tensor(12439.3818, grad_fn=<NegBackward0>) tensor(12439.3789, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12439.3759765625
tensor(12439.3789, grad_fn=<NegBackward0>) tensor(12439.3760, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12439.3720703125
tensor(12439.3760, grad_fn=<NegBackward0>) tensor(12439.3721, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12439.3701171875
tensor(12439.3721, grad_fn=<NegBackward0>) tensor(12439.3701, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12439.3681640625
tensor(12439.3701, grad_fn=<NegBackward0>) tensor(12439.3682, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12439.3671875
tensor(12439.3682, grad_fn=<NegBackward0>) tensor(12439.3672, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12439.3662109375
tensor(12439.3672, grad_fn=<NegBackward0>) tensor(12439.3662, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12439.365234375
tensor(12439.3662, grad_fn=<NegBackward0>) tensor(12439.3652, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12439.3623046875
tensor(12439.3652, grad_fn=<NegBackward0>) tensor(12439.3623, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12439.361328125
tensor(12439.3623, grad_fn=<NegBackward0>) tensor(12439.3613, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12439.3603515625
tensor(12439.3613, grad_fn=<NegBackward0>) tensor(12439.3604, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12439.3583984375
tensor(12439.3604, grad_fn=<NegBackward0>) tensor(12439.3584, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12439.357421875
tensor(12439.3584, grad_fn=<NegBackward0>) tensor(12439.3574, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12439.35546875
tensor(12439.3574, grad_fn=<NegBackward0>) tensor(12439.3555, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12439.3544921875
tensor(12439.3555, grad_fn=<NegBackward0>) tensor(12439.3545, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12439.353515625
tensor(12439.3545, grad_fn=<NegBackward0>) tensor(12439.3535, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12439.3525390625
tensor(12439.3535, grad_fn=<NegBackward0>) tensor(12439.3525, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12439.349609375
tensor(12439.3525, grad_fn=<NegBackward0>) tensor(12439.3496, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12439.3486328125
tensor(12439.3496, grad_fn=<NegBackward0>) tensor(12439.3486, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12439.34765625
tensor(12439.3486, grad_fn=<NegBackward0>) tensor(12439.3477, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12439.34375
tensor(12439.3477, grad_fn=<NegBackward0>) tensor(12439.3438, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12439.341796875
tensor(12439.3438, grad_fn=<NegBackward0>) tensor(12439.3418, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12439.337890625
tensor(12439.3418, grad_fn=<NegBackward0>) tensor(12439.3379, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12439.3359375
tensor(12439.3379, grad_fn=<NegBackward0>) tensor(12439.3359, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12439.3291015625
tensor(12439.3359, grad_fn=<NegBackward0>) tensor(12439.3291, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12439.3251953125
tensor(12439.3291, grad_fn=<NegBackward0>) tensor(12439.3252, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12439.3212890625
tensor(12439.3252, grad_fn=<NegBackward0>) tensor(12439.3213, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12439.31640625
tensor(12439.3213, grad_fn=<NegBackward0>) tensor(12439.3164, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12439.3125
tensor(12439.3164, grad_fn=<NegBackward0>) tensor(12439.3125, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12439.3115234375
tensor(12439.3125, grad_fn=<NegBackward0>) tensor(12439.3115, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12439.3095703125
tensor(12439.3115, grad_fn=<NegBackward0>) tensor(12439.3096, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12439.3134765625
tensor(12439.3096, grad_fn=<NegBackward0>) tensor(12439.3135, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12439.3095703125
tensor(12439.3096, grad_fn=<NegBackward0>) tensor(12439.3096, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12439.3076171875
tensor(12439.3096, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12439.30859375
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12439.30859375
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12439.30859375
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12439.3076171875
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12439.31640625
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3164, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12439.3076171875
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12439.3505859375
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3506, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12439.3076171875
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12439.306640625
tensor(12439.3076, grad_fn=<NegBackward0>) tensor(12439.3066, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12439.30859375
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12439.30859375
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3086, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12439.3076171875
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3076, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -12439.3095703125
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3096, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -12439.3115234375
tensor(12439.3066, grad_fn=<NegBackward0>) tensor(12439.3115, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6900 due to no improvement.
pi: tensor([[0.0238, 0.9762],
        [0.2860, 0.7140]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9890, 0.0110], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2078, 0.2076],
         [0.5618, 0.1985]],

        [[0.5789, 0.1884],
         [0.6384, 0.6290]],

        [[0.5891, 0.2032],
         [0.6003, 0.5911]],

        [[0.7084, 0.2057],
         [0.5221, 0.7234]],

        [[0.6982, 0.2011],
         [0.7165, 0.6386]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001302378301401357
Average Adjusted Rand Index: 0.0
[-0.0001302378301401357, -0.0001302378301401357] [0.0, 0.0] [12439.3125, 12439.3115234375]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11809.158562724544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19482.373046875
inf tensor(19482.3730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12311.138671875
tensor(19482.3730, grad_fn=<NegBackward0>) tensor(12311.1387, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12310.970703125
tensor(12311.1387, grad_fn=<NegBackward0>) tensor(12310.9707, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12310.8994140625
tensor(12310.9707, grad_fn=<NegBackward0>) tensor(12310.8994, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12310.7998046875
tensor(12310.8994, grad_fn=<NegBackward0>) tensor(12310.7998, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12310.6044921875
tensor(12310.7998, grad_fn=<NegBackward0>) tensor(12310.6045, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12310.08984375
tensor(12310.6045, grad_fn=<NegBackward0>) tensor(12310.0898, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12308.9873046875
tensor(12310.0898, grad_fn=<NegBackward0>) tensor(12308.9873, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12308.5947265625
tensor(12308.9873, grad_fn=<NegBackward0>) tensor(12308.5947, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12308.470703125
tensor(12308.5947, grad_fn=<NegBackward0>) tensor(12308.4707, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12308.3720703125
tensor(12308.4707, grad_fn=<NegBackward0>) tensor(12308.3721, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12289.744140625
tensor(12308.3721, grad_fn=<NegBackward0>) tensor(12289.7441, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11980.6650390625
tensor(12289.7441, grad_fn=<NegBackward0>) tensor(11980.6650, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11910.2802734375
tensor(11980.6650, grad_fn=<NegBackward0>) tensor(11910.2803, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11853.087890625
tensor(11910.2803, grad_fn=<NegBackward0>) tensor(11853.0879, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11843.73046875
tensor(11853.0879, grad_fn=<NegBackward0>) tensor(11843.7305, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11840.1875
tensor(11843.7305, grad_fn=<NegBackward0>) tensor(11840.1875, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11830.0458984375
tensor(11840.1875, grad_fn=<NegBackward0>) tensor(11830.0459, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11802.74609375
tensor(11830.0459, grad_fn=<NegBackward0>) tensor(11802.7461, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11802.720703125
tensor(11802.7461, grad_fn=<NegBackward0>) tensor(11802.7207, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11802.7060546875
tensor(11802.7207, grad_fn=<NegBackward0>) tensor(11802.7061, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11802.6953125
tensor(11802.7061, grad_fn=<NegBackward0>) tensor(11802.6953, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11802.6875
tensor(11802.6953, grad_fn=<NegBackward0>) tensor(11802.6875, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11802.671875
tensor(11802.6875, grad_fn=<NegBackward0>) tensor(11802.6719, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11802.654296875
tensor(11802.6719, grad_fn=<NegBackward0>) tensor(11802.6543, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11800.8017578125
tensor(11802.6543, grad_fn=<NegBackward0>) tensor(11800.8018, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11800.7958984375
tensor(11800.8018, grad_fn=<NegBackward0>) tensor(11800.7959, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11800.79296875
tensor(11800.7959, grad_fn=<NegBackward0>) tensor(11800.7930, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11800.7900390625
tensor(11800.7930, grad_fn=<NegBackward0>) tensor(11800.7900, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11800.7861328125
tensor(11800.7900, grad_fn=<NegBackward0>) tensor(11800.7861, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11800.7783203125
tensor(11800.7861, grad_fn=<NegBackward0>) tensor(11800.7783, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11800.7021484375
tensor(11800.7783, grad_fn=<NegBackward0>) tensor(11800.7021, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11800.7001953125
tensor(11800.7021, grad_fn=<NegBackward0>) tensor(11800.7002, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11800.6884765625
tensor(11800.7002, grad_fn=<NegBackward0>) tensor(11800.6885, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11800.6875
tensor(11800.6885, grad_fn=<NegBackward0>) tensor(11800.6875, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11800.6875
tensor(11800.6875, grad_fn=<NegBackward0>) tensor(11800.6875, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11800.6845703125
tensor(11800.6875, grad_fn=<NegBackward0>) tensor(11800.6846, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11800.68359375
tensor(11800.6846, grad_fn=<NegBackward0>) tensor(11800.6836, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11800.6826171875
tensor(11800.6836, grad_fn=<NegBackward0>) tensor(11800.6826, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11800.69140625
tensor(11800.6826, grad_fn=<NegBackward0>) tensor(11800.6914, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -11800.6826171875
tensor(11800.6826, grad_fn=<NegBackward0>) tensor(11800.6826, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11800.681640625
tensor(11800.6826, grad_fn=<NegBackward0>) tensor(11800.6816, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11800.6806640625
tensor(11800.6816, grad_fn=<NegBackward0>) tensor(11800.6807, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11800.6787109375
tensor(11800.6807, grad_fn=<NegBackward0>) tensor(11800.6787, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11800.6796875
tensor(11800.6787, grad_fn=<NegBackward0>) tensor(11800.6797, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11800.6787109375
tensor(11800.6787, grad_fn=<NegBackward0>) tensor(11800.6787, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11800.6787109375
tensor(11800.6787, grad_fn=<NegBackward0>) tensor(11800.6787, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11800.677734375
tensor(11800.6787, grad_fn=<NegBackward0>) tensor(11800.6777, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11800.6767578125
tensor(11800.6777, grad_fn=<NegBackward0>) tensor(11800.6768, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11800.6767578125
tensor(11800.6768, grad_fn=<NegBackward0>) tensor(11800.6768, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11800.6767578125
tensor(11800.6768, grad_fn=<NegBackward0>) tensor(11800.6768, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11800.67578125
tensor(11800.6768, grad_fn=<NegBackward0>) tensor(11800.6758, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11800.67578125
tensor(11800.6758, grad_fn=<NegBackward0>) tensor(11800.6758, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11800.6767578125
tensor(11800.6758, grad_fn=<NegBackward0>) tensor(11800.6768, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11800.67578125
tensor(11800.6758, grad_fn=<NegBackward0>) tensor(11800.6758, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11800.6923828125
tensor(11800.6758, grad_fn=<NegBackward0>) tensor(11800.6924, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11800.6748046875
tensor(11800.6758, grad_fn=<NegBackward0>) tensor(11800.6748, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11800.673828125
tensor(11800.6748, grad_fn=<NegBackward0>) tensor(11800.6738, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11800.677734375
tensor(11800.6738, grad_fn=<NegBackward0>) tensor(11800.6777, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -11800.6826171875
tensor(11800.6738, grad_fn=<NegBackward0>) tensor(11800.6826, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -11800.6748046875
tensor(11800.6738, grad_fn=<NegBackward0>) tensor(11800.6748, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -11800.6728515625
tensor(11800.6738, grad_fn=<NegBackward0>) tensor(11800.6729, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11800.673828125
tensor(11800.6729, grad_fn=<NegBackward0>) tensor(11800.6738, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -11800.6748046875
tensor(11800.6729, grad_fn=<NegBackward0>) tensor(11800.6748, grad_fn=<NegBackward0>)
2
Iteration 6400: Loss = -11800.677734375
tensor(11800.6729, grad_fn=<NegBackward0>) tensor(11800.6777, grad_fn=<NegBackward0>)
3
Iteration 6500: Loss = -11800.673828125
tensor(11800.6729, grad_fn=<NegBackward0>) tensor(11800.6738, grad_fn=<NegBackward0>)
4
Iteration 6600: Loss = -11800.6728515625
tensor(11800.6729, grad_fn=<NegBackward0>) tensor(11800.6729, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11800.671875
tensor(11800.6729, grad_fn=<NegBackward0>) tensor(11800.6719, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11800.673828125
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6738, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11800.6904296875
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6904, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -11800.6728515625
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6729, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -11800.671875
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6719, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11800.673828125
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6738, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11800.67578125
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6758, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -11800.66015625
tensor(11800.6719, grad_fn=<NegBackward0>) tensor(11800.6602, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11800.669921875
tensor(11800.6602, grad_fn=<NegBackward0>) tensor(11800.6699, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11800.6591796875
tensor(11800.6602, grad_fn=<NegBackward0>) tensor(11800.6592, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11800.677734375
tensor(11800.6592, grad_fn=<NegBackward0>) tensor(11800.6777, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11800.658203125
tensor(11800.6592, grad_fn=<NegBackward0>) tensor(11800.6582, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11800.3896484375
tensor(11800.6582, grad_fn=<NegBackward0>) tensor(11800.3896, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11800.369140625
tensor(11800.3896, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11800.369140625
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11800.3720703125
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3721, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -11800.4287109375
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.4287, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -11800.3681640625
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3682, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11800.369140625
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11800.369140625
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -11800.369140625
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -11800.3681640625
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3682, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11800.365234375
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11800.39453125
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3945, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11800.365234375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11800.3662109375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11800.365234375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11800.365234375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11800.3671875
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11800.375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3750, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11800.3798828125
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3799, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11800.3662109375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
4
Iteration 9900: Loss = -11800.365234375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
pi: tensor([[0.2577, 0.7423],
        [0.6719, 0.3281]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4782, 0.5218], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2999, 0.0986],
         [0.6768, 0.2975]],

        [[0.5037, 0.0979],
         [0.7010, 0.6609]],

        [[0.7162, 0.0951],
         [0.6345, 0.6901]],

        [[0.6623, 0.1063],
         [0.6054, 0.6502]],

        [[0.5877, 0.0913],
         [0.7310, 0.7019]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03648609432902387
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23180.357421875
inf tensor(23180.3574, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12046.78125
tensor(23180.3574, grad_fn=<NegBackward0>) tensor(12046.7812, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11962.1923828125
tensor(12046.7812, grad_fn=<NegBackward0>) tensor(11962.1924, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11887.4833984375
tensor(11962.1924, grad_fn=<NegBackward0>) tensor(11887.4834, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11801.037109375
tensor(11887.4834, grad_fn=<NegBackward0>) tensor(11801.0371, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11800.8818359375
tensor(11801.0371, grad_fn=<NegBackward0>) tensor(11800.8818, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11800.8203125
tensor(11800.8818, grad_fn=<NegBackward0>) tensor(11800.8203, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11800.775390625
tensor(11800.8203, grad_fn=<NegBackward0>) tensor(11800.7754, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11800.4677734375
tensor(11800.7754, grad_fn=<NegBackward0>) tensor(11800.4678, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11800.44921875
tensor(11800.4678, grad_fn=<NegBackward0>) tensor(11800.4492, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11800.4345703125
tensor(11800.4492, grad_fn=<NegBackward0>) tensor(11800.4346, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11800.41796875
tensor(11800.4346, grad_fn=<NegBackward0>) tensor(11800.4180, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11800.4111328125
tensor(11800.4180, grad_fn=<NegBackward0>) tensor(11800.4111, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11800.40625
tensor(11800.4111, grad_fn=<NegBackward0>) tensor(11800.4062, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11800.40234375
tensor(11800.4062, grad_fn=<NegBackward0>) tensor(11800.4023, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11800.396484375
tensor(11800.4023, grad_fn=<NegBackward0>) tensor(11800.3965, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11800.390625
tensor(11800.3965, grad_fn=<NegBackward0>) tensor(11800.3906, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11800.388671875
tensor(11800.3906, grad_fn=<NegBackward0>) tensor(11800.3887, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11800.3857421875
tensor(11800.3887, grad_fn=<NegBackward0>) tensor(11800.3857, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11800.3857421875
tensor(11800.3857, grad_fn=<NegBackward0>) tensor(11800.3857, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11800.3828125
tensor(11800.3857, grad_fn=<NegBackward0>) tensor(11800.3828, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11800.380859375
tensor(11800.3828, grad_fn=<NegBackward0>) tensor(11800.3809, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11800.37890625
tensor(11800.3809, grad_fn=<NegBackward0>) tensor(11800.3789, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11800.37890625
tensor(11800.3789, grad_fn=<NegBackward0>) tensor(11800.3789, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11800.376953125
tensor(11800.3789, grad_fn=<NegBackward0>) tensor(11800.3770, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11800.3779296875
tensor(11800.3770, grad_fn=<NegBackward0>) tensor(11800.3779, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11800.375
tensor(11800.3770, grad_fn=<NegBackward0>) tensor(11800.3750, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11800.3740234375
tensor(11800.3750, grad_fn=<NegBackward0>) tensor(11800.3740, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11800.373046875
tensor(11800.3740, grad_fn=<NegBackward0>) tensor(11800.3730, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11800.3701171875
tensor(11800.3730, grad_fn=<NegBackward0>) tensor(11800.3701, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11800.37109375
tensor(11800.3701, grad_fn=<NegBackward0>) tensor(11800.3711, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11800.37109375
tensor(11800.3701, grad_fn=<NegBackward0>) tensor(11800.3711, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -11800.369140625
tensor(11800.3701, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11800.369140625
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11800.3818359375
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3818, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11800.369140625
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11800.3681640625
tensor(11800.3691, grad_fn=<NegBackward0>) tensor(11800.3682, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11800.369140625
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3691, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -11800.3681640625
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3682, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11800.3671875
tensor(11800.3682, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11800.3671875
tensor(11800.3672, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11800.3671875
tensor(11800.3672, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11800.3662109375
tensor(11800.3672, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11800.3662109375
tensor(11800.3662, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11800.3671875
tensor(11800.3662, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11800.3662109375
tensor(11800.3662, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11800.3662109375
tensor(11800.3662, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11800.365234375
tensor(11800.3662, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11800.3671875
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11800.365234375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11800.3662109375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11800.3671875
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11800.37109375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3711, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11800.375
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3750, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -11800.3642578125
tensor(11800.3652, grad_fn=<NegBackward0>) tensor(11800.3643, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11800.365234375
tensor(11800.3643, grad_fn=<NegBackward0>) tensor(11800.3652, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -11800.3671875
tensor(11800.3643, grad_fn=<NegBackward0>) tensor(11800.3672, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -11800.37109375
tensor(11800.3643, grad_fn=<NegBackward0>) tensor(11800.3711, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -11800.3701171875
tensor(11800.3643, grad_fn=<NegBackward0>) tensor(11800.3701, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -11800.3662109375
tensor(11800.3643, grad_fn=<NegBackward0>) tensor(11800.3662, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.3295, 0.6705],
        [0.7425, 0.2575]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5246, 0.4754], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2969, 0.0986],
         [0.6375, 0.3003]],

        [[0.5970, 0.0979],
         [0.5098, 0.5867]],

        [[0.5823, 0.0951],
         [0.5172, 0.5292]],

        [[0.6589, 0.1062],
         [0.6773, 0.6788]],

        [[0.5953, 0.0913],
         [0.5394, 0.5315]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03648609432902387
Average Adjusted Rand Index: 0.9919995611635631
[0.03648609432902387, 0.03648609432902387] [0.9919995611635631, 0.9919995611635631] [11800.3720703125, 11800.3662109375]
-------------------------------------
This iteration is 78
True Objective function: Loss = -11854.171832704336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21191.443359375
inf tensor(21191.4434, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12318.5830078125
tensor(21191.4434, grad_fn=<NegBackward0>) tensor(12318.5830, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12318.296875
tensor(12318.5830, grad_fn=<NegBackward0>) tensor(12318.2969, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12318.189453125
tensor(12318.2969, grad_fn=<NegBackward0>) tensor(12318.1895, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12318.0107421875
tensor(12318.1895, grad_fn=<NegBackward0>) tensor(12318.0107, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12317.080078125
tensor(12318.0107, grad_fn=<NegBackward0>) tensor(12317.0801, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12316.9013671875
tensor(12317.0801, grad_fn=<NegBackward0>) tensor(12316.9014, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12316.7509765625
tensor(12316.9014, grad_fn=<NegBackward0>) tensor(12316.7510, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12316.619140625
tensor(12316.7510, grad_fn=<NegBackward0>) tensor(12316.6191, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12316.5107421875
tensor(12316.6191, grad_fn=<NegBackward0>) tensor(12316.5107, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12316.4248046875
tensor(12316.5107, grad_fn=<NegBackward0>) tensor(12316.4248, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12316.3671875
tensor(12316.4248, grad_fn=<NegBackward0>) tensor(12316.3672, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12316.3271484375
tensor(12316.3672, grad_fn=<NegBackward0>) tensor(12316.3271, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12316.298828125
tensor(12316.3271, grad_fn=<NegBackward0>) tensor(12316.2988, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12316.2763671875
tensor(12316.2988, grad_fn=<NegBackward0>) tensor(12316.2764, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12316.2578125
tensor(12316.2764, grad_fn=<NegBackward0>) tensor(12316.2578, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12316.2392578125
tensor(12316.2578, grad_fn=<NegBackward0>) tensor(12316.2393, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12316.22265625
tensor(12316.2393, grad_fn=<NegBackward0>) tensor(12316.2227, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12316.208984375
tensor(12316.2227, grad_fn=<NegBackward0>) tensor(12316.2090, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12316.1962890625
tensor(12316.2090, grad_fn=<NegBackward0>) tensor(12316.1963, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12316.1826171875
tensor(12316.1963, grad_fn=<NegBackward0>) tensor(12316.1826, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12316.1728515625
tensor(12316.1826, grad_fn=<NegBackward0>) tensor(12316.1729, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12316.1640625
tensor(12316.1729, grad_fn=<NegBackward0>) tensor(12316.1641, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12316.15625
tensor(12316.1641, grad_fn=<NegBackward0>) tensor(12316.1562, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12316.15234375
tensor(12316.1562, grad_fn=<NegBackward0>) tensor(12316.1523, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12316.146484375
tensor(12316.1523, grad_fn=<NegBackward0>) tensor(12316.1465, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12316.1435546875
tensor(12316.1465, grad_fn=<NegBackward0>) tensor(12316.1436, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12316.1396484375
tensor(12316.1436, grad_fn=<NegBackward0>) tensor(12316.1396, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12316.1396484375
tensor(12316.1396, grad_fn=<NegBackward0>) tensor(12316.1396, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12316.1376953125
tensor(12316.1396, grad_fn=<NegBackward0>) tensor(12316.1377, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12316.13671875
tensor(12316.1377, grad_fn=<NegBackward0>) tensor(12316.1367, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12316.13671875
tensor(12316.1367, grad_fn=<NegBackward0>) tensor(12316.1367, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12316.134765625
tensor(12316.1367, grad_fn=<NegBackward0>) tensor(12316.1348, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12316.134765625
tensor(12316.1348, grad_fn=<NegBackward0>) tensor(12316.1348, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12316.1337890625
tensor(12316.1348, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12316.1328125
tensor(12316.1338, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12316.1337890625
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12316.1337890625
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -12316.1337890625
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -12316.1318359375
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12316.1328125
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12316.1318359375
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12316.130859375
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12316.1328125
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12316.1318359375
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -12316.1318359375
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
3
Iteration 4600: Loss = -12316.130859375
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12316.1328125
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12316.130859375
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12316.1298828125
tensor(12316.1309, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12316.130859375
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12316.12890625
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12316.1298828125
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12316.1298828125
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12316.1298828125
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12316.12890625
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12316.126953125
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12316.1298828125
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12316.1279296875
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1279, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12316.12890625
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -12316.1279296875
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1279, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -12316.12890625
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.0056, 0.9944],
        [0.0456, 0.9544]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0198, 0.9802], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2488, 0.3121],
         [0.6213, 0.1954]],

        [[0.5511, 0.2497],
         [0.5559, 0.6466]],

        [[0.6210, 0.1983],
         [0.5736, 0.6127]],

        [[0.6916, 0.2077],
         [0.7277, 0.6914]],

        [[0.5773, 0.2226],
         [0.6862, 0.5906]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007535694595274068
Average Adjusted Rand Index: -0.0016467245245553673
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22349.080078125
inf tensor(22349.0801, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12318.916015625
tensor(22349.0801, grad_fn=<NegBackward0>) tensor(12318.9160, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12318.48046875
tensor(12318.9160, grad_fn=<NegBackward0>) tensor(12318.4805, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12318.3349609375
tensor(12318.4805, grad_fn=<NegBackward0>) tensor(12318.3350, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12318.21484375
tensor(12318.3350, grad_fn=<NegBackward0>) tensor(12318.2148, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12317.8603515625
tensor(12318.2148, grad_fn=<NegBackward0>) tensor(12317.8604, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12316.8515625
tensor(12317.8604, grad_fn=<NegBackward0>) tensor(12316.8516, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12316.7275390625
tensor(12316.8516, grad_fn=<NegBackward0>) tensor(12316.7275, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12316.6220703125
tensor(12316.7275, grad_fn=<NegBackward0>) tensor(12316.6221, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12316.5322265625
tensor(12316.6221, grad_fn=<NegBackward0>) tensor(12316.5322, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12316.4619140625
tensor(12316.5322, grad_fn=<NegBackward0>) tensor(12316.4619, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12316.408203125
tensor(12316.4619, grad_fn=<NegBackward0>) tensor(12316.4082, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12316.37109375
tensor(12316.4082, grad_fn=<NegBackward0>) tensor(12316.3711, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12316.3408203125
tensor(12316.3711, grad_fn=<NegBackward0>) tensor(12316.3408, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12316.3173828125
tensor(12316.3408, grad_fn=<NegBackward0>) tensor(12316.3174, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12316.294921875
tensor(12316.3174, grad_fn=<NegBackward0>) tensor(12316.2949, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12316.2783203125
tensor(12316.2949, grad_fn=<NegBackward0>) tensor(12316.2783, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12316.2607421875
tensor(12316.2783, grad_fn=<NegBackward0>) tensor(12316.2607, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12316.2431640625
tensor(12316.2607, grad_fn=<NegBackward0>) tensor(12316.2432, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12316.2265625
tensor(12316.2432, grad_fn=<NegBackward0>) tensor(12316.2266, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12316.2119140625
tensor(12316.2266, grad_fn=<NegBackward0>) tensor(12316.2119, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12316.1982421875
tensor(12316.2119, grad_fn=<NegBackward0>) tensor(12316.1982, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12316.185546875
tensor(12316.1982, grad_fn=<NegBackward0>) tensor(12316.1855, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12316.1728515625
tensor(12316.1855, grad_fn=<NegBackward0>) tensor(12316.1729, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12316.166015625
tensor(12316.1729, grad_fn=<NegBackward0>) tensor(12316.1660, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12316.1572265625
tensor(12316.1660, grad_fn=<NegBackward0>) tensor(12316.1572, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12316.150390625
tensor(12316.1572, grad_fn=<NegBackward0>) tensor(12316.1504, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12316.146484375
tensor(12316.1504, grad_fn=<NegBackward0>) tensor(12316.1465, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12316.14453125
tensor(12316.1465, grad_fn=<NegBackward0>) tensor(12316.1445, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12316.140625
tensor(12316.1445, grad_fn=<NegBackward0>) tensor(12316.1406, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12316.138671875
tensor(12316.1406, grad_fn=<NegBackward0>) tensor(12316.1387, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12316.1376953125
tensor(12316.1387, grad_fn=<NegBackward0>) tensor(12316.1377, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12316.13671875
tensor(12316.1377, grad_fn=<NegBackward0>) tensor(12316.1367, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12316.13671875
tensor(12316.1367, grad_fn=<NegBackward0>) tensor(12316.1367, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12316.134765625
tensor(12316.1367, grad_fn=<NegBackward0>) tensor(12316.1348, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12316.1357421875
tensor(12316.1348, grad_fn=<NegBackward0>) tensor(12316.1357, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12316.134765625
tensor(12316.1348, grad_fn=<NegBackward0>) tensor(12316.1348, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12316.1337890625
tensor(12316.1348, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12316.134765625
tensor(12316.1338, grad_fn=<NegBackward0>) tensor(12316.1348, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12316.134765625
tensor(12316.1338, grad_fn=<NegBackward0>) tensor(12316.1348, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -12316.1337890625
tensor(12316.1338, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12316.1337890625
tensor(12316.1338, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12316.1328125
tensor(12316.1338, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12316.1337890625
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1338, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12316.1328125
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12316.1318359375
tensor(12316.1328, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12316.1318359375
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12316.1318359375
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12316.1328125
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1328, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12316.1298828125
tensor(12316.1318, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12316.130859375
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12316.130859375
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
2
Iteration 5300: Loss = -12316.1318359375
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1318, grad_fn=<NegBackward0>)
3
Iteration 5400: Loss = -12316.130859375
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
4
Iteration 5500: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12316.1298828125
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12316.12890625
tensor(12316.1299, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12316.12890625
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12316.12890625
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12316.12890625
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12316.1298828125
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12316.12890625
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12316.1298828125
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1299, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12316.12890625
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12316.1279296875
tensor(12316.1289, grad_fn=<NegBackward0>) tensor(12316.1279, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12316.126953125
tensor(12316.1279, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12316.126953125
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12316.12890625
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12316.12890625
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12316.126953125
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12316.1376953125
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1377, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12316.255859375
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.2559, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12316.126953125
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12316.12890625
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1289, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12316.2333984375
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.2334, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12316.1259765625
tensor(12316.1270, grad_fn=<NegBackward0>) tensor(12316.1260, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12316.2060546875
tensor(12316.1260, grad_fn=<NegBackward0>) tensor(12316.2061, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12316.126953125
tensor(12316.1260, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12316.130859375
tensor(12316.1260, grad_fn=<NegBackward0>) tensor(12316.1309, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -12316.126953125
tensor(12316.1260, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -12316.126953125
tensor(12316.1260, grad_fn=<NegBackward0>) tensor(12316.1270, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8100 due to no improvement.
pi: tensor([[0.0032, 0.9968],
        [0.0454, 0.9546]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0198, 0.9802], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2489, 0.3122],
         [0.5302, 0.1954]],

        [[0.6265, 0.2498],
         [0.7215, 0.6051]],

        [[0.6066, 0.1982],
         [0.5403, 0.7268]],

        [[0.6016, 0.2078],
         [0.5994, 0.6136]],

        [[0.7026, 0.2226],
         [0.5523, 0.7136]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: -0.008233622622776837
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0007535694595274068
Average Adjusted Rand Index: -0.0016467245245553673
[-0.0007535694595274068, -0.0007535694595274068] [-0.0016467245245553673, -0.0016467245245553673] [12316.12890625, 12316.126953125]
-------------------------------------
This iteration is 79
True Objective function: Loss = -11794.050287430035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22273.630859375
inf tensor(22273.6309, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12254.2880859375
tensor(22273.6309, grad_fn=<NegBackward0>) tensor(12254.2881, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12253.791015625
tensor(12254.2881, grad_fn=<NegBackward0>) tensor(12253.7910, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12253.6640625
tensor(12253.7910, grad_fn=<NegBackward0>) tensor(12253.6641, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12253.5791015625
tensor(12253.6641, grad_fn=<NegBackward0>) tensor(12253.5791, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12253.509765625
tensor(12253.5791, grad_fn=<NegBackward0>) tensor(12253.5098, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12253.443359375
tensor(12253.5098, grad_fn=<NegBackward0>) tensor(12253.4434, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12253.3720703125
tensor(12253.4434, grad_fn=<NegBackward0>) tensor(12253.3721, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12253.2783203125
tensor(12253.3721, grad_fn=<NegBackward0>) tensor(12253.2783, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12253.1337890625
tensor(12253.2783, grad_fn=<NegBackward0>) tensor(12253.1338, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12252.9736328125
tensor(12253.1338, grad_fn=<NegBackward0>) tensor(12252.9736, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12252.884765625
tensor(12252.9736, grad_fn=<NegBackward0>) tensor(12252.8848, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12252.837890625
tensor(12252.8848, grad_fn=<NegBackward0>) tensor(12252.8379, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12252.8115234375
tensor(12252.8379, grad_fn=<NegBackward0>) tensor(12252.8115, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12252.787109375
tensor(12252.8115, grad_fn=<NegBackward0>) tensor(12252.7871, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12252.767578125
tensor(12252.7871, grad_fn=<NegBackward0>) tensor(12252.7676, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12252.7529296875
tensor(12252.7676, grad_fn=<NegBackward0>) tensor(12252.7529, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12252.7392578125
tensor(12252.7529, grad_fn=<NegBackward0>) tensor(12252.7393, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12252.7255859375
tensor(12252.7393, grad_fn=<NegBackward0>) tensor(12252.7256, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12252.7119140625
tensor(12252.7256, grad_fn=<NegBackward0>) tensor(12252.7119, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12252.69921875
tensor(12252.7119, grad_fn=<NegBackward0>) tensor(12252.6992, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12252.68359375
tensor(12252.6992, grad_fn=<NegBackward0>) tensor(12252.6836, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12252.666015625
tensor(12252.6836, grad_fn=<NegBackward0>) tensor(12252.6660, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12252.646484375
tensor(12252.6660, grad_fn=<NegBackward0>) tensor(12252.6465, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12252.619140625
tensor(12252.6465, grad_fn=<NegBackward0>) tensor(12252.6191, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12252.5849609375
tensor(12252.6191, grad_fn=<NegBackward0>) tensor(12252.5850, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12252.5400390625
tensor(12252.5850, grad_fn=<NegBackward0>) tensor(12252.5400, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12252.48046875
tensor(12252.5400, grad_fn=<NegBackward0>) tensor(12252.4805, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12252.40625
tensor(12252.4805, grad_fn=<NegBackward0>) tensor(12252.4062, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12252.314453125
tensor(12252.4062, grad_fn=<NegBackward0>) tensor(12252.3145, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12252.2041015625
tensor(12252.3145, grad_fn=<NegBackward0>) tensor(12252.2041, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12252.0595703125
tensor(12252.2041, grad_fn=<NegBackward0>) tensor(12252.0596, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12251.712890625
tensor(12252.0596, grad_fn=<NegBackward0>) tensor(12251.7129, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12238.7998046875
tensor(12251.7129, grad_fn=<NegBackward0>) tensor(12238.7998, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12101.58203125
tensor(12238.7998, grad_fn=<NegBackward0>) tensor(12101.5820, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11906.1142578125
tensor(12101.5820, grad_fn=<NegBackward0>) tensor(11906.1143, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11905.8310546875
tensor(11906.1143, grad_fn=<NegBackward0>) tensor(11905.8311, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11905.7119140625
tensor(11905.8311, grad_fn=<NegBackward0>) tensor(11905.7119, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11901.90625
tensor(11905.7119, grad_fn=<NegBackward0>) tensor(11901.9062, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11901.1708984375
tensor(11901.9062, grad_fn=<NegBackward0>) tensor(11901.1709, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11897.6669921875
tensor(11901.1709, grad_fn=<NegBackward0>) tensor(11897.6670, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11869.9052734375
tensor(11897.6670, grad_fn=<NegBackward0>) tensor(11869.9053, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11866.4130859375
tensor(11869.9053, grad_fn=<NegBackward0>) tensor(11866.4131, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11853.1435546875
tensor(11866.4131, grad_fn=<NegBackward0>) tensor(11853.1436, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11844.2880859375
tensor(11853.1436, grad_fn=<NegBackward0>) tensor(11844.2881, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11833.61328125
tensor(11844.2881, grad_fn=<NegBackward0>) tensor(11833.6133, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11817.826171875
tensor(11833.6133, grad_fn=<NegBackward0>) tensor(11817.8262, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11816.54296875
tensor(11817.8262, grad_fn=<NegBackward0>) tensor(11816.5430, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11816.5283203125
tensor(11816.5430, grad_fn=<NegBackward0>) tensor(11816.5283, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11808.9423828125
tensor(11816.5283, grad_fn=<NegBackward0>) tensor(11808.9424, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11805.3466796875
tensor(11808.9424, grad_fn=<NegBackward0>) tensor(11805.3467, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11800.7861328125
tensor(11805.3467, grad_fn=<NegBackward0>) tensor(11800.7861, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11800.7333984375
tensor(11800.7861, grad_fn=<NegBackward0>) tensor(11800.7334, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11800.7294921875
tensor(11800.7334, grad_fn=<NegBackward0>) tensor(11800.7295, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11800.7255859375
tensor(11800.7295, grad_fn=<NegBackward0>) tensor(11800.7256, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11796.912109375
tensor(11800.7256, grad_fn=<NegBackward0>) tensor(11796.9121, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11787.3447265625
tensor(11796.9121, grad_fn=<NegBackward0>) tensor(11787.3447, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11787.341796875
tensor(11787.3447, grad_fn=<NegBackward0>) tensor(11787.3418, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11787.3408203125
tensor(11787.3418, grad_fn=<NegBackward0>) tensor(11787.3408, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11787.34375
tensor(11787.3408, grad_fn=<NegBackward0>) tensor(11787.3438, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11787.337890625
tensor(11787.3408, grad_fn=<NegBackward0>) tensor(11787.3379, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11784.11328125
tensor(11787.3379, grad_fn=<NegBackward0>) tensor(11784.1133, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11784.107421875
tensor(11784.1133, grad_fn=<NegBackward0>) tensor(11784.1074, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11784.1005859375
tensor(11784.1074, grad_fn=<NegBackward0>) tensor(11784.1006, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11784.1005859375
tensor(11784.1006, grad_fn=<NegBackward0>) tensor(11784.1006, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11784.099609375
tensor(11784.1006, grad_fn=<NegBackward0>) tensor(11784.0996, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11784.1025390625
tensor(11784.0996, grad_fn=<NegBackward0>) tensor(11784.1025, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -11784.1015625
tensor(11784.0996, grad_fn=<NegBackward0>) tensor(11784.1016, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -11784.099609375
tensor(11784.0996, grad_fn=<NegBackward0>) tensor(11784.0996, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11784.0986328125
tensor(11784.0996, grad_fn=<NegBackward0>) tensor(11784.0986, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11784.095703125
tensor(11784.0986, grad_fn=<NegBackward0>) tensor(11784.0957, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11784.0498046875
tensor(11784.0957, grad_fn=<NegBackward0>) tensor(11784.0498, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11784.037109375
tensor(11784.0498, grad_fn=<NegBackward0>) tensor(11784.0371, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11784.0361328125
tensor(11784.0371, grad_fn=<NegBackward0>) tensor(11784.0361, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11784.03515625
tensor(11784.0361, grad_fn=<NegBackward0>) tensor(11784.0352, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11784.037109375
tensor(11784.0352, grad_fn=<NegBackward0>) tensor(11784.0371, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11784.0341796875
tensor(11784.0352, grad_fn=<NegBackward0>) tensor(11784.0342, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11784.03515625
tensor(11784.0342, grad_fn=<NegBackward0>) tensor(11784.0352, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11784.0322265625
tensor(11784.0342, grad_fn=<NegBackward0>) tensor(11784.0322, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11784.037109375
tensor(11784.0322, grad_fn=<NegBackward0>) tensor(11784.0371, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11784.0322265625
tensor(11784.0322, grad_fn=<NegBackward0>) tensor(11784.0322, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11784.0830078125
tensor(11784.0322, grad_fn=<NegBackward0>) tensor(11784.0830, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11784.04296875
tensor(11784.0322, grad_fn=<NegBackward0>) tensor(11784.0430, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11784.0078125
tensor(11784.0322, grad_fn=<NegBackward0>) tensor(11784.0078, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11782.736328125
tensor(11784.0078, grad_fn=<NegBackward0>) tensor(11782.7363, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11782.548828125
tensor(11782.7363, grad_fn=<NegBackward0>) tensor(11782.5488, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11782.5478515625
tensor(11782.5488, grad_fn=<NegBackward0>) tensor(11782.5479, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11782.5478515625
tensor(11782.5479, grad_fn=<NegBackward0>) tensor(11782.5479, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11782.546875
tensor(11782.5479, grad_fn=<NegBackward0>) tensor(11782.5469, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11782.587890625
tensor(11782.5469, grad_fn=<NegBackward0>) tensor(11782.5879, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11782.5517578125
tensor(11782.5469, grad_fn=<NegBackward0>) tensor(11782.5518, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11782.55078125
tensor(11782.5469, grad_fn=<NegBackward0>) tensor(11782.5508, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11782.552734375
tensor(11782.5469, grad_fn=<NegBackward0>) tensor(11782.5527, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11782.5458984375
tensor(11782.5469, grad_fn=<NegBackward0>) tensor(11782.5459, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11782.6083984375
tensor(11782.5459, grad_fn=<NegBackward0>) tensor(11782.6084, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -11782.546875
tensor(11782.5459, grad_fn=<NegBackward0>) tensor(11782.5469, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -11782.552734375
tensor(11782.5459, grad_fn=<NegBackward0>) tensor(11782.5527, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -11782.5146484375
tensor(11782.5459, grad_fn=<NegBackward0>) tensor(11782.5146, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11782.5146484375
tensor(11782.5146, grad_fn=<NegBackward0>) tensor(11782.5146, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11782.6103515625
tensor(11782.5146, grad_fn=<NegBackward0>) tensor(11782.6104, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7374, 0.2626],
        [0.3050, 0.6950]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5256, 0.4744], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2916, 0.1109],
         [0.5710, 0.2947]],

        [[0.5800, 0.1022],
         [0.6357, 0.6704]],

        [[0.7300, 0.0903],
         [0.5260, 0.7232]],

        [[0.6376, 0.0960],
         [0.5558, 0.7171]],

        [[0.5335, 0.0950],
         [0.6389, 0.6587]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999854008352
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22511.8359375
inf tensor(22511.8359, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12253.8173828125
tensor(22511.8359, grad_fn=<NegBackward0>) tensor(12253.8174, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12253.5361328125
tensor(12253.8174, grad_fn=<NegBackward0>) tensor(12253.5361, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12253.4306640625
tensor(12253.5361, grad_fn=<NegBackward0>) tensor(12253.4307, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12253.3388671875
tensor(12253.4307, grad_fn=<NegBackward0>) tensor(12253.3389, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12253.2060546875
tensor(12253.3389, grad_fn=<NegBackward0>) tensor(12253.2061, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12253.03515625
tensor(12253.2061, grad_fn=<NegBackward0>) tensor(12253.0352, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12252.912109375
tensor(12253.0352, grad_fn=<NegBackward0>) tensor(12252.9121, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12252.8623046875
tensor(12252.9121, grad_fn=<NegBackward0>) tensor(12252.8623, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12252.837890625
tensor(12252.8623, grad_fn=<NegBackward0>) tensor(12252.8379, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12252.8203125
tensor(12252.8379, grad_fn=<NegBackward0>) tensor(12252.8203, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12252.8076171875
tensor(12252.8203, grad_fn=<NegBackward0>) tensor(12252.8076, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12252.796875
tensor(12252.8076, grad_fn=<NegBackward0>) tensor(12252.7969, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12252.787109375
tensor(12252.7969, grad_fn=<NegBackward0>) tensor(12252.7871, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12252.779296875
tensor(12252.7871, grad_fn=<NegBackward0>) tensor(12252.7793, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12252.7646484375
tensor(12252.7793, grad_fn=<NegBackward0>) tensor(12252.7646, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12252.7509765625
tensor(12252.7646, grad_fn=<NegBackward0>) tensor(12252.7510, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12252.7333984375
tensor(12252.7510, grad_fn=<NegBackward0>) tensor(12252.7334, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12252.70703125
tensor(12252.7334, grad_fn=<NegBackward0>) tensor(12252.7070, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12252.66796875
tensor(12252.7070, grad_fn=<NegBackward0>) tensor(12252.6680, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12252.599609375
tensor(12252.6680, grad_fn=<NegBackward0>) tensor(12252.5996, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12252.478515625
tensor(12252.5996, grad_fn=<NegBackward0>) tensor(12252.4785, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12252.298828125
tensor(12252.4785, grad_fn=<NegBackward0>) tensor(12252.2988, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12252.08203125
tensor(12252.2988, grad_fn=<NegBackward0>) tensor(12252.0820, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12251.6494140625
tensor(12252.0820, grad_fn=<NegBackward0>) tensor(12251.6494, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12218.3935546875
tensor(12251.6494, grad_fn=<NegBackward0>) tensor(12218.3936, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11922.740234375
tensor(12218.3936, grad_fn=<NegBackward0>) tensor(11922.7402, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11911.4619140625
tensor(11922.7402, grad_fn=<NegBackward0>) tensor(11911.4619, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11905.32421875
tensor(11911.4619, grad_fn=<NegBackward0>) tensor(11905.3242, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11905.26953125
tensor(11905.3242, grad_fn=<NegBackward0>) tensor(11905.2695, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11905.2236328125
tensor(11905.2695, grad_fn=<NegBackward0>) tensor(11905.2236, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11905.2099609375
tensor(11905.2236, grad_fn=<NegBackward0>) tensor(11905.2100, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11905.19921875
tensor(11905.2100, grad_fn=<NegBackward0>) tensor(11905.1992, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11905.185546875
tensor(11905.1992, grad_fn=<NegBackward0>) tensor(11905.1855, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11905.0625
tensor(11905.1855, grad_fn=<NegBackward0>) tensor(11905.0625, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11905.05859375
tensor(11905.0625, grad_fn=<NegBackward0>) tensor(11905.0586, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11905.0458984375
tensor(11905.0586, grad_fn=<NegBackward0>) tensor(11905.0459, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11905.03515625
tensor(11905.0459, grad_fn=<NegBackward0>) tensor(11905.0352, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11904.9208984375
tensor(11905.0352, grad_fn=<NegBackward0>) tensor(11904.9209, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11899.80078125
tensor(11904.9209, grad_fn=<NegBackward0>) tensor(11899.8008, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11898.7412109375
tensor(11899.8008, grad_fn=<NegBackward0>) tensor(11898.7412, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11897.9482421875
tensor(11898.7412, grad_fn=<NegBackward0>) tensor(11897.9482, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11897.4814453125
tensor(11897.9482, grad_fn=<NegBackward0>) tensor(11897.4814, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11897.416015625
tensor(11897.4814, grad_fn=<NegBackward0>) tensor(11897.4160, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11894.6669921875
tensor(11897.4160, grad_fn=<NegBackward0>) tensor(11894.6670, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11894.4873046875
tensor(11894.6670, grad_fn=<NegBackward0>) tensor(11894.4873, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11894.451171875
tensor(11894.4873, grad_fn=<NegBackward0>) tensor(11894.4512, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11894.3671875
tensor(11894.4512, grad_fn=<NegBackward0>) tensor(11894.3672, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11894.28515625
tensor(11894.3672, grad_fn=<NegBackward0>) tensor(11894.2852, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11894.2236328125
tensor(11894.2852, grad_fn=<NegBackward0>) tensor(11894.2236, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11894.220703125
tensor(11894.2236, grad_fn=<NegBackward0>) tensor(11894.2207, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11894.2099609375
tensor(11894.2207, grad_fn=<NegBackward0>) tensor(11894.2100, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11894.2060546875
tensor(11894.2100, grad_fn=<NegBackward0>) tensor(11894.2061, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11894.205078125
tensor(11894.2061, grad_fn=<NegBackward0>) tensor(11894.2051, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11894.201171875
tensor(11894.2051, grad_fn=<NegBackward0>) tensor(11894.2012, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11878.390625
tensor(11894.2012, grad_fn=<NegBackward0>) tensor(11878.3906, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11872.5009765625
tensor(11878.3906, grad_fn=<NegBackward0>) tensor(11872.5010, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11872.4951171875
tensor(11872.5010, grad_fn=<NegBackward0>) tensor(11872.4951, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11869.9609375
tensor(11872.4951, grad_fn=<NegBackward0>) tensor(11869.9609, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11869.94140625
tensor(11869.9609, grad_fn=<NegBackward0>) tensor(11869.9414, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11864.1259765625
tensor(11869.9414, grad_fn=<NegBackward0>) tensor(11864.1260, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11863.541015625
tensor(11864.1260, grad_fn=<NegBackward0>) tensor(11863.5410, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11863.53125
tensor(11863.5410, grad_fn=<NegBackward0>) tensor(11863.5312, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11860.048828125
tensor(11863.5312, grad_fn=<NegBackward0>) tensor(11860.0488, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11854.2333984375
tensor(11860.0488, grad_fn=<NegBackward0>) tensor(11854.2334, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11844.6298828125
tensor(11854.2334, grad_fn=<NegBackward0>) tensor(11844.6299, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11844.47265625
tensor(11844.6299, grad_fn=<NegBackward0>) tensor(11844.4727, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11842.6796875
tensor(11844.4727, grad_fn=<NegBackward0>) tensor(11842.6797, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11837.328125
tensor(11842.6797, grad_fn=<NegBackward0>) tensor(11837.3281, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11827.611328125
tensor(11837.3281, grad_fn=<NegBackward0>) tensor(11827.6113, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11827.59765625
tensor(11827.6113, grad_fn=<NegBackward0>) tensor(11827.5977, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11811.49609375
tensor(11827.5977, grad_fn=<NegBackward0>) tensor(11811.4961, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11811.46484375
tensor(11811.4961, grad_fn=<NegBackward0>) tensor(11811.4648, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11802.0546875
tensor(11811.4648, grad_fn=<NegBackward0>) tensor(11802.0547, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11802.048828125
tensor(11802.0547, grad_fn=<NegBackward0>) tensor(11802.0488, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11802.056640625
tensor(11802.0488, grad_fn=<NegBackward0>) tensor(11802.0566, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -11802.048828125
tensor(11802.0488, grad_fn=<NegBackward0>) tensor(11802.0488, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11802.046875
tensor(11802.0488, grad_fn=<NegBackward0>) tensor(11802.0469, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11802.0458984375
tensor(11802.0469, grad_fn=<NegBackward0>) tensor(11802.0459, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -11802.0478515625
tensor(11802.0459, grad_fn=<NegBackward0>) tensor(11802.0479, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -11802.046875
tensor(11802.0459, grad_fn=<NegBackward0>) tensor(11802.0469, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -11802.0458984375
tensor(11802.0459, grad_fn=<NegBackward0>) tensor(11802.0459, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11797.755859375
tensor(11802.0459, grad_fn=<NegBackward0>) tensor(11797.7559, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11797.7177734375
tensor(11797.7559, grad_fn=<NegBackward0>) tensor(11797.7178, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11797.71484375
tensor(11797.7178, grad_fn=<NegBackward0>) tensor(11797.7148, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11797.71875
tensor(11797.7148, grad_fn=<NegBackward0>) tensor(11797.7188, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -11797.7138671875
tensor(11797.7148, grad_fn=<NegBackward0>) tensor(11797.7139, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11797.712890625
tensor(11797.7139, grad_fn=<NegBackward0>) tensor(11797.7129, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11797.7255859375
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7256, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -11797.7138671875
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7139, grad_fn=<NegBackward0>)
2
Iteration 9000: Loss = -11797.712890625
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7129, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11797.744140625
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7441, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -11797.7275390625
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7275, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -11797.7734375
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7734, grad_fn=<NegBackward0>)
3
Iteration 9400: Loss = -11797.716796875
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7168, grad_fn=<NegBackward0>)
4
Iteration 9500: Loss = -11797.712890625
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7129, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11797.712890625
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7129, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11797.712890625
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7129, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11797.7255859375
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7256, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11797.71484375
tensor(11797.7129, grad_fn=<NegBackward0>) tensor(11797.7148, grad_fn=<NegBackward0>)
2
pi: tensor([[0.6837, 0.3163],
        [0.2692, 0.7308]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5090, 0.4910], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2898, 0.1149],
         [0.5299, 0.2945]],

        [[0.6351, 0.1021],
         [0.5824, 0.7217]],

        [[0.7070, 0.0901],
         [0.6256, 0.7168]],

        [[0.5824, 0.0963],
         [0.5192, 0.6865]],

        [[0.6231, 0.0951],
         [0.5740, 0.6005]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.960320666779223
Average Adjusted Rand Index: 0.9609677516362213
[0.9919999854008352, 0.960320666779223] [0.992, 0.9609677516362213] [11782.5, 11797.7138671875]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11970.116004451182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19757.935546875
inf tensor(19757.9355, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12482.96484375
tensor(19757.9355, grad_fn=<NegBackward0>) tensor(12482.9648, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12482.646484375
tensor(12482.9648, grad_fn=<NegBackward0>) tensor(12482.6465, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12482.5556640625
tensor(12482.6465, grad_fn=<NegBackward0>) tensor(12482.5557, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12482.4423828125
tensor(12482.5557, grad_fn=<NegBackward0>) tensor(12482.4424, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12482.279296875
tensor(12482.4424, grad_fn=<NegBackward0>) tensor(12482.2793, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12481.9912109375
tensor(12482.2793, grad_fn=<NegBackward0>) tensor(12481.9912, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12481.6376953125
tensor(12481.9912, grad_fn=<NegBackward0>) tensor(12481.6377, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12481.4111328125
tensor(12481.6377, grad_fn=<NegBackward0>) tensor(12481.4111, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12481.267578125
tensor(12481.4111, grad_fn=<NegBackward0>) tensor(12481.2676, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12481.087890625
tensor(12481.2676, grad_fn=<NegBackward0>) tensor(12481.0879, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12480.4091796875
tensor(12481.0879, grad_fn=<NegBackward0>) tensor(12480.4092, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12479.6748046875
tensor(12480.4092, grad_fn=<NegBackward0>) tensor(12479.6748, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12479.2392578125
tensor(12479.6748, grad_fn=<NegBackward0>) tensor(12479.2393, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12477.7451171875
tensor(12479.2393, grad_fn=<NegBackward0>) tensor(12477.7451, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12476.890625
tensor(12477.7451, grad_fn=<NegBackward0>) tensor(12476.8906, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12476.5947265625
tensor(12476.8906, grad_fn=<NegBackward0>) tensor(12476.5947, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12476.4697265625
tensor(12476.5947, grad_fn=<NegBackward0>) tensor(12476.4697, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12476.390625
tensor(12476.4697, grad_fn=<NegBackward0>) tensor(12476.3906, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12476.3330078125
tensor(12476.3906, grad_fn=<NegBackward0>) tensor(12476.3330, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12476.2880859375
tensor(12476.3330, grad_fn=<NegBackward0>) tensor(12476.2881, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12476.2607421875
tensor(12476.2881, grad_fn=<NegBackward0>) tensor(12476.2607, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12476.2392578125
tensor(12476.2607, grad_fn=<NegBackward0>) tensor(12476.2393, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12476.224609375
tensor(12476.2393, grad_fn=<NegBackward0>) tensor(12476.2246, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12476.21484375
tensor(12476.2246, grad_fn=<NegBackward0>) tensor(12476.2148, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12476.20703125
tensor(12476.2148, grad_fn=<NegBackward0>) tensor(12476.2070, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12476.1953125
tensor(12476.2070, grad_fn=<NegBackward0>) tensor(12476.1953, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12476.18359375
tensor(12476.1953, grad_fn=<NegBackward0>) tensor(12476.1836, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12476.173828125
tensor(12476.1836, grad_fn=<NegBackward0>) tensor(12476.1738, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12476.1591796875
tensor(12476.1738, grad_fn=<NegBackward0>) tensor(12476.1592, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12476.13671875
tensor(12476.1592, grad_fn=<NegBackward0>) tensor(12476.1367, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12476.083984375
tensor(12476.1367, grad_fn=<NegBackward0>) tensor(12476.0840, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12476.0703125
tensor(12476.0840, grad_fn=<NegBackward0>) tensor(12476.0703, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12476.0634765625
tensor(12476.0703, grad_fn=<NegBackward0>) tensor(12476.0635, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12476.0576171875
tensor(12476.0635, grad_fn=<NegBackward0>) tensor(12476.0576, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12476.037109375
tensor(12476.0576, grad_fn=<NegBackward0>) tensor(12476.0371, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12476.017578125
tensor(12476.0371, grad_fn=<NegBackward0>) tensor(12476.0176, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12476.01171875
tensor(12476.0176, grad_fn=<NegBackward0>) tensor(12476.0117, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12476.0107421875
tensor(12476.0117, grad_fn=<NegBackward0>) tensor(12476.0107, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12476.009765625
tensor(12476.0107, grad_fn=<NegBackward0>) tensor(12476.0098, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12476.0078125
tensor(12476.0098, grad_fn=<NegBackward0>) tensor(12476.0078, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12476.00390625
tensor(12476.0078, grad_fn=<NegBackward0>) tensor(12476.0039, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12476.001953125
tensor(12476.0039, grad_fn=<NegBackward0>) tensor(12476.0020, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12476.001953125
tensor(12476.0020, grad_fn=<NegBackward0>) tensor(12476.0020, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12475.9990234375
tensor(12476.0020, grad_fn=<NegBackward0>) tensor(12475.9990, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12475.998046875
tensor(12475.9990, grad_fn=<NegBackward0>) tensor(12475.9980, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12475.998046875
tensor(12475.9980, grad_fn=<NegBackward0>) tensor(12475.9980, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12475.998046875
tensor(12475.9980, grad_fn=<NegBackward0>) tensor(12475.9980, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12475.9951171875
tensor(12475.9980, grad_fn=<NegBackward0>) tensor(12475.9951, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12475.990234375
tensor(12475.9951, grad_fn=<NegBackward0>) tensor(12475.9902, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12475.9736328125
tensor(12475.9902, grad_fn=<NegBackward0>) tensor(12475.9736, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12475.9609375
tensor(12475.9736, grad_fn=<NegBackward0>) tensor(12475.9609, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12475.947265625
tensor(12475.9609, grad_fn=<NegBackward0>) tensor(12475.9473, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12475.9423828125
tensor(12475.9473, grad_fn=<NegBackward0>) tensor(12475.9424, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12475.9384765625
tensor(12475.9424, grad_fn=<NegBackward0>) tensor(12475.9385, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12475.9375
tensor(12475.9385, grad_fn=<NegBackward0>) tensor(12475.9375, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12475.931640625
tensor(12475.9375, grad_fn=<NegBackward0>) tensor(12475.9316, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12305.298828125
tensor(12475.9316, grad_fn=<NegBackward0>) tensor(12305.2988, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12129.783203125
tensor(12305.2988, grad_fn=<NegBackward0>) tensor(12129.7832, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12081.1259765625
tensor(12129.7832, grad_fn=<NegBackward0>) tensor(12081.1260, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12067.1181640625
tensor(12081.1260, grad_fn=<NegBackward0>) tensor(12067.1182, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12038.8271484375
tensor(12067.1182, grad_fn=<NegBackward0>) tensor(12038.8271, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12026.00390625
tensor(12038.8271, grad_fn=<NegBackward0>) tensor(12026.0039, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12025.9931640625
tensor(12026.0039, grad_fn=<NegBackward0>) tensor(12025.9932, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12025.984375
tensor(12025.9932, grad_fn=<NegBackward0>) tensor(12025.9844, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12016.916015625
tensor(12025.9844, grad_fn=<NegBackward0>) tensor(12016.9160, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12010.982421875
tensor(12016.9160, grad_fn=<NegBackward0>) tensor(12010.9824, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12010.97265625
tensor(12010.9824, grad_fn=<NegBackward0>) tensor(12010.9727, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12010.9736328125
tensor(12010.9727, grad_fn=<NegBackward0>) tensor(12010.9736, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12010.970703125
tensor(12010.9727, grad_fn=<NegBackward0>) tensor(12010.9707, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12010.9697265625
tensor(12010.9707, grad_fn=<NegBackward0>) tensor(12010.9697, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12008.23828125
tensor(12010.9697, grad_fn=<NegBackward0>) tensor(12008.2383, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12008.2373046875
tensor(12008.2383, grad_fn=<NegBackward0>) tensor(12008.2373, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12008.23828125
tensor(12008.2373, grad_fn=<NegBackward0>) tensor(12008.2383, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12008.23828125
tensor(12008.2373, grad_fn=<NegBackward0>) tensor(12008.2383, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12008.2265625
tensor(12008.2373, grad_fn=<NegBackward0>) tensor(12008.2266, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12008.2060546875
tensor(12008.2266, grad_fn=<NegBackward0>) tensor(12008.2061, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12008.20703125
tensor(12008.2061, grad_fn=<NegBackward0>) tensor(12008.2070, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12008.20703125
tensor(12008.2061, grad_fn=<NegBackward0>) tensor(12008.2070, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12008.20703125
tensor(12008.2061, grad_fn=<NegBackward0>) tensor(12008.2070, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -12008.2021484375
tensor(12008.2061, grad_fn=<NegBackward0>) tensor(12008.2021, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12007.634765625
tensor(12008.2021, grad_fn=<NegBackward0>) tensor(12007.6348, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12007.634765625
tensor(12007.6348, grad_fn=<NegBackward0>) tensor(12007.6348, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12007.6318359375
tensor(12007.6348, grad_fn=<NegBackward0>) tensor(12007.6318, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12007.642578125
tensor(12007.6318, grad_fn=<NegBackward0>) tensor(12007.6426, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12007.6318359375
tensor(12007.6318, grad_fn=<NegBackward0>) tensor(12007.6318, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12007.6064453125
tensor(12007.6318, grad_fn=<NegBackward0>) tensor(12007.6064, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12003.0361328125
tensor(12007.6064, grad_fn=<NegBackward0>) tensor(12003.0361, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11978.908203125
tensor(12003.0361, grad_fn=<NegBackward0>) tensor(11978.9082, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11978.892578125
tensor(11978.9082, grad_fn=<NegBackward0>) tensor(11978.8926, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11978.9189453125
tensor(11978.8926, grad_fn=<NegBackward0>) tensor(11978.9189, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11978.8916015625
tensor(11978.8926, grad_fn=<NegBackward0>) tensor(11978.8916, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11978.8916015625
tensor(11978.8916, grad_fn=<NegBackward0>) tensor(11978.8916, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11978.900390625
tensor(11978.8916, grad_fn=<NegBackward0>) tensor(11978.9004, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11978.8896484375
tensor(11978.8916, grad_fn=<NegBackward0>) tensor(11978.8896, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11978.84765625
tensor(11978.8896, grad_fn=<NegBackward0>) tensor(11978.8477, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11978.8515625
tensor(11978.8477, grad_fn=<NegBackward0>) tensor(11978.8516, grad_fn=<NegBackward0>)
1
Iteration 9700: Loss = -11978.8310546875
tensor(11978.8477, grad_fn=<NegBackward0>) tensor(11978.8311, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -11978.8525390625
tensor(11978.8311, grad_fn=<NegBackward0>) tensor(11978.8525, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -11978.8681640625
tensor(11978.8311, grad_fn=<NegBackward0>) tensor(11978.8682, grad_fn=<NegBackward0>)
2
pi: tensor([[0.7676, 0.2324],
        [0.2419, 0.7581]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5905, 0.4095], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3039, 0.1009],
         [0.5157, 0.2991]],

        [[0.6726, 0.1041],
         [0.6798, 0.5572]],

        [[0.5648, 0.1094],
         [0.6470, 0.5346]],

        [[0.7287, 0.0932],
         [0.5770, 0.6911]],

        [[0.5976, 0.1072],
         [0.5460, 0.5888]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9681910930009615
Average Adjusted Rand Index: 0.9679952658369197
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22415.9140625
inf tensor(22415.9141, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12482.8310546875
tensor(22415.9141, grad_fn=<NegBackward0>) tensor(12482.8311, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12482.6298828125
tensor(12482.8311, grad_fn=<NegBackward0>) tensor(12482.6299, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12482.5703125
tensor(12482.6299, grad_fn=<NegBackward0>) tensor(12482.5703, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12482.51953125
tensor(12482.5703, grad_fn=<NegBackward0>) tensor(12482.5195, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12482.451171875
tensor(12482.5195, grad_fn=<NegBackward0>) tensor(12482.4512, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12482.3388671875
tensor(12482.4512, grad_fn=<NegBackward0>) tensor(12482.3389, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12482.2685546875
tensor(12482.3389, grad_fn=<NegBackward0>) tensor(12482.2686, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12482.234375
tensor(12482.2686, grad_fn=<NegBackward0>) tensor(12482.2344, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12482.2021484375
tensor(12482.2344, grad_fn=<NegBackward0>) tensor(12482.2021, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12482.16796875
tensor(12482.2021, grad_fn=<NegBackward0>) tensor(12482.1680, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12482.1357421875
tensor(12482.1680, grad_fn=<NegBackward0>) tensor(12482.1357, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12482.09765625
tensor(12482.1357, grad_fn=<NegBackward0>) tensor(12482.0977, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12482.0556640625
tensor(12482.0977, grad_fn=<NegBackward0>) tensor(12482.0557, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12481.9990234375
tensor(12482.0557, grad_fn=<NegBackward0>) tensor(12481.9990, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12481.916015625
tensor(12481.9990, grad_fn=<NegBackward0>) tensor(12481.9160, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12481.7783203125
tensor(12481.9160, grad_fn=<NegBackward0>) tensor(12481.7783, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12481.57421875
tensor(12481.7783, grad_fn=<NegBackward0>) tensor(12481.5742, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12481.3974609375
tensor(12481.5742, grad_fn=<NegBackward0>) tensor(12481.3975, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12481.265625
tensor(12481.3975, grad_fn=<NegBackward0>) tensor(12481.2656, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12481.1083984375
tensor(12481.2656, grad_fn=<NegBackward0>) tensor(12481.1084, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12480.4052734375
tensor(12481.1084, grad_fn=<NegBackward0>) tensor(12480.4053, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12479.5986328125
tensor(12480.4053, grad_fn=<NegBackward0>) tensor(12479.5986, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12478.330078125
tensor(12479.5986, grad_fn=<NegBackward0>) tensor(12478.3301, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12477.2197265625
tensor(12478.3301, grad_fn=<NegBackward0>) tensor(12477.2197, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12476.63671875
tensor(12477.2197, grad_fn=<NegBackward0>) tensor(12476.6367, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12316.84375
tensor(12476.6367, grad_fn=<NegBackward0>) tensor(12316.8438, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12275.6630859375
tensor(12316.8438, grad_fn=<NegBackward0>) tensor(12275.6631, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12250.1181640625
tensor(12275.6631, grad_fn=<NegBackward0>) tensor(12250.1182, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12239.7216796875
tensor(12250.1182, grad_fn=<NegBackward0>) tensor(12239.7217, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12221.576171875
tensor(12239.7217, grad_fn=<NegBackward0>) tensor(12221.5762, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12210.5927734375
tensor(12221.5762, grad_fn=<NegBackward0>) tensor(12210.5928, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12210.33203125
tensor(12210.5928, grad_fn=<NegBackward0>) tensor(12210.3320, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12199.296875
tensor(12210.3320, grad_fn=<NegBackward0>) tensor(12199.2969, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12185.662109375
tensor(12199.2969, grad_fn=<NegBackward0>) tensor(12185.6621, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12184.0400390625
tensor(12185.6621, grad_fn=<NegBackward0>) tensor(12184.0400, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12183.9755859375
tensor(12184.0400, grad_fn=<NegBackward0>) tensor(12183.9756, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12177.6826171875
tensor(12183.9756, grad_fn=<NegBackward0>) tensor(12177.6826, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12177.443359375
tensor(12177.6826, grad_fn=<NegBackward0>) tensor(12177.4434, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12177.4384765625
tensor(12177.4434, grad_fn=<NegBackward0>) tensor(12177.4385, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12177.4375
tensor(12177.4385, grad_fn=<NegBackward0>) tensor(12177.4375, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12177.4345703125
tensor(12177.4375, grad_fn=<NegBackward0>) tensor(12177.4346, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12177.43359375
tensor(12177.4346, grad_fn=<NegBackward0>) tensor(12177.4336, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12177.431640625
tensor(12177.4336, grad_fn=<NegBackward0>) tensor(12177.4316, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12177.4306640625
tensor(12177.4316, grad_fn=<NegBackward0>) tensor(12177.4307, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12177.4296875
tensor(12177.4307, grad_fn=<NegBackward0>) tensor(12177.4297, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12177.4267578125
tensor(12177.4297, grad_fn=<NegBackward0>) tensor(12177.4268, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12177.423828125
tensor(12177.4268, grad_fn=<NegBackward0>) tensor(12177.4238, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12177.421875
tensor(12177.4238, grad_fn=<NegBackward0>) tensor(12177.4219, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12177.4013671875
tensor(12177.4219, grad_fn=<NegBackward0>) tensor(12177.4014, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12173.7578125
tensor(12177.4014, grad_fn=<NegBackward0>) tensor(12173.7578, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12173.5458984375
tensor(12173.7578, grad_fn=<NegBackward0>) tensor(12173.5459, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12173.4814453125
tensor(12173.5459, grad_fn=<NegBackward0>) tensor(12173.4814, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12173.458984375
tensor(12173.4814, grad_fn=<NegBackward0>) tensor(12173.4590, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12173.28515625
tensor(12173.4590, grad_fn=<NegBackward0>) tensor(12173.2852, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12173.2822265625
tensor(12173.2852, grad_fn=<NegBackward0>) tensor(12173.2822, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12173.2783203125
tensor(12173.2822, grad_fn=<NegBackward0>) tensor(12173.2783, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12171.6318359375
tensor(12173.2783, grad_fn=<NegBackward0>) tensor(12171.6318, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12171.2333984375
tensor(12171.6318, grad_fn=<NegBackward0>) tensor(12171.2334, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12169.57421875
tensor(12171.2334, grad_fn=<NegBackward0>) tensor(12169.5742, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12169.5634765625
tensor(12169.5742, grad_fn=<NegBackward0>) tensor(12169.5635, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12152.05078125
tensor(12169.5635, grad_fn=<NegBackward0>) tensor(12152.0508, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12151.779296875
tensor(12152.0508, grad_fn=<NegBackward0>) tensor(12151.7793, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12151.77734375
tensor(12151.7793, grad_fn=<NegBackward0>) tensor(12151.7773, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12138.2041015625
tensor(12151.7773, grad_fn=<NegBackward0>) tensor(12138.2041, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12137.8671875
tensor(12138.2041, grad_fn=<NegBackward0>) tensor(12137.8672, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12121.96484375
tensor(12137.8672, grad_fn=<NegBackward0>) tensor(12121.9648, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12121.71484375
tensor(12121.9648, grad_fn=<NegBackward0>) tensor(12121.7148, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12121.7138671875
tensor(12121.7148, grad_fn=<NegBackward0>) tensor(12121.7139, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12114.447265625
tensor(12121.7139, grad_fn=<NegBackward0>) tensor(12114.4473, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12114.443359375
tensor(12114.4473, grad_fn=<NegBackward0>) tensor(12114.4434, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12114.41015625
tensor(12114.4434, grad_fn=<NegBackward0>) tensor(12114.4102, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12108.275390625
tensor(12114.4102, grad_fn=<NegBackward0>) tensor(12108.2754, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12091.419921875
tensor(12108.2754, grad_fn=<NegBackward0>) tensor(12091.4199, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12078.32421875
tensor(12091.4199, grad_fn=<NegBackward0>) tensor(12078.3242, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12053.5888671875
tensor(12078.3242, grad_fn=<NegBackward0>) tensor(12053.5889, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12053.580078125
tensor(12053.5889, grad_fn=<NegBackward0>) tensor(12053.5801, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12040.9033203125
tensor(12053.5801, grad_fn=<NegBackward0>) tensor(12040.9033, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12035.7216796875
tensor(12040.9033, grad_fn=<NegBackward0>) tensor(12035.7217, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12035.595703125
tensor(12035.7217, grad_fn=<NegBackward0>) tensor(12035.5957, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12000.5244140625
tensor(12035.5957, grad_fn=<NegBackward0>) tensor(12000.5244, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12000.4697265625
tensor(12000.5244, grad_fn=<NegBackward0>) tensor(12000.4697, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12000.4677734375
tensor(12000.4697, grad_fn=<NegBackward0>) tensor(12000.4678, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11991.6181640625
tensor(12000.4678, grad_fn=<NegBackward0>) tensor(11991.6182, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11991.623046875
tensor(11991.6182, grad_fn=<NegBackward0>) tensor(11991.6230, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11991.62890625
tensor(11991.6182, grad_fn=<NegBackward0>) tensor(11991.6289, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -11991.6181640625
tensor(11991.6182, grad_fn=<NegBackward0>) tensor(11991.6182, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11981.6962890625
tensor(11991.6182, grad_fn=<NegBackward0>) tensor(11981.6963, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11981.5615234375
tensor(11981.6963, grad_fn=<NegBackward0>) tensor(11981.5615, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11981.568359375
tensor(11981.5615, grad_fn=<NegBackward0>) tensor(11981.5684, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11981.5615234375
tensor(11981.5615, grad_fn=<NegBackward0>) tensor(11981.5615, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11981.5615234375
tensor(11981.5615, grad_fn=<NegBackward0>) tensor(11981.5615, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11981.568359375
tensor(11981.5615, grad_fn=<NegBackward0>) tensor(11981.5684, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11981.556640625
tensor(11981.5615, grad_fn=<NegBackward0>) tensor(11981.5566, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11965.22265625
tensor(11981.5566, grad_fn=<NegBackward0>) tensor(11965.2227, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11959.7626953125
tensor(11965.2227, grad_fn=<NegBackward0>) tensor(11959.7627, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11959.76171875
tensor(11959.7627, grad_fn=<NegBackward0>) tensor(11959.7617, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11959.771484375
tensor(11959.7617, grad_fn=<NegBackward0>) tensor(11959.7715, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11959.76171875
tensor(11959.7617, grad_fn=<NegBackward0>) tensor(11959.7617, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11959.791015625
tensor(11959.7617, grad_fn=<NegBackward0>) tensor(11959.7910, grad_fn=<NegBackward0>)
1
pi: tensor([[0.7480, 0.2520],
        [0.2425, 0.7575]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4199, 0.5801], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2985, 0.0990],
         [0.6549, 0.3071]],

        [[0.5082, 0.1016],
         [0.5274, 0.5054]],

        [[0.6960, 0.1087],
         [0.5505, 0.6161]],

        [[0.6010, 0.0932],
         [0.6704, 0.5491]],

        [[0.7228, 0.1075],
         [0.5874, 0.6382]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9760956584656854
Average Adjusted Rand Index: 0.9759989969312853
[0.9681910930009615, 0.9760956584656854] [0.9679952658369197, 0.9759989969312853] [11978.833984375, 11959.87890625]
-------------------------------------
This iteration is 81
True Objective function: Loss = -11989.228977843703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22580.373046875
inf tensor(22580.3730, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12466.7275390625
tensor(22580.3730, grad_fn=<NegBackward0>) tensor(12466.7275, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12466.138671875
tensor(12466.7275, grad_fn=<NegBackward0>) tensor(12466.1387, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12465.279296875
tensor(12466.1387, grad_fn=<NegBackward0>) tensor(12465.2793, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12464.6455078125
tensor(12465.2793, grad_fn=<NegBackward0>) tensor(12464.6455, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12464.361328125
tensor(12464.6455, grad_fn=<NegBackward0>) tensor(12464.3613, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12464.1884765625
tensor(12464.3613, grad_fn=<NegBackward0>) tensor(12464.1885, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12464.0810546875
tensor(12464.1885, grad_fn=<NegBackward0>) tensor(12464.0811, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12464.0009765625
tensor(12464.0811, grad_fn=<NegBackward0>) tensor(12464.0010, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12463.9287109375
tensor(12464.0010, grad_fn=<NegBackward0>) tensor(12463.9287, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12463.8671875
tensor(12463.9287, grad_fn=<NegBackward0>) tensor(12463.8672, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12463.8125
tensor(12463.8672, grad_fn=<NegBackward0>) tensor(12463.8125, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12463.76953125
tensor(12463.8125, grad_fn=<NegBackward0>) tensor(12463.7695, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12463.7353515625
tensor(12463.7695, grad_fn=<NegBackward0>) tensor(12463.7354, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12463.7119140625
tensor(12463.7354, grad_fn=<NegBackward0>) tensor(12463.7119, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12463.6953125
tensor(12463.7119, grad_fn=<NegBackward0>) tensor(12463.6953, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12463.68359375
tensor(12463.6953, grad_fn=<NegBackward0>) tensor(12463.6836, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12463.673828125
tensor(12463.6836, grad_fn=<NegBackward0>) tensor(12463.6738, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12463.6650390625
tensor(12463.6738, grad_fn=<NegBackward0>) tensor(12463.6650, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12463.65625
tensor(12463.6650, grad_fn=<NegBackward0>) tensor(12463.6562, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12463.646484375
tensor(12463.6562, grad_fn=<NegBackward0>) tensor(12463.6465, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12463.62890625
tensor(12463.6465, grad_fn=<NegBackward0>) tensor(12463.6289, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12463.599609375
tensor(12463.6289, grad_fn=<NegBackward0>) tensor(12463.5996, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12463.5283203125
tensor(12463.5996, grad_fn=<NegBackward0>) tensor(12463.5283, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12463.3330078125
tensor(12463.5283, grad_fn=<NegBackward0>) tensor(12463.3330, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12462.90234375
tensor(12463.3330, grad_fn=<NegBackward0>) tensor(12462.9023, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12462.4794921875
tensor(12462.9023, grad_fn=<NegBackward0>) tensor(12462.4795, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12462.1708984375
tensor(12462.4795, grad_fn=<NegBackward0>) tensor(12462.1709, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12461.9169921875
tensor(12462.1709, grad_fn=<NegBackward0>) tensor(12461.9170, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12461.6787109375
tensor(12461.9170, grad_fn=<NegBackward0>) tensor(12461.6787, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12393.875
tensor(12461.6787, grad_fn=<NegBackward0>) tensor(12393.8750, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12235.529296875
tensor(12393.8750, grad_fn=<NegBackward0>) tensor(12235.5293, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12200.9560546875
tensor(12235.5293, grad_fn=<NegBackward0>) tensor(12200.9561, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12184.8857421875
tensor(12200.9561, grad_fn=<NegBackward0>) tensor(12184.8857, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12176.0966796875
tensor(12184.8857, grad_fn=<NegBackward0>) tensor(12176.0967, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12175.892578125
tensor(12176.0967, grad_fn=<NegBackward0>) tensor(12175.8926, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12161.611328125
tensor(12175.8926, grad_fn=<NegBackward0>) tensor(12161.6113, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12151.7109375
tensor(12161.6113, grad_fn=<NegBackward0>) tensor(12151.7109, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12129.81640625
tensor(12151.7109, grad_fn=<NegBackward0>) tensor(12129.8164, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12117.1865234375
tensor(12129.8164, grad_fn=<NegBackward0>) tensor(12117.1865, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12110.2724609375
tensor(12117.1865, grad_fn=<NegBackward0>) tensor(12110.2725, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12109.7578125
tensor(12110.2725, grad_fn=<NegBackward0>) tensor(12109.7578, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12107.1982421875
tensor(12109.7578, grad_fn=<NegBackward0>) tensor(12107.1982, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12107.0322265625
tensor(12107.1982, grad_fn=<NegBackward0>) tensor(12107.0322, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12096.71484375
tensor(12107.0322, grad_fn=<NegBackward0>) tensor(12096.7148, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12090.05859375
tensor(12096.7148, grad_fn=<NegBackward0>) tensor(12090.0586, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12087.369140625
tensor(12090.0586, grad_fn=<NegBackward0>) tensor(12087.3691, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12076.9326171875
tensor(12087.3691, grad_fn=<NegBackward0>) tensor(12076.9326, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12076.90625
tensor(12076.9326, grad_fn=<NegBackward0>) tensor(12076.9062, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12076.8994140625
tensor(12076.9062, grad_fn=<NegBackward0>) tensor(12076.8994, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12076.78125
tensor(12076.8994, grad_fn=<NegBackward0>) tensor(12076.7812, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12075.9013671875
tensor(12076.7812, grad_fn=<NegBackward0>) tensor(12075.9014, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12063.8388671875
tensor(12075.9014, grad_fn=<NegBackward0>) tensor(12063.8389, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12063.80078125
tensor(12063.8389, grad_fn=<NegBackward0>) tensor(12063.8008, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12063.796875
tensor(12063.8008, grad_fn=<NegBackward0>) tensor(12063.7969, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12063.70703125
tensor(12063.7969, grad_fn=<NegBackward0>) tensor(12063.7070, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12063.7021484375
tensor(12063.7070, grad_fn=<NegBackward0>) tensor(12063.7021, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12063.7021484375
tensor(12063.7021, grad_fn=<NegBackward0>) tensor(12063.7021, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12063.7001953125
tensor(12063.7021, grad_fn=<NegBackward0>) tensor(12063.7002, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12060.068359375
tensor(12063.7002, grad_fn=<NegBackward0>) tensor(12060.0684, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12057.9677734375
tensor(12060.0684, grad_fn=<NegBackward0>) tensor(12057.9678, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12057.9638671875
tensor(12057.9678, grad_fn=<NegBackward0>) tensor(12057.9639, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12057.962890625
tensor(12057.9639, grad_fn=<NegBackward0>) tensor(12057.9629, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12057.9609375
tensor(12057.9629, grad_fn=<NegBackward0>) tensor(12057.9609, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12057.9609375
tensor(12057.9609, grad_fn=<NegBackward0>) tensor(12057.9609, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12045.5615234375
tensor(12057.9609, grad_fn=<NegBackward0>) tensor(12045.5615, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12045.4716796875
tensor(12045.5615, grad_fn=<NegBackward0>) tensor(12045.4717, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12045.4814453125
tensor(12045.4717, grad_fn=<NegBackward0>) tensor(12045.4814, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12045.4716796875
tensor(12045.4717, grad_fn=<NegBackward0>) tensor(12045.4717, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12026.095703125
tensor(12045.4717, grad_fn=<NegBackward0>) tensor(12026.0957, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12026.0888671875
tensor(12026.0957, grad_fn=<NegBackward0>) tensor(12026.0889, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12026.0888671875
tensor(12026.0889, grad_fn=<NegBackward0>) tensor(12026.0889, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12026.087890625
tensor(12026.0889, grad_fn=<NegBackward0>) tensor(12026.0879, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12026.091796875
tensor(12026.0879, grad_fn=<NegBackward0>) tensor(12026.0918, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12026.0869140625
tensor(12026.0879, grad_fn=<NegBackward0>) tensor(12026.0869, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12015.4931640625
tensor(12026.0869, grad_fn=<NegBackward0>) tensor(12015.4932, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12015.4482421875
tensor(12015.4932, grad_fn=<NegBackward0>) tensor(12015.4482, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12015.4892578125
tensor(12015.4482, grad_fn=<NegBackward0>) tensor(12015.4893, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12015.4560546875
tensor(12015.4482, grad_fn=<NegBackward0>) tensor(12015.4561, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12015.4462890625
tensor(12015.4482, grad_fn=<NegBackward0>) tensor(12015.4463, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12015.4453125
tensor(12015.4463, grad_fn=<NegBackward0>) tensor(12015.4453, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12015.4638671875
tensor(12015.4453, grad_fn=<NegBackward0>) tensor(12015.4639, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12015.4521484375
tensor(12015.4453, grad_fn=<NegBackward0>) tensor(12015.4521, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12015.4443359375
tensor(12015.4453, grad_fn=<NegBackward0>) tensor(12015.4443, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12015.423828125
tensor(12015.4443, grad_fn=<NegBackward0>) tensor(12015.4238, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12015.42578125
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4258, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12015.42578125
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4258, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12015.55859375
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.5586, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12015.423828125
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4238, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12015.4296875
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4297, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -12015.4248046875
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4248, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -12015.4248046875
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4248, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -12015.4482421875
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4482, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -12015.4228515625
tensor(12015.4238, grad_fn=<NegBackward0>) tensor(12015.4229, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12015.4296875
tensor(12015.4229, grad_fn=<NegBackward0>) tensor(12015.4297, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12015.462890625
tensor(12015.4229, grad_fn=<NegBackward0>) tensor(12015.4629, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12015.4228515625
tensor(12015.4229, grad_fn=<NegBackward0>) tensor(12015.4229, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12015.42578125
tensor(12015.4229, grad_fn=<NegBackward0>) tensor(12015.4258, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12015.4375
tensor(12015.4229, grad_fn=<NegBackward0>) tensor(12015.4375, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12015.423828125
tensor(12015.4229, grad_fn=<NegBackward0>) tensor(12015.4238, grad_fn=<NegBackward0>)
3
pi: tensor([[0.5376, 0.4624],
        [0.3238, 0.6762]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5744, 0.4256], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2966, 0.1049],
         [0.6587, 0.3013]],

        [[0.6611, 0.1006],
         [0.5706, 0.6844]],

        [[0.6069, 0.0973],
         [0.5952, 0.7229]],

        [[0.5660, 0.1048],
         [0.6440, 0.6353]],

        [[0.6441, 0.1034],
         [0.6777, 0.5732]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.358648135930369
Average Adjusted Rand Index: 0.9683221230253964
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21579.65234375
inf tensor(21579.6523, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12466.724609375
tensor(21579.6523, grad_fn=<NegBackward0>) tensor(12466.7246, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12466.166015625
tensor(12466.7246, grad_fn=<NegBackward0>) tensor(12466.1660, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12465.60546875
tensor(12466.1660, grad_fn=<NegBackward0>) tensor(12465.6055, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12465.083984375
tensor(12465.6055, grad_fn=<NegBackward0>) tensor(12465.0840, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12464.693359375
tensor(12465.0840, grad_fn=<NegBackward0>) tensor(12464.6934, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12464.3740234375
tensor(12464.6934, grad_fn=<NegBackward0>) tensor(12464.3740, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12464.1748046875
tensor(12464.3740, grad_fn=<NegBackward0>) tensor(12464.1748, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12464.0615234375
tensor(12464.1748, grad_fn=<NegBackward0>) tensor(12464.0615, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12463.984375
tensor(12464.0615, grad_fn=<NegBackward0>) tensor(12463.9844, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12463.9228515625
tensor(12463.9844, grad_fn=<NegBackward0>) tensor(12463.9229, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12463.87109375
tensor(12463.9229, grad_fn=<NegBackward0>) tensor(12463.8711, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12463.826171875
tensor(12463.8711, grad_fn=<NegBackward0>) tensor(12463.8262, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12463.7919921875
tensor(12463.8262, grad_fn=<NegBackward0>) tensor(12463.7920, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12463.763671875
tensor(12463.7920, grad_fn=<NegBackward0>) tensor(12463.7637, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12463.7412109375
tensor(12463.7637, grad_fn=<NegBackward0>) tensor(12463.7412, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12463.7236328125
tensor(12463.7412, grad_fn=<NegBackward0>) tensor(12463.7236, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12463.7109375
tensor(12463.7236, grad_fn=<NegBackward0>) tensor(12463.7109, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12463.703125
tensor(12463.7109, grad_fn=<NegBackward0>) tensor(12463.7031, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12463.6982421875
tensor(12463.7031, grad_fn=<NegBackward0>) tensor(12463.6982, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12463.6953125
tensor(12463.6982, grad_fn=<NegBackward0>) tensor(12463.6953, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12463.69140625
tensor(12463.6953, grad_fn=<NegBackward0>) tensor(12463.6914, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12463.6884765625
tensor(12463.6914, grad_fn=<NegBackward0>) tensor(12463.6885, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12463.689453125
tensor(12463.6885, grad_fn=<NegBackward0>) tensor(12463.6895, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -12463.685546875
tensor(12463.6885, grad_fn=<NegBackward0>) tensor(12463.6855, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12463.685546875
tensor(12463.6855, grad_fn=<NegBackward0>) tensor(12463.6855, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12463.6845703125
tensor(12463.6855, grad_fn=<NegBackward0>) tensor(12463.6846, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12463.6826171875
tensor(12463.6846, grad_fn=<NegBackward0>) tensor(12463.6826, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12463.6806640625
tensor(12463.6826, grad_fn=<NegBackward0>) tensor(12463.6807, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12463.6796875
tensor(12463.6807, grad_fn=<NegBackward0>) tensor(12463.6797, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12463.677734375
tensor(12463.6797, grad_fn=<NegBackward0>) tensor(12463.6777, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12463.6748046875
tensor(12463.6777, grad_fn=<NegBackward0>) tensor(12463.6748, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12463.66796875
tensor(12463.6748, grad_fn=<NegBackward0>) tensor(12463.6680, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12463.6650390625
tensor(12463.6680, grad_fn=<NegBackward0>) tensor(12463.6650, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12463.6533203125
tensor(12463.6650, grad_fn=<NegBackward0>) tensor(12463.6533, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12463.6298828125
tensor(12463.6533, grad_fn=<NegBackward0>) tensor(12463.6299, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12463.5693359375
tensor(12463.6299, grad_fn=<NegBackward0>) tensor(12463.5693, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12463.3173828125
tensor(12463.5693, grad_fn=<NegBackward0>) tensor(12463.3174, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12462.6806640625
tensor(12463.3174, grad_fn=<NegBackward0>) tensor(12462.6807, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12462.2138671875
tensor(12462.6807, grad_fn=<NegBackward0>) tensor(12462.2139, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12461.8671875
tensor(12462.2139, grad_fn=<NegBackward0>) tensor(12461.8672, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12459.509765625
tensor(12461.8672, grad_fn=<NegBackward0>) tensor(12459.5098, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12279.9775390625
tensor(12459.5098, grad_fn=<NegBackward0>) tensor(12279.9775, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12201.3427734375
tensor(12279.9775, grad_fn=<NegBackward0>) tensor(12201.3428, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12178.9609375
tensor(12201.3428, grad_fn=<NegBackward0>) tensor(12178.9609, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12169.765625
tensor(12178.9609, grad_fn=<NegBackward0>) tensor(12169.7656, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12155.5927734375
tensor(12169.7656, grad_fn=<NegBackward0>) tensor(12155.5928, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12145.12109375
tensor(12155.5928, grad_fn=<NegBackward0>) tensor(12145.1211, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12128.62890625
tensor(12145.1211, grad_fn=<NegBackward0>) tensor(12128.6289, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12123.560546875
tensor(12128.6289, grad_fn=<NegBackward0>) tensor(12123.5605, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12109.3759765625
tensor(12123.5605, grad_fn=<NegBackward0>) tensor(12109.3760, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12107.08203125
tensor(12109.3760, grad_fn=<NegBackward0>) tensor(12107.0820, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12105.2041015625
tensor(12107.0820, grad_fn=<NegBackward0>) tensor(12105.2041, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12103.9208984375
tensor(12105.2041, grad_fn=<NegBackward0>) tensor(12103.9209, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12103.099609375
tensor(12103.9209, grad_fn=<NegBackward0>) tensor(12103.0996, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12097.05078125
tensor(12103.0996, grad_fn=<NegBackward0>) tensor(12097.0508, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12058.951171875
tensor(12097.0508, grad_fn=<NegBackward0>) tensor(12058.9512, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12011.0029296875
tensor(12058.9512, grad_fn=<NegBackward0>) tensor(12011.0029, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12008.5869140625
tensor(12011.0029, grad_fn=<NegBackward0>) tensor(12008.5869, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12005.1123046875
tensor(12008.5869, grad_fn=<NegBackward0>) tensor(12005.1123, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12004.4248046875
tensor(12005.1123, grad_fn=<NegBackward0>) tensor(12004.4248, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12004.4130859375
tensor(12004.4248, grad_fn=<NegBackward0>) tensor(12004.4131, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12004.3701171875
tensor(12004.4131, grad_fn=<NegBackward0>) tensor(12004.3701, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12004.0732421875
tensor(12004.3701, grad_fn=<NegBackward0>) tensor(12004.0732, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12004.07421875
tensor(12004.0732, grad_fn=<NegBackward0>) tensor(12004.0742, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12004.072265625
tensor(12004.0732, grad_fn=<NegBackward0>) tensor(12004.0723, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12004.072265625
tensor(12004.0723, grad_fn=<NegBackward0>) tensor(12004.0723, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12004.0712890625
tensor(12004.0723, grad_fn=<NegBackward0>) tensor(12004.0713, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12004.0703125
tensor(12004.0713, grad_fn=<NegBackward0>) tensor(12004.0703, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12004.0810546875
tensor(12004.0703, grad_fn=<NegBackward0>) tensor(12004.0811, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12004.07421875
tensor(12004.0703, grad_fn=<NegBackward0>) tensor(12004.0742, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12004.107421875
tensor(12004.0703, grad_fn=<NegBackward0>) tensor(12004.1074, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12004.0693359375
tensor(12004.0703, grad_fn=<NegBackward0>) tensor(12004.0693, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12004.068359375
tensor(12004.0693, grad_fn=<NegBackward0>) tensor(12004.0684, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12004.068359375
tensor(12004.0684, grad_fn=<NegBackward0>) tensor(12004.0684, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12004.0673828125
tensor(12004.0684, grad_fn=<NegBackward0>) tensor(12004.0674, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12004.0654296875
tensor(12004.0674, grad_fn=<NegBackward0>) tensor(12004.0654, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12004.0283203125
tensor(12004.0654, grad_fn=<NegBackward0>) tensor(12004.0283, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12004.021484375
tensor(12004.0283, grad_fn=<NegBackward0>) tensor(12004.0215, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12004.025390625
tensor(12004.0215, grad_fn=<NegBackward0>) tensor(12004.0254, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12004.021484375
tensor(12004.0215, grad_fn=<NegBackward0>) tensor(12004.0215, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12004.021484375
tensor(12004.0215, grad_fn=<NegBackward0>) tensor(12004.0215, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11990.4384765625
tensor(12004.0215, grad_fn=<NegBackward0>) tensor(11990.4385, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11990.421875
tensor(11990.4385, grad_fn=<NegBackward0>) tensor(11990.4219, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -11990.443359375
tensor(11990.4219, grad_fn=<NegBackward0>) tensor(11990.4434, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -11990.4208984375
tensor(11990.4219, grad_fn=<NegBackward0>) tensor(11990.4209, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11990.419921875
tensor(11990.4209, grad_fn=<NegBackward0>) tensor(11990.4199, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11990.419921875
tensor(11990.4199, grad_fn=<NegBackward0>) tensor(11990.4199, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11990.306640625
tensor(11990.4199, grad_fn=<NegBackward0>) tensor(11990.3066, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11990.3134765625
tensor(11990.3066, grad_fn=<NegBackward0>) tensor(11990.3135, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11990.3046875
tensor(11990.3066, grad_fn=<NegBackward0>) tensor(11990.3047, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -11986.791015625
tensor(11990.3047, grad_fn=<NegBackward0>) tensor(11986.7910, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -11986.8544921875
tensor(11986.7910, grad_fn=<NegBackward0>) tensor(11986.8545, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -11986.78515625
tensor(11986.7910, grad_fn=<NegBackward0>) tensor(11986.7852, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -11986.77734375
tensor(11986.7852, grad_fn=<NegBackward0>) tensor(11986.7773, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -11986.7919921875
tensor(11986.7773, grad_fn=<NegBackward0>) tensor(11986.7920, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -11986.779296875
tensor(11986.7773, grad_fn=<NegBackward0>) tensor(11986.7793, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -11986.7783203125
tensor(11986.7773, grad_fn=<NegBackward0>) tensor(11986.7783, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -11986.7763671875
tensor(11986.7773, grad_fn=<NegBackward0>) tensor(11986.7764, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11986.787109375
tensor(11986.7764, grad_fn=<NegBackward0>) tensor(11986.7871, grad_fn=<NegBackward0>)
1
pi: tensor([[0.6687, 0.3313],
        [0.2311, 0.7689]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4276, 0.5724], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3013, 0.1058],
         [0.6279, 0.2982]],

        [[0.6570, 0.0999],
         [0.5892, 0.5965]],

        [[0.6382, 0.0973],
         [0.6648, 0.5534]],

        [[0.7224, 0.1047],
         [0.5302, 0.6656]],

        [[0.7152, 0.1035],
         [0.5682, 0.5406]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603039623460752
Average Adjusted Rand Index: 0.9604838349935619
[0.358648135930369, 0.9603039623460752] [0.9683221230253964, 0.9604838349935619] [12015.4267578125, 11986.7783203125]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11804.583716317185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23504.984375
inf tensor(23504.9844, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12306.0322265625
tensor(23504.9844, grad_fn=<NegBackward0>) tensor(12306.0322, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12305.337890625
tensor(12306.0322, grad_fn=<NegBackward0>) tensor(12305.3379, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12305.16796875
tensor(12305.3379, grad_fn=<NegBackward0>) tensor(12305.1680, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12305.041015625
tensor(12305.1680, grad_fn=<NegBackward0>) tensor(12305.0410, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12304.939453125
tensor(12305.0410, grad_fn=<NegBackward0>) tensor(12304.9395, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12304.8916015625
tensor(12304.9395, grad_fn=<NegBackward0>) tensor(12304.8916, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12304.8671875
tensor(12304.8916, grad_fn=<NegBackward0>) tensor(12304.8672, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12304.84765625
tensor(12304.8672, grad_fn=<NegBackward0>) tensor(12304.8477, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12304.8291015625
tensor(12304.8477, grad_fn=<NegBackward0>) tensor(12304.8291, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12304.8115234375
tensor(12304.8291, grad_fn=<NegBackward0>) tensor(12304.8115, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12304.7880859375
tensor(12304.8115, grad_fn=<NegBackward0>) tensor(12304.7881, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12304.765625
tensor(12304.7881, grad_fn=<NegBackward0>) tensor(12304.7656, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12304.7392578125
tensor(12304.7656, grad_fn=<NegBackward0>) tensor(12304.7393, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12304.708984375
tensor(12304.7393, grad_fn=<NegBackward0>) tensor(12304.7090, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12304.6796875
tensor(12304.7090, grad_fn=<NegBackward0>) tensor(12304.6797, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12304.6484375
tensor(12304.6797, grad_fn=<NegBackward0>) tensor(12304.6484, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12304.611328125
tensor(12304.6484, grad_fn=<NegBackward0>) tensor(12304.6113, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12304.578125
tensor(12304.6113, grad_fn=<NegBackward0>) tensor(12304.5781, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12304.544921875
tensor(12304.5781, grad_fn=<NegBackward0>) tensor(12304.5449, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12304.5146484375
tensor(12304.5449, grad_fn=<NegBackward0>) tensor(12304.5146, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12304.4873046875
tensor(12304.5146, grad_fn=<NegBackward0>) tensor(12304.4873, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12304.4658203125
tensor(12304.4873, grad_fn=<NegBackward0>) tensor(12304.4658, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12304.4453125
tensor(12304.4658, grad_fn=<NegBackward0>) tensor(12304.4453, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12304.427734375
tensor(12304.4453, grad_fn=<NegBackward0>) tensor(12304.4277, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12304.4111328125
tensor(12304.4277, grad_fn=<NegBackward0>) tensor(12304.4111, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12304.3955078125
tensor(12304.4111, grad_fn=<NegBackward0>) tensor(12304.3955, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12304.3818359375
tensor(12304.3955, grad_fn=<NegBackward0>) tensor(12304.3818, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12304.3681640625
tensor(12304.3818, grad_fn=<NegBackward0>) tensor(12304.3682, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12304.35546875
tensor(12304.3682, grad_fn=<NegBackward0>) tensor(12304.3555, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12304.3486328125
tensor(12304.3555, grad_fn=<NegBackward0>) tensor(12304.3486, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12304.3427734375
tensor(12304.3486, grad_fn=<NegBackward0>) tensor(12304.3428, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12304.3427734375
tensor(12304.3428, grad_fn=<NegBackward0>) tensor(12304.3428, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12304.3408203125
tensor(12304.3428, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12304.3408203125
tensor(12304.3408, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12304.3408203125
tensor(12304.3408, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12304.3388671875
tensor(12304.3408, grad_fn=<NegBackward0>) tensor(12304.3389, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12304.33984375
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12304.3388671875
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3389, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12304.3408203125
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12304.3408203125
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -12304.33984375
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -12304.33984375
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -12304.33984375
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4300 due to no improvement.
pi: tensor([[0.0248, 0.9752],
        [0.6426, 0.3574]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9698, 0.0302], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.2197],
         [0.6833, 0.1953]],

        [[0.6940, 0.2127],
         [0.5586, 0.5253]],

        [[0.5797, 0.2037],
         [0.6100, 0.5873]],

        [[0.5372, 0.1935],
         [0.6873, 0.6011]],

        [[0.6670, 0.1906],
         [0.6636, 0.5346]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.016449682236070833
Global Adjusted Rand Index: -0.0019451727476374374
Average Adjusted Rand Index: -0.0032899364472141666
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21283.544921875
inf tensor(21283.5449, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12305.8935546875
tensor(21283.5449, grad_fn=<NegBackward0>) tensor(12305.8936, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12305.6455078125
tensor(12305.8936, grad_fn=<NegBackward0>) tensor(12305.6455, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12305.55859375
tensor(12305.6455, grad_fn=<NegBackward0>) tensor(12305.5586, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12305.4794921875
tensor(12305.5586, grad_fn=<NegBackward0>) tensor(12305.4795, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12305.3720703125
tensor(12305.4795, grad_fn=<NegBackward0>) tensor(12305.3721, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12305.2626953125
tensor(12305.3721, grad_fn=<NegBackward0>) tensor(12305.2627, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12305.173828125
tensor(12305.2627, grad_fn=<NegBackward0>) tensor(12305.1738, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12305.0849609375
tensor(12305.1738, grad_fn=<NegBackward0>) tensor(12305.0850, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12305.0400390625
tensor(12305.0850, grad_fn=<NegBackward0>) tensor(12305.0400, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12305.005859375
tensor(12305.0400, grad_fn=<NegBackward0>) tensor(12305.0059, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12304.9765625
tensor(12305.0059, grad_fn=<NegBackward0>) tensor(12304.9766, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12304.947265625
tensor(12304.9766, grad_fn=<NegBackward0>) tensor(12304.9473, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12304.9169921875
tensor(12304.9473, grad_fn=<NegBackward0>) tensor(12304.9170, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12304.8896484375
tensor(12304.9170, grad_fn=<NegBackward0>) tensor(12304.8896, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12304.865234375
tensor(12304.8896, grad_fn=<NegBackward0>) tensor(12304.8652, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12304.841796875
tensor(12304.8652, grad_fn=<NegBackward0>) tensor(12304.8418, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12304.8212890625
tensor(12304.8418, grad_fn=<NegBackward0>) tensor(12304.8213, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12304.7978515625
tensor(12304.8213, grad_fn=<NegBackward0>) tensor(12304.7979, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12304.7724609375
tensor(12304.7979, grad_fn=<NegBackward0>) tensor(12304.7725, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12304.7451171875
tensor(12304.7725, grad_fn=<NegBackward0>) tensor(12304.7451, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12304.7138671875
tensor(12304.7451, grad_fn=<NegBackward0>) tensor(12304.7139, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12304.681640625
tensor(12304.7139, grad_fn=<NegBackward0>) tensor(12304.6816, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12304.6474609375
tensor(12304.6816, grad_fn=<NegBackward0>) tensor(12304.6475, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12304.6123046875
tensor(12304.6475, grad_fn=<NegBackward0>) tensor(12304.6123, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12304.576171875
tensor(12304.6123, grad_fn=<NegBackward0>) tensor(12304.5762, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12304.544921875
tensor(12304.5762, grad_fn=<NegBackward0>) tensor(12304.5449, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12304.513671875
tensor(12304.5449, grad_fn=<NegBackward0>) tensor(12304.5137, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12304.4873046875
tensor(12304.5137, grad_fn=<NegBackward0>) tensor(12304.4873, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12304.46484375
tensor(12304.4873, grad_fn=<NegBackward0>) tensor(12304.4648, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12304.4462890625
tensor(12304.4648, grad_fn=<NegBackward0>) tensor(12304.4463, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12304.4296875
tensor(12304.4463, grad_fn=<NegBackward0>) tensor(12304.4297, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12304.4140625
tensor(12304.4297, grad_fn=<NegBackward0>) tensor(12304.4141, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12304.3994140625
tensor(12304.4141, grad_fn=<NegBackward0>) tensor(12304.3994, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12304.3828125
tensor(12304.3994, grad_fn=<NegBackward0>) tensor(12304.3828, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12304.369140625
tensor(12304.3828, grad_fn=<NegBackward0>) tensor(12304.3691, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12304.357421875
tensor(12304.3691, grad_fn=<NegBackward0>) tensor(12304.3574, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12304.3505859375
tensor(12304.3574, grad_fn=<NegBackward0>) tensor(12304.3506, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12304.3447265625
tensor(12304.3506, grad_fn=<NegBackward0>) tensor(12304.3447, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12304.341796875
tensor(12304.3447, grad_fn=<NegBackward0>) tensor(12304.3418, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12304.341796875
tensor(12304.3418, grad_fn=<NegBackward0>) tensor(12304.3418, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12304.33984375
tensor(12304.3418, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12304.3408203125
tensor(12304.3398, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12304.3388671875
tensor(12304.3398, grad_fn=<NegBackward0>) tensor(12304.3389, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12304.3447265625
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3447, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12304.3408203125
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3408, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12304.37890625
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3789, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12304.33984375
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -12304.33984375
tensor(12304.3389, grad_fn=<NegBackward0>) tensor(12304.3398, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4800 due to no improvement.
pi: tensor([[0.3576, 0.6424],
        [0.9753, 0.0247]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0302, 0.9698], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1953, 0.2198],
         [0.6233, 0.2003]],

        [[0.7131, 0.2127],
         [0.5961, 0.7255]],

        [[0.6193, 0.2037],
         [0.5522, 0.6614]],

        [[0.6192, 0.1935],
         [0.6502, 0.6040]],

        [[0.6009, 0.1906],
         [0.7284, 0.6016]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.016449682236070833
Global Adjusted Rand Index: -0.0019451727476374374
Average Adjusted Rand Index: -0.0032899364472141666
[-0.0019451727476374374, -0.0019451727476374374] [-0.0032899364472141666, -0.0032899364472141666] [12304.33984375, 12304.33984375]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11841.405981942185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22450.486328125
inf tensor(22450.4863, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12296.4404296875
tensor(22450.4863, grad_fn=<NegBackward0>) tensor(12296.4404, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12296.052734375
tensor(12296.4404, grad_fn=<NegBackward0>) tensor(12296.0527, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12295.962890625
tensor(12296.0527, grad_fn=<NegBackward0>) tensor(12295.9629, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12295.900390625
tensor(12295.9629, grad_fn=<NegBackward0>) tensor(12295.9004, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12295.8515625
tensor(12295.9004, grad_fn=<NegBackward0>) tensor(12295.8516, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12295.8154296875
tensor(12295.8516, grad_fn=<NegBackward0>) tensor(12295.8154, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12295.78515625
tensor(12295.8154, grad_fn=<NegBackward0>) tensor(12295.7852, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12295.759765625
tensor(12295.7852, grad_fn=<NegBackward0>) tensor(12295.7598, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12295.7412109375
tensor(12295.7598, grad_fn=<NegBackward0>) tensor(12295.7412, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12295.7236328125
tensor(12295.7412, grad_fn=<NegBackward0>) tensor(12295.7236, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12295.7109375
tensor(12295.7236, grad_fn=<NegBackward0>) tensor(12295.7109, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12295.7001953125
tensor(12295.7109, grad_fn=<NegBackward0>) tensor(12295.7002, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12295.6904296875
tensor(12295.7002, grad_fn=<NegBackward0>) tensor(12295.6904, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12295.68359375
tensor(12295.6904, grad_fn=<NegBackward0>) tensor(12295.6836, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12295.6748046875
tensor(12295.6836, grad_fn=<NegBackward0>) tensor(12295.6748, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12295.66796875
tensor(12295.6748, grad_fn=<NegBackward0>) tensor(12295.6680, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12295.662109375
tensor(12295.6680, grad_fn=<NegBackward0>) tensor(12295.6621, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12295.6572265625
tensor(12295.6621, grad_fn=<NegBackward0>) tensor(12295.6572, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12295.650390625
tensor(12295.6572, grad_fn=<NegBackward0>) tensor(12295.6504, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12295.646484375
tensor(12295.6504, grad_fn=<NegBackward0>) tensor(12295.6465, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12295.640625
tensor(12295.6465, grad_fn=<NegBackward0>) tensor(12295.6406, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12295.634765625
tensor(12295.6406, grad_fn=<NegBackward0>) tensor(12295.6348, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12295.630859375
tensor(12295.6348, grad_fn=<NegBackward0>) tensor(12295.6309, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12295.6259765625
tensor(12295.6309, grad_fn=<NegBackward0>) tensor(12295.6260, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12295.6201171875
tensor(12295.6260, grad_fn=<NegBackward0>) tensor(12295.6201, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12295.6162109375
tensor(12295.6201, grad_fn=<NegBackward0>) tensor(12295.6162, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12295.609375
tensor(12295.6162, grad_fn=<NegBackward0>) tensor(12295.6094, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12295.6044921875
tensor(12295.6094, grad_fn=<NegBackward0>) tensor(12295.6045, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12295.6015625
tensor(12295.6045, grad_fn=<NegBackward0>) tensor(12295.6016, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12295.5947265625
tensor(12295.6016, grad_fn=<NegBackward0>) tensor(12295.5947, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12295.58984375
tensor(12295.5947, grad_fn=<NegBackward0>) tensor(12295.5898, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12295.5830078125
tensor(12295.5898, grad_fn=<NegBackward0>) tensor(12295.5830, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12295.578125
tensor(12295.5830, grad_fn=<NegBackward0>) tensor(12295.5781, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12295.5712890625
tensor(12295.5781, grad_fn=<NegBackward0>) tensor(12295.5713, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12295.5634765625
tensor(12295.5713, grad_fn=<NegBackward0>) tensor(12295.5635, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12295.5556640625
tensor(12295.5635, grad_fn=<NegBackward0>) tensor(12295.5557, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12295.5478515625
tensor(12295.5557, grad_fn=<NegBackward0>) tensor(12295.5479, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12295.5419921875
tensor(12295.5479, grad_fn=<NegBackward0>) tensor(12295.5420, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12295.5341796875
tensor(12295.5420, grad_fn=<NegBackward0>) tensor(12295.5342, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12295.52734375
tensor(12295.5342, grad_fn=<NegBackward0>) tensor(12295.5273, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12295.51953125
tensor(12295.5273, grad_fn=<NegBackward0>) tensor(12295.5195, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12295.5302734375
tensor(12295.5195, grad_fn=<NegBackward0>) tensor(12295.5303, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12295.5078125
tensor(12295.5195, grad_fn=<NegBackward0>) tensor(12295.5078, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12295.50390625
tensor(12295.5078, grad_fn=<NegBackward0>) tensor(12295.5039, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12295.4990234375
tensor(12295.5039, grad_fn=<NegBackward0>) tensor(12295.4990, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12295.4951171875
tensor(12295.4990, grad_fn=<NegBackward0>) tensor(12295.4951, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12295.4951171875
tensor(12295.4951, grad_fn=<NegBackward0>) tensor(12295.4951, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12295.4892578125
tensor(12295.4951, grad_fn=<NegBackward0>) tensor(12295.4893, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12295.486328125
tensor(12295.4893, grad_fn=<NegBackward0>) tensor(12295.4863, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12295.486328125
tensor(12295.4863, grad_fn=<NegBackward0>) tensor(12295.4863, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12295.4833984375
tensor(12295.4863, grad_fn=<NegBackward0>) tensor(12295.4834, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12295.482421875
tensor(12295.4834, grad_fn=<NegBackward0>) tensor(12295.4824, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12295.4814453125
tensor(12295.4824, grad_fn=<NegBackward0>) tensor(12295.4814, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12295.4794921875
tensor(12295.4814, grad_fn=<NegBackward0>) tensor(12295.4795, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12295.4794921875
tensor(12295.4795, grad_fn=<NegBackward0>) tensor(12295.4795, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12295.4765625
tensor(12295.4795, grad_fn=<NegBackward0>) tensor(12295.4766, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12295.5068359375
tensor(12295.4766, grad_fn=<NegBackward0>) tensor(12295.5068, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12295.4755859375
tensor(12295.4766, grad_fn=<NegBackward0>) tensor(12295.4756, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12295.4736328125
tensor(12295.4756, grad_fn=<NegBackward0>) tensor(12295.4736, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12295.4716796875
tensor(12295.4736, grad_fn=<NegBackward0>) tensor(12295.4717, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12295.4697265625
tensor(12295.4717, grad_fn=<NegBackward0>) tensor(12295.4697, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12295.4677734375
tensor(12295.4697, grad_fn=<NegBackward0>) tensor(12295.4678, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12295.458984375
tensor(12295.4678, grad_fn=<NegBackward0>) tensor(12295.4590, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12295.451171875
tensor(12295.4590, grad_fn=<NegBackward0>) tensor(12295.4512, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12295.4384765625
tensor(12295.4512, grad_fn=<NegBackward0>) tensor(12295.4385, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12295.423828125
tensor(12295.4385, grad_fn=<NegBackward0>) tensor(12295.4238, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12295.412109375
tensor(12295.4238, grad_fn=<NegBackward0>) tensor(12295.4121, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12295.3994140625
tensor(12295.4121, grad_fn=<NegBackward0>) tensor(12295.3994, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12295.419921875
tensor(12295.3994, grad_fn=<NegBackward0>) tensor(12295.4199, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12295.38671875
tensor(12295.3994, grad_fn=<NegBackward0>) tensor(12295.3867, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12295.384765625
tensor(12295.3867, grad_fn=<NegBackward0>) tensor(12295.3848, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12295.3828125
tensor(12295.3848, grad_fn=<NegBackward0>) tensor(12295.3828, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12295.380859375
tensor(12295.3828, grad_fn=<NegBackward0>) tensor(12295.3809, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12295.3798828125
tensor(12295.3809, grad_fn=<NegBackward0>) tensor(12295.3799, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12295.37890625
tensor(12295.3799, grad_fn=<NegBackward0>) tensor(12295.3789, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12295.3779296875
tensor(12295.3789, grad_fn=<NegBackward0>) tensor(12295.3779, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12295.37890625
tensor(12295.3779, grad_fn=<NegBackward0>) tensor(12295.3789, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12295.376953125
tensor(12295.3779, grad_fn=<NegBackward0>) tensor(12295.3770, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12295.3837890625
tensor(12295.3770, grad_fn=<NegBackward0>) tensor(12295.3838, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12295.3740234375
tensor(12295.3770, grad_fn=<NegBackward0>) tensor(12295.3740, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12295.3759765625
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3760, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12295.3759765625
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3760, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12295.3740234375
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3740, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12295.375
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3750, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12295.3740234375
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3740, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12295.375
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3750, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12295.375
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3750, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12295.376953125
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3770, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12295.37890625
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3789, grad_fn=<NegBackward0>)
4
Iteration 9000: Loss = -12295.373046875
tensor(12295.3740, grad_fn=<NegBackward0>) tensor(12295.3730, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12295.373046875
tensor(12295.3730, grad_fn=<NegBackward0>) tensor(12295.3730, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12295.3740234375
tensor(12295.3730, grad_fn=<NegBackward0>) tensor(12295.3740, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12295.3798828125
tensor(12295.3730, grad_fn=<NegBackward0>) tensor(12295.3799, grad_fn=<NegBackward0>)
2
Iteration 9400: Loss = -12295.6279296875
tensor(12295.3730, grad_fn=<NegBackward0>) tensor(12295.6279, grad_fn=<NegBackward0>)
3
Iteration 9500: Loss = -12295.3720703125
tensor(12295.3730, grad_fn=<NegBackward0>) tensor(12295.3721, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12295.37109375
tensor(12295.3721, grad_fn=<NegBackward0>) tensor(12295.3711, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12295.3720703125
tensor(12295.3711, grad_fn=<NegBackward0>) tensor(12295.3721, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12295.3779296875
tensor(12295.3711, grad_fn=<NegBackward0>) tensor(12295.3779, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12295.4091796875
tensor(12295.3711, grad_fn=<NegBackward0>) tensor(12295.4092, grad_fn=<NegBackward0>)
3
pi: tensor([[0.9878, 0.0122],
        [0.2414, 0.7586]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0422, 0.9578], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2015, 0.2037],
         [0.5761, 0.1947]],

        [[0.5703, 0.1940],
         [0.5357, 0.5420]],

        [[0.7134, 0.1997],
         [0.5460, 0.6634]],

        [[0.5873, 0.1994],
         [0.5085, 0.6404]],

        [[0.6180, 0.1974],
         [0.6547, 0.5569]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0051170444960952485
Average Adjusted Rand Index: -0.00026141350014004293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23158.21484375
inf tensor(23158.2148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12296.2412109375
tensor(23158.2148, grad_fn=<NegBackward0>) tensor(12296.2412, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12295.865234375
tensor(12296.2412, grad_fn=<NegBackward0>) tensor(12295.8652, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12295.8017578125
tensor(12295.8652, grad_fn=<NegBackward0>) tensor(12295.8018, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12295.7685546875
tensor(12295.8018, grad_fn=<NegBackward0>) tensor(12295.7686, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12295.75
tensor(12295.7686, grad_fn=<NegBackward0>) tensor(12295.7500, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12295.7353515625
tensor(12295.7500, grad_fn=<NegBackward0>) tensor(12295.7354, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12295.7265625
tensor(12295.7354, grad_fn=<NegBackward0>) tensor(12295.7266, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12295.7197265625
tensor(12295.7266, grad_fn=<NegBackward0>) tensor(12295.7197, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12295.7099609375
tensor(12295.7197, grad_fn=<NegBackward0>) tensor(12295.7100, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12295.701171875
tensor(12295.7100, grad_fn=<NegBackward0>) tensor(12295.7012, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12295.6953125
tensor(12295.7012, grad_fn=<NegBackward0>) tensor(12295.6953, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12295.68359375
tensor(12295.6953, grad_fn=<NegBackward0>) tensor(12295.6836, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12295.671875
tensor(12295.6836, grad_fn=<NegBackward0>) tensor(12295.6719, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12295.6552734375
tensor(12295.6719, grad_fn=<NegBackward0>) tensor(12295.6553, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12295.63671875
tensor(12295.6553, grad_fn=<NegBackward0>) tensor(12295.6367, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12295.61328125
tensor(12295.6367, grad_fn=<NegBackward0>) tensor(12295.6133, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12295.583984375
tensor(12295.6133, grad_fn=<NegBackward0>) tensor(12295.5840, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12295.556640625
tensor(12295.5840, grad_fn=<NegBackward0>) tensor(12295.5566, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12295.5341796875
tensor(12295.5566, grad_fn=<NegBackward0>) tensor(12295.5342, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12295.5185546875
tensor(12295.5342, grad_fn=<NegBackward0>) tensor(12295.5186, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12295.5068359375
tensor(12295.5186, grad_fn=<NegBackward0>) tensor(12295.5068, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12295.4931640625
tensor(12295.5068, grad_fn=<NegBackward0>) tensor(12295.4932, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12295.470703125
tensor(12295.4932, grad_fn=<NegBackward0>) tensor(12295.4707, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12295.443359375
tensor(12295.4707, grad_fn=<NegBackward0>) tensor(12295.4434, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12295.3955078125
tensor(12295.4434, grad_fn=<NegBackward0>) tensor(12295.3955, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12295.1640625
tensor(12295.3955, grad_fn=<NegBackward0>) tensor(12295.1641, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12294.6640625
tensor(12295.1641, grad_fn=<NegBackward0>) tensor(12294.6641, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12294.5341796875
tensor(12294.6641, grad_fn=<NegBackward0>) tensor(12294.5342, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12294.4638671875
tensor(12294.5342, grad_fn=<NegBackward0>) tensor(12294.4639, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12294.423828125
tensor(12294.4639, grad_fn=<NegBackward0>) tensor(12294.4238, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12294.3994140625
tensor(12294.4238, grad_fn=<NegBackward0>) tensor(12294.3994, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12294.37890625
tensor(12294.3994, grad_fn=<NegBackward0>) tensor(12294.3789, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12294.3583984375
tensor(12294.3789, grad_fn=<NegBackward0>) tensor(12294.3584, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12294.3447265625
tensor(12294.3584, grad_fn=<NegBackward0>) tensor(12294.3447, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12294.33203125
tensor(12294.3447, grad_fn=<NegBackward0>) tensor(12294.3320, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12294.318359375
tensor(12294.3320, grad_fn=<NegBackward0>) tensor(12294.3184, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12294.3076171875
tensor(12294.3184, grad_fn=<NegBackward0>) tensor(12294.3076, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12294.2958984375
tensor(12294.3076, grad_fn=<NegBackward0>) tensor(12294.2959, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12294.28515625
tensor(12294.2959, grad_fn=<NegBackward0>) tensor(12294.2852, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12294.2783203125
tensor(12294.2852, grad_fn=<NegBackward0>) tensor(12294.2783, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12294.2724609375
tensor(12294.2783, grad_fn=<NegBackward0>) tensor(12294.2725, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12294.267578125
tensor(12294.2725, grad_fn=<NegBackward0>) tensor(12294.2676, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12294.263671875
tensor(12294.2676, grad_fn=<NegBackward0>) tensor(12294.2637, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12294.2646484375
tensor(12294.2637, grad_fn=<NegBackward0>) tensor(12294.2646, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12294.26171875
tensor(12294.2637, grad_fn=<NegBackward0>) tensor(12294.2617, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12294.2607421875
tensor(12294.2617, grad_fn=<NegBackward0>) tensor(12294.2607, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12294.2587890625
tensor(12294.2607, grad_fn=<NegBackward0>) tensor(12294.2588, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12294.2587890625
tensor(12294.2588, grad_fn=<NegBackward0>) tensor(12294.2588, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12294.2568359375
tensor(12294.2588, grad_fn=<NegBackward0>) tensor(12294.2568, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12294.2568359375
tensor(12294.2568, grad_fn=<NegBackward0>) tensor(12294.2568, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12294.255859375
tensor(12294.2568, grad_fn=<NegBackward0>) tensor(12294.2559, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12294.25390625
tensor(12294.2559, grad_fn=<NegBackward0>) tensor(12294.2539, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12294.2529296875
tensor(12294.2539, grad_fn=<NegBackward0>) tensor(12294.2529, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12294.25390625
tensor(12294.2529, grad_fn=<NegBackward0>) tensor(12294.2539, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12294.2529296875
tensor(12294.2529, grad_fn=<NegBackward0>) tensor(12294.2529, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12294.2529296875
tensor(12294.2529, grad_fn=<NegBackward0>) tensor(12294.2529, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12294.2529296875
tensor(12294.2529, grad_fn=<NegBackward0>) tensor(12294.2529, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12294.2509765625
tensor(12294.2529, grad_fn=<NegBackward0>) tensor(12294.2510, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12294.25
tensor(12294.2510, grad_fn=<NegBackward0>) tensor(12294.2500, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12294.251953125
tensor(12294.2500, grad_fn=<NegBackward0>) tensor(12294.2520, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12294.25
tensor(12294.2500, grad_fn=<NegBackward0>) tensor(12294.2500, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12294.2490234375
tensor(12294.2500, grad_fn=<NegBackward0>) tensor(12294.2490, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12294.25
tensor(12294.2490, grad_fn=<NegBackward0>) tensor(12294.2500, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12294.2490234375
tensor(12294.2490, grad_fn=<NegBackward0>) tensor(12294.2490, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12294.2490234375
tensor(12294.2490, grad_fn=<NegBackward0>) tensor(12294.2490, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12294.248046875
tensor(12294.2490, grad_fn=<NegBackward0>) tensor(12294.2480, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12294.248046875
tensor(12294.2480, grad_fn=<NegBackward0>) tensor(12294.2480, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12294.248046875
tensor(12294.2480, grad_fn=<NegBackward0>) tensor(12294.2480, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12294.248046875
tensor(12294.2480, grad_fn=<NegBackward0>) tensor(12294.2480, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12294.2470703125
tensor(12294.2480, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12294.2470703125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12294.2490234375
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2490, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12294.2470703125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12294.2490234375
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2490, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12294.2548828125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2549, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12294.2470703125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12294.25
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2500, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12294.2470703125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12294.2470703125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12294.2470703125
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12294.24609375
tensor(12294.2471, grad_fn=<NegBackward0>) tensor(12294.2461, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12294.2451171875
tensor(12294.2461, grad_fn=<NegBackward0>) tensor(12294.2451, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12294.255859375
tensor(12294.2451, grad_fn=<NegBackward0>) tensor(12294.2559, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12294.2451171875
tensor(12294.2451, grad_fn=<NegBackward0>) tensor(12294.2451, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12294.244140625
tensor(12294.2451, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12294.244140625
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12294.2451171875
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2451, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12294.24609375
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2461, grad_fn=<NegBackward0>)
2
Iteration 8900: Loss = -12294.244140625
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12294.2509765625
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2510, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12294.244140625
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12294.2431640625
tensor(12294.2441, grad_fn=<NegBackward0>) tensor(12294.2432, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12294.2431640625
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2432, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12294.244140625
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12294.2470703125
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2471, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12294.244140625
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
3
Iteration 9700: Loss = -12294.244140625
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2441, grad_fn=<NegBackward0>)
4
Iteration 9800: Loss = -12294.2431640625
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2432, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12294.2431640625
tensor(12294.2432, grad_fn=<NegBackward0>) tensor(12294.2432, grad_fn=<NegBackward0>)
pi: tensor([[9.9999e-01, 1.0714e-05],
        [3.6020e-04, 9.9964e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9695, 0.0305], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1993, 0.1931],
         [0.5722, 0.2695]],

        [[0.5522, 0.1530],
         [0.7087, 0.5945]],

        [[0.5467, 0.2278],
         [0.6841, 0.6362]],

        [[0.6252, 0.1173],
         [0.7100, 0.6787]],

        [[0.7043, 0.1696],
         [0.5414, 0.5930]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.018778022358394132
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0025508831553451555
Average Adjusted Rand Index: 0.0006443691279030215
[0.0051170444960952485, 0.0025508831553451555] [-0.00026141350014004293, 0.0006443691279030215] [12295.37109375, 12294.244140625]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11902.079666400363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24286.677734375
inf tensor(24286.6777, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.607421875
tensor(24286.6777, grad_fn=<NegBackward0>) tensor(12356.6074, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12356.0712890625
tensor(12356.6074, grad_fn=<NegBackward0>) tensor(12356.0713, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12355.97265625
tensor(12356.0713, grad_fn=<NegBackward0>) tensor(12355.9727, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12355.91796875
tensor(12355.9727, grad_fn=<NegBackward0>) tensor(12355.9180, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12355.87890625
tensor(12355.9180, grad_fn=<NegBackward0>) tensor(12355.8789, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12355.8505859375
tensor(12355.8789, grad_fn=<NegBackward0>) tensor(12355.8506, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12355.826171875
tensor(12355.8506, grad_fn=<NegBackward0>) tensor(12355.8262, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12355.8095703125
tensor(12355.8262, grad_fn=<NegBackward0>) tensor(12355.8096, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12355.796875
tensor(12355.8096, grad_fn=<NegBackward0>) tensor(12355.7969, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12355.7861328125
tensor(12355.7969, grad_fn=<NegBackward0>) tensor(12355.7861, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12355.7783203125
tensor(12355.7861, grad_fn=<NegBackward0>) tensor(12355.7783, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12355.775390625
tensor(12355.7783, grad_fn=<NegBackward0>) tensor(12355.7754, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12355.76953125
tensor(12355.7754, grad_fn=<NegBackward0>) tensor(12355.7695, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12355.7666015625
tensor(12355.7695, grad_fn=<NegBackward0>) tensor(12355.7666, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12355.763671875
tensor(12355.7666, grad_fn=<NegBackward0>) tensor(12355.7637, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12355.7607421875
tensor(12355.7637, grad_fn=<NegBackward0>) tensor(12355.7607, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12355.755859375
tensor(12355.7607, grad_fn=<NegBackward0>) tensor(12355.7559, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12355.748046875
tensor(12355.7559, grad_fn=<NegBackward0>) tensor(12355.7480, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12355.73046875
tensor(12355.7480, grad_fn=<NegBackward0>) tensor(12355.7305, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12355.6865234375
tensor(12355.7305, grad_fn=<NegBackward0>) tensor(12355.6865, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12355.666015625
tensor(12355.6865, grad_fn=<NegBackward0>) tensor(12355.6660, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12355.6533203125
tensor(12355.6660, grad_fn=<NegBackward0>) tensor(12355.6533, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12355.640625
tensor(12355.6533, grad_fn=<NegBackward0>) tensor(12355.6406, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12355.6318359375
tensor(12355.6406, grad_fn=<NegBackward0>) tensor(12355.6318, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12355.6162109375
tensor(12355.6318, grad_fn=<NegBackward0>) tensor(12355.6162, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12355.560546875
tensor(12355.6162, grad_fn=<NegBackward0>) tensor(12355.5605, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12355.3583984375
tensor(12355.5605, grad_fn=<NegBackward0>) tensor(12355.3584, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12355.2236328125
tensor(12355.3584, grad_fn=<NegBackward0>) tensor(12355.2236, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12355.1396484375
tensor(12355.2236, grad_fn=<NegBackward0>) tensor(12355.1396, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12355.0517578125
tensor(12355.1396, grad_fn=<NegBackward0>) tensor(12355.0518, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12354.96875
tensor(12355.0518, grad_fn=<NegBackward0>) tensor(12354.9688, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12354.9052734375
tensor(12354.9688, grad_fn=<NegBackward0>) tensor(12354.9053, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12354.845703125
tensor(12354.9053, grad_fn=<NegBackward0>) tensor(12354.8457, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12354.6943359375
tensor(12354.8457, grad_fn=<NegBackward0>) tensor(12354.6943, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12354.6123046875
tensor(12354.6943, grad_fn=<NegBackward0>) tensor(12354.6123, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12354.544921875
tensor(12354.6123, grad_fn=<NegBackward0>) tensor(12354.5449, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12354.4921875
tensor(12354.5449, grad_fn=<NegBackward0>) tensor(12354.4922, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12354.4501953125
tensor(12354.4922, grad_fn=<NegBackward0>) tensor(12354.4502, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12354.4130859375
tensor(12354.4502, grad_fn=<NegBackward0>) tensor(12354.4131, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12354.380859375
tensor(12354.4131, grad_fn=<NegBackward0>) tensor(12354.3809, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12354.3505859375
tensor(12354.3809, grad_fn=<NegBackward0>) tensor(12354.3506, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12354.32421875
tensor(12354.3506, grad_fn=<NegBackward0>) tensor(12354.3242, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12354.298828125
tensor(12354.3242, grad_fn=<NegBackward0>) tensor(12354.2988, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12354.2744140625
tensor(12354.2988, grad_fn=<NegBackward0>) tensor(12354.2744, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12354.251953125
tensor(12354.2744, grad_fn=<NegBackward0>) tensor(12354.2520, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12354.2294921875
tensor(12354.2520, grad_fn=<NegBackward0>) tensor(12354.2295, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12354.2099609375
tensor(12354.2295, grad_fn=<NegBackward0>) tensor(12354.2100, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12354.19140625
tensor(12354.2100, grad_fn=<NegBackward0>) tensor(12354.1914, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12354.173828125
tensor(12354.1914, grad_fn=<NegBackward0>) tensor(12354.1738, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12354.158203125
tensor(12354.1738, grad_fn=<NegBackward0>) tensor(12354.1582, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12354.14453125
tensor(12354.1582, grad_fn=<NegBackward0>) tensor(12354.1445, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12354.130859375
tensor(12354.1445, grad_fn=<NegBackward0>) tensor(12354.1309, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12354.119140625
tensor(12354.1309, grad_fn=<NegBackward0>) tensor(12354.1191, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12354.1083984375
tensor(12354.1191, grad_fn=<NegBackward0>) tensor(12354.1084, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12354.09765625
tensor(12354.1084, grad_fn=<NegBackward0>) tensor(12354.0977, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12354.0888671875
tensor(12354.0977, grad_fn=<NegBackward0>) tensor(12354.0889, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12354.0791015625
tensor(12354.0889, grad_fn=<NegBackward0>) tensor(12354.0791, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12354.072265625
tensor(12354.0791, grad_fn=<NegBackward0>) tensor(12354.0723, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12354.0673828125
tensor(12354.0723, grad_fn=<NegBackward0>) tensor(12354.0674, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12354.060546875
tensor(12354.0674, grad_fn=<NegBackward0>) tensor(12354.0605, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12354.0546875
tensor(12354.0605, grad_fn=<NegBackward0>) tensor(12354.0547, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12354.0498046875
tensor(12354.0547, grad_fn=<NegBackward0>) tensor(12354.0498, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12354.0439453125
tensor(12354.0498, grad_fn=<NegBackward0>) tensor(12354.0439, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12354.0400390625
tensor(12354.0439, grad_fn=<NegBackward0>) tensor(12354.0400, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12354.0380859375
tensor(12354.0400, grad_fn=<NegBackward0>) tensor(12354.0381, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12354.033203125
tensor(12354.0381, grad_fn=<NegBackward0>) tensor(12354.0332, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12354.029296875
tensor(12354.0332, grad_fn=<NegBackward0>) tensor(12354.0293, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12354.0263671875
tensor(12354.0293, grad_fn=<NegBackward0>) tensor(12354.0264, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12354.0224609375
tensor(12354.0264, grad_fn=<NegBackward0>) tensor(12354.0225, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12354.021484375
tensor(12354.0225, grad_fn=<NegBackward0>) tensor(12354.0215, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12354.01953125
tensor(12354.0215, grad_fn=<NegBackward0>) tensor(12354.0195, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12354.017578125
tensor(12354.0195, grad_fn=<NegBackward0>) tensor(12354.0176, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12354.013671875
tensor(12354.0176, grad_fn=<NegBackward0>) tensor(12354.0137, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12354.0126953125
tensor(12354.0137, grad_fn=<NegBackward0>) tensor(12354.0127, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12354.01171875
tensor(12354.0127, grad_fn=<NegBackward0>) tensor(12354.0117, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12354.009765625
tensor(12354.0117, grad_fn=<NegBackward0>) tensor(12354.0098, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12354.0087890625
tensor(12354.0098, grad_fn=<NegBackward0>) tensor(12354.0088, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12354.0078125
tensor(12354.0088, grad_fn=<NegBackward0>) tensor(12354.0078, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12354.0068359375
tensor(12354.0078, grad_fn=<NegBackward0>) tensor(12354.0068, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -12354.025390625
tensor(12354.0068, grad_fn=<NegBackward0>) tensor(12354.0254, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -12354.0029296875
tensor(12354.0068, grad_fn=<NegBackward0>) tensor(12354.0029, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12354.001953125
tensor(12354.0029, grad_fn=<NegBackward0>) tensor(12354.0020, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12354.009765625
tensor(12354.0020, grad_fn=<NegBackward0>) tensor(12354.0098, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12354.0009765625
tensor(12354.0020, grad_fn=<NegBackward0>) tensor(12354.0010, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12354.0
tensor(12354.0010, grad_fn=<NegBackward0>) tensor(12354., grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12353.9990234375
tensor(12354., grad_fn=<NegBackward0>) tensor(12353.9990, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12353.998046875
tensor(12353.9990, grad_fn=<NegBackward0>) tensor(12353.9980, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12353.9990234375
tensor(12353.9980, grad_fn=<NegBackward0>) tensor(12353.9990, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12353.99609375
tensor(12353.9980, grad_fn=<NegBackward0>) tensor(12353.9961, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12353.998046875
tensor(12353.9961, grad_fn=<NegBackward0>) tensor(12353.9980, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12353.99609375
tensor(12353.9961, grad_fn=<NegBackward0>) tensor(12353.9961, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12353.994140625
tensor(12353.9961, grad_fn=<NegBackward0>) tensor(12353.9941, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12354.0224609375
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12354.0225, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -12353.994140625
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12353.9941, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12353.9951171875
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12353.9951, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12353.9951171875
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12353.9951, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12354.0634765625
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12354.0635, grad_fn=<NegBackward0>)
3
Iteration 9800: Loss = -12353.9912109375
tensor(12353.9941, grad_fn=<NegBackward0>) tensor(12353.9912, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12353.9912109375
tensor(12353.9912, grad_fn=<NegBackward0>) tensor(12353.9912, grad_fn=<NegBackward0>)
pi: tensor([[9.9984e-01, 1.6339e-04],
        [3.7390e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0183, 0.9817], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0022, 0.2164],
         [0.5567, 0.1976]],

        [[0.5820, 0.1965],
         [0.6724, 0.6012]],

        [[0.5144, 0.2880],
         [0.7237, 0.6412]],

        [[0.6556, 0.2963],
         [0.6430, 0.6379]],

        [[0.5536, 0.2040],
         [0.5013, 0.5780]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0007748402262652058
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: -0.0006084154285211432
Average Adjusted Rand Index: 0.005585537292244871
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20377.84375
inf tensor(20377.8438, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12356.2431640625
tensor(20377.8438, grad_fn=<NegBackward0>) tensor(12356.2432, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12355.888671875
tensor(12356.2432, grad_fn=<NegBackward0>) tensor(12355.8887, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12355.818359375
tensor(12355.8887, grad_fn=<NegBackward0>) tensor(12355.8184, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12355.7705078125
tensor(12355.8184, grad_fn=<NegBackward0>) tensor(12355.7705, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12355.7353515625
tensor(12355.7705, grad_fn=<NegBackward0>) tensor(12355.7354, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12355.7041015625
tensor(12355.7354, grad_fn=<NegBackward0>) tensor(12355.7041, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12355.6728515625
tensor(12355.7041, grad_fn=<NegBackward0>) tensor(12355.6729, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12355.638671875
tensor(12355.6729, grad_fn=<NegBackward0>) tensor(12355.6387, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12355.5986328125
tensor(12355.6387, grad_fn=<NegBackward0>) tensor(12355.5986, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12355.5595703125
tensor(12355.5986, grad_fn=<NegBackward0>) tensor(12355.5596, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12355.51953125
tensor(12355.5596, grad_fn=<NegBackward0>) tensor(12355.5195, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12355.4833984375
tensor(12355.5195, grad_fn=<NegBackward0>) tensor(12355.4834, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12355.4453125
tensor(12355.4834, grad_fn=<NegBackward0>) tensor(12355.4453, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12355.40625
tensor(12355.4453, grad_fn=<NegBackward0>) tensor(12355.4062, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12355.36328125
tensor(12355.4062, grad_fn=<NegBackward0>) tensor(12355.3633, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12355.3193359375
tensor(12355.3633, grad_fn=<NegBackward0>) tensor(12355.3193, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12355.2822265625
tensor(12355.3193, grad_fn=<NegBackward0>) tensor(12355.2822, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12355.2509765625
tensor(12355.2822, grad_fn=<NegBackward0>) tensor(12355.2510, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12355.2333984375
tensor(12355.2510, grad_fn=<NegBackward0>) tensor(12355.2334, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12355.2216796875
tensor(12355.2334, grad_fn=<NegBackward0>) tensor(12355.2217, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12355.2158203125
tensor(12355.2217, grad_fn=<NegBackward0>) tensor(12355.2158, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12355.2109375
tensor(12355.2158, grad_fn=<NegBackward0>) tensor(12355.2109, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12355.2099609375
tensor(12355.2109, grad_fn=<NegBackward0>) tensor(12355.2100, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12355.20703125
tensor(12355.2100, grad_fn=<NegBackward0>) tensor(12355.2070, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12355.2060546875
tensor(12355.2070, grad_fn=<NegBackward0>) tensor(12355.2061, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12355.2041015625
tensor(12355.2061, grad_fn=<NegBackward0>) tensor(12355.2041, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12355.201171875
tensor(12355.2041, grad_fn=<NegBackward0>) tensor(12355.2012, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12355.19921875
tensor(12355.2012, grad_fn=<NegBackward0>) tensor(12355.1992, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12355.1962890625
tensor(12355.1992, grad_fn=<NegBackward0>) tensor(12355.1963, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12355.1943359375
tensor(12355.1963, grad_fn=<NegBackward0>) tensor(12355.1943, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12355.193359375
tensor(12355.1943, grad_fn=<NegBackward0>) tensor(12355.1934, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12355.1904296875
tensor(12355.1934, grad_fn=<NegBackward0>) tensor(12355.1904, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12355.1884765625
tensor(12355.1904, grad_fn=<NegBackward0>) tensor(12355.1885, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12355.1875
tensor(12355.1885, grad_fn=<NegBackward0>) tensor(12355.1875, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12355.1875
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.1875, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12355.18359375
tensor(12355.1875, grad_fn=<NegBackward0>) tensor(12355.1836, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12355.185546875
tensor(12355.1836, grad_fn=<NegBackward0>) tensor(12355.1855, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12355.18359375
tensor(12355.1836, grad_fn=<NegBackward0>) tensor(12355.1836, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12355.1845703125
tensor(12355.1836, grad_fn=<NegBackward0>) tensor(12355.1846, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12355.1826171875
tensor(12355.1836, grad_fn=<NegBackward0>) tensor(12355.1826, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12355.1826171875
tensor(12355.1826, grad_fn=<NegBackward0>) tensor(12355.1826, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12355.1826171875
tensor(12355.1826, grad_fn=<NegBackward0>) tensor(12355.1826, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12355.1806640625
tensor(12355.1826, grad_fn=<NegBackward0>) tensor(12355.1807, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12355.1796875
tensor(12355.1807, grad_fn=<NegBackward0>) tensor(12355.1797, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12355.1806640625
tensor(12355.1797, grad_fn=<NegBackward0>) tensor(12355.1807, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12355.181640625
tensor(12355.1797, grad_fn=<NegBackward0>) tensor(12355.1816, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12355.1787109375
tensor(12355.1797, grad_fn=<NegBackward0>) tensor(12355.1787, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12355.1787109375
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1787, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12355.1796875
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1797, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12355.181640625
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1816, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12355.1787109375
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1787, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12355.1806640625
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1807, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12355.1787109375
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1787, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12355.1796875
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1797, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12355.177734375
tensor(12355.1787, grad_fn=<NegBackward0>) tensor(12355.1777, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12355.1767578125
tensor(12355.1777, grad_fn=<NegBackward0>) tensor(12355.1768, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12355.177734375
tensor(12355.1768, grad_fn=<NegBackward0>) tensor(12355.1777, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12355.1767578125
tensor(12355.1768, grad_fn=<NegBackward0>) tensor(12355.1768, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12355.177734375
tensor(12355.1768, grad_fn=<NegBackward0>) tensor(12355.1777, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12355.1767578125
tensor(12355.1768, grad_fn=<NegBackward0>) tensor(12355.1768, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12355.1767578125
tensor(12355.1768, grad_fn=<NegBackward0>) tensor(12355.1768, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12355.17578125
tensor(12355.1768, grad_fn=<NegBackward0>) tensor(12355.1758, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12355.17578125
tensor(12355.1758, grad_fn=<NegBackward0>) tensor(12355.1758, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12355.177734375
tensor(12355.1758, grad_fn=<NegBackward0>) tensor(12355.1777, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12355.177734375
tensor(12355.1758, grad_fn=<NegBackward0>) tensor(12355.1777, grad_fn=<NegBackward0>)
2
Iteration 6600: Loss = -12355.1767578125
tensor(12355.1758, grad_fn=<NegBackward0>) tensor(12355.1768, grad_fn=<NegBackward0>)
3
Iteration 6700: Loss = -12355.17578125
tensor(12355.1758, grad_fn=<NegBackward0>) tensor(12355.1758, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12355.1748046875
tensor(12355.1758, grad_fn=<NegBackward0>) tensor(12355.1748, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12355.203125
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.2031, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12355.205078125
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.2051, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12355.220703125
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.2207, grad_fn=<NegBackward0>)
3
Iteration 7200: Loss = -12355.181640625
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.1816, grad_fn=<NegBackward0>)
4
Iteration 7300: Loss = -12355.1748046875
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.1748, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12355.189453125
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.1895, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12355.1728515625
tensor(12355.1748, grad_fn=<NegBackward0>) tensor(12355.1729, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12355.1787109375
tensor(12355.1729, grad_fn=<NegBackward0>) tensor(12355.1787, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12355.1748046875
tensor(12355.1729, grad_fn=<NegBackward0>) tensor(12355.1748, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12355.181640625
tensor(12355.1729, grad_fn=<NegBackward0>) tensor(12355.1816, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -12355.17578125
tensor(12355.1729, grad_fn=<NegBackward0>) tensor(12355.1758, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -12355.3544921875
tensor(12355.1729, grad_fn=<NegBackward0>) tensor(12355.3545, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.9573, 0.0427],
        [0.9985, 0.0015]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9400, 0.0600], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.1853],
         [0.6916, 0.1609]],

        [[0.5321, 0.2111],
         [0.6731, 0.6002]],

        [[0.6186, 0.1749],
         [0.6976, 0.6934]],

        [[0.7236, 0.1475],
         [0.5343, 0.5334]],

        [[0.6392, 0.1522],
         [0.6268, 0.6236]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[-0.0006084154285211432, 0.0] [0.005585537292244871, 0.0] [12353.9921875, 12355.3544921875]
-------------------------------------
This iteration is 85
True Objective function: Loss = -11990.579974257842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21697.322265625
inf tensor(21697.3223, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12491.14453125
tensor(21697.3223, grad_fn=<NegBackward0>) tensor(12491.1445, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12490.9443359375
tensor(12491.1445, grad_fn=<NegBackward0>) tensor(12490.9443, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12490.8642578125
tensor(12490.9443, grad_fn=<NegBackward0>) tensor(12490.8643, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12490.783203125
tensor(12490.8643, grad_fn=<NegBackward0>) tensor(12490.7832, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12490.66796875
tensor(12490.7832, grad_fn=<NegBackward0>) tensor(12490.6680, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12490.4580078125
tensor(12490.6680, grad_fn=<NegBackward0>) tensor(12490.4580, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12489.8837890625
tensor(12490.4580, grad_fn=<NegBackward0>) tensor(12489.8838, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12486.623046875
tensor(12489.8838, grad_fn=<NegBackward0>) tensor(12486.6230, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12484.6875
tensor(12486.6230, grad_fn=<NegBackward0>) tensor(12484.6875, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12484.0517578125
tensor(12484.6875, grad_fn=<NegBackward0>) tensor(12484.0518, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12483.89453125
tensor(12484.0518, grad_fn=<NegBackward0>) tensor(12483.8945, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12483.841796875
tensor(12483.8945, grad_fn=<NegBackward0>) tensor(12483.8418, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12483.814453125
tensor(12483.8418, grad_fn=<NegBackward0>) tensor(12483.8145, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12483.798828125
tensor(12483.8145, grad_fn=<NegBackward0>) tensor(12483.7988, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12483.7880859375
tensor(12483.7988, grad_fn=<NegBackward0>) tensor(12483.7881, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12483.7802734375
tensor(12483.7881, grad_fn=<NegBackward0>) tensor(12483.7803, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12483.7705078125
tensor(12483.7803, grad_fn=<NegBackward0>) tensor(12483.7705, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12483.7587890625
tensor(12483.7705, grad_fn=<NegBackward0>) tensor(12483.7588, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12483.74609375
tensor(12483.7588, grad_fn=<NegBackward0>) tensor(12483.7461, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12483.7392578125
tensor(12483.7461, grad_fn=<NegBackward0>) tensor(12483.7393, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12483.732421875
tensor(12483.7393, grad_fn=<NegBackward0>) tensor(12483.7324, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12483.728515625
tensor(12483.7324, grad_fn=<NegBackward0>) tensor(12483.7285, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12483.7236328125
tensor(12483.7285, grad_fn=<NegBackward0>) tensor(12483.7236, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12483.716796875
tensor(12483.7236, grad_fn=<NegBackward0>) tensor(12483.7168, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12483.703125
tensor(12483.7168, grad_fn=<NegBackward0>) tensor(12483.7031, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12483.6962890625
tensor(12483.7031, grad_fn=<NegBackward0>) tensor(12483.6963, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12483.6904296875
tensor(12483.6963, grad_fn=<NegBackward0>) tensor(12483.6904, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12483.6826171875
tensor(12483.6904, grad_fn=<NegBackward0>) tensor(12483.6826, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12483.673828125
tensor(12483.6826, grad_fn=<NegBackward0>) tensor(12483.6738, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12483.6669921875
tensor(12483.6738, grad_fn=<NegBackward0>) tensor(12483.6670, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12483.6640625
tensor(12483.6670, grad_fn=<NegBackward0>) tensor(12483.6641, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12483.662109375
tensor(12483.6641, grad_fn=<NegBackward0>) tensor(12483.6621, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12483.662109375
tensor(12483.6621, grad_fn=<NegBackward0>) tensor(12483.6621, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12483.662109375
tensor(12483.6621, grad_fn=<NegBackward0>) tensor(12483.6621, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12483.6591796875
tensor(12483.6621, grad_fn=<NegBackward0>) tensor(12483.6592, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12483.6376953125
tensor(12483.6592, grad_fn=<NegBackward0>) tensor(12483.6377, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12483.619140625
tensor(12483.6377, grad_fn=<NegBackward0>) tensor(12483.6191, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12483.619140625
tensor(12483.6191, grad_fn=<NegBackward0>) tensor(12483.6191, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12483.6171875
tensor(12483.6191, grad_fn=<NegBackward0>) tensor(12483.6172, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12483.611328125
tensor(12483.6172, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12483.6123046875
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
1
Iteration 4200: Loss = -12483.6123046875
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
2
Iteration 4300: Loss = -12483.611328125
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12483.6103515625
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12483.6123046875
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12483.6123046875
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
2
Iteration 4700: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
3
Iteration 4800: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12483.6123046875
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12483.609375
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6094, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12483.611328125
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
2
Iteration 5900: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
3
Iteration 6000: Loss = -12483.611328125
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
4
Iteration 6100: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6100 due to no improvement.
pi: tensor([[0.1783, 0.8217],
        [0.0644, 0.9356]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0196, 0.9804], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3221, 0.2717],
         [0.6789, 0.2025]],

        [[0.5069, 0.2409],
         [0.5592, 0.5365]],

        [[0.6514, 0.2332],
         [0.6971, 0.5503]],

        [[0.5347, 0.2373],
         [0.5244, 0.5916]],

        [[0.5172, 0.1042],
         [0.5837, 0.7287]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 65
Adjusted Rand Index: 0.07000822034691945
Global Adjusted Rand Index: 0.0029058223422229563
Average Adjusted Rand Index: 0.013274371342111162
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21094.1875
inf tensor(21094.1875, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12491.3916015625
tensor(21094.1875, grad_fn=<NegBackward0>) tensor(12491.3916, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12491.021484375
tensor(12491.3916, grad_fn=<NegBackward0>) tensor(12491.0215, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12490.8583984375
tensor(12491.0215, grad_fn=<NegBackward0>) tensor(12490.8584, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12490.572265625
tensor(12490.8584, grad_fn=<NegBackward0>) tensor(12490.5723, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12488.5263671875
tensor(12490.5723, grad_fn=<NegBackward0>) tensor(12488.5264, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12484.9228515625
tensor(12488.5264, grad_fn=<NegBackward0>) tensor(12484.9229, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12484.140625
tensor(12484.9229, grad_fn=<NegBackward0>) tensor(12484.1406, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12483.890625
tensor(12484.1406, grad_fn=<NegBackward0>) tensor(12483.8906, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12483.80078125
tensor(12483.8906, grad_fn=<NegBackward0>) tensor(12483.8008, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12483.7666015625
tensor(12483.8008, grad_fn=<NegBackward0>) tensor(12483.7666, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12483.74609375
tensor(12483.7666, grad_fn=<NegBackward0>) tensor(12483.7461, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12483.7275390625
tensor(12483.7461, grad_fn=<NegBackward0>) tensor(12483.7275, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12483.7119140625
tensor(12483.7275, grad_fn=<NegBackward0>) tensor(12483.7119, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12483.69140625
tensor(12483.7119, grad_fn=<NegBackward0>) tensor(12483.6914, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12483.6630859375
tensor(12483.6914, grad_fn=<NegBackward0>) tensor(12483.6631, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12483.6455078125
tensor(12483.6631, grad_fn=<NegBackward0>) tensor(12483.6455, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12483.6318359375
tensor(12483.6455, grad_fn=<NegBackward0>) tensor(12483.6318, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12483.6240234375
tensor(12483.6318, grad_fn=<NegBackward0>) tensor(12483.6240, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12483.6171875
tensor(12483.6240, grad_fn=<NegBackward0>) tensor(12483.6172, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12483.615234375
tensor(12483.6172, grad_fn=<NegBackward0>) tensor(12483.6152, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12483.6142578125
tensor(12483.6152, grad_fn=<NegBackward0>) tensor(12483.6143, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12483.6142578125
tensor(12483.6143, grad_fn=<NegBackward0>) tensor(12483.6143, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12483.6123046875
tensor(12483.6143, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12483.611328125
tensor(12483.6123, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12483.6142578125
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6143, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -12483.6123046875
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6123, grad_fn=<NegBackward0>)
2
Iteration 2700: Loss = -12483.611328125
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12483.611328125
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12483.611328125
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12483.611328125
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12483.6103515625
tensor(12483.6113, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -12483.611328125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -12483.6142578125
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6143, grad_fn=<NegBackward0>)
3
Iteration 4100: Loss = -12483.6103515625
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12483.609375
tensor(12483.6104, grad_fn=<NegBackward0>) tensor(12483.6094, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12483.611328125
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
2
Iteration 4500: Loss = -12483.609375
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6094, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12483.611328125
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12483.609375
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6094, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12483.609375
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6094, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12483.611328125
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6113, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
4
Iteration 5300: Loss = -12483.6103515625
tensor(12483.6094, grad_fn=<NegBackward0>) tensor(12483.6104, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5300 due to no improvement.
pi: tensor([[0.9355, 0.0645],
        [0.8214, 0.1786]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9804, 0.0196], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2025, 0.2717],
         [0.6660, 0.3222]],

        [[0.5701, 0.2409],
         [0.5619, 0.5132]],

        [[0.5263, 0.2332],
         [0.5328, 0.6549]],

        [[0.6305, 0.2373],
         [0.5356, 0.6317]],

        [[0.6616, 0.1042],
         [0.6792, 0.5899]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.07000822034691945
Global Adjusted Rand Index: 0.0029058223422229563
Average Adjusted Rand Index: 0.013274371342111162
[0.0029058223422229563, 0.0029058223422229563] [0.013274371342111162, 0.013274371342111162] [12483.6103515625, 12483.6103515625]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11943.116837346859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23311.880859375
inf tensor(23311.8809, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12428.0810546875
tensor(23311.8809, grad_fn=<NegBackward0>) tensor(12428.0811, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12427.7626953125
tensor(12428.0811, grad_fn=<NegBackward0>) tensor(12427.7627, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12427.5419921875
tensor(12427.7627, grad_fn=<NegBackward0>) tensor(12427.5420, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12427.2001953125
tensor(12427.5420, grad_fn=<NegBackward0>) tensor(12427.2002, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12426.9951171875
tensor(12427.2002, grad_fn=<NegBackward0>) tensor(12426.9951, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12426.853515625
tensor(12426.9951, grad_fn=<NegBackward0>) tensor(12426.8535, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12426.7294921875
tensor(12426.8535, grad_fn=<NegBackward0>) tensor(12426.7295, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12426.6181640625
tensor(12426.7295, grad_fn=<NegBackward0>) tensor(12426.6182, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12426.5126953125
tensor(12426.6182, grad_fn=<NegBackward0>) tensor(12426.5127, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12425.96484375
tensor(12426.5127, grad_fn=<NegBackward0>) tensor(12425.9648, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12347.8046875
tensor(12425.9648, grad_fn=<NegBackward0>) tensor(12347.8047, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12347.3505859375
tensor(12347.8047, grad_fn=<NegBackward0>) tensor(12347.3506, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12347.0615234375
tensor(12347.3506, grad_fn=<NegBackward0>) tensor(12347.0615, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12346.7734375
tensor(12347.0615, grad_fn=<NegBackward0>) tensor(12346.7734, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12346.59375
tensor(12346.7734, grad_fn=<NegBackward0>) tensor(12346.5938, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12346.4658203125
tensor(12346.5938, grad_fn=<NegBackward0>) tensor(12346.4658, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12346.28125
tensor(12346.4658, grad_fn=<NegBackward0>) tensor(12346.2812, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12346.11328125
tensor(12346.2812, grad_fn=<NegBackward0>) tensor(12346.1133, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12346.01953125
tensor(12346.1133, grad_fn=<NegBackward0>) tensor(12346.0195, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12345.9716796875
tensor(12346.0195, grad_fn=<NegBackward0>) tensor(12345.9717, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12345.94140625
tensor(12345.9717, grad_fn=<NegBackward0>) tensor(12345.9414, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12345.919921875
tensor(12345.9414, grad_fn=<NegBackward0>) tensor(12345.9199, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12345.9052734375
tensor(12345.9199, grad_fn=<NegBackward0>) tensor(12345.9053, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12345.8935546875
tensor(12345.9053, grad_fn=<NegBackward0>) tensor(12345.8936, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12345.8837890625
tensor(12345.8936, grad_fn=<NegBackward0>) tensor(12345.8838, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12345.876953125
tensor(12345.8838, grad_fn=<NegBackward0>) tensor(12345.8770, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12345.87109375
tensor(12345.8770, grad_fn=<NegBackward0>) tensor(12345.8711, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12345.8671875
tensor(12345.8711, grad_fn=<NegBackward0>) tensor(12345.8672, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12345.8642578125
tensor(12345.8672, grad_fn=<NegBackward0>) tensor(12345.8643, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12345.859375
tensor(12345.8643, grad_fn=<NegBackward0>) tensor(12345.8594, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12345.857421875
tensor(12345.8594, grad_fn=<NegBackward0>) tensor(12345.8574, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12345.8544921875
tensor(12345.8574, grad_fn=<NegBackward0>) tensor(12345.8545, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12345.8525390625
tensor(12345.8545, grad_fn=<NegBackward0>) tensor(12345.8525, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12345.849609375
tensor(12345.8525, grad_fn=<NegBackward0>) tensor(12345.8496, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12345.849609375
tensor(12345.8496, grad_fn=<NegBackward0>) tensor(12345.8496, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12345.8486328125
tensor(12345.8496, grad_fn=<NegBackward0>) tensor(12345.8486, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12345.8466796875
tensor(12345.8486, grad_fn=<NegBackward0>) tensor(12345.8467, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12345.845703125
tensor(12345.8467, grad_fn=<NegBackward0>) tensor(12345.8457, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12345.8447265625
tensor(12345.8457, grad_fn=<NegBackward0>) tensor(12345.8447, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12345.84375
tensor(12345.8447, grad_fn=<NegBackward0>) tensor(12345.8438, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12345.84375
tensor(12345.8438, grad_fn=<NegBackward0>) tensor(12345.8438, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12345.841796875
tensor(12345.8438, grad_fn=<NegBackward0>) tensor(12345.8418, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12345.8408203125
tensor(12345.8418, grad_fn=<NegBackward0>) tensor(12345.8408, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12345.8408203125
tensor(12345.8408, grad_fn=<NegBackward0>) tensor(12345.8408, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12345.83984375
tensor(12345.8408, grad_fn=<NegBackward0>) tensor(12345.8398, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12345.83984375
tensor(12345.8398, grad_fn=<NegBackward0>) tensor(12345.8398, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12345.8388671875
tensor(12345.8398, grad_fn=<NegBackward0>) tensor(12345.8389, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12345.8388671875
tensor(12345.8389, grad_fn=<NegBackward0>) tensor(12345.8389, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12345.841796875
tensor(12345.8389, grad_fn=<NegBackward0>) tensor(12345.8418, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12345.8369140625
tensor(12345.8389, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12345.8369140625
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12345.837890625
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8379, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12345.837890625
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8379, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12345.8349609375
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12345.8369140625
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12345.8359375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12345.8359375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12345.8349609375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12345.833984375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12345.845703125
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8457, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12345.8349609375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -12345.8349609375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
3
Iteration 6300: Loss = -12345.833984375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12345.8330078125
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12345.8330078125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12345.8349609375
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12345.83203125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12345.8349609375
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12345.8330078125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12345.833984375
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
3
Iteration 7100: Loss = -12345.8310546875
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
3
Iteration 7500: Loss = -12345.8662109375
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8662, grad_fn=<NegBackward0>)
4
Iteration 7600: Loss = -12345.837890625
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8379, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[2.2650e-05, 9.9998e-01],
        [2.4492e-02, 9.7551e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5634, 0.4366], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3200, 0.1035],
         [0.5439, 0.2051]],

        [[0.6508, 0.1056],
         [0.5333, 0.6829]],

        [[0.5108, 0.1855],
         [0.7008, 0.6274]],

        [[0.5028, 0.2288],
         [0.7167, 0.6584]],

        [[0.6951, 0.1167],
         [0.7167, 0.7066]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.040242465118768625
Average Adjusted Rand Index: 0.20069575300768228
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21300.81640625
inf tensor(21300.8164, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12427.8623046875
tensor(21300.8164, grad_fn=<NegBackward0>) tensor(12427.8623, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12427.6123046875
tensor(12427.8623, grad_fn=<NegBackward0>) tensor(12427.6123, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12427.3984375
tensor(12427.6123, grad_fn=<NegBackward0>) tensor(12427.3984, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12427.119140625
tensor(12427.3984, grad_fn=<NegBackward0>) tensor(12427.1191, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12426.9384765625
tensor(12427.1191, grad_fn=<NegBackward0>) tensor(12426.9385, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12426.7275390625
tensor(12426.9385, grad_fn=<NegBackward0>) tensor(12426.7275, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12426.494140625
tensor(12426.7275, grad_fn=<NegBackward0>) tensor(12426.4941, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12426.2890625
tensor(12426.4941, grad_fn=<NegBackward0>) tensor(12426.2891, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12426.16015625
tensor(12426.2891, grad_fn=<NegBackward0>) tensor(12426.1602, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12426.10546875
tensor(12426.1602, grad_fn=<NegBackward0>) tensor(12426.1055, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12426.072265625
tensor(12426.1055, grad_fn=<NegBackward0>) tensor(12426.0723, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12426.021484375
tensor(12426.0723, grad_fn=<NegBackward0>) tensor(12426.0215, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12348.2412109375
tensor(12426.0215, grad_fn=<NegBackward0>) tensor(12348.2412, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12346.818359375
tensor(12348.2412, grad_fn=<NegBackward0>) tensor(12346.8184, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12346.69921875
tensor(12346.8184, grad_fn=<NegBackward0>) tensor(12346.6992, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12346.615234375
tensor(12346.6992, grad_fn=<NegBackward0>) tensor(12346.6152, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12346.521484375
tensor(12346.6152, grad_fn=<NegBackward0>) tensor(12346.5215, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12346.4208984375
tensor(12346.5215, grad_fn=<NegBackward0>) tensor(12346.4209, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12346.2294921875
tensor(12346.4209, grad_fn=<NegBackward0>) tensor(12346.2295, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12346.0400390625
tensor(12346.2295, grad_fn=<NegBackward0>) tensor(12346.0400, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12345.95703125
tensor(12346.0400, grad_fn=<NegBackward0>) tensor(12345.9570, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12345.921875
tensor(12345.9570, grad_fn=<NegBackward0>) tensor(12345.9219, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12345.9013671875
tensor(12345.9219, grad_fn=<NegBackward0>) tensor(12345.9014, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12345.88671875
tensor(12345.9014, grad_fn=<NegBackward0>) tensor(12345.8867, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12345.8779296875
tensor(12345.8867, grad_fn=<NegBackward0>) tensor(12345.8779, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12345.8701171875
tensor(12345.8779, grad_fn=<NegBackward0>) tensor(12345.8701, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12345.8642578125
tensor(12345.8701, grad_fn=<NegBackward0>) tensor(12345.8643, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12345.8603515625
tensor(12345.8643, grad_fn=<NegBackward0>) tensor(12345.8604, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12345.857421875
tensor(12345.8604, grad_fn=<NegBackward0>) tensor(12345.8574, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12345.853515625
tensor(12345.8574, grad_fn=<NegBackward0>) tensor(12345.8535, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12345.8505859375
tensor(12345.8535, grad_fn=<NegBackward0>) tensor(12345.8506, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12345.8486328125
tensor(12345.8506, grad_fn=<NegBackward0>) tensor(12345.8486, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12345.84765625
tensor(12345.8486, grad_fn=<NegBackward0>) tensor(12345.8477, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12345.845703125
tensor(12345.8477, grad_fn=<NegBackward0>) tensor(12345.8457, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12345.845703125
tensor(12345.8457, grad_fn=<NegBackward0>) tensor(12345.8457, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12345.84375
tensor(12345.8457, grad_fn=<NegBackward0>) tensor(12345.8438, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12345.841796875
tensor(12345.8438, grad_fn=<NegBackward0>) tensor(12345.8418, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12345.841796875
tensor(12345.8418, grad_fn=<NegBackward0>) tensor(12345.8418, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12345.83984375
tensor(12345.8418, grad_fn=<NegBackward0>) tensor(12345.8398, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12345.83984375
tensor(12345.8398, grad_fn=<NegBackward0>) tensor(12345.8398, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12345.8388671875
tensor(12345.8398, grad_fn=<NegBackward0>) tensor(12345.8389, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12345.8388671875
tensor(12345.8389, grad_fn=<NegBackward0>) tensor(12345.8389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12345.83984375
tensor(12345.8389, grad_fn=<NegBackward0>) tensor(12345.8398, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12345.8369140625
tensor(12345.8389, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12345.8369140625
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12345.837890625
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8379, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12345.8369140625
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12345.8349609375
tensor(12345.8369, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12345.8359375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12345.8359375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12345.8359375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
3
Iteration 5200: Loss = -12345.8349609375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12345.833984375
tensor(12345.8350, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12345.8349609375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12345.8349609375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12345.8349609375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12345.8359375
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12345.8330078125
tensor(12345.8340, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12345.8330078125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12345.8330078125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12345.8330078125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12345.833984375
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
1
Iteration 6300: Loss = -12345.8330078125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12345.8330078125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12345.833984375
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12345.83203125
tensor(12345.8330, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12345.8349609375
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12345.83203125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12345.8330078125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12345.8408203125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8408, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12345.83203125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12345.83203125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12345.841796875
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8418, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12345.83203125
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12345.833984375
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12345.8310546875
tensor(12345.8320, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12345.8369140625
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8369, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12345.8359375
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8359, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
4
Iteration 8100: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12346.0830078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12346.0830, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12345.83203125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12345.9111328125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.9111, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12345.833984375
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8340, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -12345.8349609375
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8350, grad_fn=<NegBackward0>)
1
Iteration 8900: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12345.83203125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12346.0244140625
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12346.0244, grad_fn=<NegBackward0>)
2
Iteration 9300: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12345.8330078125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8330, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12345.83203125
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8320, grad_fn=<NegBackward0>)
2
Iteration 9900: Loss = -12345.8310546875
tensor(12345.8311, grad_fn=<NegBackward0>) tensor(12345.8311, grad_fn=<NegBackward0>)
pi: tensor([[9.7542e-01, 2.4576e-02],
        [9.9999e-01, 7.9228e-06]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4367, 0.5633], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2055, 0.1035],
         [0.7084, 0.3201]],

        [[0.6899, 0.1055],
         [0.6669, 0.5891]],

        [[0.7004, 0.1855],
         [0.5582, 0.6270]],

        [[0.6834, 0.2288],
         [0.5611, 0.6023]],

        [[0.5861, 0.1167],
         [0.6531, 0.5305]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.040242465118768625
Average Adjusted Rand Index: 0.20069575300768228
[0.040242465118768625, 0.040242465118768625] [0.20069575300768228, 0.20069575300768228] [12345.837890625, 12345.8310546875]
-------------------------------------
This iteration is 87
True Objective function: Loss = -12031.005919698497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22639.8984375
inf tensor(22639.8984, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12499.294921875
tensor(22639.8984, grad_fn=<NegBackward0>) tensor(12499.2949, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12498.8330078125
tensor(12499.2949, grad_fn=<NegBackward0>) tensor(12498.8330, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12498.505859375
tensor(12498.8330, grad_fn=<NegBackward0>) tensor(12498.5059, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12498.1572265625
tensor(12498.5059, grad_fn=<NegBackward0>) tensor(12498.1572, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12497.9755859375
tensor(12498.1572, grad_fn=<NegBackward0>) tensor(12497.9756, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12497.853515625
tensor(12497.9756, grad_fn=<NegBackward0>) tensor(12497.8535, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12497.7607421875
tensor(12497.8535, grad_fn=<NegBackward0>) tensor(12497.7607, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12497.6982421875
tensor(12497.7607, grad_fn=<NegBackward0>) tensor(12497.6982, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12497.658203125
tensor(12497.6982, grad_fn=<NegBackward0>) tensor(12497.6582, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12497.6337890625
tensor(12497.6582, grad_fn=<NegBackward0>) tensor(12497.6338, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12497.6162109375
tensor(12497.6338, grad_fn=<NegBackward0>) tensor(12497.6162, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12497.607421875
tensor(12497.6162, grad_fn=<NegBackward0>) tensor(12497.6074, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12497.6025390625
tensor(12497.6074, grad_fn=<NegBackward0>) tensor(12497.6025, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12497.59765625
tensor(12497.6025, grad_fn=<NegBackward0>) tensor(12497.5977, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12497.595703125
tensor(12497.5977, grad_fn=<NegBackward0>) tensor(12497.5957, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12497.5947265625
tensor(12497.5957, grad_fn=<NegBackward0>) tensor(12497.5947, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12497.5927734375
tensor(12497.5947, grad_fn=<NegBackward0>) tensor(12497.5928, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12497.5908203125
tensor(12497.5928, grad_fn=<NegBackward0>) tensor(12497.5908, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12497.591796875
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5918, grad_fn=<NegBackward0>)
1
Iteration 2000: Loss = -12497.5908203125
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5908, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12497.591796875
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5918, grad_fn=<NegBackward0>)
1
Iteration 2200: Loss = -12497.5908203125
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5908, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12497.5908203125
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5908, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12497.58984375
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5898, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12497.58984375
tensor(12497.5898, grad_fn=<NegBackward0>) tensor(12497.5898, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12497.58984375
tensor(12497.5898, grad_fn=<NegBackward0>) tensor(12497.5898, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12497.5888671875
tensor(12497.5898, grad_fn=<NegBackward0>) tensor(12497.5889, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12497.58984375
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5898, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12497.587890625
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12497.5888671875
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5889, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -12497.587890625
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12497.587890625
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12497.5869140625
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12497.5869140625
tensor(12497.5869, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12497.5869140625
tensor(12497.5869, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12497.587890625
tensor(12497.5869, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12497.5869140625
tensor(12497.5869, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12497.5859375
tensor(12497.5869, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12497.5859375
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
4
Iteration 4400: Loss = -12497.5859375
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12497.5859375
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -12497.5859375
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12497.5849609375
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5850, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12497.5859375
tensor(12497.5850, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12497.5859375
tensor(12497.5850, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12497.5859375
tensor(12497.5850, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -12497.5859375
tensor(12497.5850, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -12497.59375
tensor(12497.5850, grad_fn=<NegBackward0>) tensor(12497.5938, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.0044, 0.9956],
        [0.0902, 0.9098]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0138, 0.9862], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2379, 0.1153],
         [0.7059, 0.2015]],

        [[0.6048, 0.2223],
         [0.5099, 0.5630]],

        [[0.7251, 0.1989],
         [0.5466, 0.5330]],

        [[0.5684, 0.2067],
         [0.5830, 0.6299]],

        [[0.5532, 0.2446],
         [0.5291, 0.6409]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23060.7578125
inf tensor(23060.7578, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12500.23828125
tensor(23060.7578, grad_fn=<NegBackward0>) tensor(12500.2383, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12498.9609375
tensor(12500.2383, grad_fn=<NegBackward0>) tensor(12498.9609, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12498.857421875
tensor(12498.9609, grad_fn=<NegBackward0>) tensor(12498.8574, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12498.7822265625
tensor(12498.8574, grad_fn=<NegBackward0>) tensor(12498.7822, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12498.724609375
tensor(12498.7822, grad_fn=<NegBackward0>) tensor(12498.7246, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12498.68359375
tensor(12498.7246, grad_fn=<NegBackward0>) tensor(12498.6836, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12498.65234375
tensor(12498.6836, grad_fn=<NegBackward0>) tensor(12498.6523, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12498.625
tensor(12498.6523, grad_fn=<NegBackward0>) tensor(12498.6250, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12498.5986328125
tensor(12498.6250, grad_fn=<NegBackward0>) tensor(12498.5986, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12498.556640625
tensor(12498.5986, grad_fn=<NegBackward0>) tensor(12498.5566, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12498.396484375
tensor(12498.5566, grad_fn=<NegBackward0>) tensor(12498.3965, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12497.8837890625
tensor(12498.3965, grad_fn=<NegBackward0>) tensor(12497.8838, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12497.767578125
tensor(12497.8838, grad_fn=<NegBackward0>) tensor(12497.7676, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12497.6875
tensor(12497.7676, grad_fn=<NegBackward0>) tensor(12497.6875, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12497.626953125
tensor(12497.6875, grad_fn=<NegBackward0>) tensor(12497.6270, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12497.6044921875
tensor(12497.6270, grad_fn=<NegBackward0>) tensor(12497.6045, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12497.5966796875
tensor(12497.6045, grad_fn=<NegBackward0>) tensor(12497.5967, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12497.59375
tensor(12497.5967, grad_fn=<NegBackward0>) tensor(12497.5938, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12497.59375
tensor(12497.5938, grad_fn=<NegBackward0>) tensor(12497.5938, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12497.59375
tensor(12497.5938, grad_fn=<NegBackward0>) tensor(12497.5938, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12497.591796875
tensor(12497.5938, grad_fn=<NegBackward0>) tensor(12497.5918, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12497.5908203125
tensor(12497.5918, grad_fn=<NegBackward0>) tensor(12497.5908, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12497.591796875
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5918, grad_fn=<NegBackward0>)
1
Iteration 2400: Loss = -12497.5888671875
tensor(12497.5908, grad_fn=<NegBackward0>) tensor(12497.5889, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12497.5888671875
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5889, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12497.5888671875
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5889, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12497.58984375
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5898, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12497.5888671875
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5889, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12497.587890625
tensor(12497.5889, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12497.5908203125
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5908, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -12497.587890625
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12497.587890625
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12497.5859375
tensor(12497.5879, grad_fn=<NegBackward0>) tensor(12497.5859, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
2
Iteration 3600: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
3
Iteration 3700: Loss = -12497.587890625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5879, grad_fn=<NegBackward0>)
4
Iteration 3800: Loss = -12497.5869140625
tensor(12497.5859, grad_fn=<NegBackward0>) tensor(12497.5869, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3800 due to no improvement.
pi: tensor([[0.0091, 0.9909],
        [0.0892, 0.9108]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0137, 0.9863], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2379, 0.1154],
         [0.6606, 0.2015]],

        [[0.6510, 0.2222],
         [0.6144, 0.6557]],

        [[0.5119, 0.1990],
         [0.5116, 0.5459]],

        [[0.6355, 0.2068],
         [0.6252, 0.6129]],

        [[0.6050, 0.2445],
         [0.6257, 0.5168]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.0, 0.0] [0.0, 0.0] [12497.59375, 12497.5869140625]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11896.932164415199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20657.361328125
inf tensor(20657.3613, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12454.5576171875
tensor(20657.3613, grad_fn=<NegBackward0>) tensor(12454.5576, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12454.1044921875
tensor(12454.5576, grad_fn=<NegBackward0>) tensor(12454.1045, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12453.912109375
tensor(12454.1045, grad_fn=<NegBackward0>) tensor(12453.9121, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12453.76953125
tensor(12453.9121, grad_fn=<NegBackward0>) tensor(12453.7695, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12453.62890625
tensor(12453.7695, grad_fn=<NegBackward0>) tensor(12453.6289, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12453.4560546875
tensor(12453.6289, grad_fn=<NegBackward0>) tensor(12453.4561, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12453.2099609375
tensor(12453.4561, grad_fn=<NegBackward0>) tensor(12453.2100, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12452.931640625
tensor(12453.2100, grad_fn=<NegBackward0>) tensor(12452.9316, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12452.7119140625
tensor(12452.9316, grad_fn=<NegBackward0>) tensor(12452.7119, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12452.525390625
tensor(12452.7119, grad_fn=<NegBackward0>) tensor(12452.5254, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12452.3359375
tensor(12452.5254, grad_fn=<NegBackward0>) tensor(12452.3359, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12452.11328125
tensor(12452.3359, grad_fn=<NegBackward0>) tensor(12452.1133, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12451.7890625
tensor(12452.1133, grad_fn=<NegBackward0>) tensor(12451.7891, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12451.0390625
tensor(12451.7891, grad_fn=<NegBackward0>) tensor(12451.0391, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12445.9833984375
tensor(12451.0391, grad_fn=<NegBackward0>) tensor(12445.9834, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12127.8056640625
tensor(12445.9834, grad_fn=<NegBackward0>) tensor(12127.8057, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12086.7470703125
tensor(12127.8057, grad_fn=<NegBackward0>) tensor(12086.7471, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11950.2353515625
tensor(12086.7471, grad_fn=<NegBackward0>) tensor(11950.2354, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11884.8056640625
tensor(11950.2354, grad_fn=<NegBackward0>) tensor(11884.8057, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11884.5498046875
tensor(11884.8057, grad_fn=<NegBackward0>) tensor(11884.5498, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11884.48828125
tensor(11884.5498, grad_fn=<NegBackward0>) tensor(11884.4883, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11884.453125
tensor(11884.4883, grad_fn=<NegBackward0>) tensor(11884.4531, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11884.4296875
tensor(11884.4531, grad_fn=<NegBackward0>) tensor(11884.4297, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11884.4140625
tensor(11884.4297, grad_fn=<NegBackward0>) tensor(11884.4141, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11884.4013671875
tensor(11884.4141, grad_fn=<NegBackward0>) tensor(11884.4014, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11884.392578125
tensor(11884.4014, grad_fn=<NegBackward0>) tensor(11884.3926, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11884.396484375
tensor(11884.3926, grad_fn=<NegBackward0>) tensor(11884.3965, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -11884.37890625
tensor(11884.3926, grad_fn=<NegBackward0>) tensor(11884.3789, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11884.375
tensor(11884.3789, grad_fn=<NegBackward0>) tensor(11884.3750, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11884.369140625
tensor(11884.3750, grad_fn=<NegBackward0>) tensor(11884.3691, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11884.3642578125
tensor(11884.3691, grad_fn=<NegBackward0>) tensor(11884.3643, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11884.3701171875
tensor(11884.3643, grad_fn=<NegBackward0>) tensor(11884.3701, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11884.3583984375
tensor(11884.3643, grad_fn=<NegBackward0>) tensor(11884.3584, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11884.3564453125
tensor(11884.3584, grad_fn=<NegBackward0>) tensor(11884.3564, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11884.35546875
tensor(11884.3564, grad_fn=<NegBackward0>) tensor(11884.3555, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11884.3525390625
tensor(11884.3555, grad_fn=<NegBackward0>) tensor(11884.3525, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11884.3515625
tensor(11884.3525, grad_fn=<NegBackward0>) tensor(11884.3516, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11884.3525390625
tensor(11884.3516, grad_fn=<NegBackward0>) tensor(11884.3525, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11884.3486328125
tensor(11884.3516, grad_fn=<NegBackward0>) tensor(11884.3486, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11884.345703125
tensor(11884.3486, grad_fn=<NegBackward0>) tensor(11884.3457, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11884.34375
tensor(11884.3457, grad_fn=<NegBackward0>) tensor(11884.3438, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11884.3388671875
tensor(11884.3438, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11884.326171875
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3262, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11884.32421875
tensor(11884.3262, grad_fn=<NegBackward0>) tensor(11884.3242, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11884.326171875
tensor(11884.3242, grad_fn=<NegBackward0>) tensor(11884.3262, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11884.3203125
tensor(11884.3242, grad_fn=<NegBackward0>) tensor(11884.3203, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11884.31640625
tensor(11884.3203, grad_fn=<NegBackward0>) tensor(11884.3164, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11884.314453125
tensor(11884.3164, grad_fn=<NegBackward0>) tensor(11884.3145, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11884.3154296875
tensor(11884.3145, grad_fn=<NegBackward0>) tensor(11884.3154, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -11884.3134765625
tensor(11884.3145, grad_fn=<NegBackward0>) tensor(11884.3135, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11884.3125
tensor(11884.3135, grad_fn=<NegBackward0>) tensor(11884.3125, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11884.3125
tensor(11884.3125, grad_fn=<NegBackward0>) tensor(11884.3125, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11884.3125
tensor(11884.3125, grad_fn=<NegBackward0>) tensor(11884.3125, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11884.3125
tensor(11884.3125, grad_fn=<NegBackward0>) tensor(11884.3125, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11884.310546875
tensor(11884.3125, grad_fn=<NegBackward0>) tensor(11884.3105, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11884.3125
tensor(11884.3105, grad_fn=<NegBackward0>) tensor(11884.3125, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11884.310546875
tensor(11884.3105, grad_fn=<NegBackward0>) tensor(11884.3105, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11884.310546875
tensor(11884.3105, grad_fn=<NegBackward0>) tensor(11884.3105, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11884.3115234375
tensor(11884.3105, grad_fn=<NegBackward0>) tensor(11884.3115, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11884.310546875
tensor(11884.3105, grad_fn=<NegBackward0>) tensor(11884.3105, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11884.3095703125
tensor(11884.3105, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -11884.3095703125
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11884.310546875
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3105, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11884.3095703125
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11884.3095703125
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11884.3095703125
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11884.3095703125
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11884.30859375
tensor(11884.3096, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11884.30859375
tensor(11884.3086, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11884.3232421875
tensor(11884.3086, grad_fn=<NegBackward0>) tensor(11884.3232, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11884.30859375
tensor(11884.3086, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11884.30859375
tensor(11884.3086, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11884.3095703125
tensor(11884.3086, grad_fn=<NegBackward0>) tensor(11884.3096, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11884.3076171875
tensor(11884.3086, grad_fn=<NegBackward0>) tensor(11884.3076, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -11884.3076171875
tensor(11884.3076, grad_fn=<NegBackward0>) tensor(11884.3076, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11884.306640625
tensor(11884.3076, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11884.30859375
tensor(11884.3066, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11884.396484375
tensor(11884.3066, grad_fn=<NegBackward0>) tensor(11884.3965, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11884.306640625
tensor(11884.3066, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11884.306640625
tensor(11884.3066, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11884.3056640625
tensor(11884.3066, grad_fn=<NegBackward0>) tensor(11884.3057, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -11884.3056640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3057, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11884.306640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11884.3076171875
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3076, grad_fn=<NegBackward0>)
2
Iteration 8500: Loss = -11884.30859375
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
3
Iteration 8600: Loss = -11884.306640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
4
Iteration 8700: Loss = -11884.3056640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3057, grad_fn=<NegBackward0>)
Iteration 8800: Loss = -11884.3056640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3057, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11884.3310546875
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3311, grad_fn=<NegBackward0>)
1
Iteration 9000: Loss = -11884.3115234375
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3115, grad_fn=<NegBackward0>)
2
Iteration 9100: Loss = -11884.306640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
3
Iteration 9200: Loss = -11884.306640625
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3066, grad_fn=<NegBackward0>)
4
Iteration 9300: Loss = -11884.30859375
tensor(11884.3057, grad_fn=<NegBackward0>) tensor(11884.3086, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.7364, 0.2636],
        [0.2918, 0.7082]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5220, 0.4780], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3025, 0.0990],
         [0.6288, 0.3116]],

        [[0.6529, 0.0968],
         [0.5335, 0.6929]],

        [[0.5707, 0.1057],
         [0.6794, 0.5584]],

        [[0.6812, 0.0976],
         [0.6038, 0.6487]],

        [[0.6568, 0.0913],
         [0.5976, 0.5735]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9603206498086512
Average Adjusted Rand Index: 0.9601603309856056
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25255.05078125
inf tensor(25255.0508, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12385.955078125
tensor(25255.0508, grad_fn=<NegBackward0>) tensor(12385.9551, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12383.083984375
tensor(12385.9551, grad_fn=<NegBackward0>) tensor(12383.0840, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12382.7685546875
tensor(12383.0840, grad_fn=<NegBackward0>) tensor(12382.7686, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12382.025390625
tensor(12382.7686, grad_fn=<NegBackward0>) tensor(12382.0254, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11918.6240234375
tensor(12382.0254, grad_fn=<NegBackward0>) tensor(11918.6240, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11898.5654296875
tensor(11918.6240, grad_fn=<NegBackward0>) tensor(11898.5654, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11897.966796875
tensor(11898.5654, grad_fn=<NegBackward0>) tensor(11897.9668, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11897.8603515625
tensor(11897.9668, grad_fn=<NegBackward0>) tensor(11897.8604, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11897.17578125
tensor(11897.8604, grad_fn=<NegBackward0>) tensor(11897.1758, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11897.158203125
tensor(11897.1758, grad_fn=<NegBackward0>) tensor(11897.1582, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11885.896484375
tensor(11897.1582, grad_fn=<NegBackward0>) tensor(11885.8965, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11884.3974609375
tensor(11885.8965, grad_fn=<NegBackward0>) tensor(11884.3975, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11884.390625
tensor(11884.3975, grad_fn=<NegBackward0>) tensor(11884.3906, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11884.3837890625
tensor(11884.3906, grad_fn=<NegBackward0>) tensor(11884.3838, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11884.3798828125
tensor(11884.3838, grad_fn=<NegBackward0>) tensor(11884.3799, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11884.3759765625
tensor(11884.3799, grad_fn=<NegBackward0>) tensor(11884.3760, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11884.3740234375
tensor(11884.3760, grad_fn=<NegBackward0>) tensor(11884.3740, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11884.373046875
tensor(11884.3740, grad_fn=<NegBackward0>) tensor(11884.3730, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11884.3681640625
tensor(11884.3730, grad_fn=<NegBackward0>) tensor(11884.3682, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11884.3662109375
tensor(11884.3682, grad_fn=<NegBackward0>) tensor(11884.3662, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11884.3662109375
tensor(11884.3662, grad_fn=<NegBackward0>) tensor(11884.3662, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11884.3623046875
tensor(11884.3662, grad_fn=<NegBackward0>) tensor(11884.3623, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11884.359375
tensor(11884.3623, grad_fn=<NegBackward0>) tensor(11884.3594, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11884.3583984375
tensor(11884.3594, grad_fn=<NegBackward0>) tensor(11884.3584, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11884.3564453125
tensor(11884.3584, grad_fn=<NegBackward0>) tensor(11884.3564, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11884.353515625
tensor(11884.3564, grad_fn=<NegBackward0>) tensor(11884.3535, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11884.349609375
tensor(11884.3535, grad_fn=<NegBackward0>) tensor(11884.3496, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11884.34765625
tensor(11884.3496, grad_fn=<NegBackward0>) tensor(11884.3477, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11884.3466796875
tensor(11884.3477, grad_fn=<NegBackward0>) tensor(11884.3467, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11884.345703125
tensor(11884.3467, grad_fn=<NegBackward0>) tensor(11884.3457, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11884.345703125
tensor(11884.3457, grad_fn=<NegBackward0>) tensor(11884.3457, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11884.3427734375
tensor(11884.3457, grad_fn=<NegBackward0>) tensor(11884.3428, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11884.341796875
tensor(11884.3428, grad_fn=<NegBackward0>) tensor(11884.3418, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11884.3408203125
tensor(11884.3418, grad_fn=<NegBackward0>) tensor(11884.3408, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11884.341796875
tensor(11884.3408, grad_fn=<NegBackward0>) tensor(11884.3418, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11884.3408203125
tensor(11884.3408, grad_fn=<NegBackward0>) tensor(11884.3408, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11884.33984375
tensor(11884.3408, grad_fn=<NegBackward0>) tensor(11884.3398, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11884.341796875
tensor(11884.3398, grad_fn=<NegBackward0>) tensor(11884.3418, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11884.3408203125
tensor(11884.3398, grad_fn=<NegBackward0>) tensor(11884.3408, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11884.3388671875
tensor(11884.3398, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11884.3388671875
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11884.3388671875
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11884.3388671875
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11884.3388671875
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11884.3388671875
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11884.337890625
tensor(11884.3389, grad_fn=<NegBackward0>) tensor(11884.3379, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11884.3388671875
tensor(11884.3379, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11884.337890625
tensor(11884.3379, grad_fn=<NegBackward0>) tensor(11884.3379, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11884.337890625
tensor(11884.3379, grad_fn=<NegBackward0>) tensor(11884.3379, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11884.3466796875
tensor(11884.3379, grad_fn=<NegBackward0>) tensor(11884.3467, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11884.3388671875
tensor(11884.3379, grad_fn=<NegBackward0>) tensor(11884.3389, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11884.3369140625
tensor(11884.3379, grad_fn=<NegBackward0>) tensor(11884.3369, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11884.3369140625
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3369, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11884.337890625
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3379, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11884.337890625
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3379, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11884.337890625
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3379, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11884.3369140625
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3369, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11884.3369140625
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3369, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11884.3359375
tensor(11884.3369, grad_fn=<NegBackward0>) tensor(11884.3359, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11884.333984375
tensor(11884.3359, grad_fn=<NegBackward0>) tensor(11884.3340, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11884.3515625
tensor(11884.3340, grad_fn=<NegBackward0>) tensor(11884.3516, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11884.3330078125
tensor(11884.3340, grad_fn=<NegBackward0>) tensor(11884.3330, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11884.35546875
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3555, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11884.3330078125
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3330, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11884.3359375
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3359, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11884.3369140625
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3369, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -11884.3359375
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3359, grad_fn=<NegBackward0>)
3
Iteration 6800: Loss = -11884.3369140625
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3369, grad_fn=<NegBackward0>)
4
Iteration 6900: Loss = -11884.3330078125
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3330, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11884.3310546875
tensor(11884.3330, grad_fn=<NegBackward0>) tensor(11884.3311, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11884.3330078125
tensor(11884.3311, grad_fn=<NegBackward0>) tensor(11884.3330, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11884.33203125
tensor(11884.3311, grad_fn=<NegBackward0>) tensor(11884.3320, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -11884.33203125
tensor(11884.3311, grad_fn=<NegBackward0>) tensor(11884.3320, grad_fn=<NegBackward0>)
3
Iteration 7400: Loss = -11884.33203125
tensor(11884.3311, grad_fn=<NegBackward0>) tensor(11884.3320, grad_fn=<NegBackward0>)
4
Iteration 7500: Loss = -11884.3408203125
tensor(11884.3311, grad_fn=<NegBackward0>) tensor(11884.3408, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7107, 0.2893],
        [0.2674, 0.7326]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4779, 0.5221], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3118, 0.0990],
         [0.5086, 0.3023]],

        [[0.6773, 0.0969],
         [0.5218, 0.6903]],

        [[0.6883, 0.1056],
         [0.5112, 0.5463]],

        [[0.6555, 0.0976],
         [0.5901, 0.7022]],

        [[0.6384, 0.0913],
         [0.6027, 0.6735]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9603206498086512
Average Adjusted Rand Index: 0.9601603309856056
[0.9603206498086512, 0.9603206498086512] [0.9601603309856056, 0.9601603309856056] [11884.30859375, 11884.3408203125]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11954.526849930035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20540.548828125
inf tensor(20540.5488, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12423.8076171875
tensor(20540.5488, grad_fn=<NegBackward0>) tensor(12423.8076, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12423.5498046875
tensor(12423.8076, grad_fn=<NegBackward0>) tensor(12423.5498, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12423.4775390625
tensor(12423.5498, grad_fn=<NegBackward0>) tensor(12423.4775, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12423.404296875
tensor(12423.4775, grad_fn=<NegBackward0>) tensor(12423.4043, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12423.3134765625
tensor(12423.4043, grad_fn=<NegBackward0>) tensor(12423.3135, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12423.216796875
tensor(12423.3135, grad_fn=<NegBackward0>) tensor(12423.2168, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12423.140625
tensor(12423.2168, grad_fn=<NegBackward0>) tensor(12423.1406, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12423.091796875
tensor(12423.1406, grad_fn=<NegBackward0>) tensor(12423.0918, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12423.060546875
tensor(12423.0918, grad_fn=<NegBackward0>) tensor(12423.0605, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12423.0322265625
tensor(12423.0605, grad_fn=<NegBackward0>) tensor(12423.0322, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12423.0048828125
tensor(12423.0322, grad_fn=<NegBackward0>) tensor(12423.0049, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12422.9765625
tensor(12423.0049, grad_fn=<NegBackward0>) tensor(12422.9766, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12422.94921875
tensor(12422.9766, grad_fn=<NegBackward0>) tensor(12422.9492, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12422.9150390625
tensor(12422.9492, grad_fn=<NegBackward0>) tensor(12422.9150, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12422.8798828125
tensor(12422.9150, grad_fn=<NegBackward0>) tensor(12422.8799, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12422.85546875
tensor(12422.8799, grad_fn=<NegBackward0>) tensor(12422.8555, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12422.837890625
tensor(12422.8555, grad_fn=<NegBackward0>) tensor(12422.8379, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12422.82421875
tensor(12422.8379, grad_fn=<NegBackward0>) tensor(12422.8242, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12422.814453125
tensor(12422.8242, grad_fn=<NegBackward0>) tensor(12422.8145, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12422.8046875
tensor(12422.8145, grad_fn=<NegBackward0>) tensor(12422.8047, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12422.796875
tensor(12422.8047, grad_fn=<NegBackward0>) tensor(12422.7969, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12422.7890625
tensor(12422.7969, grad_fn=<NegBackward0>) tensor(12422.7891, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12422.7841796875
tensor(12422.7891, grad_fn=<NegBackward0>) tensor(12422.7842, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12422.779296875
tensor(12422.7842, grad_fn=<NegBackward0>) tensor(12422.7793, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12422.7744140625
tensor(12422.7793, grad_fn=<NegBackward0>) tensor(12422.7744, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12422.7705078125
tensor(12422.7744, grad_fn=<NegBackward0>) tensor(12422.7705, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12422.76953125
tensor(12422.7705, grad_fn=<NegBackward0>) tensor(12422.7695, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12422.7646484375
tensor(12422.7695, grad_fn=<NegBackward0>) tensor(12422.7646, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12422.763671875
tensor(12422.7646, grad_fn=<NegBackward0>) tensor(12422.7637, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12422.7607421875
tensor(12422.7637, grad_fn=<NegBackward0>) tensor(12422.7607, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12422.7587890625
tensor(12422.7607, grad_fn=<NegBackward0>) tensor(12422.7588, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12422.7568359375
tensor(12422.7588, grad_fn=<NegBackward0>) tensor(12422.7568, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12422.755859375
tensor(12422.7568, grad_fn=<NegBackward0>) tensor(12422.7559, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12422.75390625
tensor(12422.7559, grad_fn=<NegBackward0>) tensor(12422.7539, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12422.751953125
tensor(12422.7539, grad_fn=<NegBackward0>) tensor(12422.7520, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12422.75
tensor(12422.7520, grad_fn=<NegBackward0>) tensor(12422.7500, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12422.75
tensor(12422.7500, grad_fn=<NegBackward0>) tensor(12422.7500, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12422.7470703125
tensor(12422.7500, grad_fn=<NegBackward0>) tensor(12422.7471, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12422.7451171875
tensor(12422.7471, grad_fn=<NegBackward0>) tensor(12422.7451, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12422.744140625
tensor(12422.7451, grad_fn=<NegBackward0>) tensor(12422.7441, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12422.7412109375
tensor(12422.7441, grad_fn=<NegBackward0>) tensor(12422.7412, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12422.7373046875
tensor(12422.7412, grad_fn=<NegBackward0>) tensor(12422.7373, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12422.73046875
tensor(12422.7373, grad_fn=<NegBackward0>) tensor(12422.7305, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12422.72265625
tensor(12422.7305, grad_fn=<NegBackward0>) tensor(12422.7227, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12422.703125
tensor(12422.7227, grad_fn=<NegBackward0>) tensor(12422.7031, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12422.65625
tensor(12422.7031, grad_fn=<NegBackward0>) tensor(12422.6562, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12422.564453125
tensor(12422.6562, grad_fn=<NegBackward0>) tensor(12422.5645, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12422.49609375
tensor(12422.5645, grad_fn=<NegBackward0>) tensor(12422.4961, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12422.4765625
tensor(12422.4961, grad_fn=<NegBackward0>) tensor(12422.4766, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12422.470703125
tensor(12422.4766, grad_fn=<NegBackward0>) tensor(12422.4707, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12422.470703125
tensor(12422.4707, grad_fn=<NegBackward0>) tensor(12422.4707, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12422.46875
tensor(12422.4707, grad_fn=<NegBackward0>) tensor(12422.4688, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12422.4677734375
tensor(12422.4688, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12422.4677734375
tensor(12422.4678, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12422.4658203125
tensor(12422.4678, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12422.4677734375
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12422.466796875
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4668, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12422.4677734375
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12422.466796875
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4668, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12422.4658203125
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4658, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12422.46484375
tensor(12422.4658, grad_fn=<NegBackward0>) tensor(12422.4648, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12422.4599609375
tensor(12422.4648, grad_fn=<NegBackward0>) tensor(12422.4600, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12422.3955078125
tensor(12422.4600, grad_fn=<NegBackward0>) tensor(12422.3955, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12422.068359375
tensor(12422.3955, grad_fn=<NegBackward0>) tensor(12422.0684, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12421.8203125
tensor(12422.0684, grad_fn=<NegBackward0>) tensor(12421.8203, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12421.705078125
tensor(12421.8203, grad_fn=<NegBackward0>) tensor(12421.7051, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12421.6748046875
tensor(12421.7051, grad_fn=<NegBackward0>) tensor(12421.6748, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12421.67578125
tensor(12421.6748, grad_fn=<NegBackward0>) tensor(12421.6758, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12421.654296875
tensor(12421.6748, grad_fn=<NegBackward0>) tensor(12421.6543, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12421.6513671875
tensor(12421.6543, grad_fn=<NegBackward0>) tensor(12421.6514, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12421.6494140625
tensor(12421.6514, grad_fn=<NegBackward0>) tensor(12421.6494, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12421.744140625
tensor(12421.6494, grad_fn=<NegBackward0>) tensor(12421.7441, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12421.7197265625
tensor(12421.6494, grad_fn=<NegBackward0>) tensor(12421.7197, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12421.64453125
tensor(12421.6494, grad_fn=<NegBackward0>) tensor(12421.6445, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12421.6435546875
tensor(12421.6445, grad_fn=<NegBackward0>) tensor(12421.6436, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12421.642578125
tensor(12421.6436, grad_fn=<NegBackward0>) tensor(12421.6426, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12421.6416015625
tensor(12421.6426, grad_fn=<NegBackward0>) tensor(12421.6416, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12421.8408203125
tensor(12421.6416, grad_fn=<NegBackward0>) tensor(12421.8408, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12421.849609375
tensor(12421.6416, grad_fn=<NegBackward0>) tensor(12421.8496, grad_fn=<NegBackward0>)
2
Iteration 8600: Loss = -12421.6396484375
tensor(12421.6416, grad_fn=<NegBackward0>) tensor(12421.6396, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -12421.6474609375
tensor(12421.6396, grad_fn=<NegBackward0>) tensor(12421.6475, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -12421.6376953125
tensor(12421.6396, grad_fn=<NegBackward0>) tensor(12421.6377, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -12421.6376953125
tensor(12421.6377, grad_fn=<NegBackward0>) tensor(12421.6377, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12421.6923828125
tensor(12421.6377, grad_fn=<NegBackward0>) tensor(12421.6924, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12421.638671875
tensor(12421.6377, grad_fn=<NegBackward0>) tensor(12421.6387, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -12421.6396484375
tensor(12421.6377, grad_fn=<NegBackward0>) tensor(12421.6396, grad_fn=<NegBackward0>)
3
Iteration 9300: Loss = -12421.638671875
tensor(12421.6377, grad_fn=<NegBackward0>) tensor(12421.6387, grad_fn=<NegBackward0>)
4
Iteration 9400: Loss = -12421.6513671875
tensor(12421.6377, grad_fn=<NegBackward0>) tensor(12421.6514, grad_fn=<NegBackward0>)
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[2.2593e-05, 9.9998e-01],
        [9.9994e-01, 5.8264e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0103, 0.9897], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2057, 0.1825],
         [0.7037, 0.1993]],

        [[0.5932, 0.0961],
         [0.5701, 0.5088]],

        [[0.5170, 0.1544],
         [0.5389, 0.5750]],

        [[0.5720, 0.2041],
         [0.6742, 0.5293]],

        [[0.5174, 0.2671],
         [0.6032, 0.7297]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: -0.00224609613838606
Average Adjusted Rand Index: 0.0037794547321575813
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21292.451171875
inf tensor(21292.4512, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12424.1123046875
tensor(21292.4512, grad_fn=<NegBackward0>) tensor(12424.1123, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12423.7412109375
tensor(12424.1123, grad_fn=<NegBackward0>) tensor(12423.7412, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12423.6318359375
tensor(12423.7412, grad_fn=<NegBackward0>) tensor(12423.6318, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12423.5380859375
tensor(12423.6318, grad_fn=<NegBackward0>) tensor(12423.5381, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12423.4384765625
tensor(12423.5381, grad_fn=<NegBackward0>) tensor(12423.4385, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12423.32421875
tensor(12423.4385, grad_fn=<NegBackward0>) tensor(12423.3242, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12423.203125
tensor(12423.3242, grad_fn=<NegBackward0>) tensor(12423.2031, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12423.1240234375
tensor(12423.2031, grad_fn=<NegBackward0>) tensor(12423.1240, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12423.076171875
tensor(12423.1240, grad_fn=<NegBackward0>) tensor(12423.0762, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12423.03125
tensor(12423.0762, grad_fn=<NegBackward0>) tensor(12423.0312, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12422.9873046875
tensor(12423.0312, grad_fn=<NegBackward0>) tensor(12422.9873, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12422.9443359375
tensor(12422.9873, grad_fn=<NegBackward0>) tensor(12422.9443, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12422.904296875
tensor(12422.9443, grad_fn=<NegBackward0>) tensor(12422.9043, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12422.8759765625
tensor(12422.9043, grad_fn=<NegBackward0>) tensor(12422.8760, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12422.85546875
tensor(12422.8760, grad_fn=<NegBackward0>) tensor(12422.8555, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12422.8369140625
tensor(12422.8555, grad_fn=<NegBackward0>) tensor(12422.8369, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12422.822265625
tensor(12422.8369, grad_fn=<NegBackward0>) tensor(12422.8223, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12422.8125
tensor(12422.8223, grad_fn=<NegBackward0>) tensor(12422.8125, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12422.8017578125
tensor(12422.8125, grad_fn=<NegBackward0>) tensor(12422.8018, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12422.7919921875
tensor(12422.8018, grad_fn=<NegBackward0>) tensor(12422.7920, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12422.7861328125
tensor(12422.7920, grad_fn=<NegBackward0>) tensor(12422.7861, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12422.7802734375
tensor(12422.7861, grad_fn=<NegBackward0>) tensor(12422.7803, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12422.7763671875
tensor(12422.7803, grad_fn=<NegBackward0>) tensor(12422.7764, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12422.78125
tensor(12422.7764, grad_fn=<NegBackward0>) tensor(12422.7812, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -12422.76953125
tensor(12422.7764, grad_fn=<NegBackward0>) tensor(12422.7695, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12422.7783203125
tensor(12422.7695, grad_fn=<NegBackward0>) tensor(12422.7783, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -12422.763671875
tensor(12422.7695, grad_fn=<NegBackward0>) tensor(12422.7637, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12422.7841796875
tensor(12422.7637, grad_fn=<NegBackward0>) tensor(12422.7842, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12422.7607421875
tensor(12422.7637, grad_fn=<NegBackward0>) tensor(12422.7607, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12422.7587890625
tensor(12422.7607, grad_fn=<NegBackward0>) tensor(12422.7588, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12422.7568359375
tensor(12422.7588, grad_fn=<NegBackward0>) tensor(12422.7568, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12422.755859375
tensor(12422.7568, grad_fn=<NegBackward0>) tensor(12422.7559, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12422.7529296875
tensor(12422.7559, grad_fn=<NegBackward0>) tensor(12422.7529, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12422.751953125
tensor(12422.7529, grad_fn=<NegBackward0>) tensor(12422.7520, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12422.75
tensor(12422.7520, grad_fn=<NegBackward0>) tensor(12422.7500, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12422.74609375
tensor(12422.7500, grad_fn=<NegBackward0>) tensor(12422.7461, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12422.75390625
tensor(12422.7461, grad_fn=<NegBackward0>) tensor(12422.7539, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12422.7392578125
tensor(12422.7461, grad_fn=<NegBackward0>) tensor(12422.7393, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12422.7333984375
tensor(12422.7393, grad_fn=<NegBackward0>) tensor(12422.7334, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12422.720703125
tensor(12422.7334, grad_fn=<NegBackward0>) tensor(12422.7207, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12422.7060546875
tensor(12422.7207, grad_fn=<NegBackward0>) tensor(12422.7061, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12422.630859375
tensor(12422.7061, grad_fn=<NegBackward0>) tensor(12422.6309, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12422.53125
tensor(12422.6309, grad_fn=<NegBackward0>) tensor(12422.5312, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12422.4873046875
tensor(12422.5312, grad_fn=<NegBackward0>) tensor(12422.4873, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12422.4765625
tensor(12422.4873, grad_fn=<NegBackward0>) tensor(12422.4766, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12422.47265625
tensor(12422.4766, grad_fn=<NegBackward0>) tensor(12422.4727, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12422.470703125
tensor(12422.4727, grad_fn=<NegBackward0>) tensor(12422.4707, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12422.4697265625
tensor(12422.4707, grad_fn=<NegBackward0>) tensor(12422.4697, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12422.4697265625
tensor(12422.4697, grad_fn=<NegBackward0>) tensor(12422.4697, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12422.4697265625
tensor(12422.4697, grad_fn=<NegBackward0>) tensor(12422.4697, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12422.46875
tensor(12422.4697, grad_fn=<NegBackward0>) tensor(12422.4688, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12422.4677734375
tensor(12422.4688, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12422.466796875
tensor(12422.4678, grad_fn=<NegBackward0>) tensor(12422.4668, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12422.46875
tensor(12422.4668, grad_fn=<NegBackward0>) tensor(12422.4688, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12422.4677734375
tensor(12422.4668, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12422.4697265625
tensor(12422.4668, grad_fn=<NegBackward0>) tensor(12422.4697, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12422.4677734375
tensor(12422.4668, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12422.4677734375
tensor(12422.4668, grad_fn=<NegBackward0>) tensor(12422.4678, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.0065, 0.9935],
        [0.8616, 0.1384]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0412, 0.9588], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2089, 0.2323],
         [0.5860, 0.1967]],

        [[0.6710, 0.1934],
         [0.5460, 0.5254]],

        [[0.7010, 0.1894],
         [0.7109, 0.7126]],

        [[0.6643, 0.2001],
         [0.6970, 0.7310]],

        [[0.6926, 0.2104],
         [0.5698, 0.6793]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002150639657162737
Average Adjusted Rand Index: 0.0
[-0.00224609613838606, -0.002150639657162737] [0.0037794547321575813, 0.0] [12421.6513671875, 12422.4677734375]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11791.18499148354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22268.802734375
inf tensor(22268.8027, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12324.0703125
tensor(22268.8027, grad_fn=<NegBackward0>) tensor(12324.0703, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12323.787109375
tensor(12324.0703, grad_fn=<NegBackward0>) tensor(12323.7871, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12323.7294921875
tensor(12323.7871, grad_fn=<NegBackward0>) tensor(12323.7295, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12323.701171875
tensor(12323.7295, grad_fn=<NegBackward0>) tensor(12323.7012, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12323.68359375
tensor(12323.7012, grad_fn=<NegBackward0>) tensor(12323.6836, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12323.666015625
tensor(12323.6836, grad_fn=<NegBackward0>) tensor(12323.6660, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12323.654296875
tensor(12323.6660, grad_fn=<NegBackward0>) tensor(12323.6543, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12323.640625
tensor(12323.6543, grad_fn=<NegBackward0>) tensor(12323.6406, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12323.625
tensor(12323.6406, grad_fn=<NegBackward0>) tensor(12323.6250, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12323.607421875
tensor(12323.6250, grad_fn=<NegBackward0>) tensor(12323.6074, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12323.583984375
tensor(12323.6074, grad_fn=<NegBackward0>) tensor(12323.5840, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12323.54296875
tensor(12323.5840, grad_fn=<NegBackward0>) tensor(12323.5430, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12323.4853515625
tensor(12323.5430, grad_fn=<NegBackward0>) tensor(12323.4854, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12323.4365234375
tensor(12323.4854, grad_fn=<NegBackward0>) tensor(12323.4365, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12323.416015625
tensor(12323.4365, grad_fn=<NegBackward0>) tensor(12323.4160, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12323.4072265625
tensor(12323.4160, grad_fn=<NegBackward0>) tensor(12323.4072, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12323.3974609375
tensor(12323.4072, grad_fn=<NegBackward0>) tensor(12323.3975, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12323.373046875
tensor(12323.3975, grad_fn=<NegBackward0>) tensor(12323.3730, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12323.197265625
tensor(12323.3730, grad_fn=<NegBackward0>) tensor(12323.1973, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12322.623046875
tensor(12323.1973, grad_fn=<NegBackward0>) tensor(12322.6230, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12322.138671875
tensor(12322.6230, grad_fn=<NegBackward0>) tensor(12322.1387, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12321.88671875
tensor(12322.1387, grad_fn=<NegBackward0>) tensor(12321.8867, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12321.771484375
tensor(12321.8867, grad_fn=<NegBackward0>) tensor(12321.7715, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12321.71484375
tensor(12321.7715, grad_fn=<NegBackward0>) tensor(12321.7148, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12321.6796875
tensor(12321.7148, grad_fn=<NegBackward0>) tensor(12321.6797, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12321.6572265625
tensor(12321.6797, grad_fn=<NegBackward0>) tensor(12321.6572, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12321.642578125
tensor(12321.6572, grad_fn=<NegBackward0>) tensor(12321.6426, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12321.6318359375
tensor(12321.6426, grad_fn=<NegBackward0>) tensor(12321.6318, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12321.625
tensor(12321.6318, grad_fn=<NegBackward0>) tensor(12321.6250, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12321.6162109375
tensor(12321.6250, grad_fn=<NegBackward0>) tensor(12321.6162, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12321.609375
tensor(12321.6162, grad_fn=<NegBackward0>) tensor(12321.6094, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12321.60546875
tensor(12321.6094, grad_fn=<NegBackward0>) tensor(12321.6055, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12321.603515625
tensor(12321.6055, grad_fn=<NegBackward0>) tensor(12321.6035, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12321.6005859375
tensor(12321.6035, grad_fn=<NegBackward0>) tensor(12321.6006, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12321.5966796875
tensor(12321.6006, grad_fn=<NegBackward0>) tensor(12321.5967, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12321.5947265625
tensor(12321.5967, grad_fn=<NegBackward0>) tensor(12321.5947, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12321.5927734375
tensor(12321.5947, grad_fn=<NegBackward0>) tensor(12321.5928, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12321.5908203125
tensor(12321.5928, grad_fn=<NegBackward0>) tensor(12321.5908, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12321.591796875
tensor(12321.5908, grad_fn=<NegBackward0>) tensor(12321.5918, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12321.58984375
tensor(12321.5908, grad_fn=<NegBackward0>) tensor(12321.5898, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12321.5869140625
tensor(12321.5898, grad_fn=<NegBackward0>) tensor(12321.5869, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12321.587890625
tensor(12321.5869, grad_fn=<NegBackward0>) tensor(12321.5879, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12321.5869140625
tensor(12321.5869, grad_fn=<NegBackward0>) tensor(12321.5869, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12321.583984375
tensor(12321.5869, grad_fn=<NegBackward0>) tensor(12321.5840, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12321.5849609375
tensor(12321.5840, grad_fn=<NegBackward0>) tensor(12321.5850, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -12321.5830078125
tensor(12321.5840, grad_fn=<NegBackward0>) tensor(12321.5830, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12321.5830078125
tensor(12321.5830, grad_fn=<NegBackward0>) tensor(12321.5830, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12321.5830078125
tensor(12321.5830, grad_fn=<NegBackward0>) tensor(12321.5830, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12321.58203125
tensor(12321.5830, grad_fn=<NegBackward0>) tensor(12321.5820, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12321.580078125
tensor(12321.5820, grad_fn=<NegBackward0>) tensor(12321.5801, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12321.5810546875
tensor(12321.5801, grad_fn=<NegBackward0>) tensor(12321.5811, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12321.5791015625
tensor(12321.5801, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12321.580078125
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5801, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12321.580078125
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5801, grad_fn=<NegBackward0>)
2
Iteration 5500: Loss = -12321.5771484375
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12321.5791015625
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -12321.578125
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5781, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -12321.5791015625
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -12321.5771484375
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12321.5751953125
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12321.5771484375
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12321.576171875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12321.5771484375
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -12321.5751953125
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12321.5771484375
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12321.576171875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12321.5751953125
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12321.57421875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12321.5751953125
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12321.5751953125
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12321.5791015625
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12321.5751953125
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12321.70703125
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.7070, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12321.57421875
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12321.572265625
tensor(12321.5742, grad_fn=<NegBackward0>) tensor(12321.5723, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12321.576171875
tensor(12321.5723, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12321.5751953125
tensor(12321.5723, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12321.5732421875
tensor(12321.5723, grad_fn=<NegBackward0>) tensor(12321.5732, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12321.576171875
tensor(12321.5723, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
4
Iteration 8600: Loss = -12321.57421875
tensor(12321.5723, grad_fn=<NegBackward0>) tensor(12321.5742, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[9.9945e-01, 5.4509e-04],
        [8.6365e-06, 9.9999e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2552, 0.1530],
         [0.5760, 0.1975]],

        [[0.6253, 0.3203],
         [0.6119, 0.5062]],

        [[0.7133, 0.2888],
         [0.5380, 0.5675]],

        [[0.7240, 0.2022],
         [0.6275, 0.5232]],

        [[0.5450, 0.2120],
         [0.5561, 0.7244]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.0011370400292945396
Average Adjusted Rand Index: 0.0008722755306145265
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23717.60546875
inf tensor(23717.6055, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12324.5859375
tensor(23717.6055, grad_fn=<NegBackward0>) tensor(12324.5859, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12324.0361328125
tensor(12324.5859, grad_fn=<NegBackward0>) tensor(12324.0361, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12323.927734375
tensor(12324.0361, grad_fn=<NegBackward0>) tensor(12323.9277, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12323.8603515625
tensor(12323.9277, grad_fn=<NegBackward0>) tensor(12323.8604, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12323.8115234375
tensor(12323.8604, grad_fn=<NegBackward0>) tensor(12323.8115, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12323.7705078125
tensor(12323.8115, grad_fn=<NegBackward0>) tensor(12323.7705, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12323.7373046875
tensor(12323.7705, grad_fn=<NegBackward0>) tensor(12323.7373, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12323.7041015625
tensor(12323.7373, grad_fn=<NegBackward0>) tensor(12323.7041, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12323.673828125
tensor(12323.7041, grad_fn=<NegBackward0>) tensor(12323.6738, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12323.642578125
tensor(12323.6738, grad_fn=<NegBackward0>) tensor(12323.6426, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12323.60546875
tensor(12323.6426, grad_fn=<NegBackward0>) tensor(12323.6055, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12323.56640625
tensor(12323.6055, grad_fn=<NegBackward0>) tensor(12323.5664, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12323.5283203125
tensor(12323.5664, grad_fn=<NegBackward0>) tensor(12323.5283, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12323.4970703125
tensor(12323.5283, grad_fn=<NegBackward0>) tensor(12323.4971, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12323.4755859375
tensor(12323.4971, grad_fn=<NegBackward0>) tensor(12323.4756, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12323.458984375
tensor(12323.4756, grad_fn=<NegBackward0>) tensor(12323.4590, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12323.447265625
tensor(12323.4590, grad_fn=<NegBackward0>) tensor(12323.4473, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12323.439453125
tensor(12323.4473, grad_fn=<NegBackward0>) tensor(12323.4395, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12323.4306640625
tensor(12323.4395, grad_fn=<NegBackward0>) tensor(12323.4307, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12323.4228515625
tensor(12323.4307, grad_fn=<NegBackward0>) tensor(12323.4229, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12323.4169921875
tensor(12323.4229, grad_fn=<NegBackward0>) tensor(12323.4170, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12323.4033203125
tensor(12323.4170, grad_fn=<NegBackward0>) tensor(12323.4033, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12323.37890625
tensor(12323.4033, grad_fn=<NegBackward0>) tensor(12323.3789, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12323.3056640625
tensor(12323.3789, grad_fn=<NegBackward0>) tensor(12323.3057, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12323.1103515625
tensor(12323.3057, grad_fn=<NegBackward0>) tensor(12323.1104, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12322.8115234375
tensor(12323.1104, grad_fn=<NegBackward0>) tensor(12322.8115, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12322.4541015625
tensor(12322.8115, grad_fn=<NegBackward0>) tensor(12322.4541, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12322.1728515625
tensor(12322.4541, grad_fn=<NegBackward0>) tensor(12322.1729, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12321.9970703125
tensor(12322.1729, grad_fn=<NegBackward0>) tensor(12321.9971, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12321.8896484375
tensor(12321.9971, grad_fn=<NegBackward0>) tensor(12321.8896, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12321.8203125
tensor(12321.8896, grad_fn=<NegBackward0>) tensor(12321.8203, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12321.76953125
tensor(12321.8203, grad_fn=<NegBackward0>) tensor(12321.7695, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12321.734375
tensor(12321.7695, grad_fn=<NegBackward0>) tensor(12321.7344, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12321.708984375
tensor(12321.7344, grad_fn=<NegBackward0>) tensor(12321.7090, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12321.6875
tensor(12321.7090, grad_fn=<NegBackward0>) tensor(12321.6875, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12321.6708984375
tensor(12321.6875, grad_fn=<NegBackward0>) tensor(12321.6709, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12321.6591796875
tensor(12321.6709, grad_fn=<NegBackward0>) tensor(12321.6592, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12321.6494140625
tensor(12321.6592, grad_fn=<NegBackward0>) tensor(12321.6494, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12321.6416015625
tensor(12321.6494, grad_fn=<NegBackward0>) tensor(12321.6416, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12321.6337890625
tensor(12321.6416, grad_fn=<NegBackward0>) tensor(12321.6338, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12321.6298828125
tensor(12321.6338, grad_fn=<NegBackward0>) tensor(12321.6299, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12321.6240234375
tensor(12321.6299, grad_fn=<NegBackward0>) tensor(12321.6240, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12321.619140625
tensor(12321.6240, grad_fn=<NegBackward0>) tensor(12321.6191, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12321.615234375
tensor(12321.6191, grad_fn=<NegBackward0>) tensor(12321.6152, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12321.611328125
tensor(12321.6152, grad_fn=<NegBackward0>) tensor(12321.6113, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12321.607421875
tensor(12321.6113, grad_fn=<NegBackward0>) tensor(12321.6074, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12321.60546875
tensor(12321.6074, grad_fn=<NegBackward0>) tensor(12321.6055, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12321.6015625
tensor(12321.6055, grad_fn=<NegBackward0>) tensor(12321.6016, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12321.6005859375
tensor(12321.6016, grad_fn=<NegBackward0>) tensor(12321.6006, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12321.5986328125
tensor(12321.6006, grad_fn=<NegBackward0>) tensor(12321.5986, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12321.5986328125
tensor(12321.5986, grad_fn=<NegBackward0>) tensor(12321.5986, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12321.595703125
tensor(12321.5986, grad_fn=<NegBackward0>) tensor(12321.5957, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12321.5947265625
tensor(12321.5957, grad_fn=<NegBackward0>) tensor(12321.5947, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12321.5927734375
tensor(12321.5947, grad_fn=<NegBackward0>) tensor(12321.5928, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12321.591796875
tensor(12321.5928, grad_fn=<NegBackward0>) tensor(12321.5918, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12321.5908203125
tensor(12321.5918, grad_fn=<NegBackward0>) tensor(12321.5908, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12321.58984375
tensor(12321.5908, grad_fn=<NegBackward0>) tensor(12321.5898, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12321.58984375
tensor(12321.5898, grad_fn=<NegBackward0>) tensor(12321.5898, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12321.5869140625
tensor(12321.5898, grad_fn=<NegBackward0>) tensor(12321.5869, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12321.5869140625
tensor(12321.5869, grad_fn=<NegBackward0>) tensor(12321.5869, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12321.5859375
tensor(12321.5869, grad_fn=<NegBackward0>) tensor(12321.5859, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12321.5849609375
tensor(12321.5859, grad_fn=<NegBackward0>) tensor(12321.5850, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12321.583984375
tensor(12321.5850, grad_fn=<NegBackward0>) tensor(12321.5840, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12321.5830078125
tensor(12321.5840, grad_fn=<NegBackward0>) tensor(12321.5830, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12321.583984375
tensor(12321.5830, grad_fn=<NegBackward0>) tensor(12321.5840, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12321.5830078125
tensor(12321.5830, grad_fn=<NegBackward0>) tensor(12321.5830, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12321.58203125
tensor(12321.5830, grad_fn=<NegBackward0>) tensor(12321.5820, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12321.58203125
tensor(12321.5820, grad_fn=<NegBackward0>) tensor(12321.5820, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12321.58203125
tensor(12321.5820, grad_fn=<NegBackward0>) tensor(12321.5820, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12321.58203125
tensor(12321.5820, grad_fn=<NegBackward0>) tensor(12321.5820, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12321.580078125
tensor(12321.5820, grad_fn=<NegBackward0>) tensor(12321.5801, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12321.5791015625
tensor(12321.5801, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -12321.580078125
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5801, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -12321.580078125
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5801, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -12321.64453125
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.6445, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -12321.5791015625
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12321.5771484375
tensor(12321.5791, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12321.5771484375
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12321.5791015625
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5791, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12321.939453125
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.9395, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12321.578125
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5781, grad_fn=<NegBackward0>)
3
Iteration 8200: Loss = -12321.576171875
tensor(12321.5771, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12321.609375
tensor(12321.5762, grad_fn=<NegBackward0>) tensor(12321.6094, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -12321.5751953125
tensor(12321.5762, grad_fn=<NegBackward0>) tensor(12321.5752, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -12321.576171875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
1
Iteration 8600: Loss = -12321.576171875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
2
Iteration 8700: Loss = -12321.576171875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
3
Iteration 8800: Loss = -12321.5771484375
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5771, grad_fn=<NegBackward0>)
4
Iteration 8900: Loss = -12321.576171875
tensor(12321.5752, grad_fn=<NegBackward0>) tensor(12321.5762, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[9.9999e-01, 8.9295e-06],
        [1.0064e-03, 9.9899e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.1530],
         [0.6603, 0.2621]],

        [[0.6385, 0.3204],
         [0.5576, 0.7073]],

        [[0.6765, 0.2889],
         [0.6777, 0.6035]],

        [[0.7116, 0.2022],
         [0.7027, 0.6804]],

        [[0.6853, 0.2120],
         [0.5413, 0.5488]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.0011370400292945396
Average Adjusted Rand Index: 0.0008722755306145265
[0.0011370400292945396, 0.0011370400292945396] [0.0008722755306145265, 0.0008722755306145265] [12321.57421875, 12321.576171875]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11852.670979284829
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20209.74609375
inf tensor(20209.7461, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12340.6240234375
tensor(20209.7461, grad_fn=<NegBackward0>) tensor(12340.6240, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12340.376953125
tensor(12340.6240, grad_fn=<NegBackward0>) tensor(12340.3770, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12340.30078125
tensor(12340.3770, grad_fn=<NegBackward0>) tensor(12340.3008, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12340.23828125
tensor(12340.3008, grad_fn=<NegBackward0>) tensor(12340.2383, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12340.177734375
tensor(12340.2383, grad_fn=<NegBackward0>) tensor(12340.1777, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12340.12109375
tensor(12340.1777, grad_fn=<NegBackward0>) tensor(12340.1211, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12340.0830078125
tensor(12340.1211, grad_fn=<NegBackward0>) tensor(12340.0830, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12340.0634765625
tensor(12340.0830, grad_fn=<NegBackward0>) tensor(12340.0635, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12340.0517578125
tensor(12340.0635, grad_fn=<NegBackward0>) tensor(12340.0518, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12340.0439453125
tensor(12340.0518, grad_fn=<NegBackward0>) tensor(12340.0439, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12340.0380859375
tensor(12340.0439, grad_fn=<NegBackward0>) tensor(12340.0381, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12340.033203125
tensor(12340.0381, grad_fn=<NegBackward0>) tensor(12340.0332, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12340.0244140625
tensor(12340.0332, grad_fn=<NegBackward0>) tensor(12340.0244, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12340.013671875
tensor(12340.0244, grad_fn=<NegBackward0>) tensor(12340.0137, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12340.001953125
tensor(12340.0137, grad_fn=<NegBackward0>) tensor(12340.0020, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12339.9873046875
tensor(12340.0020, grad_fn=<NegBackward0>) tensor(12339.9873, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12339.96875
tensor(12339.9873, grad_fn=<NegBackward0>) tensor(12339.9688, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12339.9453125
tensor(12339.9688, grad_fn=<NegBackward0>) tensor(12339.9453, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12339.9169921875
tensor(12339.9453, grad_fn=<NegBackward0>) tensor(12339.9170, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12339.8837890625
tensor(12339.9170, grad_fn=<NegBackward0>) tensor(12339.8838, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12339.84375
tensor(12339.8838, grad_fn=<NegBackward0>) tensor(12339.8438, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12339.794921875
tensor(12339.8438, grad_fn=<NegBackward0>) tensor(12339.7949, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12339.70703125
tensor(12339.7949, grad_fn=<NegBackward0>) tensor(12339.7070, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12339.419921875
tensor(12339.7070, grad_fn=<NegBackward0>) tensor(12339.4199, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12339.08203125
tensor(12339.4199, grad_fn=<NegBackward0>) tensor(12339.0820, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12338.7109375
tensor(12339.0820, grad_fn=<NegBackward0>) tensor(12338.7109, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12336.7412109375
tensor(12338.7109, grad_fn=<NegBackward0>) tensor(12336.7412, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12335.4013671875
tensor(12336.7412, grad_fn=<NegBackward0>) tensor(12335.4014, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12335.0263671875
tensor(12335.4014, grad_fn=<NegBackward0>) tensor(12335.0264, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12334.8955078125
tensor(12335.0264, grad_fn=<NegBackward0>) tensor(12334.8955, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12334.833984375
tensor(12334.8955, grad_fn=<NegBackward0>) tensor(12334.8340, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12334.80078125
tensor(12334.8340, grad_fn=<NegBackward0>) tensor(12334.8008, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12334.7783203125
tensor(12334.8008, grad_fn=<NegBackward0>) tensor(12334.7783, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12334.763671875
tensor(12334.7783, grad_fn=<NegBackward0>) tensor(12334.7637, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12334.7509765625
tensor(12334.7637, grad_fn=<NegBackward0>) tensor(12334.7510, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12334.7412109375
tensor(12334.7510, grad_fn=<NegBackward0>) tensor(12334.7412, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12334.7333984375
tensor(12334.7412, grad_fn=<NegBackward0>) tensor(12334.7334, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12334.728515625
tensor(12334.7334, grad_fn=<NegBackward0>) tensor(12334.7285, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12334.7236328125
tensor(12334.7285, grad_fn=<NegBackward0>) tensor(12334.7236, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12334.71875
tensor(12334.7236, grad_fn=<NegBackward0>) tensor(12334.7188, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12334.71484375
tensor(12334.7188, grad_fn=<NegBackward0>) tensor(12334.7148, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12334.7119140625
tensor(12334.7148, grad_fn=<NegBackward0>) tensor(12334.7119, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12334.708984375
tensor(12334.7119, grad_fn=<NegBackward0>) tensor(12334.7090, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12334.7060546875
tensor(12334.7090, grad_fn=<NegBackward0>) tensor(12334.7061, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12334.7041015625
tensor(12334.7061, grad_fn=<NegBackward0>) tensor(12334.7041, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12334.7001953125
tensor(12334.7041, grad_fn=<NegBackward0>) tensor(12334.7002, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12334.701171875
tensor(12334.7002, grad_fn=<NegBackward0>) tensor(12334.7012, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12334.69921875
tensor(12334.7002, grad_fn=<NegBackward0>) tensor(12334.6992, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12334.6982421875
tensor(12334.6992, grad_fn=<NegBackward0>) tensor(12334.6982, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12334.6953125
tensor(12334.6982, grad_fn=<NegBackward0>) tensor(12334.6953, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12334.6943359375
tensor(12334.6953, grad_fn=<NegBackward0>) tensor(12334.6943, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12334.693359375
tensor(12334.6943, grad_fn=<NegBackward0>) tensor(12334.6934, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12334.6923828125
tensor(12334.6934, grad_fn=<NegBackward0>) tensor(12334.6924, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12334.6904296875
tensor(12334.6924, grad_fn=<NegBackward0>) tensor(12334.6904, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12334.6923828125
tensor(12334.6904, grad_fn=<NegBackward0>) tensor(12334.6924, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12334.689453125
tensor(12334.6904, grad_fn=<NegBackward0>) tensor(12334.6895, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12334.6884765625
tensor(12334.6895, grad_fn=<NegBackward0>) tensor(12334.6885, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12334.6884765625
tensor(12334.6885, grad_fn=<NegBackward0>) tensor(12334.6885, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12334.6875
tensor(12334.6885, grad_fn=<NegBackward0>) tensor(12334.6875, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12334.6875
tensor(12334.6875, grad_fn=<NegBackward0>) tensor(12334.6875, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12334.6865234375
tensor(12334.6875, grad_fn=<NegBackward0>) tensor(12334.6865, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12334.6845703125
tensor(12334.6865, grad_fn=<NegBackward0>) tensor(12334.6846, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12334.6845703125
tensor(12334.6846, grad_fn=<NegBackward0>) tensor(12334.6846, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12334.6845703125
tensor(12334.6846, grad_fn=<NegBackward0>) tensor(12334.6846, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12334.68359375
tensor(12334.6846, grad_fn=<NegBackward0>) tensor(12334.6836, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12334.68359375
tensor(12334.6836, grad_fn=<NegBackward0>) tensor(12334.6836, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12334.75
tensor(12334.6836, grad_fn=<NegBackward0>) tensor(12334.7500, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -12334.681640625
tensor(12334.6836, grad_fn=<NegBackward0>) tensor(12334.6816, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12334.6875
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12334.6875, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12334.681640625
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12334.6816, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12335.126953125
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12335.1270, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -12334.6826171875
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12334.6826, grad_fn=<NegBackward0>)
2
Iteration 7300: Loss = -12334.681640625
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12334.6816, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12335.0341796875
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12335.0342, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12334.6806640625
tensor(12334.6816, grad_fn=<NegBackward0>) tensor(12334.6807, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12334.6806640625
tensor(12334.6807, grad_fn=<NegBackward0>) tensor(12334.6807, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -12334.775390625
tensor(12334.6807, grad_fn=<NegBackward0>) tensor(12334.7754, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -12334.681640625
tensor(12334.6807, grad_fn=<NegBackward0>) tensor(12334.6816, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -12334.681640625
tensor(12334.6807, grad_fn=<NegBackward0>) tensor(12334.6816, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -12334.6787109375
tensor(12334.6807, grad_fn=<NegBackward0>) tensor(12334.6787, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -12334.6806640625
tensor(12334.6787, grad_fn=<NegBackward0>) tensor(12334.6807, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -12334.6796875
tensor(12334.6787, grad_fn=<NegBackward0>) tensor(12334.6797, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -12334.6806640625
tensor(12334.6787, grad_fn=<NegBackward0>) tensor(12334.6807, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -12335.12890625
tensor(12334.6787, grad_fn=<NegBackward0>) tensor(12335.1289, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -12334.6796875
tensor(12334.6787, grad_fn=<NegBackward0>) tensor(12334.6797, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[9.9999e-01, 6.0031e-06],
        [1.0894e-04, 9.9989e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9506, 0.0494], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1826],
         [0.5137, 0.0412]],

        [[0.5213, 0.2409],
         [0.6036, 0.5886]],

        [[0.7161, 0.1798],
         [0.5046, 0.5557]],

        [[0.6793, 0.2496],
         [0.6121, 0.6417]],

        [[0.7097, 0.2914],
         [0.6896, 0.5401]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.012897625100762696
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022474640102047015
Global Adjusted Rand Index: -0.0013679843120587899
Average Adjusted Rand Index: -0.0021387090065487363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21474.953125
inf tensor(21474.9531, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12340.552734375
tensor(21474.9531, grad_fn=<NegBackward0>) tensor(12340.5527, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12340.2392578125
tensor(12340.5527, grad_fn=<NegBackward0>) tensor(12340.2393, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12340.1240234375
tensor(12340.2393, grad_fn=<NegBackward0>) tensor(12340.1240, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12339.8974609375
tensor(12340.1240, grad_fn=<NegBackward0>) tensor(12339.8975, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12339.314453125
tensor(12339.8975, grad_fn=<NegBackward0>) tensor(12339.3145, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12338.80078125
tensor(12339.3145, grad_fn=<NegBackward0>) tensor(12338.8008, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12338.6142578125
tensor(12338.8008, grad_fn=<NegBackward0>) tensor(12338.6143, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12338.55078125
tensor(12338.6143, grad_fn=<NegBackward0>) tensor(12338.5508, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12338.521484375
tensor(12338.5508, grad_fn=<NegBackward0>) tensor(12338.5215, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12338.505859375
tensor(12338.5215, grad_fn=<NegBackward0>) tensor(12338.5059, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12338.4931640625
tensor(12338.5059, grad_fn=<NegBackward0>) tensor(12338.4932, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12338.4853515625
tensor(12338.4932, grad_fn=<NegBackward0>) tensor(12338.4854, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12338.4765625
tensor(12338.4854, grad_fn=<NegBackward0>) tensor(12338.4766, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12338.4697265625
tensor(12338.4766, grad_fn=<NegBackward0>) tensor(12338.4697, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12338.462890625
tensor(12338.4697, grad_fn=<NegBackward0>) tensor(12338.4629, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12338.458984375
tensor(12338.4629, grad_fn=<NegBackward0>) tensor(12338.4590, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12338.453125
tensor(12338.4590, grad_fn=<NegBackward0>) tensor(12338.4531, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12338.4501953125
tensor(12338.4531, grad_fn=<NegBackward0>) tensor(12338.4502, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12338.447265625
tensor(12338.4502, grad_fn=<NegBackward0>) tensor(12338.4473, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12338.4423828125
tensor(12338.4473, grad_fn=<NegBackward0>) tensor(12338.4424, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12338.439453125
tensor(12338.4424, grad_fn=<NegBackward0>) tensor(12338.4395, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12338.43359375
tensor(12338.4395, grad_fn=<NegBackward0>) tensor(12338.4336, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12338.42578125
tensor(12338.4336, grad_fn=<NegBackward0>) tensor(12338.4258, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12338.419921875
tensor(12338.4258, grad_fn=<NegBackward0>) tensor(12338.4199, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12338.4140625
tensor(12338.4199, grad_fn=<NegBackward0>) tensor(12338.4141, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12338.408203125
tensor(12338.4141, grad_fn=<NegBackward0>) tensor(12338.4082, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12338.404296875
tensor(12338.4082, grad_fn=<NegBackward0>) tensor(12338.4043, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12338.4013671875
tensor(12338.4043, grad_fn=<NegBackward0>) tensor(12338.4014, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12338.3984375
tensor(12338.4014, grad_fn=<NegBackward0>) tensor(12338.3984, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12338.396484375
tensor(12338.3984, grad_fn=<NegBackward0>) tensor(12338.3965, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12338.3955078125
tensor(12338.3965, grad_fn=<NegBackward0>) tensor(12338.3955, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12338.392578125
tensor(12338.3955, grad_fn=<NegBackward0>) tensor(12338.3926, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12338.3916015625
tensor(12338.3926, grad_fn=<NegBackward0>) tensor(12338.3916, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12338.390625
tensor(12338.3916, grad_fn=<NegBackward0>) tensor(12338.3906, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12338.390625
tensor(12338.3906, grad_fn=<NegBackward0>) tensor(12338.3906, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12338.3896484375
tensor(12338.3906, grad_fn=<NegBackward0>) tensor(12338.3896, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12338.388671875
tensor(12338.3896, grad_fn=<NegBackward0>) tensor(12338.3887, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12338.3876953125
tensor(12338.3887, grad_fn=<NegBackward0>) tensor(12338.3877, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12338.388671875
tensor(12338.3877, grad_fn=<NegBackward0>) tensor(12338.3887, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12338.388671875
tensor(12338.3877, grad_fn=<NegBackward0>) tensor(12338.3887, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -12338.388671875
tensor(12338.3877, grad_fn=<NegBackward0>) tensor(12338.3887, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -12338.3876953125
tensor(12338.3877, grad_fn=<NegBackward0>) tensor(12338.3877, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12338.4033203125
tensor(12338.3877, grad_fn=<NegBackward0>) tensor(12338.4033, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12338.38671875
tensor(12338.3877, grad_fn=<NegBackward0>) tensor(12338.3867, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12338.3857421875
tensor(12338.3867, grad_fn=<NegBackward0>) tensor(12338.3857, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12338.38671875
tensor(12338.3857, grad_fn=<NegBackward0>) tensor(12338.3867, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12338.3857421875
tensor(12338.3857, grad_fn=<NegBackward0>) tensor(12338.3857, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12338.38671875
tensor(12338.3857, grad_fn=<NegBackward0>) tensor(12338.3867, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12338.3857421875
tensor(12338.3857, grad_fn=<NegBackward0>) tensor(12338.3857, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12338.384765625
tensor(12338.3857, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12338.3857421875
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3857, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12338.4013671875
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.4014, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12338.384765625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12338.3837890625
tensor(12338.3848, grad_fn=<NegBackward0>) tensor(12338.3838, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12338.384765625
tensor(12338.3838, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -12338.384765625
tensor(12338.3838, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -12338.3896484375
tensor(12338.3838, grad_fn=<NegBackward0>) tensor(12338.3896, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -12338.384765625
tensor(12338.3838, grad_fn=<NegBackward0>) tensor(12338.3848, grad_fn=<NegBackward0>)
4
Iteration 6500: Loss = -12338.3857421875
tensor(12338.3838, grad_fn=<NegBackward0>) tensor(12338.3857, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6500 due to no improvement.
pi: tensor([[0.1994, 0.8006],
        [0.0122, 0.9878]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9783, 0.0217], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.1996],
         [0.5672, 0.2000]],

        [[0.5521, 0.2045],
         [0.7108, 0.6629]],

        [[0.6470, 0.1575],
         [0.6537, 0.5639]],

        [[0.6682, 0.1998],
         [0.6914, 0.7144]],

        [[0.5648, 0.0836],
         [0.5491, 0.6518]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0017539084200509098
Average Adjusted Rand Index: -0.00015692302765368048
[-0.0013679843120587899, -0.0017539084200509098] [-0.0021387090065487363, -0.00015692302765368048] [12334.6796875, 12338.3857421875]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11688.53395267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22057.47265625
inf tensor(22057.4727, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12238.556640625
tensor(22057.4727, grad_fn=<NegBackward0>) tensor(12238.5566, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12238.115234375
tensor(12238.5566, grad_fn=<NegBackward0>) tensor(12238.1152, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12238.0224609375
tensor(12238.1152, grad_fn=<NegBackward0>) tensor(12238.0225, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12237.9638671875
tensor(12238.0225, grad_fn=<NegBackward0>) tensor(12237.9639, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12237.9208984375
tensor(12237.9639, grad_fn=<NegBackward0>) tensor(12237.9209, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12237.8828125
tensor(12237.9209, grad_fn=<NegBackward0>) tensor(12237.8828, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12237.84765625
tensor(12237.8828, grad_fn=<NegBackward0>) tensor(12237.8477, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12237.814453125
tensor(12237.8477, grad_fn=<NegBackward0>) tensor(12237.8145, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12237.7744140625
tensor(12237.8145, grad_fn=<NegBackward0>) tensor(12237.7744, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12237.732421875
tensor(12237.7744, grad_fn=<NegBackward0>) tensor(12237.7324, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12237.6787109375
tensor(12237.7324, grad_fn=<NegBackward0>) tensor(12237.6787, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12237.6171875
tensor(12237.6787, grad_fn=<NegBackward0>) tensor(12237.6172, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12237.5458984375
tensor(12237.6172, grad_fn=<NegBackward0>) tensor(12237.5459, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12237.46484375
tensor(12237.5459, grad_fn=<NegBackward0>) tensor(12237.4648, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12237.37109375
tensor(12237.4648, grad_fn=<NegBackward0>) tensor(12237.3711, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12237.2578125
tensor(12237.3711, grad_fn=<NegBackward0>) tensor(12237.2578, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11961.6748046875
tensor(12237.2578, grad_fn=<NegBackward0>) tensor(11961.6748, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11679.8310546875
tensor(11961.6748, grad_fn=<NegBackward0>) tensor(11679.8311, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11679.798828125
tensor(11679.8311, grad_fn=<NegBackward0>) tensor(11679.7988, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11679.7822265625
tensor(11679.7988, grad_fn=<NegBackward0>) tensor(11679.7822, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11679.767578125
tensor(11679.7822, grad_fn=<NegBackward0>) tensor(11679.7676, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11679.6572265625
tensor(11679.7676, grad_fn=<NegBackward0>) tensor(11679.6572, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11679.6533203125
tensor(11679.6572, grad_fn=<NegBackward0>) tensor(11679.6533, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11679.650390625
tensor(11679.6533, grad_fn=<NegBackward0>) tensor(11679.6504, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11679.646484375
tensor(11679.6504, grad_fn=<NegBackward0>) tensor(11679.6465, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11679.6396484375
tensor(11679.6465, grad_fn=<NegBackward0>) tensor(11679.6396, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11679.6318359375
tensor(11679.6396, grad_fn=<NegBackward0>) tensor(11679.6318, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11679.619140625
tensor(11679.6318, grad_fn=<NegBackward0>) tensor(11679.6191, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11679.6181640625
tensor(11679.6191, grad_fn=<NegBackward0>) tensor(11679.6182, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11679.560546875
tensor(11679.6182, grad_fn=<NegBackward0>) tensor(11679.5605, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11679.560546875
tensor(11679.5605, grad_fn=<NegBackward0>) tensor(11679.5605, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11679.55859375
tensor(11679.5605, grad_fn=<NegBackward0>) tensor(11679.5586, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11679.3203125
tensor(11679.5586, grad_fn=<NegBackward0>) tensor(11679.3203, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11679.3095703125
tensor(11679.3203, grad_fn=<NegBackward0>) tensor(11679.3096, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11679.310546875
tensor(11679.3096, grad_fn=<NegBackward0>) tensor(11679.3105, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11679.30859375
tensor(11679.3096, grad_fn=<NegBackward0>) tensor(11679.3086, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11679.3076171875
tensor(11679.3086, grad_fn=<NegBackward0>) tensor(11679.3076, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11679.30859375
tensor(11679.3076, grad_fn=<NegBackward0>) tensor(11679.3086, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11679.3095703125
tensor(11679.3076, grad_fn=<NegBackward0>) tensor(11679.3096, grad_fn=<NegBackward0>)
2
Iteration 4000: Loss = -11679.306640625
tensor(11679.3076, grad_fn=<NegBackward0>) tensor(11679.3066, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11679.3046875
tensor(11679.3066, grad_fn=<NegBackward0>) tensor(11679.3047, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11679.3046875
tensor(11679.3047, grad_fn=<NegBackward0>) tensor(11679.3047, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11679.3017578125
tensor(11679.3047, grad_fn=<NegBackward0>) tensor(11679.3018, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11679.302734375
tensor(11679.3018, grad_fn=<NegBackward0>) tensor(11679.3027, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11679.30078125
tensor(11679.3018, grad_fn=<NegBackward0>) tensor(11679.3008, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11679.298828125
tensor(11679.3008, grad_fn=<NegBackward0>) tensor(11679.2988, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11679.2998046875
tensor(11679.2988, grad_fn=<NegBackward0>) tensor(11679.2998, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -11679.3017578125
tensor(11679.2988, grad_fn=<NegBackward0>) tensor(11679.3018, grad_fn=<NegBackward0>)
2
Iteration 4900: Loss = -11679.296875
tensor(11679.2988, grad_fn=<NegBackward0>) tensor(11679.2969, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11679.3017578125
tensor(11679.2969, grad_fn=<NegBackward0>) tensor(11679.3018, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11679.298828125
tensor(11679.2969, grad_fn=<NegBackward0>) tensor(11679.2988, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11679.2978515625
tensor(11679.2969, grad_fn=<NegBackward0>) tensor(11679.2979, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11679.2958984375
tensor(11679.2969, grad_fn=<NegBackward0>) tensor(11679.2959, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11679.2998046875
tensor(11679.2959, grad_fn=<NegBackward0>) tensor(11679.2998, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11679.296875
tensor(11679.2959, grad_fn=<NegBackward0>) tensor(11679.2969, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11679.2958984375
tensor(11679.2959, grad_fn=<NegBackward0>) tensor(11679.2959, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -11679.2958984375
tensor(11679.2959, grad_fn=<NegBackward0>) tensor(11679.2959, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11679.2958984375
tensor(11679.2959, grad_fn=<NegBackward0>) tensor(11679.2959, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11679.294921875
tensor(11679.2959, grad_fn=<NegBackward0>) tensor(11679.2949, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11679.2939453125
tensor(11679.2949, grad_fn=<NegBackward0>) tensor(11679.2939, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11679.2998046875
tensor(11679.2939, grad_fn=<NegBackward0>) tensor(11679.2998, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11679.2958984375
tensor(11679.2939, grad_fn=<NegBackward0>) tensor(11679.2959, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11679.2998046875
tensor(11679.2939, grad_fn=<NegBackward0>) tensor(11679.2998, grad_fn=<NegBackward0>)
3
Iteration 6400: Loss = -11679.2939453125
tensor(11679.2939, grad_fn=<NegBackward0>) tensor(11679.2939, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11679.2822265625
tensor(11679.2939, grad_fn=<NegBackward0>) tensor(11679.2822, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11679.2822265625
tensor(11679.2822, grad_fn=<NegBackward0>) tensor(11679.2822, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11679.296875
tensor(11679.2822, grad_fn=<NegBackward0>) tensor(11679.2969, grad_fn=<NegBackward0>)
1
Iteration 6800: Loss = -11679.279296875
tensor(11679.2822, grad_fn=<NegBackward0>) tensor(11679.2793, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -11679.28125
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2812, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -11679.279296875
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2793, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -11679.279296875
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2793, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -11679.279296875
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2793, grad_fn=<NegBackward0>)
Iteration 7700: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
1
Iteration 7800: Loss = -11679.28125
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2812, grad_fn=<NegBackward0>)
2
Iteration 7900: Loss = -11679.291015625
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2910, grad_fn=<NegBackward0>)
3
Iteration 8000: Loss = -11679.279296875
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2793, grad_fn=<NegBackward0>)
Iteration 8100: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
1
Iteration 8200: Loss = -11679.3515625
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.3516, grad_fn=<NegBackward0>)
2
Iteration 8300: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
3
Iteration 8400: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
4
Iteration 8500: Loss = -11679.2802734375
tensor(11679.2793, grad_fn=<NegBackward0>) tensor(11679.2803, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.2656, 0.7344],
        [0.7696, 0.2304]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4153, 0.5847], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2955, 0.1002],
         [0.5070, 0.2995]],

        [[0.6711, 0.0832],
         [0.6003, 0.6999]],

        [[0.6316, 0.0925],
         [0.6930, 0.5518]],

        [[0.5512, 0.0998],
         [0.5614, 0.7072]],

        [[0.7168, 0.0966],
         [0.5634, 0.5938]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03648588234065676
Average Adjusted Rand Index: 0.9919971467023199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21210.876953125
inf tensor(21210.8770, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12238.103515625
tensor(21210.8770, grad_fn=<NegBackward0>) tensor(12238.1035, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12237.0947265625
tensor(12238.1035, grad_fn=<NegBackward0>) tensor(12237.0947, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12236.666015625
tensor(12237.0947, grad_fn=<NegBackward0>) tensor(12236.6660, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12236.5849609375
tensor(12236.6660, grad_fn=<NegBackward0>) tensor(12236.5850, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12236.5341796875
tensor(12236.5850, grad_fn=<NegBackward0>) tensor(12236.5342, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12236.4951171875
tensor(12236.5342, grad_fn=<NegBackward0>) tensor(12236.4951, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12236.4638671875
tensor(12236.4951, grad_fn=<NegBackward0>) tensor(12236.4639, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12236.435546875
tensor(12236.4639, grad_fn=<NegBackward0>) tensor(12236.4355, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12236.412109375
tensor(12236.4355, grad_fn=<NegBackward0>) tensor(12236.4121, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12236.3955078125
tensor(12236.4121, grad_fn=<NegBackward0>) tensor(12236.3955, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12236.3818359375
tensor(12236.3955, grad_fn=<NegBackward0>) tensor(12236.3818, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12236.373046875
tensor(12236.3818, grad_fn=<NegBackward0>) tensor(12236.3730, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12236.3662109375
tensor(12236.3730, grad_fn=<NegBackward0>) tensor(12236.3662, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12236.3583984375
tensor(12236.3662, grad_fn=<NegBackward0>) tensor(12236.3584, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12236.3544921875
tensor(12236.3584, grad_fn=<NegBackward0>) tensor(12236.3545, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12236.34765625
tensor(12236.3545, grad_fn=<NegBackward0>) tensor(12236.3477, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12236.345703125
tensor(12236.3477, grad_fn=<NegBackward0>) tensor(12236.3457, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12236.3408203125
tensor(12236.3457, grad_fn=<NegBackward0>) tensor(12236.3408, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12236.3369140625
tensor(12236.3408, grad_fn=<NegBackward0>) tensor(12236.3369, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12236.330078125
tensor(12236.3369, grad_fn=<NegBackward0>) tensor(12236.3301, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12236.3251953125
tensor(12236.3301, grad_fn=<NegBackward0>) tensor(12236.3252, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12236.3173828125
tensor(12236.3252, grad_fn=<NegBackward0>) tensor(12236.3174, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12236.310546875
tensor(12236.3174, grad_fn=<NegBackward0>) tensor(12236.3105, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12236.2978515625
tensor(12236.3105, grad_fn=<NegBackward0>) tensor(12236.2979, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12236.28515625
tensor(12236.2979, grad_fn=<NegBackward0>) tensor(12236.2852, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12236.2666015625
tensor(12236.2852, grad_fn=<NegBackward0>) tensor(12236.2666, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12236.2392578125
tensor(12236.2666, grad_fn=<NegBackward0>) tensor(12236.2393, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12236.203125
tensor(12236.2393, grad_fn=<NegBackward0>) tensor(12236.2031, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12236.1533203125
tensor(12236.2031, grad_fn=<NegBackward0>) tensor(12236.1533, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12236.09765625
tensor(12236.1533, grad_fn=<NegBackward0>) tensor(12236.0977, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12236.04296875
tensor(12236.0977, grad_fn=<NegBackward0>) tensor(12236.0430, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12235.9990234375
tensor(12236.0430, grad_fn=<NegBackward0>) tensor(12235.9990, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12235.966796875
tensor(12235.9990, grad_fn=<NegBackward0>) tensor(12235.9668, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12235.943359375
tensor(12235.9668, grad_fn=<NegBackward0>) tensor(12235.9434, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12235.9267578125
tensor(12235.9434, grad_fn=<NegBackward0>) tensor(12235.9268, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12235.912109375
tensor(12235.9268, grad_fn=<NegBackward0>) tensor(12235.9121, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12235.900390625
tensor(12235.9121, grad_fn=<NegBackward0>) tensor(12235.9004, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12235.890625
tensor(12235.9004, grad_fn=<NegBackward0>) tensor(12235.8906, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12235.87890625
tensor(12235.8906, grad_fn=<NegBackward0>) tensor(12235.8789, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12235.8671875
tensor(12235.8789, grad_fn=<NegBackward0>) tensor(12235.8672, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12235.849609375
tensor(12235.8672, grad_fn=<NegBackward0>) tensor(12235.8496, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12235.8271484375
tensor(12235.8496, grad_fn=<NegBackward0>) tensor(12235.8271, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12235.8037109375
tensor(12235.8271, grad_fn=<NegBackward0>) tensor(12235.8037, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12235.779296875
tensor(12235.8037, grad_fn=<NegBackward0>) tensor(12235.7793, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12235.7490234375
tensor(12235.7793, grad_fn=<NegBackward0>) tensor(12235.7490, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12235.669921875
tensor(12235.7490, grad_fn=<NegBackward0>) tensor(12235.6699, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12234.6123046875
tensor(12235.6699, grad_fn=<NegBackward0>) tensor(12234.6123, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12234.1630859375
tensor(12234.6123, grad_fn=<NegBackward0>) tensor(12234.1631, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12234.0869140625
tensor(12234.1631, grad_fn=<NegBackward0>) tensor(12234.0869, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12234.0634765625
tensor(12234.0869, grad_fn=<NegBackward0>) tensor(12234.0635, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12234.048828125
tensor(12234.0635, grad_fn=<NegBackward0>) tensor(12234.0488, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12234.04296875
tensor(12234.0488, grad_fn=<NegBackward0>) tensor(12234.0430, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12234.0380859375
tensor(12234.0430, grad_fn=<NegBackward0>) tensor(12234.0381, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12234.033203125
tensor(12234.0381, grad_fn=<NegBackward0>) tensor(12234.0332, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12234.03125
tensor(12234.0332, grad_fn=<NegBackward0>) tensor(12234.0312, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12234.029296875
tensor(12234.0312, grad_fn=<NegBackward0>) tensor(12234.0293, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12234.025390625
tensor(12234.0293, grad_fn=<NegBackward0>) tensor(12234.0254, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12234.0244140625
tensor(12234.0254, grad_fn=<NegBackward0>) tensor(12234.0244, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12234.0224609375
tensor(12234.0244, grad_fn=<NegBackward0>) tensor(12234.0225, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12234.0234375
tensor(12234.0225, grad_fn=<NegBackward0>) tensor(12234.0234, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -12234.0224609375
tensor(12234.0225, grad_fn=<NegBackward0>) tensor(12234.0225, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12234.021484375
tensor(12234.0225, grad_fn=<NegBackward0>) tensor(12234.0215, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12234.01953125
tensor(12234.0215, grad_fn=<NegBackward0>) tensor(12234.0195, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12234.01953125
tensor(12234.0195, grad_fn=<NegBackward0>) tensor(12234.0195, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12234.01953125
tensor(12234.0195, grad_fn=<NegBackward0>) tensor(12234.0195, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12234.0185546875
tensor(12234.0195, grad_fn=<NegBackward0>) tensor(12234.0186, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12234.0185546875
tensor(12234.0186, grad_fn=<NegBackward0>) tensor(12234.0186, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12234.0166015625
tensor(12234.0186, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12234.0361328125
tensor(12234.0166, grad_fn=<NegBackward0>) tensor(12234.0361, grad_fn=<NegBackward0>)
1
Iteration 7000: Loss = -12234.017578125
tensor(12234.0166, grad_fn=<NegBackward0>) tensor(12234.0176, grad_fn=<NegBackward0>)
2
Iteration 7100: Loss = -12234.0166015625
tensor(12234.0166, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12234.1650390625
tensor(12234.0166, grad_fn=<NegBackward0>) tensor(12234.1650, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12234.017578125
tensor(12234.0166, grad_fn=<NegBackward0>) tensor(12234.0176, grad_fn=<NegBackward0>)
2
Iteration 7400: Loss = -12234.015625
tensor(12234.0166, grad_fn=<NegBackward0>) tensor(12234.0156, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12234.0146484375
tensor(12234.0156, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12234.0166015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12234.015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0156, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 7900: Loss = -12234.015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0156, grad_fn=<NegBackward0>)
1
Iteration 8000: Loss = -12234.015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0156, grad_fn=<NegBackward0>)
2
Iteration 8100: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 8400: Loss = -12234.0166015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
1
Iteration 8500: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12234.0166015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12234.0185546875
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0186, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12234.0166015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12234.0185546875
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0186, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -12234.0146484375
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
Iteration 9200: Loss = -12234.0166015625
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0166, grad_fn=<NegBackward0>)
1
Iteration 9300: Loss = -12234.013671875
tensor(12234.0146, grad_fn=<NegBackward0>) tensor(12234.0137, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12234.013671875
tensor(12234.0137, grad_fn=<NegBackward0>) tensor(12234.0137, grad_fn=<NegBackward0>)
Iteration 9500: Loss = -12234.015625
tensor(12234.0137, grad_fn=<NegBackward0>) tensor(12234.0156, grad_fn=<NegBackward0>)
1
Iteration 9600: Loss = -12234.087890625
tensor(12234.0137, grad_fn=<NegBackward0>) tensor(12234.0879, grad_fn=<NegBackward0>)
2
Iteration 9700: Loss = -12234.0126953125
tensor(12234.0137, grad_fn=<NegBackward0>) tensor(12234.0127, grad_fn=<NegBackward0>)
Iteration 9800: Loss = -12234.0146484375
tensor(12234.0127, grad_fn=<NegBackward0>) tensor(12234.0146, grad_fn=<NegBackward0>)
1
Iteration 9900: Loss = -12234.013671875
tensor(12234.0127, grad_fn=<NegBackward0>) tensor(12234.0137, grad_fn=<NegBackward0>)
2
pi: tensor([[1.0843e-05, 9.9999e-01],
        [9.9999e-01, 1.0054e-05]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0240, 0.9760], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.3010],
         [0.6095, 0.1935]],

        [[0.5398, 0.1809],
         [0.7211, 0.5372]],

        [[0.7122, 0.1350],
         [0.5020, 0.5467]],

        [[0.5930, 0.1573],
         [0.6975, 0.5740]],

        [[0.6549, 0.2340],
         [0.6913, 0.6281]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: -0.011374456256342739
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.0018036122246310666
Average Adjusted Rand Index: -0.00245506189708676
[0.03648588234065676, -0.0018036122246310666] [0.9919971467023199, -0.00245506189708676] [11679.2802734375, 12234.013671875]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11972.076109055504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23141.236328125
inf tensor(23141.2363, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12496.626953125
tensor(23141.2363, grad_fn=<NegBackward0>) tensor(12496.6270, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12496.287109375
tensor(12496.6270, grad_fn=<NegBackward0>) tensor(12496.2871, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12496.236328125
tensor(12496.2871, grad_fn=<NegBackward0>) tensor(12496.2363, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12496.2041015625
tensor(12496.2363, grad_fn=<NegBackward0>) tensor(12496.2041, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12496.1787109375
tensor(12496.2041, grad_fn=<NegBackward0>) tensor(12496.1787, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12496.1494140625
tensor(12496.1787, grad_fn=<NegBackward0>) tensor(12496.1494, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12496.1142578125
tensor(12496.1494, grad_fn=<NegBackward0>) tensor(12496.1143, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12496.0732421875
tensor(12496.1143, grad_fn=<NegBackward0>) tensor(12496.0732, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12496.0146484375
tensor(12496.0732, grad_fn=<NegBackward0>) tensor(12496.0146, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12495.8984375
tensor(12496.0146, grad_fn=<NegBackward0>) tensor(12495.8984, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12495.6640625
tensor(12495.8984, grad_fn=<NegBackward0>) tensor(12495.6641, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12495.4287109375
tensor(12495.6641, grad_fn=<NegBackward0>) tensor(12495.4287, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12495.3291015625
tensor(12495.4287, grad_fn=<NegBackward0>) tensor(12495.3291, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12495.2763671875
tensor(12495.3291, grad_fn=<NegBackward0>) tensor(12495.2764, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12495.2421875
tensor(12495.2764, grad_fn=<NegBackward0>) tensor(12495.2422, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12495.21875
tensor(12495.2422, grad_fn=<NegBackward0>) tensor(12495.2188, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12495.2041015625
tensor(12495.2188, grad_fn=<NegBackward0>) tensor(12495.2041, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12495.1953125
tensor(12495.2041, grad_fn=<NegBackward0>) tensor(12495.1953, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12495.1904296875
tensor(12495.1953, grad_fn=<NegBackward0>) tensor(12495.1904, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12495.1884765625
tensor(12495.1904, grad_fn=<NegBackward0>) tensor(12495.1885, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12495.1875
tensor(12495.1885, grad_fn=<NegBackward0>) tensor(12495.1875, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12495.185546875
tensor(12495.1875, grad_fn=<NegBackward0>) tensor(12495.1855, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12495.185546875
tensor(12495.1855, grad_fn=<NegBackward0>) tensor(12495.1855, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12495.185546875
tensor(12495.1855, grad_fn=<NegBackward0>) tensor(12495.1855, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12495.1845703125
tensor(12495.1855, grad_fn=<NegBackward0>) tensor(12495.1846, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12495.185546875
tensor(12495.1846, grad_fn=<NegBackward0>) tensor(12495.1855, grad_fn=<NegBackward0>)
1
Iteration 2700: Loss = -12495.1845703125
tensor(12495.1846, grad_fn=<NegBackward0>) tensor(12495.1846, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12495.1845703125
tensor(12495.1846, grad_fn=<NegBackward0>) tensor(12495.1846, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12495.18359375
tensor(12495.1846, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12495.18359375
tensor(12495.1836, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12495.18359375
tensor(12495.1836, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12495.18359375
tensor(12495.1836, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12495.18359375
tensor(12495.1836, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12495.1826171875
tensor(12495.1836, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12495.18359375
tensor(12495.1826, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12495.1826171875
tensor(12495.1826, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12495.18359375
tensor(12495.1826, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12495.181640625
tensor(12495.1826, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12495.181640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12495.193359375
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1934, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -12495.181640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12495.189453125
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1895, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12495.18359375
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12495.18359375
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1836, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
4
Iteration 4800: Loss = -12495.181640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12495.1845703125
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1846, grad_fn=<NegBackward0>)
1
Iteration 5000: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
2
Iteration 5100: Loss = -12495.181640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12495.181640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12495.181640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12495.1826171875
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12495.1806640625
tensor(12495.1816, grad_fn=<NegBackward0>) tensor(12495.1807, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12495.1826171875
tensor(12495.1807, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
1
Iteration 5900: Loss = -12495.1826171875
tensor(12495.1807, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
2
Iteration 6000: Loss = -12495.1826171875
tensor(12495.1807, grad_fn=<NegBackward0>) tensor(12495.1826, grad_fn=<NegBackward0>)
3
Iteration 6100: Loss = -12495.181640625
tensor(12495.1807, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
4
Iteration 6200: Loss = -12495.181640625
tensor(12495.1807, grad_fn=<NegBackward0>) tensor(12495.1816, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6200 due to no improvement.
pi: tensor([[0.8495, 0.1505],
        [0.9751, 0.0249]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0087, 0.9913], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.2095],
         [0.6236, 0.2088]],

        [[0.5300, 0.2391],
         [0.5819, 0.5495]],

        [[0.6350, 0.1932],
         [0.5059, 0.5307]],

        [[0.5557, 0.2166],
         [0.6605, 0.6355]],

        [[0.6478, 0.1983],
         [0.6778, 0.7024]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011954054510527122
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20136.6171875
inf tensor(20136.6172, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12496.5400390625
tensor(20136.6172, grad_fn=<NegBackward0>) tensor(12496.5400, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12496.37890625
tensor(12496.5400, grad_fn=<NegBackward0>) tensor(12496.3789, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12496.333984375
tensor(12496.3789, grad_fn=<NegBackward0>) tensor(12496.3340, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12496.294921875
tensor(12496.3340, grad_fn=<NegBackward0>) tensor(12496.2949, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12496.2490234375
tensor(12496.2949, grad_fn=<NegBackward0>) tensor(12496.2490, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12496.19140625
tensor(12496.2490, grad_fn=<NegBackward0>) tensor(12496.1914, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12496.1240234375
tensor(12496.1914, grad_fn=<NegBackward0>) tensor(12496.1240, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12496.0625
tensor(12496.1240, grad_fn=<NegBackward0>) tensor(12496.0625, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12496.017578125
tensor(12496.0625, grad_fn=<NegBackward0>) tensor(12496.0176, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12495.982421875
tensor(12496.0176, grad_fn=<NegBackward0>) tensor(12495.9824, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12495.9453125
tensor(12495.9824, grad_fn=<NegBackward0>) tensor(12495.9453, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12495.8935546875
tensor(12495.9453, grad_fn=<NegBackward0>) tensor(12495.8936, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12495.81640625
tensor(12495.8936, grad_fn=<NegBackward0>) tensor(12495.8164, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12495.6943359375
tensor(12495.8164, grad_fn=<NegBackward0>) tensor(12495.6943, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12495.5458984375
tensor(12495.6943, grad_fn=<NegBackward0>) tensor(12495.5459, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12495.447265625
tensor(12495.5459, grad_fn=<NegBackward0>) tensor(12495.4473, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12495.396484375
tensor(12495.4473, grad_fn=<NegBackward0>) tensor(12495.3965, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12495.373046875
tensor(12495.3965, grad_fn=<NegBackward0>) tensor(12495.3730, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12495.36328125
tensor(12495.3730, grad_fn=<NegBackward0>) tensor(12495.3633, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12495.359375
tensor(12495.3633, grad_fn=<NegBackward0>) tensor(12495.3594, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12495.357421875
tensor(12495.3594, grad_fn=<NegBackward0>) tensor(12495.3574, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12495.35546875
tensor(12495.3574, grad_fn=<NegBackward0>) tensor(12495.3555, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12495.3515625
tensor(12495.3555, grad_fn=<NegBackward0>) tensor(12495.3516, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12495.349609375
tensor(12495.3516, grad_fn=<NegBackward0>) tensor(12495.3496, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12495.3466796875
tensor(12495.3496, grad_fn=<NegBackward0>) tensor(12495.3467, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12495.3330078125
tensor(12495.3467, grad_fn=<NegBackward0>) tensor(12495.3330, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12495.3115234375
tensor(12495.3330, grad_fn=<NegBackward0>) tensor(12495.3115, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12495.2734375
tensor(12495.3115, grad_fn=<NegBackward0>) tensor(12495.2734, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12495.2373046875
tensor(12495.2734, grad_fn=<NegBackward0>) tensor(12495.2373, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12495.1591796875
tensor(12495.2373, grad_fn=<NegBackward0>) tensor(12495.1592, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12495.14453125
tensor(12495.1592, grad_fn=<NegBackward0>) tensor(12495.1445, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12495.14453125
tensor(12495.1445, grad_fn=<NegBackward0>) tensor(12495.1445, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12495.1376953125
tensor(12495.1445, grad_fn=<NegBackward0>) tensor(12495.1377, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12495.1328125
tensor(12495.1377, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12495.1337890625
tensor(12495.1328, grad_fn=<NegBackward0>) tensor(12495.1338, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12495.13671875
tensor(12495.1328, grad_fn=<NegBackward0>) tensor(12495.1367, grad_fn=<NegBackward0>)
2
Iteration 3700: Loss = -12495.1328125
tensor(12495.1328, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12495.1328125
tensor(12495.1328, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12495.1318359375
tensor(12495.1328, grad_fn=<NegBackward0>) tensor(12495.1318, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12495.134765625
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1348, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12495.1328125
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
2
Iteration 4200: Loss = -12495.1328125
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
3
Iteration 4300: Loss = -12495.1318359375
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1318, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12495.13671875
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1367, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12495.1328125
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12495.1328125
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12495.1318359375
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1318, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12495.1328125
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12495.1318359375
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1318, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12495.1328125
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12495.130859375
tensor(12495.1318, grad_fn=<NegBackward0>) tensor(12495.1309, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12495.1328125
tensor(12495.1309, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -12495.1318359375
tensor(12495.1309, grad_fn=<NegBackward0>) tensor(12495.1318, grad_fn=<NegBackward0>)
2
Iteration 5400: Loss = -12495.1328125
tensor(12495.1309, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
3
Iteration 5500: Loss = -12495.1328125
tensor(12495.1309, grad_fn=<NegBackward0>) tensor(12495.1328, grad_fn=<NegBackward0>)
4
Iteration 5600: Loss = -12495.1318359375
tensor(12495.1309, grad_fn=<NegBackward0>) tensor(12495.1318, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.7486, 0.2514],
        [0.5853, 0.4147]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0106, 0.9894], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.2091],
         [0.7295, 0.2090]],

        [[0.7268, 0.2048],
         [0.6638, 0.5787]],

        [[0.7023, 0.1971],
         [0.7135, 0.6297]],

        [[0.6138, 0.2112],
         [0.6868, 0.6599]],

        [[0.6081, 0.2005],
         [0.6501, 0.6710]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000955237711961403
Average Adjusted Rand Index: 0.001301815690304899
[-0.0011954054510527122, -0.000955237711961403] [0.0, 0.001301815690304899] [12495.181640625, 12495.1318359375]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11852.715449635527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22188.3203125
inf tensor(22188.3203, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12379.8359375
tensor(22188.3203, grad_fn=<NegBackward0>) tensor(12379.8359, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12379.59375
tensor(12379.8359, grad_fn=<NegBackward0>) tensor(12379.5938, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12379.52734375
tensor(12379.5938, grad_fn=<NegBackward0>) tensor(12379.5273, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12379.4892578125
tensor(12379.5273, grad_fn=<NegBackward0>) tensor(12379.4893, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12379.4599609375
tensor(12379.4893, grad_fn=<NegBackward0>) tensor(12379.4600, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12379.4404296875
tensor(12379.4600, grad_fn=<NegBackward0>) tensor(12379.4404, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12379.421875
tensor(12379.4404, grad_fn=<NegBackward0>) tensor(12379.4219, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12379.404296875
tensor(12379.4219, grad_fn=<NegBackward0>) tensor(12379.4043, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12379.3818359375
tensor(12379.4043, grad_fn=<NegBackward0>) tensor(12379.3818, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12379.34375
tensor(12379.3818, grad_fn=<NegBackward0>) tensor(12379.3438, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12379.2431640625
tensor(12379.3438, grad_fn=<NegBackward0>) tensor(12379.2432, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12379.0576171875
tensor(12379.2432, grad_fn=<NegBackward0>) tensor(12379.0576, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12378.9775390625
tensor(12379.0576, grad_fn=<NegBackward0>) tensor(12378.9775, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12378.9580078125
tensor(12378.9775, grad_fn=<NegBackward0>) tensor(12378.9580, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12378.9482421875
tensor(12378.9580, grad_fn=<NegBackward0>) tensor(12378.9482, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12378.943359375
tensor(12378.9482, grad_fn=<NegBackward0>) tensor(12378.9434, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12378.943359375
tensor(12378.9434, grad_fn=<NegBackward0>) tensor(12378.9434, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12378.9423828125
tensor(12378.9434, grad_fn=<NegBackward0>) tensor(12378.9424, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12378.9404296875
tensor(12378.9424, grad_fn=<NegBackward0>) tensor(12378.9404, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12378.939453125
tensor(12378.9404, grad_fn=<NegBackward0>) tensor(12378.9395, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12378.9375
tensor(12378.9395, grad_fn=<NegBackward0>) tensor(12378.9375, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12378.9384765625
tensor(12378.9375, grad_fn=<NegBackward0>) tensor(12378.9385, grad_fn=<NegBackward0>)
1
Iteration 2300: Loss = -12378.9365234375
tensor(12378.9375, grad_fn=<NegBackward0>) tensor(12378.9365, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12378.9345703125
tensor(12378.9365, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12378.9365234375
tensor(12378.9346, grad_fn=<NegBackward0>) tensor(12378.9365, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -12378.9365234375
tensor(12378.9346, grad_fn=<NegBackward0>) tensor(12378.9365, grad_fn=<NegBackward0>)
2
Iteration 2700: Loss = -12378.93359375
tensor(12378.9346, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12378.935546875
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9355, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -12378.93359375
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12378.9345703125
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -12378.935546875
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9355, grad_fn=<NegBackward0>)
2
Iteration 3200: Loss = -12378.9326171875
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9326, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12378.9345703125
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12378.93359375
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -12378.9345703125
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
3
Iteration 3600: Loss = -12378.9345703125
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
4
Iteration 3700: Loss = -12378.93359375
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
5
Stopping early at iteration 3700 due to no improvement.
pi: tensor([[0.9915, 0.0085],
        [0.9847, 0.0153]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9925, 0.0075], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.2093],
         [0.5522, 0.2273]],

        [[0.5627, 0.2135],
         [0.7237, 0.7005]],

        [[0.5853, 0.1797],
         [0.5277, 0.6983]],

        [[0.6101, 0.3275],
         [0.6853, 0.6145]],

        [[0.5013, 0.1886],
         [0.5868, 0.6646]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.212902401625345e-05
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24469.142578125
inf tensor(24469.1426, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12380.1904296875
tensor(24469.1426, grad_fn=<NegBackward0>) tensor(12380.1904, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12379.548828125
tensor(12380.1904, grad_fn=<NegBackward0>) tensor(12379.5488, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12379.412109375
tensor(12379.5488, grad_fn=<NegBackward0>) tensor(12379.4121, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12379.3212890625
tensor(12379.4121, grad_fn=<NegBackward0>) tensor(12379.3213, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12379.23046875
tensor(12379.3213, grad_fn=<NegBackward0>) tensor(12379.2305, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12379.1416015625
tensor(12379.2305, grad_fn=<NegBackward0>) tensor(12379.1416, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12379.076171875
tensor(12379.1416, grad_fn=<NegBackward0>) tensor(12379.0762, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12379.0302734375
tensor(12379.0762, grad_fn=<NegBackward0>) tensor(12379.0303, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12379.001953125
tensor(12379.0303, grad_fn=<NegBackward0>) tensor(12379.0020, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12378.9833984375
tensor(12379.0020, grad_fn=<NegBackward0>) tensor(12378.9834, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12378.9716796875
tensor(12378.9834, grad_fn=<NegBackward0>) tensor(12378.9717, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12378.9638671875
tensor(12378.9717, grad_fn=<NegBackward0>) tensor(12378.9639, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12378.9580078125
tensor(12378.9639, grad_fn=<NegBackward0>) tensor(12378.9580, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12378.9541015625
tensor(12378.9580, grad_fn=<NegBackward0>) tensor(12378.9541, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12378.9501953125
tensor(12378.9541, grad_fn=<NegBackward0>) tensor(12378.9502, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12378.94921875
tensor(12378.9502, grad_fn=<NegBackward0>) tensor(12378.9492, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12378.9462890625
tensor(12378.9492, grad_fn=<NegBackward0>) tensor(12378.9463, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12378.943359375
tensor(12378.9463, grad_fn=<NegBackward0>) tensor(12378.9434, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12378.9443359375
tensor(12378.9434, grad_fn=<NegBackward0>) tensor(12378.9443, grad_fn=<NegBackward0>)
1
Iteration 2000: Loss = -12378.9423828125
tensor(12378.9434, grad_fn=<NegBackward0>) tensor(12378.9424, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12378.94140625
tensor(12378.9424, grad_fn=<NegBackward0>) tensor(12378.9414, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12378.94140625
tensor(12378.9414, grad_fn=<NegBackward0>) tensor(12378.9414, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12378.9404296875
tensor(12378.9414, grad_fn=<NegBackward0>) tensor(12378.9404, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12378.939453125
tensor(12378.9404, grad_fn=<NegBackward0>) tensor(12378.9395, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12378.9375
tensor(12378.9395, grad_fn=<NegBackward0>) tensor(12378.9375, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12378.9375
tensor(12378.9375, grad_fn=<NegBackward0>) tensor(12378.9375, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12378.9375
tensor(12378.9375, grad_fn=<NegBackward0>) tensor(12378.9375, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12378.9375
tensor(12378.9375, grad_fn=<NegBackward0>) tensor(12378.9375, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12378.9365234375
tensor(12378.9375, grad_fn=<NegBackward0>) tensor(12378.9365, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12378.9365234375
tensor(12378.9365, grad_fn=<NegBackward0>) tensor(12378.9365, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12378.9375
tensor(12378.9365, grad_fn=<NegBackward0>) tensor(12378.9375, grad_fn=<NegBackward0>)
1
Iteration 3200: Loss = -12378.9345703125
tensor(12378.9365, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12378.935546875
tensor(12378.9346, grad_fn=<NegBackward0>) tensor(12378.9355, grad_fn=<NegBackward0>)
1
Iteration 3400: Loss = -12378.935546875
tensor(12378.9346, grad_fn=<NegBackward0>) tensor(12378.9355, grad_fn=<NegBackward0>)
2
Iteration 3500: Loss = -12378.93359375
tensor(12378.9346, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12378.9345703125
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12378.9345703125
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -12378.9345703125
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -12378.93359375
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12378.93359375
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9336, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12378.9326171875
tensor(12378.9336, grad_fn=<NegBackward0>) tensor(12378.9326, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12378.9326171875
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9326, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12378.9345703125
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9346, grad_fn=<NegBackward0>)
1
Iteration 4400: Loss = -12378.9326171875
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9326, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12378.9326171875
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9326, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12378.931640625
tensor(12378.9326, grad_fn=<NegBackward0>) tensor(12378.9316, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12378.931640625
tensor(12378.9316, grad_fn=<NegBackward0>) tensor(12378.9316, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12378.931640625
tensor(12378.9316, grad_fn=<NegBackward0>) tensor(12378.9316, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12378.9296875
tensor(12378.9316, grad_fn=<NegBackward0>) tensor(12378.9297, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12378.9306640625
tensor(12378.9297, grad_fn=<NegBackward0>) tensor(12378.9307, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -12378.931640625
tensor(12378.9297, grad_fn=<NegBackward0>) tensor(12378.9316, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -12378.931640625
tensor(12378.9297, grad_fn=<NegBackward0>) tensor(12378.9316, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -12378.9306640625
tensor(12378.9297, grad_fn=<NegBackward0>) tensor(12378.9307, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -12378.9306640625
tensor(12378.9297, grad_fn=<NegBackward0>) tensor(12378.9307, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.9915, 0.0085],
        [0.9835, 0.0165]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9685, 0.0315], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.2058],
         [0.6970, 0.2134]],

        [[0.5321, 0.2132],
         [0.5781, 0.7234]],

        [[0.6508, 0.1796],
         [0.5550, 0.6062]],

        [[0.5102, 0.3279],
         [0.6714, 0.5084]],

        [[0.6002, 0.1875],
         [0.5512, 0.6823]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 3.212902401625345e-05
Average Adjusted Rand Index: -0.0004529465619428516
[3.212902401625345e-05, 3.212902401625345e-05] [-0.0004529465619428516, -0.0004529465619428516] [12378.93359375, 12378.9306640625]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11962.719437980853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20203.078125
inf tensor(20203.0781, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12513.0068359375
tensor(20203.0781, grad_fn=<NegBackward0>) tensor(12513.0068, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12512.486328125
tensor(12513.0068, grad_fn=<NegBackward0>) tensor(12512.4863, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12511.9130859375
tensor(12512.4863, grad_fn=<NegBackward0>) tensor(12511.9131, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12510.505859375
tensor(12511.9131, grad_fn=<NegBackward0>) tensor(12510.5059, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12509.96875
tensor(12510.5059, grad_fn=<NegBackward0>) tensor(12509.9688, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12509.5546875
tensor(12509.9688, grad_fn=<NegBackward0>) tensor(12509.5547, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12509.2802734375
tensor(12509.5547, grad_fn=<NegBackward0>) tensor(12509.2803, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12509.150390625
tensor(12509.2803, grad_fn=<NegBackward0>) tensor(12509.1504, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12509.0849609375
tensor(12509.1504, grad_fn=<NegBackward0>) tensor(12509.0850, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12509.0400390625
tensor(12509.0850, grad_fn=<NegBackward0>) tensor(12509.0400, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12509.0
tensor(12509.0400, grad_fn=<NegBackward0>) tensor(12509., grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12508.9560546875
tensor(12509., grad_fn=<NegBackward0>) tensor(12508.9561, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12508.8955078125
tensor(12508.9561, grad_fn=<NegBackward0>) tensor(12508.8955, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12508.828125
tensor(12508.8955, grad_fn=<NegBackward0>) tensor(12508.8281, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12508.7685546875
tensor(12508.8281, grad_fn=<NegBackward0>) tensor(12508.7686, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12508.728515625
tensor(12508.7686, grad_fn=<NegBackward0>) tensor(12508.7285, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12508.7109375
tensor(12508.7285, grad_fn=<NegBackward0>) tensor(12508.7109, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12508.703125
tensor(12508.7109, grad_fn=<NegBackward0>) tensor(12508.7031, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12508.6982421875
tensor(12508.7031, grad_fn=<NegBackward0>) tensor(12508.6982, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12508.6962890625
tensor(12508.6982, grad_fn=<NegBackward0>) tensor(12508.6963, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12508.6953125
tensor(12508.6963, grad_fn=<NegBackward0>) tensor(12508.6953, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12508.6943359375
tensor(12508.6953, grad_fn=<NegBackward0>) tensor(12508.6943, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12508.6923828125
tensor(12508.6943, grad_fn=<NegBackward0>) tensor(12508.6924, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12508.693359375
tensor(12508.6924, grad_fn=<NegBackward0>) tensor(12508.6934, grad_fn=<NegBackward0>)
1
Iteration 2500: Loss = -12508.6904296875
tensor(12508.6924, grad_fn=<NegBackward0>) tensor(12508.6904, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12508.6904296875
tensor(12508.6904, grad_fn=<NegBackward0>) tensor(12508.6904, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12508.69140625
tensor(12508.6904, grad_fn=<NegBackward0>) tensor(12508.6914, grad_fn=<NegBackward0>)
1
Iteration 2800: Loss = -12508.6904296875
tensor(12508.6904, grad_fn=<NegBackward0>) tensor(12508.6904, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12508.689453125
tensor(12508.6904, grad_fn=<NegBackward0>) tensor(12508.6895, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12508.689453125
tensor(12508.6895, grad_fn=<NegBackward0>) tensor(12508.6895, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12508.6884765625
tensor(12508.6895, grad_fn=<NegBackward0>) tensor(12508.6885, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12508.6884765625
tensor(12508.6885, grad_fn=<NegBackward0>) tensor(12508.6885, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12508.6884765625
tensor(12508.6885, grad_fn=<NegBackward0>) tensor(12508.6885, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12508.6875
tensor(12508.6885, grad_fn=<NegBackward0>) tensor(12508.6875, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12508.6875
tensor(12508.6875, grad_fn=<NegBackward0>) tensor(12508.6875, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12508.6875
tensor(12508.6875, grad_fn=<NegBackward0>) tensor(12508.6875, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12508.6865234375
tensor(12508.6875, grad_fn=<NegBackward0>) tensor(12508.6865, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12508.6845703125
tensor(12508.6865, grad_fn=<NegBackward0>) tensor(12508.6846, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12508.6875
tensor(12508.6846, grad_fn=<NegBackward0>) tensor(12508.6875, grad_fn=<NegBackward0>)
1
Iteration 4000: Loss = -12508.6875
tensor(12508.6846, grad_fn=<NegBackward0>) tensor(12508.6875, grad_fn=<NegBackward0>)
2
Iteration 4100: Loss = -12508.6875
tensor(12508.6846, grad_fn=<NegBackward0>) tensor(12508.6875, grad_fn=<NegBackward0>)
3
Iteration 4200: Loss = -12508.6865234375
tensor(12508.6846, grad_fn=<NegBackward0>) tensor(12508.6865, grad_fn=<NegBackward0>)
4
Iteration 4300: Loss = -12508.6865234375
tensor(12508.6846, grad_fn=<NegBackward0>) tensor(12508.6865, grad_fn=<NegBackward0>)
5
Stopping early at iteration 4300 due to no improvement.
pi: tensor([[0.4972, 0.5028],
        [0.1139, 0.8861]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0069, 0.9931], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2517, 0.2096],
         [0.5111, 0.1957]],

        [[0.5930, 0.2577],
         [0.5189, 0.5920]],

        [[0.6812, 0.2190],
         [0.6724, 0.6784]],

        [[0.5534, 0.2366],
         [0.5792, 0.6319]],

        [[0.5656, 0.2061],
         [0.6341, 0.5164]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.012897625100762696
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002143614483191185
Average Adjusted Rand Index: -0.003193534484584708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20896.52734375
inf tensor(20896.5273, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12513.1513671875
tensor(20896.5273, grad_fn=<NegBackward0>) tensor(12513.1514, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12512.55078125
tensor(12513.1514, grad_fn=<NegBackward0>) tensor(12512.5508, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12511.98046875
tensor(12512.5508, grad_fn=<NegBackward0>) tensor(12511.9805, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12510.9814453125
tensor(12511.9805, grad_fn=<NegBackward0>) tensor(12510.9814, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12510.15234375
tensor(12510.9814, grad_fn=<NegBackward0>) tensor(12510.1523, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12509.615234375
tensor(12510.1523, grad_fn=<NegBackward0>) tensor(12509.6152, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12509.3076171875
tensor(12509.6152, grad_fn=<NegBackward0>) tensor(12509.3076, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12509.095703125
tensor(12509.3076, grad_fn=<NegBackward0>) tensor(12509.0957, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12508.9521484375
tensor(12509.0957, grad_fn=<NegBackward0>) tensor(12508.9521, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12508.8642578125
tensor(12508.9521, grad_fn=<NegBackward0>) tensor(12508.8643, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12508.8095703125
tensor(12508.8643, grad_fn=<NegBackward0>) tensor(12508.8096, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12508.771484375
tensor(12508.8096, grad_fn=<NegBackward0>) tensor(12508.7715, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12508.7421875
tensor(12508.7715, grad_fn=<NegBackward0>) tensor(12508.7422, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12508.72265625
tensor(12508.7422, grad_fn=<NegBackward0>) tensor(12508.7227, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12508.7021484375
tensor(12508.7227, grad_fn=<NegBackward0>) tensor(12508.7021, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12508.6845703125
tensor(12508.7021, grad_fn=<NegBackward0>) tensor(12508.6846, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12508.6669921875
tensor(12508.6846, grad_fn=<NegBackward0>) tensor(12508.6670, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12508.6474609375
tensor(12508.6670, grad_fn=<NegBackward0>) tensor(12508.6475, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12508.6240234375
tensor(12508.6475, grad_fn=<NegBackward0>) tensor(12508.6240, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12508.5986328125
tensor(12508.6240, grad_fn=<NegBackward0>) tensor(12508.5986, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12508.56640625
tensor(12508.5986, grad_fn=<NegBackward0>) tensor(12508.5664, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12508.53125
tensor(12508.5664, grad_fn=<NegBackward0>) tensor(12508.5312, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12508.49609375
tensor(12508.5312, grad_fn=<NegBackward0>) tensor(12508.4961, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12508.4658203125
tensor(12508.4961, grad_fn=<NegBackward0>) tensor(12508.4658, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12508.4453125
tensor(12508.4658, grad_fn=<NegBackward0>) tensor(12508.4453, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12508.4296875
tensor(12508.4453, grad_fn=<NegBackward0>) tensor(12508.4297, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12508.419921875
tensor(12508.4297, grad_fn=<NegBackward0>) tensor(12508.4199, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12508.4150390625
tensor(12508.4199, grad_fn=<NegBackward0>) tensor(12508.4150, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12508.41015625
tensor(12508.4150, grad_fn=<NegBackward0>) tensor(12508.4102, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12508.41015625
tensor(12508.4102, grad_fn=<NegBackward0>) tensor(12508.4102, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12508.408203125
tensor(12508.4102, grad_fn=<NegBackward0>) tensor(12508.4082, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12508.40625
tensor(12508.4082, grad_fn=<NegBackward0>) tensor(12508.4062, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12508.4052734375
tensor(12508.4062, grad_fn=<NegBackward0>) tensor(12508.4053, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12508.4052734375
tensor(12508.4053, grad_fn=<NegBackward0>) tensor(12508.4053, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12508.4052734375
tensor(12508.4053, grad_fn=<NegBackward0>) tensor(12508.4053, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12508.4033203125
tensor(12508.4053, grad_fn=<NegBackward0>) tensor(12508.4033, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12508.404296875
tensor(12508.4033, grad_fn=<NegBackward0>) tensor(12508.4043, grad_fn=<NegBackward0>)
1
Iteration 3800: Loss = -12508.40234375
tensor(12508.4033, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12508.40234375
tensor(12508.4023, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12508.404296875
tensor(12508.4023, grad_fn=<NegBackward0>) tensor(12508.4043, grad_fn=<NegBackward0>)
1
Iteration 4100: Loss = -12508.40234375
tensor(12508.4023, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12508.40234375
tensor(12508.4023, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12508.4013671875
tensor(12508.4023, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12508.4033203125
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4033, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12508.4033203125
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4033, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12508.4052734375
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4053, grad_fn=<NegBackward0>)
3
Iteration 4700: Loss = -12508.4013671875
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12508.40234375
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12508.40234375
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -12508.4013671875
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12508.40234375
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
1
Iteration 5200: Loss = -12508.4013671875
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12508.400390625
tensor(12508.4014, grad_fn=<NegBackward0>) tensor(12508.4004, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12508.40234375
tensor(12508.4004, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12508.4013671875
tensor(12508.4004, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12508.4013671875
tensor(12508.4004, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -12508.40234375
tensor(12508.4004, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
4
Iteration 5800: Loss = -12508.3994140625
tensor(12508.4004, grad_fn=<NegBackward0>) tensor(12508.3994, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12508.400390625
tensor(12508.3994, grad_fn=<NegBackward0>) tensor(12508.4004, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12508.4013671875
tensor(12508.3994, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
2
Iteration 6100: Loss = -12508.40234375
tensor(12508.3994, grad_fn=<NegBackward0>) tensor(12508.4023, grad_fn=<NegBackward0>)
3
Iteration 6200: Loss = -12508.400390625
tensor(12508.3994, grad_fn=<NegBackward0>) tensor(12508.4004, grad_fn=<NegBackward0>)
4
Iteration 6300: Loss = -12508.4013671875
tensor(12508.3994, grad_fn=<NegBackward0>) tensor(12508.4014, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6300 due to no improvement.
pi: tensor([[0.4738, 0.5262],
        [0.1004, 0.8996]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0159, 0.9841], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2523, 0.1152],
         [0.6695, 0.1974]],

        [[0.6068, 0.2586],
         [0.5948, 0.6258]],

        [[0.7138, 0.2194],
         [0.6054, 0.6929]],

        [[0.6975, 0.2382],
         [0.7029, 0.7193]],

        [[0.6949, 0.2051],
         [0.6223, 0.6737]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.004267232452421997
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001390164742435905
Average Adjusted Rand Index: -0.0013181654109866345
[-0.002143614483191185, -0.001390164742435905] [-0.003193534484584708, -0.0013181654109866345] [12508.6865234375, 12508.4013671875]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11834.82789798985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20081.501953125
inf tensor(20081.5020, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12396.322265625
tensor(20081.5020, grad_fn=<NegBackward0>) tensor(12396.3223, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12395.9599609375
tensor(12396.3223, grad_fn=<NegBackward0>) tensor(12395.9600, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12395.7021484375
tensor(12395.9600, grad_fn=<NegBackward0>) tensor(12395.7021, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12394.298828125
tensor(12395.7021, grad_fn=<NegBackward0>) tensor(12394.2988, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12384.3642578125
tensor(12394.2988, grad_fn=<NegBackward0>) tensor(12384.3643, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12382.8662109375
tensor(12384.3643, grad_fn=<NegBackward0>) tensor(12382.8662, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12375.908203125
tensor(12382.8662, grad_fn=<NegBackward0>) tensor(12375.9082, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12356.419921875
tensor(12375.9082, grad_fn=<NegBackward0>) tensor(12356.4199, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12021.5830078125
tensor(12356.4199, grad_fn=<NegBackward0>) tensor(12021.5830, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11920.6025390625
tensor(12021.5830, grad_fn=<NegBackward0>) tensor(11920.6025, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11917.6142578125
tensor(11920.6025, grad_fn=<NegBackward0>) tensor(11917.6143, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11906.890625
tensor(11917.6143, grad_fn=<NegBackward0>) tensor(11906.8906, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11883.5322265625
tensor(11906.8906, grad_fn=<NegBackward0>) tensor(11883.5322, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11883.4501953125
tensor(11883.5322, grad_fn=<NegBackward0>) tensor(11883.4502, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11872.2236328125
tensor(11883.4502, grad_fn=<NegBackward0>) tensor(11872.2236, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11861.537109375
tensor(11872.2236, grad_fn=<NegBackward0>) tensor(11861.5371, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11861.3203125
tensor(11861.5371, grad_fn=<NegBackward0>) tensor(11861.3203, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11861.3056640625
tensor(11861.3203, grad_fn=<NegBackward0>) tensor(11861.3057, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11861.2373046875
tensor(11861.3057, grad_fn=<NegBackward0>) tensor(11861.2373, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11851.6328125
tensor(11861.2373, grad_fn=<NegBackward0>) tensor(11851.6328, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11851.5703125
tensor(11851.6328, grad_fn=<NegBackward0>) tensor(11851.5703, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11851.5595703125
tensor(11851.5703, grad_fn=<NegBackward0>) tensor(11851.5596, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11851.5400390625
tensor(11851.5596, grad_fn=<NegBackward0>) tensor(11851.5400, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11841.5625
tensor(11851.5400, grad_fn=<NegBackward0>) tensor(11841.5625, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11841.55859375
tensor(11841.5625, grad_fn=<NegBackward0>) tensor(11841.5586, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11841.5537109375
tensor(11841.5586, grad_fn=<NegBackward0>) tensor(11841.5537, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11841.5478515625
tensor(11841.5537, grad_fn=<NegBackward0>) tensor(11841.5479, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11841.54296875
tensor(11841.5479, grad_fn=<NegBackward0>) tensor(11841.5430, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11841.53515625
tensor(11841.5430, grad_fn=<NegBackward0>) tensor(11841.5352, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11841.5361328125
tensor(11841.5352, grad_fn=<NegBackward0>) tensor(11841.5361, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11841.53125
tensor(11841.5352, grad_fn=<NegBackward0>) tensor(11841.5312, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11841.529296875
tensor(11841.5312, grad_fn=<NegBackward0>) tensor(11841.5293, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11841.52734375
tensor(11841.5293, grad_fn=<NegBackward0>) tensor(11841.5273, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11841.525390625
tensor(11841.5273, grad_fn=<NegBackward0>) tensor(11841.5254, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11841.5244140625
tensor(11841.5254, grad_fn=<NegBackward0>) tensor(11841.5244, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11841.52734375
tensor(11841.5244, grad_fn=<NegBackward0>) tensor(11841.5273, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -11841.521484375
tensor(11841.5244, grad_fn=<NegBackward0>) tensor(11841.5215, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11841.517578125
tensor(11841.5215, grad_fn=<NegBackward0>) tensor(11841.5176, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11831.6669921875
tensor(11841.5176, grad_fn=<NegBackward0>) tensor(11831.6670, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11831.6611328125
tensor(11831.6670, grad_fn=<NegBackward0>) tensor(11831.6611, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11831.6611328125
tensor(11831.6611, grad_fn=<NegBackward0>) tensor(11831.6611, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11831.66015625
tensor(11831.6611, grad_fn=<NegBackward0>) tensor(11831.6602, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11831.66015625
tensor(11831.6602, grad_fn=<NegBackward0>) tensor(11831.6602, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11831.6328125
tensor(11831.6602, grad_fn=<NegBackward0>) tensor(11831.6328, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11831.626953125
tensor(11831.6328, grad_fn=<NegBackward0>) tensor(11831.6270, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11831.626953125
tensor(11831.6270, grad_fn=<NegBackward0>) tensor(11831.6270, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11831.625
tensor(11831.6270, grad_fn=<NegBackward0>) tensor(11831.6250, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11831.625
tensor(11831.6250, grad_fn=<NegBackward0>) tensor(11831.6250, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11831.6240234375
tensor(11831.6250, grad_fn=<NegBackward0>) tensor(11831.6240, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11831.6240234375
tensor(11831.6240, grad_fn=<NegBackward0>) tensor(11831.6240, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11831.6240234375
tensor(11831.6240, grad_fn=<NegBackward0>) tensor(11831.6240, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11831.6259765625
tensor(11831.6240, grad_fn=<NegBackward0>) tensor(11831.6260, grad_fn=<NegBackward0>)
1
Iteration 5300: Loss = -11831.623046875
tensor(11831.6240, grad_fn=<NegBackward0>) tensor(11831.6230, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11831.6240234375
tensor(11831.6230, grad_fn=<NegBackward0>) tensor(11831.6240, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11831.6240234375
tensor(11831.6230, grad_fn=<NegBackward0>) tensor(11831.6240, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -11831.6318359375
tensor(11831.6230, grad_fn=<NegBackward0>) tensor(11831.6318, grad_fn=<NegBackward0>)
3
Iteration 5700: Loss = -11831.623046875
tensor(11831.6230, grad_fn=<NegBackward0>) tensor(11831.6230, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11831.623046875
tensor(11831.6230, grad_fn=<NegBackward0>) tensor(11831.6230, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11831.62109375
tensor(11831.6230, grad_fn=<NegBackward0>) tensor(11831.6211, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -11831.6259765625
tensor(11831.6211, grad_fn=<NegBackward0>) tensor(11831.6260, grad_fn=<NegBackward0>)
1
Iteration 6100: Loss = -11831.623046875
tensor(11831.6211, grad_fn=<NegBackward0>) tensor(11831.6230, grad_fn=<NegBackward0>)
2
Iteration 6200: Loss = -11831.619140625
tensor(11831.6211, grad_fn=<NegBackward0>) tensor(11831.6191, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -11831.62890625
tensor(11831.6191, grad_fn=<NegBackward0>) tensor(11831.6289, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -11831.619140625
tensor(11831.6191, grad_fn=<NegBackward0>) tensor(11831.6191, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -11831.625
tensor(11831.6191, grad_fn=<NegBackward0>) tensor(11831.6250, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -11831.60546875
tensor(11831.6191, grad_fn=<NegBackward0>) tensor(11831.6055, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11831.6044921875
tensor(11831.6055, grad_fn=<NegBackward0>) tensor(11831.6045, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11831.60546875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6055, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11831.6044921875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6045, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11831.6044921875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6045, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -11831.60546875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6055, grad_fn=<NegBackward0>)
1
Iteration 7200: Loss = -11831.6044921875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6045, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11831.6083984375
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6084, grad_fn=<NegBackward0>)
1
Iteration 7400: Loss = -11831.6083984375
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6084, grad_fn=<NegBackward0>)
2
Iteration 7500: Loss = -11831.615234375
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6152, grad_fn=<NegBackward0>)
3
Iteration 7600: Loss = -11831.60546875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6055, grad_fn=<NegBackward0>)
4
Iteration 7700: Loss = -11831.6044921875
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6045, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -11831.642578125
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6426, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -11831.603515625
tensor(11831.6045, grad_fn=<NegBackward0>) tensor(11831.6035, grad_fn=<NegBackward0>)
Iteration 8000: Loss = -11831.6044921875
tensor(11831.6035, grad_fn=<NegBackward0>) tensor(11831.6045, grad_fn=<NegBackward0>)
1
Iteration 8100: Loss = -11831.6064453125
tensor(11831.6035, grad_fn=<NegBackward0>) tensor(11831.6064, grad_fn=<NegBackward0>)
2
Iteration 8200: Loss = -11831.6025390625
tensor(11831.6035, grad_fn=<NegBackward0>) tensor(11831.6025, grad_fn=<NegBackward0>)
Iteration 8300: Loss = -11831.603515625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6035, grad_fn=<NegBackward0>)
1
Iteration 8400: Loss = -11831.6025390625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6025, grad_fn=<NegBackward0>)
Iteration 8500: Loss = -11831.6025390625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6025, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -11831.6025390625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6025, grad_fn=<NegBackward0>)
Iteration 8700: Loss = -11831.6142578125
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6143, grad_fn=<NegBackward0>)
1
Iteration 8800: Loss = -11831.6025390625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6025, grad_fn=<NegBackward0>)
Iteration 8900: Loss = -11831.6025390625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6025, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -11831.603515625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6035, grad_fn=<NegBackward0>)
1
Iteration 9100: Loss = -11831.603515625
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.6035, grad_fn=<NegBackward0>)
2
Iteration 9200: Loss = -11831.580078125
tensor(11831.6025, grad_fn=<NegBackward0>) tensor(11831.5801, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -11831.5830078125
tensor(11831.5801, grad_fn=<NegBackward0>) tensor(11831.5830, grad_fn=<NegBackward0>)
1
Iteration 9400: Loss = -11831.7041015625
tensor(11831.5801, grad_fn=<NegBackward0>) tensor(11831.7041, grad_fn=<NegBackward0>)
2
Iteration 9500: Loss = -11831.5712890625
tensor(11831.5801, grad_fn=<NegBackward0>) tensor(11831.5713, grad_fn=<NegBackward0>)
Iteration 9600: Loss = -11831.5712890625
tensor(11831.5713, grad_fn=<NegBackward0>) tensor(11831.5713, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -11831.76953125
tensor(11831.5713, grad_fn=<NegBackward0>) tensor(11831.7695, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -11831.5703125
tensor(11831.5713, grad_fn=<NegBackward0>) tensor(11831.5703, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -11831.611328125
tensor(11831.5703, grad_fn=<NegBackward0>) tensor(11831.6113, grad_fn=<NegBackward0>)
1
pi: tensor([[0.2290, 0.7710],
        [0.7322, 0.2678]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5300, 0.4700], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3077, 0.0955],
         [0.5624, 0.2989]],

        [[0.7178, 0.0988],
         [0.7216, 0.6327]],

        [[0.5716, 0.0972],
         [0.5083, 0.6881]],

        [[0.7101, 0.0964],
         [0.7153, 0.5803]],

        [[0.7211, 0.0981],
         [0.5819, 0.5402]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03807902028560263
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22787.19140625
inf tensor(22787.1914, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12396.314453125
tensor(22787.1914, grad_fn=<NegBackward0>) tensor(12396.3145, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12396.05859375
tensor(12396.3145, grad_fn=<NegBackward0>) tensor(12396.0586, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12395.9970703125
tensor(12396.0586, grad_fn=<NegBackward0>) tensor(12395.9971, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12395.9404296875
tensor(12395.9971, grad_fn=<NegBackward0>) tensor(12395.9404, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12395.8828125
tensor(12395.9404, grad_fn=<NegBackward0>) tensor(12395.8828, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12395.8251953125
tensor(12395.8828, grad_fn=<NegBackward0>) tensor(12395.8252, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12395.765625
tensor(12395.8252, grad_fn=<NegBackward0>) tensor(12395.7656, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12395.720703125
tensor(12395.7656, grad_fn=<NegBackward0>) tensor(12395.7207, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12395.685546875
tensor(12395.7207, grad_fn=<NegBackward0>) tensor(12395.6855, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12395.6533203125
tensor(12395.6855, grad_fn=<NegBackward0>) tensor(12395.6533, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12395.6259765625
tensor(12395.6533, grad_fn=<NegBackward0>) tensor(12395.6260, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12395.599609375
tensor(12395.6260, grad_fn=<NegBackward0>) tensor(12395.5996, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12395.5791015625
tensor(12395.5996, grad_fn=<NegBackward0>) tensor(12395.5791, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12395.55859375
tensor(12395.5791, grad_fn=<NegBackward0>) tensor(12395.5586, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12395.5400390625
tensor(12395.5586, grad_fn=<NegBackward0>) tensor(12395.5400, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12395.5166015625
tensor(12395.5400, grad_fn=<NegBackward0>) tensor(12395.5166, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12395.484375
tensor(12395.5166, grad_fn=<NegBackward0>) tensor(12395.4844, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12395.4150390625
tensor(12395.4844, grad_fn=<NegBackward0>) tensor(12395.4150, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12395.33203125
tensor(12395.4150, grad_fn=<NegBackward0>) tensor(12395.3320, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12395.2763671875
tensor(12395.3320, grad_fn=<NegBackward0>) tensor(12395.2764, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12395.2421875
tensor(12395.2764, grad_fn=<NegBackward0>) tensor(12395.2422, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12395.220703125
tensor(12395.2422, grad_fn=<NegBackward0>) tensor(12395.2207, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12395.2021484375
tensor(12395.2207, grad_fn=<NegBackward0>) tensor(12395.2021, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12395.1865234375
tensor(12395.2021, grad_fn=<NegBackward0>) tensor(12395.1865, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12395.171875
tensor(12395.1865, grad_fn=<NegBackward0>) tensor(12395.1719, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12395.162109375
tensor(12395.1719, grad_fn=<NegBackward0>) tensor(12395.1621, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12395.146484375
tensor(12395.1621, grad_fn=<NegBackward0>) tensor(12395.1465, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12395.13671875
tensor(12395.1465, grad_fn=<NegBackward0>) tensor(12395.1367, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12395.12890625
tensor(12395.1367, grad_fn=<NegBackward0>) tensor(12395.1289, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12395.12109375
tensor(12395.1289, grad_fn=<NegBackward0>) tensor(12395.1211, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12395.11328125
tensor(12395.1211, grad_fn=<NegBackward0>) tensor(12395.1133, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12395.1083984375
tensor(12395.1133, grad_fn=<NegBackward0>) tensor(12395.1084, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12395.1044921875
tensor(12395.1084, grad_fn=<NegBackward0>) tensor(12395.1045, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12395.0986328125
tensor(12395.1045, grad_fn=<NegBackward0>) tensor(12395.0986, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12395.099609375
tensor(12395.0986, grad_fn=<NegBackward0>) tensor(12395.0996, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -12395.0927734375
tensor(12395.0986, grad_fn=<NegBackward0>) tensor(12395.0928, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12395.091796875
tensor(12395.0928, grad_fn=<NegBackward0>) tensor(12395.0918, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12395.0888671875
tensor(12395.0918, grad_fn=<NegBackward0>) tensor(12395.0889, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12395.087890625
tensor(12395.0889, grad_fn=<NegBackward0>) tensor(12395.0879, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12395.087890625
tensor(12395.0879, grad_fn=<NegBackward0>) tensor(12395.0879, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12395.0859375
tensor(12395.0879, grad_fn=<NegBackward0>) tensor(12395.0859, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12395.0849609375
tensor(12395.0859, grad_fn=<NegBackward0>) tensor(12395.0850, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12395.0830078125
tensor(12395.0850, grad_fn=<NegBackward0>) tensor(12395.0830, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12395.083984375
tensor(12395.0830, grad_fn=<NegBackward0>) tensor(12395.0840, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12395.0859375
tensor(12395.0830, grad_fn=<NegBackward0>) tensor(12395.0859, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12395.0830078125
tensor(12395.0830, grad_fn=<NegBackward0>) tensor(12395.0830, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12395.08203125
tensor(12395.0830, grad_fn=<NegBackward0>) tensor(12395.0820, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12395.0810546875
tensor(12395.0820, grad_fn=<NegBackward0>) tensor(12395.0811, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12395.0810546875
tensor(12395.0811, grad_fn=<NegBackward0>) tensor(12395.0811, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12395.0810546875
tensor(12395.0811, grad_fn=<NegBackward0>) tensor(12395.0811, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12395.0810546875
tensor(12395.0811, grad_fn=<NegBackward0>) tensor(12395.0811, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12395.0810546875
tensor(12395.0811, grad_fn=<NegBackward0>) tensor(12395.0811, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12395.080078125
tensor(12395.0811, grad_fn=<NegBackward0>) tensor(12395.0801, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12395.0791015625
tensor(12395.0801, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12395.080078125
tensor(12395.0791, grad_fn=<NegBackward0>) tensor(12395.0801, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12395.0791015625
tensor(12395.0791, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12395.080078125
tensor(12395.0791, grad_fn=<NegBackward0>) tensor(12395.0801, grad_fn=<NegBackward0>)
1
Iteration 5800: Loss = -12395.0791015625
tensor(12395.0791, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12395.080078125
tensor(12395.0791, grad_fn=<NegBackward0>) tensor(12395.0801, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -12395.078125
tensor(12395.0791, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12395.078125
tensor(12395.0781, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12395.078125
tensor(12395.0781, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12395.0791015625
tensor(12395.0781, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
1
Iteration 6400: Loss = -12395.078125
tensor(12395.0781, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12395.0771484375
tensor(12395.0781, grad_fn=<NegBackward0>) tensor(12395.0771, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12395.0791015625
tensor(12395.0771, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
1
Iteration 6700: Loss = -12395.08203125
tensor(12395.0771, grad_fn=<NegBackward0>) tensor(12395.0820, grad_fn=<NegBackward0>)
2
Iteration 6800: Loss = -12395.0791015625
tensor(12395.0771, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
3
Iteration 6900: Loss = -12395.076171875
tensor(12395.0771, grad_fn=<NegBackward0>) tensor(12395.0762, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12395.0791015625
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0791, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -12395.078125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -12395.0830078125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0830, grad_fn=<NegBackward0>)
3
Iteration 7300: Loss = -12395.078125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
4
Iteration 7400: Loss = -12395.076171875
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0762, grad_fn=<NegBackward0>)
Iteration 7500: Loss = -12395.078125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0781, grad_fn=<NegBackward0>)
1
Iteration 7600: Loss = -12395.0849609375
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0850, grad_fn=<NegBackward0>)
2
Iteration 7700: Loss = -12395.08203125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0820, grad_fn=<NegBackward0>)
3
Iteration 7800: Loss = -12395.08203125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0820, grad_fn=<NegBackward0>)
4
Iteration 7900: Loss = -12395.08203125
tensor(12395.0762, grad_fn=<NegBackward0>) tensor(12395.0820, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7900 due to no improvement.
pi: tensor([[0.0144, 0.9856],
        [0.0144, 0.9856]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8012, 0.1988], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2049, 0.2019],
         [0.5481, 0.1987]],

        [[0.5394, 0.2732],
         [0.5610, 0.6604]],

        [[0.5724, 0.2385],
         [0.7302, 0.5452]],

        [[0.5402, 0.3004],
         [0.6178, 0.7284]],

        [[0.7169, 0.1283],
         [0.6672, 0.6022]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009687642749099427
Average Adjusted Rand Index: -0.0003077958928485548
[0.03807902028560263, -0.0009687642749099427] [1.0, -0.0003077958928485548] [11831.5712890625, 12395.08203125]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11963.39528080235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22609.697265625
inf tensor(22609.6973, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12514.52734375
tensor(22609.6973, grad_fn=<NegBackward0>) tensor(12514.5273, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12514.138671875
tensor(12514.5273, grad_fn=<NegBackward0>) tensor(12514.1387, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12513.9716796875
tensor(12514.1387, grad_fn=<NegBackward0>) tensor(12513.9717, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12513.8408203125
tensor(12513.9717, grad_fn=<NegBackward0>) tensor(12513.8408, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12513.7255859375
tensor(12513.8408, grad_fn=<NegBackward0>) tensor(12513.7256, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12513.6259765625
tensor(12513.7256, grad_fn=<NegBackward0>) tensor(12513.6260, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12513.5322265625
tensor(12513.6260, grad_fn=<NegBackward0>) tensor(12513.5322, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12513.421875
tensor(12513.5322, grad_fn=<NegBackward0>) tensor(12513.4219, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12513.2236328125
tensor(12513.4219, grad_fn=<NegBackward0>) tensor(12513.2236, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12512.890625
tensor(12513.2236, grad_fn=<NegBackward0>) tensor(12512.8906, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12512.6962890625
tensor(12512.8906, grad_fn=<NegBackward0>) tensor(12512.6963, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12512.6103515625
tensor(12512.6963, grad_fn=<NegBackward0>) tensor(12512.6104, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12512.5625
tensor(12512.6104, grad_fn=<NegBackward0>) tensor(12512.5625, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12512.5361328125
tensor(12512.5625, grad_fn=<NegBackward0>) tensor(12512.5361, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12512.517578125
tensor(12512.5361, grad_fn=<NegBackward0>) tensor(12512.5176, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12512.5068359375
tensor(12512.5176, grad_fn=<NegBackward0>) tensor(12512.5068, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12512.4970703125
tensor(12512.5068, grad_fn=<NegBackward0>) tensor(12512.4971, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12512.4912109375
tensor(12512.4971, grad_fn=<NegBackward0>) tensor(12512.4912, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12512.486328125
tensor(12512.4912, grad_fn=<NegBackward0>) tensor(12512.4863, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12512.482421875
tensor(12512.4863, grad_fn=<NegBackward0>) tensor(12512.4824, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12512.48046875
tensor(12512.4824, grad_fn=<NegBackward0>) tensor(12512.4805, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12512.4755859375
tensor(12512.4805, grad_fn=<NegBackward0>) tensor(12512.4756, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12512.474609375
tensor(12512.4756, grad_fn=<NegBackward0>) tensor(12512.4746, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12512.4736328125
tensor(12512.4746, grad_fn=<NegBackward0>) tensor(12512.4736, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12512.470703125
tensor(12512.4736, grad_fn=<NegBackward0>) tensor(12512.4707, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12512.470703125
tensor(12512.4707, grad_fn=<NegBackward0>) tensor(12512.4707, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12512.4677734375
tensor(12512.4707, grad_fn=<NegBackward0>) tensor(12512.4678, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12512.4677734375
tensor(12512.4678, grad_fn=<NegBackward0>) tensor(12512.4678, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12512.466796875
tensor(12512.4678, grad_fn=<NegBackward0>) tensor(12512.4668, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12512.4638671875
tensor(12512.4668, grad_fn=<NegBackward0>) tensor(12512.4639, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12512.4638671875
tensor(12512.4639, grad_fn=<NegBackward0>) tensor(12512.4639, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12512.4638671875
tensor(12512.4639, grad_fn=<NegBackward0>) tensor(12512.4639, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12512.4609375
tensor(12512.4639, grad_fn=<NegBackward0>) tensor(12512.4609, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12512.4609375
tensor(12512.4609, grad_fn=<NegBackward0>) tensor(12512.4609, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12512.4599609375
tensor(12512.4609, grad_fn=<NegBackward0>) tensor(12512.4600, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12512.458984375
tensor(12512.4600, grad_fn=<NegBackward0>) tensor(12512.4590, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12512.4580078125
tensor(12512.4590, grad_fn=<NegBackward0>) tensor(12512.4580, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12512.4580078125
tensor(12512.4580, grad_fn=<NegBackward0>) tensor(12512.4580, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12512.4580078125
tensor(12512.4580, grad_fn=<NegBackward0>) tensor(12512.4580, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12512.45703125
tensor(12512.4580, grad_fn=<NegBackward0>) tensor(12512.4570, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12512.45703125
tensor(12512.4570, grad_fn=<NegBackward0>) tensor(12512.4570, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12512.455078125
tensor(12512.4570, grad_fn=<NegBackward0>) tensor(12512.4551, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12512.455078125
tensor(12512.4551, grad_fn=<NegBackward0>) tensor(12512.4551, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12512.4541015625
tensor(12512.4551, grad_fn=<NegBackward0>) tensor(12512.4541, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -12512.453125
tensor(12512.4541, grad_fn=<NegBackward0>) tensor(12512.4531, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12512.4521484375
tensor(12512.4531, grad_fn=<NegBackward0>) tensor(12512.4521, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12512.453125
tensor(12512.4521, grad_fn=<NegBackward0>) tensor(12512.4531, grad_fn=<NegBackward0>)
1
Iteration 4800: Loss = -12512.451171875
tensor(12512.4521, grad_fn=<NegBackward0>) tensor(12512.4512, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -12512.451171875
tensor(12512.4512, grad_fn=<NegBackward0>) tensor(12512.4512, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12512.451171875
tensor(12512.4512, grad_fn=<NegBackward0>) tensor(12512.4512, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12512.4501953125
tensor(12512.4512, grad_fn=<NegBackward0>) tensor(12512.4502, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12512.4501953125
tensor(12512.4502, grad_fn=<NegBackward0>) tensor(12512.4502, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12512.4482421875
tensor(12512.4502, grad_fn=<NegBackward0>) tensor(12512.4482, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12512.44921875
tensor(12512.4482, grad_fn=<NegBackward0>) tensor(12512.4492, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -12512.44921875
tensor(12512.4482, grad_fn=<NegBackward0>) tensor(12512.4492, grad_fn=<NegBackward0>)
2
Iteration 5600: Loss = -12512.4482421875
tensor(12512.4482, grad_fn=<NegBackward0>) tensor(12512.4482, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12512.4462890625
tensor(12512.4482, grad_fn=<NegBackward0>) tensor(12512.4463, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12512.4462890625
tensor(12512.4463, grad_fn=<NegBackward0>) tensor(12512.4463, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12512.4453125
tensor(12512.4463, grad_fn=<NegBackward0>) tensor(12512.4453, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12512.44140625
tensor(12512.4453, grad_fn=<NegBackward0>) tensor(12512.4414, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12512.4326171875
tensor(12512.4414, grad_fn=<NegBackward0>) tensor(12512.4326, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12512.2998046875
tensor(12512.4326, grad_fn=<NegBackward0>) tensor(12512.2998, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12512.162109375
tensor(12512.2998, grad_fn=<NegBackward0>) tensor(12512.1621, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12512.1611328125
tensor(12512.1621, grad_fn=<NegBackward0>) tensor(12512.1611, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12512.1640625
tensor(12512.1611, grad_fn=<NegBackward0>) tensor(12512.1641, grad_fn=<NegBackward0>)
1
Iteration 6600: Loss = -12512.22265625
tensor(12512.1611, grad_fn=<NegBackward0>) tensor(12512.2227, grad_fn=<NegBackward0>)
2
Iteration 6700: Loss = -12512.1591796875
tensor(12512.1611, grad_fn=<NegBackward0>) tensor(12512.1592, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12512.16015625
tensor(12512.1592, grad_fn=<NegBackward0>) tensor(12512.1602, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -12512.171875
tensor(12512.1592, grad_fn=<NegBackward0>) tensor(12512.1719, grad_fn=<NegBackward0>)
2
Iteration 7000: Loss = -12512.158203125
tensor(12512.1592, grad_fn=<NegBackward0>) tensor(12512.1582, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12512.158203125
tensor(12512.1582, grad_fn=<NegBackward0>) tensor(12512.1582, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12512.16015625
tensor(12512.1582, grad_fn=<NegBackward0>) tensor(12512.1602, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12512.1572265625
tensor(12512.1582, grad_fn=<NegBackward0>) tensor(12512.1572, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12512.478515625
tensor(12512.1572, grad_fn=<NegBackward0>) tensor(12512.4785, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12512.15625
tensor(12512.1572, grad_fn=<NegBackward0>) tensor(12512.1562, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -12512.1572265625
tensor(12512.1562, grad_fn=<NegBackward0>) tensor(12512.1572, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -12512.15625
tensor(12512.1562, grad_fn=<NegBackward0>) tensor(12512.1562, grad_fn=<NegBackward0>)
Iteration 7800: Loss = -12512.1572265625
tensor(12512.1562, grad_fn=<NegBackward0>) tensor(12512.1572, grad_fn=<NegBackward0>)
1
Iteration 7900: Loss = -12512.3056640625
tensor(12512.1562, grad_fn=<NegBackward0>) tensor(12512.3057, grad_fn=<NegBackward0>)
2
Iteration 8000: Loss = -12512.1572265625
tensor(12512.1562, grad_fn=<NegBackward0>) tensor(12512.1572, grad_fn=<NegBackward0>)
3
Iteration 8100: Loss = -12512.1552734375
tensor(12512.1562, grad_fn=<NegBackward0>) tensor(12512.1553, grad_fn=<NegBackward0>)
Iteration 8200: Loss = -12512.1728515625
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1729, grad_fn=<NegBackward0>)
1
Iteration 8300: Loss = -12512.1572265625
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1572, grad_fn=<NegBackward0>)
2
Iteration 8400: Loss = -12512.2998046875
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.2998, grad_fn=<NegBackward0>)
3
Iteration 8500: Loss = -12512.1552734375
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1553, grad_fn=<NegBackward0>)
Iteration 8600: Loss = -12512.1953125
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1953, grad_fn=<NegBackward0>)
1
Iteration 8700: Loss = -12512.15625
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1562, grad_fn=<NegBackward0>)
2
Iteration 8800: Loss = -12512.15625
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1562, grad_fn=<NegBackward0>)
3
Iteration 8900: Loss = -12512.1552734375
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1553, grad_fn=<NegBackward0>)
Iteration 9000: Loss = -12512.1552734375
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1553, grad_fn=<NegBackward0>)
Iteration 9100: Loss = -12512.4208984375
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.4209, grad_fn=<NegBackward0>)
1
Iteration 9200: Loss = -12512.1552734375
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1553, grad_fn=<NegBackward0>)
Iteration 9300: Loss = -12512.154296875
tensor(12512.1553, grad_fn=<NegBackward0>) tensor(12512.1543, grad_fn=<NegBackward0>)
Iteration 9400: Loss = -12512.16015625
tensor(12512.1543, grad_fn=<NegBackward0>) tensor(12512.1602, grad_fn=<NegBackward0>)
1
Iteration 9500: Loss = -12512.2158203125
tensor(12512.1543, grad_fn=<NegBackward0>) tensor(12512.2158, grad_fn=<NegBackward0>)
2
Iteration 9600: Loss = -12512.1533203125
tensor(12512.1543, grad_fn=<NegBackward0>) tensor(12512.1533, grad_fn=<NegBackward0>)
Iteration 9700: Loss = -12512.1953125
tensor(12512.1533, grad_fn=<NegBackward0>) tensor(12512.1953, grad_fn=<NegBackward0>)
1
Iteration 9800: Loss = -12512.1533203125
tensor(12512.1533, grad_fn=<NegBackward0>) tensor(12512.1533, grad_fn=<NegBackward0>)
Iteration 9900: Loss = -12512.236328125
tensor(12512.1533, grad_fn=<NegBackward0>) tensor(12512.2363, grad_fn=<NegBackward0>)
1
pi: tensor([[8.3010e-04, 9.9917e-01],
        [1.7414e-02, 9.8259e-01]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2221, 0.7779], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1720, 0.1867],
         [0.5442, 0.2040]],

        [[0.6610, 0.1992],
         [0.5834, 0.6507]],

        [[0.6629, 0.2092],
         [0.6657, 0.6211]],

        [[0.7219, 0.2832],
         [0.6757, 0.5260]],

        [[0.5647, 0.3159],
         [0.5000, 0.6676]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
Global Adjusted Rand Index: -0.0005372996754200452
Average Adjusted Rand Index: -0.001219209991574565
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21864.560546875
inf tensor(21864.5605, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12514.3076171875
tensor(21864.5605, grad_fn=<NegBackward0>) tensor(12514.3076, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12514.08984375
tensor(12514.3076, grad_fn=<NegBackward0>) tensor(12514.0898, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12514.0048828125
tensor(12514.0898, grad_fn=<NegBackward0>) tensor(12514.0049, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12513.8984375
tensor(12514.0049, grad_fn=<NegBackward0>) tensor(12513.8984, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12513.765625
tensor(12513.8984, grad_fn=<NegBackward0>) tensor(12513.7656, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12513.6259765625
tensor(12513.7656, grad_fn=<NegBackward0>) tensor(12513.6260, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12513.443359375
tensor(12513.6260, grad_fn=<NegBackward0>) tensor(12513.4434, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12512.90234375
tensor(12513.4434, grad_fn=<NegBackward0>) tensor(12512.9023, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12512.5673828125
tensor(12512.9023, grad_fn=<NegBackward0>) tensor(12512.5674, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12512.46484375
tensor(12512.5674, grad_fn=<NegBackward0>) tensor(12512.4648, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12512.40625
tensor(12512.4648, grad_fn=<NegBackward0>) tensor(12512.4062, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12512.359375
tensor(12512.4062, grad_fn=<NegBackward0>) tensor(12512.3594, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12512.318359375
tensor(12512.3594, grad_fn=<NegBackward0>) tensor(12512.3184, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12512.2822265625
tensor(12512.3184, grad_fn=<NegBackward0>) tensor(12512.2822, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12512.2529296875
tensor(12512.2822, grad_fn=<NegBackward0>) tensor(12512.2529, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12512.2294921875
tensor(12512.2529, grad_fn=<NegBackward0>) tensor(12512.2295, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12512.2080078125
tensor(12512.2295, grad_fn=<NegBackward0>) tensor(12512.2080, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12512.1865234375
tensor(12512.2080, grad_fn=<NegBackward0>) tensor(12512.1865, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12512.162109375
tensor(12512.1865, grad_fn=<NegBackward0>) tensor(12512.1621, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12512.1328125
tensor(12512.1621, grad_fn=<NegBackward0>) tensor(12512.1328, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12512.087890625
tensor(12512.1328, grad_fn=<NegBackward0>) tensor(12512.0879, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12512.0087890625
tensor(12512.0879, grad_fn=<NegBackward0>) tensor(12512.0088, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12511.8310546875
tensor(12512.0088, grad_fn=<NegBackward0>) tensor(12511.8311, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12511.6513671875
tensor(12511.8311, grad_fn=<NegBackward0>) tensor(12511.6514, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12511.54296875
tensor(12511.6514, grad_fn=<NegBackward0>) tensor(12511.5430, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12511.4775390625
tensor(12511.5430, grad_fn=<NegBackward0>) tensor(12511.4775, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12511.4345703125
tensor(12511.4775, grad_fn=<NegBackward0>) tensor(12511.4346, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12511.40234375
tensor(12511.4346, grad_fn=<NegBackward0>) tensor(12511.4023, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12511.3798828125
tensor(12511.4023, grad_fn=<NegBackward0>) tensor(12511.3799, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12511.359375
tensor(12511.3799, grad_fn=<NegBackward0>) tensor(12511.3594, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12511.3466796875
tensor(12511.3594, grad_fn=<NegBackward0>) tensor(12511.3467, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12511.3359375
tensor(12511.3467, grad_fn=<NegBackward0>) tensor(12511.3359, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12511.33203125
tensor(12511.3359, grad_fn=<NegBackward0>) tensor(12511.3320, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12511.326171875
tensor(12511.3320, grad_fn=<NegBackward0>) tensor(12511.3262, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12511.322265625
tensor(12511.3262, grad_fn=<NegBackward0>) tensor(12511.3223, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12511.3193359375
tensor(12511.3223, grad_fn=<NegBackward0>) tensor(12511.3193, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -12511.3173828125
tensor(12511.3193, grad_fn=<NegBackward0>) tensor(12511.3174, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -12511.314453125
tensor(12511.3174, grad_fn=<NegBackward0>) tensor(12511.3145, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -12511.314453125
tensor(12511.3145, grad_fn=<NegBackward0>) tensor(12511.3145, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -12511.3125
tensor(12511.3145, grad_fn=<NegBackward0>) tensor(12511.3125, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12511.3115234375
tensor(12511.3125, grad_fn=<NegBackward0>) tensor(12511.3115, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12511.3115234375
tensor(12511.3115, grad_fn=<NegBackward0>) tensor(12511.3115, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -12511.3095703125
tensor(12511.3115, grad_fn=<NegBackward0>) tensor(12511.3096, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12511.310546875
tensor(12511.3096, grad_fn=<NegBackward0>) tensor(12511.3105, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12511.3095703125
tensor(12511.3096, grad_fn=<NegBackward0>) tensor(12511.3096, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -12511.310546875
tensor(12511.3096, grad_fn=<NegBackward0>) tensor(12511.3105, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -12511.310546875
tensor(12511.3096, grad_fn=<NegBackward0>) tensor(12511.3105, grad_fn=<NegBackward0>)
2
Iteration 4800: Loss = -12511.310546875
tensor(12511.3096, grad_fn=<NegBackward0>) tensor(12511.3105, grad_fn=<NegBackward0>)
3
Iteration 4900: Loss = -12511.30859375
tensor(12511.3096, grad_fn=<NegBackward0>) tensor(12511.3086, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -12511.30859375
tensor(12511.3086, grad_fn=<NegBackward0>) tensor(12511.3086, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12511.30859375
tensor(12511.3086, grad_fn=<NegBackward0>) tensor(12511.3086, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12511.3076171875
tensor(12511.3086, grad_fn=<NegBackward0>) tensor(12511.3076, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12511.30859375
tensor(12511.3076, grad_fn=<NegBackward0>) tensor(12511.3086, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -12511.306640625
tensor(12511.3076, grad_fn=<NegBackward0>) tensor(12511.3066, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12511.30859375
tensor(12511.3066, grad_fn=<NegBackward0>) tensor(12511.3086, grad_fn=<NegBackward0>)
1
Iteration 5600: Loss = -12511.3095703125
tensor(12511.3066, grad_fn=<NegBackward0>) tensor(12511.3096, grad_fn=<NegBackward0>)
2
Iteration 5700: Loss = -12511.30859375
tensor(12511.3066, grad_fn=<NegBackward0>) tensor(12511.3086, grad_fn=<NegBackward0>)
3
Iteration 5800: Loss = -12511.3095703125
tensor(12511.3066, grad_fn=<NegBackward0>) tensor(12511.3096, grad_fn=<NegBackward0>)
4
Iteration 5900: Loss = -12511.3076171875
tensor(12511.3066, grad_fn=<NegBackward0>) tensor(12511.3076, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5900 due to no improvement.
pi: tensor([[0.7217, 0.2783],
        [0.0042, 0.9958]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0309, 0.9691], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0967, 0.1483],
         [0.6430, 0.2020]],

        [[0.5528, 0.3004],
         [0.7079, 0.6010]],

        [[0.6960, 0.2438],
         [0.6585, 0.6583]],

        [[0.5774, 0.2415],
         [0.6939, 0.7089]],

        [[0.7158, 0.3084],
         [0.7107, 0.6141]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.011374456256342739
Global Adjusted Rand Index: -0.0013651965464886934
Average Adjusted Rand Index: -0.003114806798842399
[-0.0005372996754200452, -0.0013651965464886934] [-0.001219209991574565, -0.003114806798842399] [12512.1552734375, 12511.3076171875]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11954.412489498376
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22471.21484375
inf tensor(22471.2148, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12169.5517578125
tensor(22471.2148, grad_fn=<NegBackward0>) tensor(12169.5518, grad_fn=<NegBackward0>)
Iteration 200: Loss = -11985.490234375
tensor(12169.5518, grad_fn=<NegBackward0>) tensor(11985.4902, grad_fn=<NegBackward0>)
Iteration 300: Loss = -11967.6044921875
tensor(11985.4902, grad_fn=<NegBackward0>) tensor(11967.6045, grad_fn=<NegBackward0>)
Iteration 400: Loss = -11967.4833984375
tensor(11967.6045, grad_fn=<NegBackward0>) tensor(11967.4834, grad_fn=<NegBackward0>)
Iteration 500: Loss = -11967.4287109375
tensor(11967.4834, grad_fn=<NegBackward0>) tensor(11967.4287, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11967.3994140625
tensor(11967.4287, grad_fn=<NegBackward0>) tensor(11967.3994, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11967.3798828125
tensor(11967.3994, grad_fn=<NegBackward0>) tensor(11967.3799, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11967.3681640625
tensor(11967.3799, grad_fn=<NegBackward0>) tensor(11967.3682, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11967.3583984375
tensor(11967.3682, grad_fn=<NegBackward0>) tensor(11967.3584, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11967.3515625
tensor(11967.3584, grad_fn=<NegBackward0>) tensor(11967.3516, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11967.345703125
tensor(11967.3516, grad_fn=<NegBackward0>) tensor(11967.3457, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11967.3408203125
tensor(11967.3457, grad_fn=<NegBackward0>) tensor(11967.3408, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11967.3388671875
tensor(11967.3408, grad_fn=<NegBackward0>) tensor(11967.3389, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11967.3359375
tensor(11967.3389, grad_fn=<NegBackward0>) tensor(11967.3359, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11967.333984375
tensor(11967.3359, grad_fn=<NegBackward0>) tensor(11967.3340, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11967.3330078125
tensor(11967.3340, grad_fn=<NegBackward0>) tensor(11967.3330, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11967.330078125
tensor(11967.3330, grad_fn=<NegBackward0>) tensor(11967.3301, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11967.328125
tensor(11967.3301, grad_fn=<NegBackward0>) tensor(11967.3281, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11967.3271484375
tensor(11967.3281, grad_fn=<NegBackward0>) tensor(11967.3271, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11967.326171875
tensor(11967.3271, grad_fn=<NegBackward0>) tensor(11967.3262, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11967.326171875
tensor(11967.3262, grad_fn=<NegBackward0>) tensor(11967.3262, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11967.32421875
tensor(11967.3262, grad_fn=<NegBackward0>) tensor(11967.3242, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11967.3232421875
tensor(11967.3242, grad_fn=<NegBackward0>) tensor(11967.3232, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11967.3232421875
tensor(11967.3232, grad_fn=<NegBackward0>) tensor(11967.3232, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11967.3251953125
tensor(11967.3232, grad_fn=<NegBackward0>) tensor(11967.3252, grad_fn=<NegBackward0>)
1
Iteration 2600: Loss = -11967.3232421875
tensor(11967.3232, grad_fn=<NegBackward0>) tensor(11967.3232, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11967.322265625
tensor(11967.3232, grad_fn=<NegBackward0>) tensor(11967.3223, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11967.3232421875
tensor(11967.3223, grad_fn=<NegBackward0>) tensor(11967.3232, grad_fn=<NegBackward0>)
1
Iteration 2900: Loss = -11967.3212890625
tensor(11967.3223, grad_fn=<NegBackward0>) tensor(11967.3213, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11967.3203125
tensor(11967.3213, grad_fn=<NegBackward0>) tensor(11967.3203, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11967.3193359375
tensor(11967.3203, grad_fn=<NegBackward0>) tensor(11967.3193, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11967.3203125
tensor(11967.3193, grad_fn=<NegBackward0>) tensor(11967.3203, grad_fn=<NegBackward0>)
1
Iteration 3300: Loss = -11967.3203125
tensor(11967.3193, grad_fn=<NegBackward0>) tensor(11967.3203, grad_fn=<NegBackward0>)
2
Iteration 3400: Loss = -11967.3193359375
tensor(11967.3193, grad_fn=<NegBackward0>) tensor(11967.3193, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11967.333984375
tensor(11967.3193, grad_fn=<NegBackward0>) tensor(11967.3340, grad_fn=<NegBackward0>)
1
Iteration 3600: Loss = -11967.3193359375
tensor(11967.3193, grad_fn=<NegBackward0>) tensor(11967.3193, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11967.318359375
tensor(11967.3193, grad_fn=<NegBackward0>) tensor(11967.3184, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11967.3193359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3193, grad_fn=<NegBackward0>)
1
Iteration 3900: Loss = -11967.318359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3184, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11967.318359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3184, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11967.318359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3184, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11967.3212890625
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3213, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11967.318359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3184, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11967.3193359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3193, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -11967.318359375
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3184, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11967.3330078125
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3330, grad_fn=<NegBackward0>)
1
Iteration 4700: Loss = -11967.31640625
tensor(11967.3184, grad_fn=<NegBackward0>) tensor(11967.3164, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11967.3173828125
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3174, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -11967.31640625
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3164, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11967.3251953125
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3252, grad_fn=<NegBackward0>)
1
Iteration 5100: Loss = -11967.3173828125
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3174, grad_fn=<NegBackward0>)
2
Iteration 5200: Loss = -11967.3173828125
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3174, grad_fn=<NegBackward0>)
3
Iteration 5300: Loss = -11967.3173828125
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3174, grad_fn=<NegBackward0>)
4
Iteration 5400: Loss = -11967.3173828125
tensor(11967.3164, grad_fn=<NegBackward0>) tensor(11967.3174, grad_fn=<NegBackward0>)
5
Stopping early at iteration 5400 due to no improvement.
pi: tensor([[0.3723, 0.6277],
        [0.6193, 0.3807]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2978, 0.0965],
         [0.7193, 0.3006]],

        [[0.7150, 0.1105],
         [0.7235, 0.5892]],

        [[0.7163, 0.0988],
         [0.6270, 0.6958]],

        [[0.5134, 0.1057],
         [0.7119, 0.7148]],

        [[0.5290, 0.1067],
         [0.5013, 0.5054]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.04134437130886516
Average Adjusted Rand Index: 0.9841604758322362
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23370.296875
inf tensor(23370.2969, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12403.490234375
tensor(23370.2969, grad_fn=<NegBackward0>) tensor(12403.4902, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12403.142578125
tensor(12403.4902, grad_fn=<NegBackward0>) tensor(12403.1426, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12403.048828125
tensor(12403.1426, grad_fn=<NegBackward0>) tensor(12403.0488, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12402.9892578125
tensor(12403.0488, grad_fn=<NegBackward0>) tensor(12402.9893, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12402.9453125
tensor(12402.9893, grad_fn=<NegBackward0>) tensor(12402.9453, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12402.9072265625
tensor(12402.9453, grad_fn=<NegBackward0>) tensor(12402.9072, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12402.87109375
tensor(12402.9072, grad_fn=<NegBackward0>) tensor(12402.8711, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12402.8349609375
tensor(12402.8711, grad_fn=<NegBackward0>) tensor(12402.8350, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12402.794921875
tensor(12402.8350, grad_fn=<NegBackward0>) tensor(12402.7949, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12402.75390625
tensor(12402.7949, grad_fn=<NegBackward0>) tensor(12402.7539, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12402.7138671875
tensor(12402.7539, grad_fn=<NegBackward0>) tensor(12402.7139, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12402.67578125
tensor(12402.7139, grad_fn=<NegBackward0>) tensor(12402.6758, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12402.6396484375
tensor(12402.6758, grad_fn=<NegBackward0>) tensor(12402.6396, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12402.6083984375
tensor(12402.6396, grad_fn=<NegBackward0>) tensor(12402.6084, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12402.580078125
tensor(12402.6084, grad_fn=<NegBackward0>) tensor(12402.5801, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12402.5517578125
tensor(12402.5801, grad_fn=<NegBackward0>) tensor(12402.5518, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12402.5283203125
tensor(12402.5518, grad_fn=<NegBackward0>) tensor(12402.5283, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12402.5048828125
tensor(12402.5283, grad_fn=<NegBackward0>) tensor(12402.5049, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -12402.4853515625
tensor(12402.5049, grad_fn=<NegBackward0>) tensor(12402.4854, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -12402.4658203125
tensor(12402.4854, grad_fn=<NegBackward0>) tensor(12402.4658, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -12402.451171875
tensor(12402.4658, grad_fn=<NegBackward0>) tensor(12402.4512, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -12402.4365234375
tensor(12402.4512, grad_fn=<NegBackward0>) tensor(12402.4365, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -12402.4287109375
tensor(12402.4365, grad_fn=<NegBackward0>) tensor(12402.4287, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -12402.419921875
tensor(12402.4287, grad_fn=<NegBackward0>) tensor(12402.4199, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -12402.4169921875
tensor(12402.4199, grad_fn=<NegBackward0>) tensor(12402.4170, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -12402.4130859375
tensor(12402.4170, grad_fn=<NegBackward0>) tensor(12402.4131, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -12402.4111328125
tensor(12402.4131, grad_fn=<NegBackward0>) tensor(12402.4111, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -12402.408203125
tensor(12402.4111, grad_fn=<NegBackward0>) tensor(12402.4082, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -12402.4072265625
tensor(12402.4082, grad_fn=<NegBackward0>) tensor(12402.4072, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -12402.4052734375
tensor(12402.4072, grad_fn=<NegBackward0>) tensor(12402.4053, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -12402.404296875
tensor(12402.4053, grad_fn=<NegBackward0>) tensor(12402.4043, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -12402.404296875
tensor(12402.4043, grad_fn=<NegBackward0>) tensor(12402.4043, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -12402.4033203125
tensor(12402.4043, grad_fn=<NegBackward0>) tensor(12402.4033, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -12402.40234375
tensor(12402.4033, grad_fn=<NegBackward0>) tensor(12402.4023, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -12402.40234375
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4023, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -12402.4033203125
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4033, grad_fn=<NegBackward0>)
1
Iteration 3700: Loss = -12402.404296875
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4043, grad_fn=<NegBackward0>)
2
Iteration 3800: Loss = -12402.4033203125
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4033, grad_fn=<NegBackward0>)
3
Iteration 3900: Loss = -12402.4033203125
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4033, grad_fn=<NegBackward0>)
4
Iteration 4000: Loss = -12402.40234375
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4023, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -12402.4013671875
tensor(12402.4023, grad_fn=<NegBackward0>) tensor(12402.4014, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -12402.40234375
tensor(12402.4014, grad_fn=<NegBackward0>) tensor(12402.4023, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -12402.4013671875
tensor(12402.4014, grad_fn=<NegBackward0>) tensor(12402.4014, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -12402.40234375
tensor(12402.4014, grad_fn=<NegBackward0>) tensor(12402.4023, grad_fn=<NegBackward0>)
1
Iteration 4500: Loss = -12402.40234375
tensor(12402.4014, grad_fn=<NegBackward0>) tensor(12402.4023, grad_fn=<NegBackward0>)
2
Iteration 4600: Loss = -12402.4013671875
tensor(12402.4014, grad_fn=<NegBackward0>) tensor(12402.4014, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -12402.400390625
tensor(12402.4014, grad_fn=<NegBackward0>) tensor(12402.4004, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -12402.4013671875
tensor(12402.4004, grad_fn=<NegBackward0>) tensor(12402.4014, grad_fn=<NegBackward0>)
1
Iteration 4900: Loss = -12402.4013671875
tensor(12402.4004, grad_fn=<NegBackward0>) tensor(12402.4014, grad_fn=<NegBackward0>)
2
Iteration 5000: Loss = -12402.400390625
tensor(12402.4004, grad_fn=<NegBackward0>) tensor(12402.4004, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -12402.3994140625
tensor(12402.4004, grad_fn=<NegBackward0>) tensor(12402.3994, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -12402.3994140625
tensor(12402.3994, grad_fn=<NegBackward0>) tensor(12402.3994, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -12402.3994140625
tensor(12402.3994, grad_fn=<NegBackward0>) tensor(12402.3994, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -12402.3994140625
tensor(12402.3994, grad_fn=<NegBackward0>) tensor(12402.3994, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -12402.3984375
tensor(12402.3994, grad_fn=<NegBackward0>) tensor(12402.3984, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -12402.396484375
tensor(12402.3984, grad_fn=<NegBackward0>) tensor(12402.3965, grad_fn=<NegBackward0>)
Iteration 5700: Loss = -12402.3955078125
tensor(12402.3965, grad_fn=<NegBackward0>) tensor(12402.3955, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -12402.3955078125
tensor(12402.3955, grad_fn=<NegBackward0>) tensor(12402.3955, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -12402.3916015625
tensor(12402.3955, grad_fn=<NegBackward0>) tensor(12402.3916, grad_fn=<NegBackward0>)
Iteration 6000: Loss = -12402.390625
tensor(12402.3916, grad_fn=<NegBackward0>) tensor(12402.3906, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -12402.388671875
tensor(12402.3906, grad_fn=<NegBackward0>) tensor(12402.3887, grad_fn=<NegBackward0>)
Iteration 6200: Loss = -12402.38671875
tensor(12402.3887, grad_fn=<NegBackward0>) tensor(12402.3867, grad_fn=<NegBackward0>)
Iteration 6300: Loss = -12402.3837890625
tensor(12402.3867, grad_fn=<NegBackward0>) tensor(12402.3838, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -12402.380859375
tensor(12402.3838, grad_fn=<NegBackward0>) tensor(12402.3809, grad_fn=<NegBackward0>)
Iteration 6500: Loss = -12402.3759765625
tensor(12402.3809, grad_fn=<NegBackward0>) tensor(12402.3760, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -12402.373046875
tensor(12402.3760, grad_fn=<NegBackward0>) tensor(12402.3730, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -12402.37109375
tensor(12402.3730, grad_fn=<NegBackward0>) tensor(12402.3711, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -12402.3701171875
tensor(12402.3711, grad_fn=<NegBackward0>) tensor(12402.3701, grad_fn=<NegBackward0>)
Iteration 6900: Loss = -12402.369140625
tensor(12402.3701, grad_fn=<NegBackward0>) tensor(12402.3691, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -12402.3681640625
tensor(12402.3691, grad_fn=<NegBackward0>) tensor(12402.3682, grad_fn=<NegBackward0>)
Iteration 7100: Loss = -12402.3671875
tensor(12402.3682, grad_fn=<NegBackward0>) tensor(12402.3672, grad_fn=<NegBackward0>)
Iteration 7200: Loss = -12402.3681640625
tensor(12402.3672, grad_fn=<NegBackward0>) tensor(12402.3682, grad_fn=<NegBackward0>)
1
Iteration 7300: Loss = -12402.3662109375
tensor(12402.3672, grad_fn=<NegBackward0>) tensor(12402.3662, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -12402.3671875
tensor(12402.3662, grad_fn=<NegBackward0>) tensor(12402.3672, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -12402.4287109375
tensor(12402.3662, grad_fn=<NegBackward0>) tensor(12402.4287, grad_fn=<NegBackward0>)
2
Iteration 7600: Loss = -12402.5166015625
tensor(12402.3662, grad_fn=<NegBackward0>) tensor(12402.5166, grad_fn=<NegBackward0>)
3
Iteration 7700: Loss = -12402.3681640625
tensor(12402.3662, grad_fn=<NegBackward0>) tensor(12402.3682, grad_fn=<NegBackward0>)
4
Iteration 7800: Loss = -12402.4052734375
tensor(12402.3662, grad_fn=<NegBackward0>) tensor(12402.4053, grad_fn=<NegBackward0>)
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.0437, 0.9563],
        [0.0408, 0.9592]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3360, 0.6640], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.1985],
         [0.5978, 0.2021]],

        [[0.5345, 0.2104],
         [0.7079, 0.6306]],

        [[0.5359, 0.1481],
         [0.7076, 0.5763]],

        [[0.6683, 0.2162],
         [0.6470, 0.5839]],

        [[0.7280, 0.1980],
         [0.5467, 0.7120]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
[0.04134437130886516, 0.0] [0.9841604758322362, 0.0] [11967.3173828125, 12402.4052734375]
-------------------------------------
This iteration is 99
True Objective function: Loss = -11909.483212475014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23233.810546875
inf tensor(23233.8105, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12395.1640625
tensor(23233.8105, grad_fn=<NegBackward0>) tensor(12395.1641, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12394.1865234375
tensor(12395.1641, grad_fn=<NegBackward0>) tensor(12394.1865, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12390.9013671875
tensor(12394.1865, grad_fn=<NegBackward0>) tensor(12390.9014, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12368.5654296875
tensor(12390.9014, grad_fn=<NegBackward0>) tensor(12368.5654, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12103.9453125
tensor(12368.5654, grad_fn=<NegBackward0>) tensor(12103.9453, grad_fn=<NegBackward0>)
Iteration 600: Loss = -11973.169921875
tensor(12103.9453, grad_fn=<NegBackward0>) tensor(11973.1699, grad_fn=<NegBackward0>)
Iteration 700: Loss = -11914.1337890625
tensor(11973.1699, grad_fn=<NegBackward0>) tensor(11914.1338, grad_fn=<NegBackward0>)
Iteration 800: Loss = -11904.86328125
tensor(11914.1338, grad_fn=<NegBackward0>) tensor(11904.8633, grad_fn=<NegBackward0>)
Iteration 900: Loss = -11900.830078125
tensor(11904.8633, grad_fn=<NegBackward0>) tensor(11900.8301, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -11900.7607421875
tensor(11900.8301, grad_fn=<NegBackward0>) tensor(11900.7607, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -11900.716796875
tensor(11900.7607, grad_fn=<NegBackward0>) tensor(11900.7168, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -11900.6904296875
tensor(11900.7168, grad_fn=<NegBackward0>) tensor(11900.6904, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -11900.673828125
tensor(11900.6904, grad_fn=<NegBackward0>) tensor(11900.6738, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -11900.62890625
tensor(11900.6738, grad_fn=<NegBackward0>) tensor(11900.6289, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -11900.6181640625
tensor(11900.6289, grad_fn=<NegBackward0>) tensor(11900.6182, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -11900.6103515625
tensor(11900.6182, grad_fn=<NegBackward0>) tensor(11900.6104, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -11900.6005859375
tensor(11900.6104, grad_fn=<NegBackward0>) tensor(11900.6006, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -11900.58984375
tensor(11900.6006, grad_fn=<NegBackward0>) tensor(11900.5898, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11900.5751953125
tensor(11900.5898, grad_fn=<NegBackward0>) tensor(11900.5752, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11900.5712890625
tensor(11900.5752, grad_fn=<NegBackward0>) tensor(11900.5713, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11900.5673828125
tensor(11900.5713, grad_fn=<NegBackward0>) tensor(11900.5674, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11900.5654296875
tensor(11900.5674, grad_fn=<NegBackward0>) tensor(11900.5654, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11900.5625
tensor(11900.5654, grad_fn=<NegBackward0>) tensor(11900.5625, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11900.560546875
tensor(11900.5625, grad_fn=<NegBackward0>) tensor(11900.5605, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11900.55859375
tensor(11900.5605, grad_fn=<NegBackward0>) tensor(11900.5586, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11900.556640625
tensor(11900.5586, grad_fn=<NegBackward0>) tensor(11900.5566, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11900.5546875
tensor(11900.5566, grad_fn=<NegBackward0>) tensor(11900.5547, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11900.5537109375
tensor(11900.5547, grad_fn=<NegBackward0>) tensor(11900.5537, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11900.5517578125
tensor(11900.5537, grad_fn=<NegBackward0>) tensor(11900.5518, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11900.552734375
tensor(11900.5518, grad_fn=<NegBackward0>) tensor(11900.5527, grad_fn=<NegBackward0>)
1
Iteration 3100: Loss = -11900.5498046875
tensor(11900.5518, grad_fn=<NegBackward0>) tensor(11900.5498, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11900.5498046875
tensor(11900.5498, grad_fn=<NegBackward0>) tensor(11900.5498, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11900.548828125
tensor(11900.5498, grad_fn=<NegBackward0>) tensor(11900.5488, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11900.552734375
tensor(11900.5488, grad_fn=<NegBackward0>) tensor(11900.5527, grad_fn=<NegBackward0>)
1
Iteration 3500: Loss = -11900.5478515625
tensor(11900.5488, grad_fn=<NegBackward0>) tensor(11900.5479, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11900.546875
tensor(11900.5479, grad_fn=<NegBackward0>) tensor(11900.5469, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11900.544921875
tensor(11900.5469, grad_fn=<NegBackward0>) tensor(11900.5449, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11900.544921875
tensor(11900.5449, grad_fn=<NegBackward0>) tensor(11900.5449, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11900.5439453125
tensor(11900.5449, grad_fn=<NegBackward0>) tensor(11900.5439, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11900.5439453125
tensor(11900.5439, grad_fn=<NegBackward0>) tensor(11900.5439, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11900.54296875
tensor(11900.5439, grad_fn=<NegBackward0>) tensor(11900.5430, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11900.544921875
tensor(11900.5430, grad_fn=<NegBackward0>) tensor(11900.5449, grad_fn=<NegBackward0>)
1
Iteration 4300: Loss = -11900.54296875
tensor(11900.5430, grad_fn=<NegBackward0>) tensor(11900.5430, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11900.5419921875
tensor(11900.5430, grad_fn=<NegBackward0>) tensor(11900.5420, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11900.5419921875
tensor(11900.5420, grad_fn=<NegBackward0>) tensor(11900.5420, grad_fn=<NegBackward0>)
Iteration 4600: Loss = -11900.541015625
tensor(11900.5420, grad_fn=<NegBackward0>) tensor(11900.5410, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11900.541015625
tensor(11900.5410, grad_fn=<NegBackward0>) tensor(11900.5410, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11900.541015625
tensor(11900.5410, grad_fn=<NegBackward0>) tensor(11900.5410, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11900.541015625
tensor(11900.5410, grad_fn=<NegBackward0>) tensor(11900.5410, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11900.541015625
tensor(11900.5410, grad_fn=<NegBackward0>) tensor(11900.5410, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11900.5390625
tensor(11900.5410, grad_fn=<NegBackward0>) tensor(11900.5391, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11900.5390625
tensor(11900.5391, grad_fn=<NegBackward0>) tensor(11900.5391, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11900.5439453125
tensor(11900.5391, grad_fn=<NegBackward0>) tensor(11900.5439, grad_fn=<NegBackward0>)
1
Iteration 5400: Loss = -11900.5390625
tensor(11900.5391, grad_fn=<NegBackward0>) tensor(11900.5391, grad_fn=<NegBackward0>)
Iteration 5500: Loss = -11900.537109375
tensor(11900.5391, grad_fn=<NegBackward0>) tensor(11900.5371, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11900.5390625
tensor(11900.5371, grad_fn=<NegBackward0>) tensor(11900.5391, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11900.5380859375
tensor(11900.5371, grad_fn=<NegBackward0>) tensor(11900.5381, grad_fn=<NegBackward0>)
2
Iteration 5800: Loss = -11900.5380859375
tensor(11900.5371, grad_fn=<NegBackward0>) tensor(11900.5381, grad_fn=<NegBackward0>)
3
Iteration 5900: Loss = -11900.5400390625
tensor(11900.5371, grad_fn=<NegBackward0>) tensor(11900.5400, grad_fn=<NegBackward0>)
4
Iteration 6000: Loss = -11900.5380859375
tensor(11900.5371, grad_fn=<NegBackward0>) tensor(11900.5381, grad_fn=<NegBackward0>)
5
Stopping early at iteration 6000 due to no improvement.
pi: tensor([[0.2630, 0.7370],
        [0.7786, 0.2214]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5113, 0.4887], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3042, 0.0967],
         [0.5470, 0.2884]],

        [[0.5973, 0.1064],
         [0.7238, 0.7148]],

        [[0.6647, 0.0935],
         [0.5736, 0.6285]],

        [[0.6604, 0.0995],
         [0.6696, 0.6033]],

        [[0.6536, 0.1056],
         [0.6417, 0.5349]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.03809381261495235
Average Adjusted Rand Index: 0.9681583605609692
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22850.009765625
inf tensor(22850.0098, grad_fn=<NegBackward0>)
Iteration 100: Loss = -12394.779296875
tensor(22850.0098, grad_fn=<NegBackward0>) tensor(12394.7793, grad_fn=<NegBackward0>)
Iteration 200: Loss = -12394.4609375
tensor(12394.7793, grad_fn=<NegBackward0>) tensor(12394.4609, grad_fn=<NegBackward0>)
Iteration 300: Loss = -12394.0625
tensor(12394.4609, grad_fn=<NegBackward0>) tensor(12394.0625, grad_fn=<NegBackward0>)
Iteration 400: Loss = -12392.4013671875
tensor(12394.0625, grad_fn=<NegBackward0>) tensor(12392.4014, grad_fn=<NegBackward0>)
Iteration 500: Loss = -12390.9951171875
tensor(12392.4014, grad_fn=<NegBackward0>) tensor(12390.9951, grad_fn=<NegBackward0>)
Iteration 600: Loss = -12390.0166015625
tensor(12390.9951, grad_fn=<NegBackward0>) tensor(12390.0166, grad_fn=<NegBackward0>)
Iteration 700: Loss = -12389.4150390625
tensor(12390.0166, grad_fn=<NegBackward0>) tensor(12389.4150, grad_fn=<NegBackward0>)
Iteration 800: Loss = -12389.052734375
tensor(12389.4150, grad_fn=<NegBackward0>) tensor(12389.0527, grad_fn=<NegBackward0>)
Iteration 900: Loss = -12388.787109375
tensor(12389.0527, grad_fn=<NegBackward0>) tensor(12388.7871, grad_fn=<NegBackward0>)
Iteration 1000: Loss = -12388.5947265625
tensor(12388.7871, grad_fn=<NegBackward0>) tensor(12388.5947, grad_fn=<NegBackward0>)
Iteration 1100: Loss = -12388.466796875
tensor(12388.5947, grad_fn=<NegBackward0>) tensor(12388.4668, grad_fn=<NegBackward0>)
Iteration 1200: Loss = -12388.3837890625
tensor(12388.4668, grad_fn=<NegBackward0>) tensor(12388.3838, grad_fn=<NegBackward0>)
Iteration 1300: Loss = -12388.3310546875
tensor(12388.3838, grad_fn=<NegBackward0>) tensor(12388.3311, grad_fn=<NegBackward0>)
Iteration 1400: Loss = -12388.287109375
tensor(12388.3311, grad_fn=<NegBackward0>) tensor(12388.2871, grad_fn=<NegBackward0>)
Iteration 1500: Loss = -12388.05859375
tensor(12388.2871, grad_fn=<NegBackward0>) tensor(12388.0586, grad_fn=<NegBackward0>)
Iteration 1600: Loss = -12175.0693359375
tensor(12388.0586, grad_fn=<NegBackward0>) tensor(12175.0693, grad_fn=<NegBackward0>)
Iteration 1700: Loss = -12027.7431640625
tensor(12175.0693, grad_fn=<NegBackward0>) tensor(12027.7432, grad_fn=<NegBackward0>)
Iteration 1800: Loss = -12008.828125
tensor(12027.7432, grad_fn=<NegBackward0>) tensor(12008.8281, grad_fn=<NegBackward0>)
Iteration 1900: Loss = -11994.0419921875
tensor(12008.8281, grad_fn=<NegBackward0>) tensor(11994.0420, grad_fn=<NegBackward0>)
Iteration 2000: Loss = -11993.796875
tensor(11994.0420, grad_fn=<NegBackward0>) tensor(11993.7969, grad_fn=<NegBackward0>)
Iteration 2100: Loss = -11980.3310546875
tensor(11993.7969, grad_fn=<NegBackward0>) tensor(11980.3311, grad_fn=<NegBackward0>)
Iteration 2200: Loss = -11973.72265625
tensor(11980.3311, grad_fn=<NegBackward0>) tensor(11973.7227, grad_fn=<NegBackward0>)
Iteration 2300: Loss = -11973.662109375
tensor(11973.7227, grad_fn=<NegBackward0>) tensor(11973.6621, grad_fn=<NegBackward0>)
Iteration 2400: Loss = -11965.5888671875
tensor(11973.6621, grad_fn=<NegBackward0>) tensor(11965.5889, grad_fn=<NegBackward0>)
Iteration 2500: Loss = -11946.1123046875
tensor(11965.5889, grad_fn=<NegBackward0>) tensor(11946.1123, grad_fn=<NegBackward0>)
Iteration 2600: Loss = -11946.0791015625
tensor(11946.1123, grad_fn=<NegBackward0>) tensor(11946.0791, grad_fn=<NegBackward0>)
Iteration 2700: Loss = -11943.8525390625
tensor(11946.0791, grad_fn=<NegBackward0>) tensor(11943.8525, grad_fn=<NegBackward0>)
Iteration 2800: Loss = -11943.83984375
tensor(11943.8525, grad_fn=<NegBackward0>) tensor(11943.8398, grad_fn=<NegBackward0>)
Iteration 2900: Loss = -11943.810546875
tensor(11943.8398, grad_fn=<NegBackward0>) tensor(11943.8105, grad_fn=<NegBackward0>)
Iteration 3000: Loss = -11937.556640625
tensor(11943.8105, grad_fn=<NegBackward0>) tensor(11937.5566, grad_fn=<NegBackward0>)
Iteration 3100: Loss = -11937.5224609375
tensor(11937.5566, grad_fn=<NegBackward0>) tensor(11937.5225, grad_fn=<NegBackward0>)
Iteration 3200: Loss = -11937.517578125
tensor(11937.5225, grad_fn=<NegBackward0>) tensor(11937.5176, grad_fn=<NegBackward0>)
Iteration 3300: Loss = -11927.880859375
tensor(11937.5176, grad_fn=<NegBackward0>) tensor(11927.8809, grad_fn=<NegBackward0>)
Iteration 3400: Loss = -11921.4404296875
tensor(11927.8809, grad_fn=<NegBackward0>) tensor(11921.4404, grad_fn=<NegBackward0>)
Iteration 3500: Loss = -11921.4345703125
tensor(11921.4404, grad_fn=<NegBackward0>) tensor(11921.4346, grad_fn=<NegBackward0>)
Iteration 3600: Loss = -11921.43359375
tensor(11921.4346, grad_fn=<NegBackward0>) tensor(11921.4336, grad_fn=<NegBackward0>)
Iteration 3700: Loss = -11921.431640625
tensor(11921.4336, grad_fn=<NegBackward0>) tensor(11921.4316, grad_fn=<NegBackward0>)
Iteration 3800: Loss = -11921.4287109375
tensor(11921.4316, grad_fn=<NegBackward0>) tensor(11921.4287, grad_fn=<NegBackward0>)
Iteration 3900: Loss = -11921.427734375
tensor(11921.4287, grad_fn=<NegBackward0>) tensor(11921.4277, grad_fn=<NegBackward0>)
Iteration 4000: Loss = -11921.4248046875
tensor(11921.4277, grad_fn=<NegBackward0>) tensor(11921.4248, grad_fn=<NegBackward0>)
Iteration 4100: Loss = -11921.421875
tensor(11921.4248, grad_fn=<NegBackward0>) tensor(11921.4219, grad_fn=<NegBackward0>)
Iteration 4200: Loss = -11921.4189453125
tensor(11921.4219, grad_fn=<NegBackward0>) tensor(11921.4189, grad_fn=<NegBackward0>)
Iteration 4300: Loss = -11921.4169921875
tensor(11921.4189, grad_fn=<NegBackward0>) tensor(11921.4170, grad_fn=<NegBackward0>)
Iteration 4400: Loss = -11921.4169921875
tensor(11921.4170, grad_fn=<NegBackward0>) tensor(11921.4170, grad_fn=<NegBackward0>)
Iteration 4500: Loss = -11921.423828125
tensor(11921.4170, grad_fn=<NegBackward0>) tensor(11921.4238, grad_fn=<NegBackward0>)
1
Iteration 4600: Loss = -11921.412109375
tensor(11921.4170, grad_fn=<NegBackward0>) tensor(11921.4121, grad_fn=<NegBackward0>)
Iteration 4700: Loss = -11921.404296875
tensor(11921.4121, grad_fn=<NegBackward0>) tensor(11921.4043, grad_fn=<NegBackward0>)
Iteration 4800: Loss = -11921.4033203125
tensor(11921.4043, grad_fn=<NegBackward0>) tensor(11921.4033, grad_fn=<NegBackward0>)
Iteration 4900: Loss = -11921.4013671875
tensor(11921.4033, grad_fn=<NegBackward0>) tensor(11921.4014, grad_fn=<NegBackward0>)
Iteration 5000: Loss = -11912.1728515625
tensor(11921.4014, grad_fn=<NegBackward0>) tensor(11912.1729, grad_fn=<NegBackward0>)
Iteration 5100: Loss = -11912.1513671875
tensor(11912.1729, grad_fn=<NegBackward0>) tensor(11912.1514, grad_fn=<NegBackward0>)
Iteration 5200: Loss = -11912.150390625
tensor(11912.1514, grad_fn=<NegBackward0>) tensor(11912.1504, grad_fn=<NegBackward0>)
Iteration 5300: Loss = -11912.1484375
tensor(11912.1504, grad_fn=<NegBackward0>) tensor(11912.1484, grad_fn=<NegBackward0>)
Iteration 5400: Loss = -11912.150390625
tensor(11912.1484, grad_fn=<NegBackward0>) tensor(11912.1504, grad_fn=<NegBackward0>)
1
Iteration 5500: Loss = -11912.1474609375
tensor(11912.1484, grad_fn=<NegBackward0>) tensor(11912.1475, grad_fn=<NegBackward0>)
Iteration 5600: Loss = -11912.1494140625
tensor(11912.1475, grad_fn=<NegBackward0>) tensor(11912.1494, grad_fn=<NegBackward0>)
1
Iteration 5700: Loss = -11900.9697265625
tensor(11912.1475, grad_fn=<NegBackward0>) tensor(11900.9697, grad_fn=<NegBackward0>)
Iteration 5800: Loss = -11900.9462890625
tensor(11900.9697, grad_fn=<NegBackward0>) tensor(11900.9463, grad_fn=<NegBackward0>)
Iteration 5900: Loss = -11900.9521484375
tensor(11900.9463, grad_fn=<NegBackward0>) tensor(11900.9521, grad_fn=<NegBackward0>)
1
Iteration 6000: Loss = -11900.9443359375
tensor(11900.9463, grad_fn=<NegBackward0>) tensor(11900.9443, grad_fn=<NegBackward0>)
Iteration 6100: Loss = -11900.9453125
tensor(11900.9443, grad_fn=<NegBackward0>) tensor(11900.9453, grad_fn=<NegBackward0>)
1
Iteration 6200: Loss = -11900.9453125
tensor(11900.9443, grad_fn=<NegBackward0>) tensor(11900.9453, grad_fn=<NegBackward0>)
2
Iteration 6300: Loss = -11900.943359375
tensor(11900.9443, grad_fn=<NegBackward0>) tensor(11900.9434, grad_fn=<NegBackward0>)
Iteration 6400: Loss = -11900.94921875
tensor(11900.9434, grad_fn=<NegBackward0>) tensor(11900.9492, grad_fn=<NegBackward0>)
1
Iteration 6500: Loss = -11900.943359375
tensor(11900.9434, grad_fn=<NegBackward0>) tensor(11900.9434, grad_fn=<NegBackward0>)
Iteration 6600: Loss = -11900.9423828125
tensor(11900.9434, grad_fn=<NegBackward0>) tensor(11900.9424, grad_fn=<NegBackward0>)
Iteration 6700: Loss = -11900.9423828125
tensor(11900.9424, grad_fn=<NegBackward0>) tensor(11900.9424, grad_fn=<NegBackward0>)
Iteration 6800: Loss = -11900.943359375
tensor(11900.9424, grad_fn=<NegBackward0>) tensor(11900.9434, grad_fn=<NegBackward0>)
1
Iteration 6900: Loss = -11900.9404296875
tensor(11900.9424, grad_fn=<NegBackward0>) tensor(11900.9404, grad_fn=<NegBackward0>)
Iteration 7000: Loss = -11900.9521484375
tensor(11900.9404, grad_fn=<NegBackward0>) tensor(11900.9521, grad_fn=<NegBackward0>)
1
Iteration 7100: Loss = -11900.94140625
tensor(11900.9404, grad_fn=<NegBackward0>) tensor(11900.9414, grad_fn=<NegBackward0>)
2
Iteration 7200: Loss = -11900.857421875
tensor(11900.9404, grad_fn=<NegBackward0>) tensor(11900.8574, grad_fn=<NegBackward0>)
Iteration 7300: Loss = -11900.853515625
tensor(11900.8574, grad_fn=<NegBackward0>) tensor(11900.8535, grad_fn=<NegBackward0>)
Iteration 7400: Loss = -11900.8544921875
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.8545, grad_fn=<NegBackward0>)
1
Iteration 7500: Loss = -11900.853515625
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.8535, grad_fn=<NegBackward0>)
Iteration 7600: Loss = -11900.8642578125
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.8643, grad_fn=<NegBackward0>)
1
Iteration 7700: Loss = -11900.8720703125
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.8721, grad_fn=<NegBackward0>)
2
Iteration 7800: Loss = -11900.9072265625
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.9072, grad_fn=<NegBackward0>)
3
Iteration 7900: Loss = -11900.859375
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.8594, grad_fn=<NegBackward0>)
4
Iteration 8000: Loss = -11900.8544921875
tensor(11900.8535, grad_fn=<NegBackward0>) tensor(11900.8545, grad_fn=<NegBackward0>)
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.2601, 0.7399],
        [0.7768, 0.2232]], grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5108, 0.4892], grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3048, 0.0967],
         [0.6900, 0.2881]],

        [[0.6037, 0.1062],
         [0.6104, 0.5635]],

        [[0.5849, 0.0936],
         [0.6807, 0.7199]],

        [[0.6860, 0.0995],
         [0.5075, 0.6023]],

        [[0.6111, 0.1056],
         [0.6525, 0.5202]]], grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.03809381261495235
Average Adjusted Rand Index: 0.9681583605609692
[0.03809381261495235, 0.03809381261495235] [0.9681583605609692, 0.9681583605609692] [11900.5380859375, 11900.8544921875]
